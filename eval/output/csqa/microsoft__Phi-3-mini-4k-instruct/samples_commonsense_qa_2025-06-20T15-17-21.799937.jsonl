{"doc_id": 0, "doc": {"id": "1afa02df02c908a558b4036e80242fac", "question": "A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?", "question_concept": "revolving door", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bank", "library", "department store", "mall", "new york"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\nA. bank\nB. library\nC. department store\nD. mall\nE. new york\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\nA. bank\nB. library\nC. department store\nD. mall\nE. new york\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\nA. bank\nB. library\nC. department store\nD. mall\nE. new york\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\nA. bank\nB. library\nC. department store\nD. mall\nE. new york\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\nA. bank\nB. library\nC. department store\nD. mall\nE. new york\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9156832695007324", "True"]], [["-5.915683269500732", "False"]], [["-1.9156832695007324", "False"]], [["-3.4156832695007324", "False"]], [["-7.665683269500732", "False"]]], "filtered_resps": [["-0.9156832695007324", "True"], ["-5.915683269500732", "False"], ["-1.9156832695007324", "False"], ["-3.4156832695007324", "False"], ["-7.665683269500732", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "504955a1bd454f3705b3644021bb29c412e1049e3a566869a7fcf5608b88fed5", "prompt_hash": "143f0c04f0a4bb68ae61bc1cb92452015f3fb7a0633369459d7d513b4f69b5c7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1, "doc": {"id": "a7ab086045575bb497933726e4e6ad28", "question": "What do people aim to do at work?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["complete job", "learn from each other", "kill animals", "wear hats", "talk to each other"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do people aim to do at work?\nA. complete job\nB. learn from each other\nC. kill animals\nD. wear hats\nE. talk to each other\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people aim to do at work?\nA. complete job\nB. learn from each other\nC. kill animals\nD. wear hats\nE. talk to each other\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people aim to do at work?\nA. complete job\nB. learn from each other\nC. kill animals\nD. wear hats\nE. talk to each other\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people aim to do at work?\nA. complete job\nB. learn from each other\nC. kill animals\nD. wear hats\nE. talk to each other\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people aim to do at work?\nA. complete job\nB. learn from each other\nC. kill animals\nD. wear hats\nE. talk to each other\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0148563385009766", "True"]], [["-3.2648563385009766", "False"]], [["-6.514856338500977", "False"]], [["-7.014856338500977", "False"]], [["-6.514856338500977", "False"]]], "filtered_resps": [["-1.0148563385009766", "True"], ["-3.2648563385009766", "False"], ["-6.514856338500977", "False"], ["-7.014856338500977", "False"], ["-6.514856338500977", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f93b5b09047bad8ae7cf4f1c99125e811724445835c6cb352cf430e09a22456c", "prompt_hash": "6c8bc453a198c80c1a843ddf7d58786373a046df6cd22cac8b3caca727c95e26", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 2, "doc": {"id": "b8c0a4703079cf661d7261a60a1bcbff", "question": "Where would you find magazines along side many other printed works?", "question_concept": "magazines", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["doctor", "bookstore", "market", "train station", "mortuary"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find magazines along side many other printed works?\nA. doctor\nB. bookstore\nC. market\nD. train station\nE. mortuary\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find magazines along side many other printed works?\nA. doctor\nB. bookstore\nC. market\nD. train station\nE. mortuary\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find magazines along side many other printed works?\nA. doctor\nB. bookstore\nC. market\nD. train station\nE. mortuary\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find magazines along side many other printed works?\nA. doctor\nB. bookstore\nC. market\nD. train station\nE. mortuary\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find magazines along side many other printed works?\nA. doctor\nB. bookstore\nC. market\nD. train station\nE. mortuary\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7976932525634766", "False"]], [["-1.2976932525634766", "False"]], [["-9.547693252563477", "False"]], [["-10.047693252563477", "False"]], [["-11.797693252563477", "False"]]], "filtered_resps": [["-3.7976932525634766", "False"], ["-1.2976932525634766", "False"], ["-9.547693252563477", "False"], ["-10.047693252563477", "False"], ["-11.797693252563477", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "83a5f673074a07f8c2156d5b49ca6493fea892eed812eb2219ed7a492d1444bb", "prompt_hash": "7f4b700aa37f83f9a6e2895f234a6e086999231c17dfe13c8ba2d8828a0724e9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 3, "doc": {"id": "e68fb2448fd74e402aae9982aa76e527", "question": "Where are  you likely to find a hamburger?", "question_concept": "hamburger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fast food restaurant", "pizza", "ground up dead cows", "mouth", "cow carcus"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where are  you likely to find a hamburger?\nA. fast food restaurant\nB. pizza\nC. ground up dead cows\nD. mouth\nE. cow carcus\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are  you likely to find a hamburger?\nA. fast food restaurant\nB. pizza\nC. ground up dead cows\nD. mouth\nE. cow carcus\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are  you likely to find a hamburger?\nA. fast food restaurant\nB. pizza\nC. ground up dead cows\nD. mouth\nE. cow carcus\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are  you likely to find a hamburger?\nA. fast food restaurant\nB. pizza\nC. ground up dead cows\nD. mouth\nE. cow carcus\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are  you likely to find a hamburger?\nA. fast food restaurant\nB. pizza\nC. ground up dead cows\nD. mouth\nE. cow carcus\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6230960488319397", "True"]], [["-7.123095989227295", "False"]], [["-6.623095989227295", "False"]], [["-7.123095989227295", "False"]], [["-8.373096466064453", "False"]]], "filtered_resps": [["-0.6230960488319397", "True"], ["-7.123095989227295", "False"], ["-6.623095989227295", "False"], ["-7.123095989227295", "False"], ["-8.373096466064453", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3b18b8526e6e6f3493867567447f5b4002421fe5d9f8fa0b936433a81fa65735", "prompt_hash": "547307434198ede4a02d9e92e6caf4656f3ec5f655b0947065826b9a8fccd17d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 4, "doc": {"id": "2435de612dd69f2012b9e40d6af4ce38", "question": "James was looking for a good place to buy farmland.  Where might he look?", "question_concept": "farmland", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["midwest", "countryside", "estate", "farming areas", "illinois"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James was looking for a good place to buy farmland.  Where might he look?\nA. midwest\nB. countryside\nC. estate\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James was looking for a good place to buy farmland.  Where might he look?\nA. midwest\nB. countryside\nC. estate\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James was looking for a good place to buy farmland.  Where might he look?\nA. midwest\nB. countryside\nC. estate\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James was looking for a good place to buy farmland.  Where might he look?\nA. midwest\nB. countryside\nC. estate\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James was looking for a good place to buy farmland.  Where might he look?\nA. midwest\nB. countryside\nC. estate\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.9536445140838623", "False"]], [["-3.2036445140838623", "False"]], [["-4.703644752502441", "False"]], [["-3.4536445140838623", "False"]], [["-5.203644752502441", "False"]]], "filtered_resps": [["-1.9536445140838623", "False"], ["-3.2036445140838623", "False"], ["-4.703644752502441", "False"], ["-3.4536445140838623", "False"], ["-5.203644752502441", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a6694ebf0f28c918a164222aeb595593baaae0083b98099bdc06357b8cb4b843", "prompt_hash": "0aaa257fbe2229f1d1e7c6e658c113f9dfd3d7047d421be488322ae8da9b783d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 5, "doc": {"id": "a4892551cb4beb279653ae52d0de4c89", "question": "What island country is ferret popular?", "question_concept": "ferret", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["own home", "north carolina", "great britain", "hutch", "outdoors"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What island country is ferret popular?\nA. own home\nB. north carolina\nC. great britain\nD. hutch\nE. outdoors\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What island country is ferret popular?\nA. own home\nB. north carolina\nC. great britain\nD. hutch\nE. outdoors\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What island country is ferret popular?\nA. own home\nB. north carolina\nC. great britain\nD. hutch\nE. outdoors\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What island country is ferret popular?\nA. own home\nB. north carolina\nC. great britain\nD. hutch\nE. outdoors\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What island country is ferret popular?\nA. own home\nB. north carolina\nC. great britain\nD. hutch\nE. outdoors\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.675680160522461", "False"]], [["-7.175680160522461", "False"]], [["-2.175680160522461", "False"]], [["-7.925680160522461", "False"]], [["-9.675680160522461", "False"]]], "filtered_resps": [["-4.675680160522461", "False"], ["-7.175680160522461", "False"], ["-2.175680160522461", "False"], ["-7.925680160522461", "False"], ["-9.675680160522461", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "84bb14e23ee22adffbcc06737945c3c453e078783adbfe8455341342c498a8fd", "prompt_hash": "dced14f491565b138ea74b8b1069b327b30b48f55a14832d6c984619b17c4848", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 6, "doc": {"id": "118a9093a30695622363455e4d911866", "question": "In what Spanish speaking North American country can you get a great cup of coffee?", "question_concept": "cup of coffee", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mildred's coffee shop", "mexico", "diner", "kitchen", "canteen"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: In what Spanish speaking North American country can you get a great cup of coffee?\nA. mildred's coffee shop\nB. mexico\nC. diner\nD. kitchen\nE. canteen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: In what Spanish speaking North American country can you get a great cup of coffee?\nA. mildred's coffee shop\nB. mexico\nC. diner\nD. kitchen\nE. canteen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: In what Spanish speaking North American country can you get a great cup of coffee?\nA. mildred's coffee shop\nB. mexico\nC. diner\nD. kitchen\nE. canteen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: In what Spanish speaking North American country can you get a great cup of coffee?\nA. mildred's coffee shop\nB. mexico\nC. diner\nD. kitchen\nE. canteen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: In what Spanish speaking North American country can you get a great cup of coffee?\nA. mildred's coffee shop\nB. mexico\nC. diner\nD. kitchen\nE. canteen\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.659648895263672", "False"]], [["-1.6596488952636719", "False"]], [["-7.409648895263672", "False"]], [["-10.659648895263672", "False"]], [["-10.659648895263672", "False"]]], "filtered_resps": [["-6.659648895263672", "False"], ["-1.6596488952636719", "False"], ["-7.409648895263672", "False"], ["-10.659648895263672", "False"], ["-10.659648895263672", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65781e8dcb376340f1e35d14f2221b2e95ba72f340e958fc823c9b6640cc6688", "prompt_hash": "fd94c78b076bbf89b2ccca63cdee45403843be1c1d4cffd9d9eab8fac1a063d0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 7, "doc": {"id": "05ea49b82e8ec519e82d6633936ab8bf", "question": "What do animals do when an enemy is approaching?", "question_concept": "animals", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feel pleasure", "procreate", "pass water", "listen to each other", "sing"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What do animals do when an enemy is approaching?\nA. feel pleasure\nB. procreate\nC. pass water\nD. listen to each other\nE. sing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do animals do when an enemy is approaching?\nA. feel pleasure\nB. procreate\nC. pass water\nD. listen to each other\nE. sing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do animals do when an enemy is approaching?\nA. feel pleasure\nB. procreate\nC. pass water\nD. listen to each other\nE. sing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do animals do when an enemy is approaching?\nA. feel pleasure\nB. procreate\nC. pass water\nD. listen to each other\nE. sing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do animals do when an enemy is approaching?\nA. feel pleasure\nB. procreate\nC. pass water\nD. listen to each other\nE. sing\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3593368530273438", "False"]], [["-5.609336853027344", "False"]], [["-5.609336853027344", "False"]], [["-2.3593368530273438", "False"]], [["-4.609336853027344", "False"]]], "filtered_resps": [["-3.3593368530273438", "False"], ["-5.609336853027344", "False"], ["-5.609336853027344", "False"], ["-2.3593368530273438", "False"], ["-4.609336853027344", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7a8ed3623467c7786caff2ef8dd240f4e35aa5d724efe3fa898c3900525ca434", "prompt_hash": "a18e584c559972b1b269f2fc2c1b7f5ed2a8e40ac34b04170d84e07cf0a39e74", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 8, "doc": {"id": "c0c07ce781653b2a2c01871ba2bcba93", "question": "Reading newspaper one of many ways to practice your what?", "question_concept": "reading newspaper", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["literacy", "knowing how to read", "money", "buying", "money bank"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Reading newspaper one of many ways to practice your what?\nA. literacy\nB. knowing how to read\nC. money\nD. buying\nE. money bank\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Reading newspaper one of many ways to practice your what?\nA. literacy\nB. knowing how to read\nC. money\nD. buying\nE. money bank\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Reading newspaper one of many ways to practice your what?\nA. literacy\nB. knowing how to read\nC. money\nD. buying\nE. money bank\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Reading newspaper one of many ways to practice your what?\nA. literacy\nB. knowing how to read\nC. money\nD. buying\nE. money bank\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Reading newspaper one of many ways to practice your what?\nA. literacy\nB. knowing how to read\nC. money\nD. buying\nE. money bank\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2105293273925781", "True"]], [["-6.460529327392578", "False"]], [["-8.210529327392578", "False"]], [["-9.210529327392578", "False"]], [["-9.710529327392578", "False"]]], "filtered_resps": [["-1.2105293273925781", "True"], ["-6.460529327392578", "False"], ["-8.210529327392578", "False"], ["-9.210529327392578", "False"], ["-9.710529327392578", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "88aefd6d3a34ff52284a12f026f753583224930041791fd8ddef90a48421d495", "prompt_hash": "4089236ff097616007e70109efc9567e8913c5477800b49167c6cb93aeee52a6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 9, "doc": {"id": "1d24f406b6828492040b405d3f35119c", "question": "What do people typically do while playing guitar?", "question_concept": "playing guitar", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cry", "hear sounds", "singing", "arthritis", "making music"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What do people typically do while playing guitar?\nA. cry\nB. hear sounds\nC. singing\nD. arthritis\nE. making music\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people typically do while playing guitar?\nA. cry\nB. hear sounds\nC. singing\nD. arthritis\nE. making music\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people typically do while playing guitar?\nA. cry\nB. hear sounds\nC. singing\nD. arthritis\nE. making music\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people typically do while playing guitar?\nA. cry\nB. hear sounds\nC. singing\nD. arthritis\nE. making music\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people typically do while playing guitar?\nA. cry\nB. hear sounds\nC. singing\nD. arthritis\nE. making music\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.837461471557617", "False"]], [["-4.087461471557617", "False"]], [["-2.837461233139038", "False"]], [["-7.087461471557617", "False"]], [["-1.837461233139038", "False"]]], "filtered_resps": [["-5.837461471557617", "False"], ["-4.087461471557617", "False"], ["-2.837461233139038", "False"], ["-7.087461471557617", "False"], ["-1.837461233139038", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3826ac5c6f4df8246919e794a8a8cbe6609ff9238d7614e58b4a8703225fd7ae", "prompt_hash": "b65b3c197704b52e86aee813b7e1384b3db74349a3e0a53add81125245e3e96f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 10, "doc": {"id": "57f92025d860e32c4e780c0d51c1c20c", "question": "What would vinyl be an odd thing to replace?", "question_concept": "vinyl", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pants", "record albums", "record store", "cheese", "wallpaper"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What would vinyl be an odd thing to replace?\nA. pants\nB. record albums\nC. record store\nD. cheese\nE. wallpaper\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would vinyl be an odd thing to replace?\nA. pants\nB. record albums\nC. record store\nD. cheese\nE. wallpaper\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would vinyl be an odd thing to replace?\nA. pants\nB. record albums\nC. record store\nD. cheese\nE. wallpaper\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would vinyl be an odd thing to replace?\nA. pants\nB. record albums\nC. record store\nD. cheese\nE. wallpaper\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would vinyl be an odd thing to replace?\nA. pants\nB. record albums\nC. record store\nD. cheese\nE. wallpaper\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5005862712860107", "False"]], [["-3.7505862712860107", "False"]], [["-4.25058650970459", "False"]], [["-1.7505862712860107", "False"]], [["-6.00058650970459", "False"]]], "filtered_resps": [["-3.5005862712860107", "False"], ["-3.7505862712860107", "False"], ["-4.25058650970459", "False"], ["-1.7505862712860107", "False"], ["-6.00058650970459", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "86423849f6eed04da85e6056ffed4e2adf26aace3b3e2fbb2a8f2b3a28fef5d9", "prompt_hash": "a7894c052627d69202ca2825694bd3b6a3af072e11eeed5434a7793a1f3d4e53", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 11, "doc": {"id": "81eb4b2ee66edd8bc91ee944697c4e9f", "question": "If you want harmony, what is something you should try to do with the world?", "question_concept": "something you", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["take time", "make noise", "make war", "make peace", "make haste"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If you want harmony, what is something you should try to do with the world?\nA. take time\nB. make noise\nC. make war\nD. make peace\nE. make haste\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you want harmony, what is something you should try to do with the world?\nA. take time\nB. make noise\nC. make war\nD. make peace\nE. make haste\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you want harmony, what is something you should try to do with the world?\nA. take time\nB. make noise\nC. make war\nD. make peace\nE. make haste\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you want harmony, what is something you should try to do with the world?\nA. take time\nB. make noise\nC. make war\nD. make peace\nE. make haste\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you want harmony, what is something you should try to do with the world?\nA. take time\nB. make noise\nC. make war\nD. make peace\nE. make haste\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.4992547035217285", "False"]], [["-7.2492547035217285", "False"]], [["-7.4992547035217285", "False"]], [["-1.249254822731018", "True"]], [["-7.7492547035217285", "False"]]], "filtered_resps": [["-4.4992547035217285", "False"], ["-7.2492547035217285", "False"], ["-7.4992547035217285", "False"], ["-1.249254822731018", "True"], ["-7.7492547035217285", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f417decd7960b50e6dd22f72550ea85b4eb3737523adc2dcf7852bb1ededa945", "prompt_hash": "f3012a3c7b2c74426db8c6162617e4b609d3600e634d3ca46fcbc272e7d10a2d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 12, "doc": {"id": "d807e7ae60976324920c8d29eb42dad6", "question": "Where does a heifer's master live?", "question_concept": "heifer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["farm house", "barnyard", "stockyard", "slaughter house", "eat cake"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where does a heifer's master live?\nA. farm house\nB. barnyard\nC. stockyard\nD. slaughter house\nE. eat cake\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where does a heifer's master live?\nA. farm house\nB. barnyard\nC. stockyard\nD. slaughter house\nE. eat cake\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where does a heifer's master live?\nA. farm house\nB. barnyard\nC. stockyard\nD. slaughter house\nE. eat cake\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where does a heifer's master live?\nA. farm house\nB. barnyard\nC. stockyard\nD. slaughter house\nE. eat cake\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where does a heifer's master live?\nA. farm house\nB. barnyard\nC. stockyard\nD. slaughter house\nE. eat cake\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7073906660079956", "True"]], [["-3.457390785217285", "False"]], [["-6.457390785217285", "False"]], [["-7.207390785217285", "False"]], [["-4.207390785217285", "False"]]], "filtered_resps": [["-0.7073906660079956", "True"], ["-3.457390785217285", "False"], ["-6.457390785217285", "False"], ["-7.207390785217285", "False"], ["-4.207390785217285", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9ef10d02cfd6351c8fc10f500a6b089f2d00d49c5243d0894c2df1b140cee712", "prompt_hash": "7ec531daaa85274149f9f139761390135bf5844abbf8ed813009b11083e96d97", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 13, "doc": {"id": "7ea9f721ffc662918bb0c0937a487f04", "question": "Aside from water and nourishment what does your dog need?", "question_concept": "dog", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bone", "charm", "petted", "lots of attention", "walked"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Aside from water and nourishment what does your dog need?\nA. bone\nB. charm\nC. petted\nD. lots of attention\nE. walked\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Aside from water and nourishment what does your dog need?\nA. bone\nB. charm\nC. petted\nD. lots of attention\nE. walked\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Aside from water and nourishment what does your dog need?\nA. bone\nB. charm\nC. petted\nD. lots of attention\nE. walked\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Aside from water and nourishment what does your dog need?\nA. bone\nB. charm\nC. petted\nD. lots of attention\nE. walked\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Aside from water and nourishment what does your dog need?\nA. bone\nB. charm\nC. petted\nD. lots of attention\nE. walked\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.146004676818848", "False"]], [["-5.396004676818848", "False"]], [["-2.6460046768188477", "False"]], [["-0.8960047364234924", "True"]], [["-6.896004676818848", "False"]]], "filtered_resps": [["-4.146004676818848", "False"], ["-5.396004676818848", "False"], ["-2.6460046768188477", "False"], ["-0.8960047364234924", "True"], ["-6.896004676818848", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5e6fb12a01f8512298273b70882306c4bfcb2c767543c2ad6d0a04d3ac0a97f1", "prompt_hash": "d71525213effc977ca68fe79fce03b8912c6681daeb40633ab68a8026f093732", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 14, "doc": {"id": "fc1d33a2301a30214523c12573f81aba", "question": "Janet was watching the film because she liked what?", "question_concept": "watching film", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["erection", "laughter", "being entertained", "fear", "bordem"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Janet was watching the film because she liked what?\nA. erection\nB. laughter\nC. being entertained\nD. fear\nE. bordem\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Janet was watching the film because she liked what?\nA. erection\nB. laughter\nC. being entertained\nD. fear\nE. bordem\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Janet was watching the film because she liked what?\nA. erection\nB. laughter\nC. being entertained\nD. fear\nE. bordem\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Janet was watching the film because she liked what?\nA. erection\nB. laughter\nC. being entertained\nD. fear\nE. bordem\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Janet was watching the film because she liked what?\nA. erection\nB. laughter\nC. being entertained\nD. fear\nE. bordem\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.344198703765869", "False"]], [["-5.844198703765869", "False"]], [["-1.0941987037658691", "True"]], [["-7.344198703765869", "False"]], [["-9.094198226928711", "False"]]], "filtered_resps": [["-6.344198703765869", "False"], ["-5.844198703765869", "False"], ["-1.0941987037658691", "True"], ["-7.344198703765869", "False"], ["-9.094198226928711", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7b170af5df2455cdb64b4369347fd2eef8540f9881f5e804ba03f348a76205a9", "prompt_hash": "bf788823d95713d976f6fc1476595245a55ea0115ed62ff31e10c13ec370f566", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 15, "doc": {"id": "3b8e1d236f5169b6c833a994d6d9c39a", "question": "What are you waiting alongside with when you're in a reception area?", "question_concept": "reception area", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["motel", "chair", "hospital", "people", "hotels"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What are you waiting alongside with when you're in a reception area?\nA. motel\nB. chair\nC. hospital\nD. people\nE. hotels\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are you waiting alongside with when you're in a reception area?\nA. motel\nB. chair\nC. hospital\nD. people\nE. hotels\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are you waiting alongside with when you're in a reception area?\nA. motel\nB. chair\nC. hospital\nD. people\nE. hotels\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are you waiting alongside with when you're in a reception area?\nA. motel\nB. chair\nC. hospital\nD. people\nE. hotels\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are you waiting alongside with when you're in a reception area?\nA. motel\nB. chair\nC. hospital\nD. people\nE. hotels\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.333165168762207", "False"]], [["-3.833165168762207", "False"]], [["-6.833165168762207", "False"]], [["-1.333165168762207", "True"]], [["-7.583165168762207", "False"]]], "filtered_resps": [["-4.333165168762207", "False"], ["-3.833165168762207", "False"], ["-6.833165168762207", "False"], ["-1.333165168762207", "True"], ["-7.583165168762207", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2703e2c89d7b733d3aaca565d126d92b666255e16849eb7f0e4d7e46775c8b5e", "prompt_hash": "cc178216ddb5a59ee7f723e79e5c55dd72f4025aa7b4dca9c04826a8903da142", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 16, "doc": {"id": "c5c4166f2ed3c2b3517b79e6848e9ae2", "question": "When drinking booze what can you do to stay busy?", "question_concept": "booze", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["reach tentative agreement", "stay in bed", "stop bicycle", "examine thing", "suicide"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When drinking booze what can you do to stay busy?\nA. reach tentative agreement\nB. stay in bed\nC. stop bicycle\nD. examine thing\nE. suicide\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When drinking booze what can you do to stay busy?\nA. reach tentative agreement\nB. stay in bed\nC. stop bicycle\nD. examine thing\nE. suicide\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When drinking booze what can you do to stay busy?\nA. reach tentative agreement\nB. stay in bed\nC. stop bicycle\nD. examine thing\nE. suicide\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When drinking booze what can you do to stay busy?\nA. reach tentative agreement\nB. stay in bed\nC. stop bicycle\nD. examine thing\nE. suicide\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When drinking booze what can you do to stay busy?\nA. reach tentative agreement\nB. stay in bed\nC. stop bicycle\nD. examine thing\nE. suicide\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7408809661865234", "False"]], [["-2.4908809661865234", "True"]], [["-5.990880966186523", "False"]], [["-5.740880966186523", "False"]], [["-4.240880966186523", "False"]]], "filtered_resps": [["-2.7408809661865234", "False"], ["-2.4908809661865234", "True"], ["-5.990880966186523", "False"], ["-5.740880966186523", "False"], ["-4.240880966186523", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e9af2e0373d0427e6921c9a8202e363f5d6a8e8eb27b32ec849e10d12675b741", "prompt_hash": "5020422541c7bc739e895dea01363a3decddb0cf4f387cda141f09e459160ffb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 17, "doc": {"id": "6dc5b2884737e66543ce65f8dc40c992", "question": "A fencing thrust with a sharp sword towards a person would result in what?", "question_concept": "fencing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["injury", "small cuts", "fever", "competition", "puncture wound"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: A fencing thrust with a sharp sword towards a person would result in what?\nA. injury\nB. small cuts\nC. fever\nD. competition\nE. puncture wound\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A fencing thrust with a sharp sword towards a person would result in what?\nA. injury\nB. small cuts\nC. fever\nD. competition\nE. puncture wound\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A fencing thrust with a sharp sword towards a person would result in what?\nA. injury\nB. small cuts\nC. fever\nD. competition\nE. puncture wound\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A fencing thrust with a sharp sword towards a person would result in what?\nA. injury\nB. small cuts\nC. fever\nD. competition\nE. puncture wound\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A fencing thrust with a sharp sword towards a person would result in what?\nA. injury\nB. small cuts\nC. fever\nD. competition\nE. puncture wound\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4946393370628357", "True"]], [["-5.7446393966674805", "False"]], [["-5.7446393966674805", "False"]], [["-6.4946393966674805", "False"]], [["-2.7446393966674805", "False"]]], "filtered_resps": [["-0.4946393370628357", "True"], ["-5.7446393966674805", "False"], ["-5.7446393966674805", "False"], ["-6.4946393966674805", "False"], ["-2.7446393966674805", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ed5323e031b68a4568710dd06119d91dbde3ffca8708123358d79816fc35eea9", "prompt_hash": "70119aade3642da549143858fd342787fb8124eb4b70dae52368dc185e10c48a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 18, "doc": {"id": "8af63d58cc35061dec38e5448c325988", "question": "Unlike a spider and his many sight seers, people only have what?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tongues", "names", "brains", "feelings", "two eyes"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Unlike a spider and his many sight seers, people only have what?\nA. tongues\nB. names\nC. brains\nD. feelings\nE. two eyes\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Unlike a spider and his many sight seers, people only have what?\nA. tongues\nB. names\nC. brains\nD. feelings\nE. two eyes\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Unlike a spider and his many sight seers, people only have what?\nA. tongues\nB. names\nC. brains\nD. feelings\nE. two eyes\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Unlike a spider and his many sight seers, people only have what?\nA. tongues\nB. names\nC. brains\nD. feelings\nE. two eyes\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Unlike a spider and his many sight seers, people only have what?\nA. tongues\nB. names\nC. brains\nD. feelings\nE. two eyes\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.137629985809326", "False"]], [["-4.137629985809326", "False"]], [["-3.637629985809326", "False"]], [["-3.887629985809326", "False"]], [["-1.6376299858093262", "True"]]], "filtered_resps": [["-4.137629985809326", "False"], ["-4.137629985809326", "False"], ["-3.637629985809326", "False"], ["-3.887629985809326", "False"], ["-1.6376299858093262", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "094f41cd4a8ddf81bd4c5ea0e0afdd88e19c8b99c44268d632433ba20d23e1e4", "prompt_hash": "ba1b02d632816f38bfa72f47e79c6ec2a13f7a6afb820e32cccde5c2ce841253", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 19, "doc": {"id": "768fb09deab56046e1565b6a2556ad5c", "question": "Where do adults use glue sticks?", "question_concept": "glue stick", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["classroom", "desk drawer", "at school", "office", "kitchen drawer"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where do adults use glue sticks?\nA. classroom\nB. desk drawer\nC. at school\nD. office\nE. kitchen drawer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do adults use glue sticks?\nA. classroom\nB. desk drawer\nC. at school\nD. office\nE. kitchen drawer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do adults use glue sticks?\nA. classroom\nB. desk drawer\nC. at school\nD. office\nE. kitchen drawer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do adults use glue sticks?\nA. classroom\nB. desk drawer\nC. at school\nD. office\nE. kitchen drawer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do adults use glue sticks?\nA. classroom\nB. desk drawer\nC. at school\nD. office\nE. kitchen drawer\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.020207405090332", "False"]], [["-5.020207405090332", "False"]], [["-1.7702072858810425", "False"]], [["-4.020207405090332", "False"]], [["-6.770207405090332", "False"]]], "filtered_resps": [["-2.020207405090332", "False"], ["-5.020207405090332", "False"], ["-1.7702072858810425", "False"], ["-4.020207405090332", "False"], ["-6.770207405090332", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cab6d72283ebc6e2ec64a8dd1e992a53a9b75c8dcf1a8c3d646dba5e9b3050b8", "prompt_hash": "134c67858722a29e87d8e7839cb9c9dbccfe2b5bcc415e132e32d81ed6eee774", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 20, "doc": {"id": "cd639cf3ff82f825ace7dd2b087562bd", "question": "What could go on top of wood?", "question_concept": "wood", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lumberyard", "synagogue", "floor", "carpet", "hardware store"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What could go on top of wood?\nA. lumberyard\nB. synagogue\nC. floor\nD. carpet\nE. hardware store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could go on top of wood?\nA. lumberyard\nB. synagogue\nC. floor\nD. carpet\nE. hardware store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could go on top of wood?\nA. lumberyard\nB. synagogue\nC. floor\nD. carpet\nE. hardware store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could go on top of wood?\nA. lumberyard\nB. synagogue\nC. floor\nD. carpet\nE. hardware store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could go on top of wood?\nA. lumberyard\nB. synagogue\nC. floor\nD. carpet\nE. hardware store\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.797964334487915", "False"]], [["-4.547964096069336", "False"]], [["-2.547964334487915", "False"]], [["-0.7979642748832703", "True"]], [["-8.547964096069336", "False"]]], "filtered_resps": [["-2.797964334487915", "False"], ["-4.547964096069336", "False"], ["-2.547964334487915", "False"], ["-0.7979642748832703", "True"], ["-8.547964096069336", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "15a36f1c5e9a620e2554f5cb41b01ea130c724029e1cf2be2917b3106bac42d6", "prompt_hash": "0b06bd7e6d2f97fb3f0a15e74dd7448925d33e7d2d67d3928ab33739bc524ad6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 21, "doc": {"id": "8d79cc5e4eea11f50fab18fdea20fd4f", "question": "The artist was sitting quietly pondering, then suddenly he began to paint when what struck him?", "question_concept": "sitting quietly", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sadness", "anxiety", "inspiration", "discomfort", "insights"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The artist was sitting quietly pondering, then suddenly he began to paint when what struck him?\nA. sadness\nB. anxiety\nC. inspiration\nD. discomfort\nE. insights\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The artist was sitting quietly pondering, then suddenly he began to paint when what struck him?\nA. sadness\nB. anxiety\nC. inspiration\nD. discomfort\nE. insights\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The artist was sitting quietly pondering, then suddenly he began to paint when what struck him?\nA. sadness\nB. anxiety\nC. inspiration\nD. discomfort\nE. insights\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The artist was sitting quietly pondering, then suddenly he began to paint when what struck him?\nA. sadness\nB. anxiety\nC. inspiration\nD. discomfort\nE. insights\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The artist was sitting quietly pondering, then suddenly he began to paint when what struck him?\nA. sadness\nB. anxiety\nC. inspiration\nD. discomfort\nE. insights\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.24774169921875", "False"]], [["-4.99774169921875", "False"]], [["-0.9977415204048157", "True"]], [["-6.74774169921875", "False"]], [["-4.49774169921875", "False"]]], "filtered_resps": [["-4.24774169921875", "False"], ["-4.99774169921875", "False"], ["-0.9977415204048157", "True"], ["-6.74774169921875", "False"], ["-4.49774169921875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0fd43895750d66f2e31b66f306e321ac03f29fb0f30117ca56e73d3ae52d6a9c", "prompt_hash": "e5028f51efbe8846151e589420e9eb71d064c95d6c752fd4f36704e0ea44ceb6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 22, "doc": {"id": "e5ad2184e37ae88b2bf46bf6bc0ed2f4", "question": "Though the thin film seemed fragile, for it's intended purpose it was actually nearly what?", "question_concept": "fragile", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["indestructible", "durable", "undestroyable", "indestructible", "unbreakable"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Though the thin film seemed fragile, for it's intended purpose it was actually nearly what?\nA. indestructible\nB. durable\nC. undestroyable\nD. indestructible\nE. unbreakable\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Though the thin film seemed fragile, for it's intended purpose it was actually nearly what?\nA. indestructible\nB. durable\nC. undestroyable\nD. indestructible\nE. unbreakable\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Though the thin film seemed fragile, for it's intended purpose it was actually nearly what?\nA. indestructible\nB. durable\nC. undestroyable\nD. indestructible\nE. unbreakable\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Though the thin film seemed fragile, for it's intended purpose it was actually nearly what?\nA. indestructible\nB. durable\nC. undestroyable\nD. indestructible\nE. unbreakable\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Though the thin film seemed fragile, for it's intended purpose it was actually nearly what?\nA. indestructible\nB. durable\nC. undestroyable\nD. indestructible\nE. unbreakable\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.106168270111084", "False"]], [["-1.8561681509017944", "False"]], [["-4.856168270111084", "False"]], [["-3.106168270111084", "False"]], [["-4.106168270111084", "False"]]], "filtered_resps": [["-3.106168270111084", "False"], ["-1.8561681509017944", "False"], ["-4.856168270111084", "False"], ["-3.106168270111084", "False"], ["-4.106168270111084", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "245d67edc4e2e49120084816cc7b1325e7e7eac4b2e7e6b82786161824fc8f26", "prompt_hash": "53be5ad4419525b003a79aee47b5ac3b654ec78a8fd5ac3baeb0ee13ce1ac15c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 23, "doc": {"id": "b8b287b6277fccd4b7c9c72577177328", "question": "Where could you find a toilet that only friends can use?", "question_concept": "toilet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rest area", "school", "stadium", "apartment", "hospital"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where could you find a toilet that only friends can use?\nA. rest area\nB. school\nC. stadium\nD. apartment\nE. hospital\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could you find a toilet that only friends can use?\nA. rest area\nB. school\nC. stadium\nD. apartment\nE. hospital\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could you find a toilet that only friends can use?\nA. rest area\nB. school\nC. stadium\nD. apartment\nE. hospital\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could you find a toilet that only friends can use?\nA. rest area\nB. school\nC. stadium\nD. apartment\nE. hospital\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could you find a toilet that only friends can use?\nA. rest area\nB. school\nC. stadium\nD. apartment\nE. hospital\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.382789134979248", "False"]], [["-2.882789134979248", "False"]], [["-3.382789134979248", "False"]], [["-1.632789134979248", "True"]], [["-3.132789134979248", "False"]]], "filtered_resps": [["-2.382789134979248", "False"], ["-2.882789134979248", "False"], ["-3.382789134979248", "False"], ["-1.632789134979248", "True"], ["-3.132789134979248", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "53d21d26fc31374c0893f9cd74f6b885490c3b46ba1ca11680f868fa503e6cdb", "prompt_hash": "6226529e083c10ec8d0de3ea8d1ed715f948105429dbd954e65d2ff844f8c0a7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 24, "doc": {"id": "f646f3e064f06423fc25b98500796cf0", "question": "What is someone who isn't clever, bright, or competent called?", "question_concept": "clever", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["clumsy", "ineffectual", "dull", "clumsy", "stupid"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is someone who isn't clever, bright, or competent called?\nA. clumsy\nB. ineffectual\nC. dull\nD. clumsy\nE. stupid\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is someone who isn't clever, bright, or competent called?\nA. clumsy\nB. ineffectual\nC. dull\nD. clumsy\nE. stupid\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is someone who isn't clever, bright, or competent called?\nA. clumsy\nB. ineffectual\nC. dull\nD. clumsy\nE. stupid\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is someone who isn't clever, bright, or competent called?\nA. clumsy\nB. ineffectual\nC. dull\nD. clumsy\nE. stupid\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is someone who isn't clever, bright, or competent called?\nA. clumsy\nB. ineffectual\nC. dull\nD. clumsy\nE. stupid\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.200167417526245", "False"]], [["-4.450167655944824", "False"]], [["-2.950167417526245", "False"]], [["-7.200167655944824", "False"]], [["-2.200167417526245", "False"]]], "filtered_resps": [["-3.200167417526245", "False"], ["-4.450167655944824", "False"], ["-2.950167417526245", "False"], ["-7.200167655944824", "False"], ["-2.200167417526245", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65e4afcf1a74d613b053f9be8e4201481b123ffaad0b77415368a2c9f33a47be", "prompt_hash": "c5b09f2dbd6d3496348797f2e3c7a6995ef8c731a5260d86745953fe77cf9532", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 25, "doc": {"id": "b0f7d7978ac41c465108a92660d70e84", "question": "When wildlife reproduce we often refer to what comes out as what?", "question_concept": "reproduce", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["raise children", "have children", "photo copy", "offspring", "accidently got pregnant somehow"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When wildlife reproduce we often refer to what comes out as what?\nA. raise children\nB. have children\nC. photo copy\nD. offspring\nE. accidently got pregnant somehow\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When wildlife reproduce we often refer to what comes out as what?\nA. raise children\nB. have children\nC. photo copy\nD. offspring\nE. accidently got pregnant somehow\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When wildlife reproduce we often refer to what comes out as what?\nA. raise children\nB. have children\nC. photo copy\nD. offspring\nE. accidently got pregnant somehow\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When wildlife reproduce we often refer to what comes out as what?\nA. raise children\nB. have children\nC. photo copy\nD. offspring\nE. accidently got pregnant somehow\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When wildlife reproduce we often refer to what comes out as what?\nA. raise children\nB. have children\nC. photo copy\nD. offspring\nE. accidently got pregnant somehow\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.443668365478516", "False"]], [["-4.693668365478516", "False"]], [["-7.943668365478516", "False"]], [["-1.6936686038970947", "False"]], [["-9.693668365478516", "False"]]], "filtered_resps": [["-6.443668365478516", "False"], ["-4.693668365478516", "False"], ["-7.943668365478516", "False"], ["-1.6936686038970947", "False"], ["-9.693668365478516", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3ef6341aacaf844fd89f4a9c47cd4922a901a892f70480d0448915fdbb6a0c59", "prompt_hash": "25fd5689e483fcfdb9cc782b14b5a4937d32ee81fb87987d21aab61f9a1069f3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 26, "doc": {"id": "54075de8b8b89ecef2e4eb4eaee2713d", "question": "The weasel was becoming a problem, it kept getting into the chicken eggs kept in the what?", "question_concept": "weasel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["forrest", "barn", "public office", "out of doors", "freezer"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The weasel was becoming a problem, it kept getting into the chicken eggs kept in the what?\nA. forrest\nB. barn\nC. public office\nD. out of doors\nE. freezer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The weasel was becoming a problem, it kept getting into the chicken eggs kept in the what?\nA. forrest\nB. barn\nC. public office\nD. out of doors\nE. freezer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The weasel was becoming a problem, it kept getting into the chicken eggs kept in the what?\nA. forrest\nB. barn\nC. public office\nD. out of doors\nE. freezer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The weasel was becoming a problem, it kept getting into the chicken eggs kept in the what?\nA. forrest\nB. barn\nC. public office\nD. out of doors\nE. freezer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The weasel was becoming a problem, it kept getting into the chicken eggs kept in the what?\nA. forrest\nB. barn\nC. public office\nD. out of doors\nE. freezer\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.569490909576416", "False"]], [["-1.319490909576416", "True"]], [["-8.819490432739258", "False"]], [["-5.569490909576416", "False"]], [["-8.569490432739258", "False"]]], "filtered_resps": [["-3.569490909576416", "False"], ["-1.319490909576416", "True"], ["-8.819490432739258", "False"], ["-5.569490909576416", "False"], ["-8.569490432739258", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4b70e8fbc8e09e402406ed90e8c7872b895716ddf2d5bd20cbe1018e2a5bceba", "prompt_hash": "52a45d2accd71b5b12adac4938f819a810768b24e048eefd74c2f5bc9bfaf070", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 27, "doc": {"id": "65435b996ce9d1685bebb74b49c1ba7f", "question": "Blue read material outside of his comfort zone because he wanted to gain what?", "question_concept": "reading", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["new perspective", "entertained", "understanding", "hunger", "tired eyes"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Blue read material outside of his comfort zone because he wanted to gain what?\nA. new perspective\nB. entertained\nC. understanding\nD. hunger\nE. tired eyes\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Blue read material outside of his comfort zone because he wanted to gain what?\nA. new perspective\nB. entertained\nC. understanding\nD. hunger\nE. tired eyes\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Blue read material outside of his comfort zone because he wanted to gain what?\nA. new perspective\nB. entertained\nC. understanding\nD. hunger\nE. tired eyes\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Blue read material outside of his comfort zone because he wanted to gain what?\nA. new perspective\nB. entertained\nC. understanding\nD. hunger\nE. tired eyes\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Blue read material outside of his comfort zone because he wanted to gain what?\nA. new perspective\nB. entertained\nC. understanding\nD. hunger\nE. tired eyes\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0779774188995361", "True"]], [["-5.327977180480957", "False"]], [["-1.5779774188995361", "False"]], [["-5.327977180480957", "False"]], [["-7.827977180480957", "False"]]], "filtered_resps": [["-1.0779774188995361", "True"], ["-5.327977180480957", "False"], ["-1.5779774188995361", "False"], ["-5.327977180480957", "False"], ["-7.827977180480957", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bd9bcebf3b4826ddca9bccd4db0d478192cefafe848ee8838f1cb89844e890d8", "prompt_hash": "a5d9cd03c77fe6fe3a421140419b1669c99895e1c31eebec9c21e3257e691205", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 28, "doc": {"id": "9889e5389917d812c09d6e5d382d333d", "question": "After he got hired he hoped for success at his what?", "question_concept": "success", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["vocation", "new job", "michigan", "working hard", "manual"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: After he got hired he hoped for success at his what?\nA. vocation\nB. new job\nC. michigan\nD. working hard\nE. manual\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: After he got hired he hoped for success at his what?\nA. vocation\nB. new job\nC. michigan\nD. working hard\nE. manual\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: After he got hired he hoped for success at his what?\nA. vocation\nB. new job\nC. michigan\nD. working hard\nE. manual\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: After he got hired he hoped for success at his what?\nA. vocation\nB. new job\nC. michigan\nD. working hard\nE. manual\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: After he got hired he hoped for success at his what?\nA. vocation\nB. new job\nC. michigan\nD. working hard\nE. manual\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7160946130752563", "False"]], [["-2.466094493865967", "False"]], [["-7.966094493865967", "False"]], [["-7.966094493865967", "False"]], [["-9.716094970703125", "False"]]], "filtered_resps": [["-1.7160946130752563", "False"], ["-2.466094493865967", "False"], ["-7.966094493865967", "False"], ["-7.966094493865967", "False"], ["-9.716094970703125", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c7066f358ce5e28b7a2bd158d55626da9d43f452cbd19ec54969570d4aff7d45", "prompt_hash": "007134cb2a55bb3519e917c43ee30a1bb7a88bc6b9671824bc56125d017b0f98", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 29, "doc": {"id": "a651ffa44ac5febf0aede6748899b981", "question": "Committing perjury is a serious what?", "question_concept": "committing perjury", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["indictment", "crime", "violence", "lie", "go to jail"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Committing perjury is a serious what?\nA. indictment\nB. crime\nC. violence\nD. lie\nE. go to jail\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Committing perjury is a serious what?\nA. indictment\nB. crime\nC. violence\nD. lie\nE. go to jail\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Committing perjury is a serious what?\nA. indictment\nB. crime\nC. violence\nD. lie\nE. go to jail\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Committing perjury is a serious what?\nA. indictment\nB. crime\nC. violence\nD. lie\nE. go to jail\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Committing perjury is a serious what?\nA. indictment\nB. crime\nC. violence\nD. lie\nE. go to jail\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.647345542907715", "False"]], [["-1.147345781326294", "True"]], [["-7.897345542907715", "False"]], [["-7.147345542907715", "False"]], [["-10.397345542907715", "False"]]], "filtered_resps": [["-5.647345542907715", "False"], ["-1.147345781326294", "True"], ["-7.897345542907715", "False"], ["-7.147345542907715", "False"], ["-10.397345542907715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "869fb7894adf8267eb1bed3abb5f552514c66494fffbea583c7b40b6a1505ac7", "prompt_hash": "d9a58b110361259f698587055e3ccb79f1aaca9b5b4dc17b5cea7ec710209c1b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 30, "doc": {"id": "bdcfbe2132295d437e4c5701085f19c0", "question": "If you are prone to postpone work what will you have to do in order to finish on time?", "question_concept": "postpone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat", "hasten", "antedate", "bring forward", "advance"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you are prone to postpone work what will you have to do in order to finish on time?\nA. eat\nB. hasten\nC. antedate\nD. bring forward\nE. advance\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you are prone to postpone work what will you have to do in order to finish on time?\nA. eat\nB. hasten\nC. antedate\nD. bring forward\nE. advance\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you are prone to postpone work what will you have to do in order to finish on time?\nA. eat\nB. hasten\nC. antedate\nD. bring forward\nE. advance\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you are prone to postpone work what will you have to do in order to finish on time?\nA. eat\nB. hasten\nC. antedate\nD. bring forward\nE. advance\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you are prone to postpone work what will you have to do in order to finish on time?\nA. eat\nB. hasten\nC. antedate\nD. bring forward\nE. advance\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.137930393218994", "False"]], [["-2.387930393218994", "False"]], [["-6.137930393218994", "False"]], [["-3.137930393218994", "False"]], [["-4.137930393218994", "False"]]], "filtered_resps": [["-6.137930393218994", "False"], ["-2.387930393218994", "False"], ["-6.137930393218994", "False"], ["-3.137930393218994", "False"], ["-4.137930393218994", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0162dd1dec6a68dabba9f778c77a120a4d43651bfd39d05ab0af3605d1cb0469", "prompt_hash": "05311d9855a106e23fc4abb19917a5d64d16d58938484264465db57ffa631458", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 31, "doc": {"id": "8d3dc21a53523850ec80771daaa5ff20", "question": "James wanted to find an old underground map from the 50s.  Where might he look for one?", "question_concept": "underground map", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["library", "subway station", "county engineer's office", "super market", "home"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James wanted to find an old underground map from the 50s.  Where might he look for one?\nA. library\nB. subway station\nC. county engineer's office\nD. super market\nE. home\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James wanted to find an old underground map from the 50s.  Where might he look for one?\nA. library\nB. subway station\nC. county engineer's office\nD. super market\nE. home\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James wanted to find an old underground map from the 50s.  Where might he look for one?\nA. library\nB. subway station\nC. county engineer's office\nD. super market\nE. home\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James wanted to find an old underground map from the 50s.  Where might he look for one?\nA. library\nB. subway station\nC. county engineer's office\nD. super market\nE. home\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James wanted to find an old underground map from the 50s.  Where might he look for one?\nA. library\nB. subway station\nC. county engineer's office\nD. super market\nE. home\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9446057677268982", "True"]], [["-4.944605827331543", "False"]], [["-3.194605827331543", "False"]], [["-7.194605827331543", "False"]], [["-9.694605827331543", "False"]]], "filtered_resps": [["-0.9446057677268982", "True"], ["-4.944605827331543", "False"], ["-3.194605827331543", "False"], ["-7.194605827331543", "False"], ["-9.694605827331543", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "faac243076e5e030cd3453f8220d47b09b4699052b1abcce8c235442e67f170a", "prompt_hash": "99a4db954c81452799686729e1a543695386ba2b03c17d8a8b0d19fae1d5670d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 32, "doc": {"id": "a80ee7775e934c423012fe98e20ba28b", "question": "Sean was in a rush to get home, but the light turned yellow and he was forced to do what?", "question_concept": "rush", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["take time", "dawdle", "go slowly", "ocean", "slow down"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Sean was in a rush to get home, but the light turned yellow and he was forced to do what?\nA. take time\nB. dawdle\nC. go slowly\nD. ocean\nE. slow down\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sean was in a rush to get home, but the light turned yellow and he was forced to do what?\nA. take time\nB. dawdle\nC. go slowly\nD. ocean\nE. slow down\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sean was in a rush to get home, but the light turned yellow and he was forced to do what?\nA. take time\nB. dawdle\nC. go slowly\nD. ocean\nE. slow down\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sean was in a rush to get home, but the light turned yellow and he was forced to do what?\nA. take time\nB. dawdle\nC. go slowly\nD. ocean\nE. slow down\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sean was in a rush to get home, but the light turned yellow and he was forced to do what?\nA. take time\nB. dawdle\nC. go slowly\nD. ocean\nE. slow down\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.696188926696777", "False"]], [["-5.946188926696777", "False"]], [["-4.446188926696777", "False"]], [["-7.446188926696777", "False"]], [["-1.1961886882781982", "True"]]], "filtered_resps": [["-4.696188926696777", "False"], ["-5.946188926696777", "False"], ["-4.446188926696777", "False"], ["-7.446188926696777", "False"], ["-1.1961886882781982", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8d79605048144c9fcd5bc5640c37c3992dfb91ff8104352f55aac1605bf27662", "prompt_hash": "fa6df76f4091da8199bc7a6416aa18325793f5514ea4d922b28a1f66a04294c7", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 33, "doc": {"id": "48a315cfa3ce11f7a9d615bc854331d5", "question": "Where would a person be doing when having to wait their turn?", "question_concept": "wait turn", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have patience", "get in line", "sing", "stand in line", "turn left"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would a person be doing when having to wait their turn?\nA. have patience\nB. get in line\nC. sing\nD. stand in line\nE. turn left\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would a person be doing when having to wait their turn?\nA. have patience\nB. get in line\nC. sing\nD. stand in line\nE. turn left\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would a person be doing when having to wait their turn?\nA. have patience\nB. get in line\nC. sing\nD. stand in line\nE. turn left\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would a person be doing when having to wait their turn?\nA. have patience\nB. get in line\nC. sing\nD. stand in line\nE. turn left\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would a person be doing when having to wait their turn?\nA. have patience\nB. get in line\nC. sing\nD. stand in line\nE. turn left\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.6621973514556885", "False"]], [["-3.1621973514556885", "False"]], [["-7.912197113037109", "False"]], [["-1.6621973514556885", "True"]], [["-9.66219711303711", "False"]]], "filtered_resps": [["-2.6621973514556885", "False"], ["-3.1621973514556885", "False"], ["-7.912197113037109", "False"], ["-1.6621973514556885", "True"], ["-9.66219711303711", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e7ee313e92b978b2b46e83c83238d27d17a6c8db13ba28fdd4e4543bbb078e0d", "prompt_hash": "a001974743b5743cb0b112af618fa7074717a5902da9d96cfaf540626e77dc25", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 34, "doc": {"id": "4acd496cc78d96c2431279a5fd87de7c", "question": "She was always helping at the senior center, it brought her what?", "question_concept": "helping", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["satisfaction", "heart", "feel better", "pay", "happiness"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: She was always helping at the senior center, it brought her what?\nA. satisfaction\nB. heart\nC. feel better\nD. pay\nE. happiness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: She was always helping at the senior center, it brought her what?\nA. satisfaction\nB. heart\nC. feel better\nD. pay\nE. happiness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: She was always helping at the senior center, it brought her what?\nA. satisfaction\nB. heart\nC. feel better\nD. pay\nE. happiness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: She was always helping at the senior center, it brought her what?\nA. satisfaction\nB. heart\nC. feel better\nD. pay\nE. happiness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: She was always helping at the senior center, it brought her what?\nA. satisfaction\nB. heart\nC. feel better\nD. pay\nE. happiness\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8388128280639648", "False"]], [["-4.838812828063965", "False"]], [["-5.338812828063965", "False"]], [["-6.088812828063965", "False"]], [["-1.8388128280639648", "False"]]], "filtered_resps": [["-1.8388128280639648", "False"], ["-4.838812828063965", "False"], ["-5.338812828063965", "False"], ["-6.088812828063965", "False"], ["-1.8388128280639648", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "09e32934d9be637e27a70c41639b53c37ee3f00e9e08b37aae46baf7a5ab157f", "prompt_hash": "ccf89cd4da8a6e26a0bcb4d8ed29b8ec626050d9817c7477f7ae316241fcf2a0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 35, "doc": {"id": "91e0f4ab62c9d2fd440d73a3f5308d96", "question": "The lock kept the steering wheel from moving, but the thief still took his chances and began to work on the what?", "question_concept": "lock", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["keep cloesd", "train", "ignition switch", "drawer", "firearm"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The lock kept the steering wheel from moving, but the thief still took his chances and began to work on the what?\nA. keep cloesd\nB. train\nC. ignition switch\nD. drawer\nE. firearm\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The lock kept the steering wheel from moving, but the thief still took his chances and began to work on the what?\nA. keep cloesd\nB. train\nC. ignition switch\nD. drawer\nE. firearm\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The lock kept the steering wheel from moving, but the thief still took his chances and began to work on the what?\nA. keep cloesd\nB. train\nC. ignition switch\nD. drawer\nE. firearm\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The lock kept the steering wheel from moving, but the thief still took his chances and began to work on the what?\nA. keep cloesd\nB. train\nC. ignition switch\nD. drawer\nE. firearm\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The lock kept the steering wheel from moving, but the thief still took his chances and began to work on the what?\nA. keep cloesd\nB. train\nC. ignition switch\nD. drawer\nE. firearm\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.001458168029785", "False"]], [["-5.501458168029785", "False"]], [["-1.7514581680297852", "False"]], [["-6.751458168029785", "False"]], [["-5.251458168029785", "False"]]], "filtered_resps": [["-4.001458168029785", "False"], ["-5.501458168029785", "False"], ["-1.7514581680297852", "False"], ["-6.751458168029785", "False"], ["-5.251458168029785", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b02b7cb8da6c802766762648ca933593563b137df163a3c3e133f2b258b5e3ed", "prompt_hash": "c4c8a215e23a4fe485fb20ecaac65c8f00d86be83ee105b8fcfc2294a174aee5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 36, "doc": {"id": "b61e849e44db16a581f0b65e28ab95dc", "question": "Who is a police officer likely to work for?", "question_concept": "police officer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["beat", "direct traffic", "city", "street", "president"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Who is a police officer likely to work for?\nA. beat\nB. direct traffic\nC. city\nD. street\nE. president\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Who is a police officer likely to work for?\nA. beat\nB. direct traffic\nC. city\nD. street\nE. president\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Who is a police officer likely to work for?\nA. beat\nB. direct traffic\nC. city\nD. street\nE. president\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Who is a police officer likely to work for?\nA. beat\nB. direct traffic\nC. city\nD. street\nE. president\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Who is a police officer likely to work for?\nA. beat\nB. direct traffic\nC. city\nD. street\nE. president\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.224815845489502", "True"]], [["-5.974815845489502", "False"]], [["-1.724815845489502", "False"]], [["-7.224815845489502", "False"]], [["-7.474815845489502", "False"]]], "filtered_resps": [["-1.224815845489502", "True"], ["-5.974815845489502", "False"], ["-1.724815845489502", "False"], ["-7.224815845489502", "False"], ["-7.474815845489502", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "98d4cfc73dae7130794fb50e3696efabfb8340e08bc4d151e6f63f47b4dbf0a4", "prompt_hash": "807996c2a5825ed5725a61e0a688aa214c3640f27f813c9349aa041d3eab5315", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 37, "doc": {"id": "ba6bd1bdef02d0ebfe5370f92365ae18", "question": "If you have leftover cake, where would you put it?", "question_concept": "cake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["quandry", "refrigerator", "oven", "night stand", "bakery"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you have leftover cake, where would you put it?\nA. quandry\nB. refrigerator\nC. oven\nD. night stand\nE. bakery\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you have leftover cake, where would you put it?\nA. quandry\nB. refrigerator\nC. oven\nD. night stand\nE. bakery\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you have leftover cake, where would you put it?\nA. quandry\nB. refrigerator\nC. oven\nD. night stand\nE. bakery\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you have leftover cake, where would you put it?\nA. quandry\nB. refrigerator\nC. oven\nD. night stand\nE. bakery\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you have leftover cake, where would you put it?\nA. quandry\nB. refrigerator\nC. oven\nD. night stand\nE. bakery\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.751650810241699", "False"]], [["-0.7516508102416992", "True"]], [["-8.2516508102417", "False"]], [["-6.251650810241699", "False"]], [["-10.0016508102417", "False"]]], "filtered_resps": [["-4.751650810241699", "False"], ["-0.7516508102416992", "True"], ["-8.2516508102417", "False"], ["-6.251650810241699", "False"], ["-10.0016508102417", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "efb3421eb16c1e6036baa6d83eb9149e97dddcdfffed3758b0f2b50eae6f5bd0", "prompt_hash": "b1a5d74fc12d67f862be22590ee5996a1134e15fd19547a67c60f41a10ed72a4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 38, "doc": {"id": "dc55d473c22b04877b11d584f9548194", "question": "A human wants to submerge himself in water, what should he use?", "question_concept": "water", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["whirlpool bath", "coffee cup", "cup", "soft drink", "puddle"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A human wants to submerge himself in water, what should he use?\nA. whirlpool bath\nB. coffee cup\nC. cup\nD. soft drink\nE. puddle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A human wants to submerge himself in water, what should he use?\nA. whirlpool bath\nB. coffee cup\nC. cup\nD. soft drink\nE. puddle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A human wants to submerge himself in water, what should he use?\nA. whirlpool bath\nB. coffee cup\nC. cup\nD. soft drink\nE. puddle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A human wants to submerge himself in water, what should he use?\nA. whirlpool bath\nB. coffee cup\nC. cup\nD. soft drink\nE. puddle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A human wants to submerge himself in water, what should he use?\nA. whirlpool bath\nB. coffee cup\nC. cup\nD. soft drink\nE. puddle\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.754861831665039", "False"]], [["-4.754861831665039", "False"]], [["-4.004861831665039", "False"]], [["-5.754861831665039", "False"]], [["-5.504861831665039", "False"]]], "filtered_resps": [["-1.754861831665039", "False"], ["-4.754861831665039", "False"], ["-4.004861831665039", "False"], ["-5.754861831665039", "False"], ["-5.504861831665039", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b6b55be1671898f080e6857a337598b1c89ad5769ebba8fc5297be137650836e", "prompt_hash": "6e1f8a903118156c89b27963bd499146c0e7f03672e9937e10c03049520216bb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 39, "doc": {"id": "113aaea2b1a27a976547f54e531d99bb", "question": "Where is a doormat likely to be in front of?", "question_concept": "doormat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["facade", "front door", "doorway", "entrance porch", "hallway"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a doormat likely to be in front of?\nA. facade\nB. front door\nC. doorway\nD. entrance porch\nE. hallway\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a doormat likely to be in front of?\nA. facade\nB. front door\nC. doorway\nD. entrance porch\nE. hallway\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a doormat likely to be in front of?\nA. facade\nB. front door\nC. doorway\nD. entrance porch\nE. hallway\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a doormat likely to be in front of?\nA. facade\nB. front door\nC. doorway\nD. entrance porch\nE. hallway\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a doormat likely to be in front of?\nA. facade\nB. front door\nC. doorway\nD. entrance porch\nE. hallway\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.650934100151062", "False"]], [["-1.400934100151062", "True"]], [["-3.1509342193603516", "False"]], [["-4.400934219360352", "False"]], [["-8.900934219360352", "False"]]], "filtered_resps": [["-1.650934100151062", "False"], ["-1.400934100151062", "True"], ["-3.1509342193603516", "False"], ["-4.400934219360352", "False"], ["-8.900934219360352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "86748bf23a75017ceff0d3ddd12ef3665d9a8f3c160a6e88e91f374c7afb54f3", "prompt_hash": "dbf7c70bddd614f5edbcce1fdd6e15689e8712922bd1f86af7307051f2d489b1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 40, "doc": {"id": "ba640b9634ad6b4ad98b17b4f152e562", "question": "Bob the lizard lives in a warm place with lots of water.  Where does he probably live?", "question_concept": "lizard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rock", "tropical rainforest", "jazz club", "new mexico", "rocky places"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Bob the lizard lives in a warm place with lots of water.  Where does he probably live?\nA. rock\nB. tropical rainforest\nC. jazz club\nD. new mexico\nE. rocky places\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Bob the lizard lives in a warm place with lots of water.  Where does he probably live?\nA. rock\nB. tropical rainforest\nC. jazz club\nD. new mexico\nE. rocky places\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Bob the lizard lives in a warm place with lots of water.  Where does he probably live?\nA. rock\nB. tropical rainforest\nC. jazz club\nD. new mexico\nE. rocky places\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Bob the lizard lives in a warm place with lots of water.  Where does he probably live?\nA. rock\nB. tropical rainforest\nC. jazz club\nD. new mexico\nE. rocky places\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Bob the lizard lives in a warm place with lots of water.  Where does he probably live?\nA. rock\nB. tropical rainforest\nC. jazz club\nD. new mexico\nE. rocky places\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.462976455688477", "False"]], [["-0.9629766941070557", "True"]], [["-8.462976455688477", "False"]], [["-7.712976455688477", "False"]], [["-10.212976455688477", "False"]]], "filtered_resps": [["-4.462976455688477", "False"], ["-0.9629766941070557", "True"], ["-8.462976455688477", "False"], ["-7.712976455688477", "False"], ["-10.212976455688477", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "93e7b797f857bc457cf1e89e3fc7661c78ac440d6cd30fe429e315d2ed73a5fe", "prompt_hash": "4b20d783411bd1dcc6a1a934f36814c7f59e7724805b5b2dd1a7a60c01f651a4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 41, "doc": {"id": "750ebdf36a0b3b407be0fe2163e3700b", "question": "August needed  money because he was afraid that he'd be kicked out of his house.  What did he need money to do?", "question_concept": "money", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["control people", "pay bills", "hurt people", "buy food", "get things"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: August needed  money because he was afraid that he'd be kicked out of his house.  What did he need money to do?\nA. control people\nB. pay bills\nC. hurt people\nD. buy food\nE. get things\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: August needed  money because he was afraid that he'd be kicked out of his house.  What did he need money to do?\nA. control people\nB. pay bills\nC. hurt people\nD. buy food\nE. get things\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: August needed  money because he was afraid that he'd be kicked out of his house.  What did he need money to do?\nA. control people\nB. pay bills\nC. hurt people\nD. buy food\nE. get things\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: August needed  money because he was afraid that he'd be kicked out of his house.  What did he need money to do?\nA. control people\nB. pay bills\nC. hurt people\nD. buy food\nE. get things\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: August needed  money because he was afraid that he'd be kicked out of his house.  What did he need money to do?\nA. control people\nB. pay bills\nC. hurt people\nD. buy food\nE. get things\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.963822364807129", "False"]], [["-0.7138225436210632", "True"]], [["-8.713822364807129", "False"]], [["-5.713822364807129", "False"]], [["-6.963822364807129", "False"]]], "filtered_resps": [["-5.963822364807129", "False"], ["-0.7138225436210632", "True"], ["-8.713822364807129", "False"], ["-5.713822364807129", "False"], ["-6.963822364807129", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "960a28a80a97d0aa44703d5c5f5b096a4c1ba37f5c4cd3cc8b40287310ad3c8a", "prompt_hash": "311a51002d9b70eaa188a0024575031150eb824835608fd2292ee26e1b85ee0c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 42, "doc": {"id": "8f01273422a370a8dbda6bf473a395a0", "question": "He needed more information to fix it, so he consulted the what?", "question_concept": "information", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["chickens", "google", "newspaper", "online", "manual"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: He needed more information to fix it, so he consulted the what?\nA. chickens\nB. google\nC. newspaper\nD. online\nE. manual\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He needed more information to fix it, so he consulted the what?\nA. chickens\nB. google\nC. newspaper\nD. online\nE. manual\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He needed more information to fix it, so he consulted the what?\nA. chickens\nB. google\nC. newspaper\nD. online\nE. manual\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He needed more information to fix it, so he consulted the what?\nA. chickens\nB. google\nC. newspaper\nD. online\nE. manual\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He needed more information to fix it, so he consulted the what?\nA. chickens\nB. google\nC. newspaper\nD. online\nE. manual\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.4813432693481445", "False"]], [["-2.4813432693481445", "False"]], [["-7.4813432693481445", "False"]], [["-2.2313432693481445", "False"]], [["-1.7313432693481445", "True"]]], "filtered_resps": [["-4.4813432693481445", "False"], ["-2.4813432693481445", "False"], ["-7.4813432693481445", "False"], ["-2.2313432693481445", "False"], ["-1.7313432693481445", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e1ab2bef18a1ccf308bdad50f74f8bc3090eff6ab95b09846b6c20cd86ef6957", "prompt_hash": "cb4f023c5824cc7792aaf3ac7e7187d211226065ec2c17ab9c9726f3e84f4921", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 43, "doc": {"id": "e6586bba9fe96d38792e6e6d4f2703dc", "question": "Where can you put a picture frame when it's not hung vertically?", "question_concept": "picture", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["art show", "wall", "newspaper", "car", "table"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you put a picture frame when it's not hung vertically?\nA. art show\nB. wall\nC. newspaper\nD. car\nE. table\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you put a picture frame when it's not hung vertically?\nA. art show\nB. wall\nC. newspaper\nD. car\nE. table\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you put a picture frame when it's not hung vertically?\nA. art show\nB. wall\nC. newspaper\nD. car\nE. table\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you put a picture frame when it's not hung vertically?\nA. art show\nB. wall\nC. newspaper\nD. car\nE. table\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you put a picture frame when it's not hung vertically?\nA. art show\nB. wall\nC. newspaper\nD. car\nE. table\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7372066974639893", "False"]], [["-3.9872066974639893", "False"]], [["-9.48720645904541", "False"]], [["-9.73720645904541", "False"]], [["-1.7372066974639893", "False"]]], "filtered_resps": [["-3.7372066974639893", "False"], ["-3.9872066974639893", "False"], ["-9.48720645904541", "False"], ["-9.73720645904541", "False"], ["-1.7372066974639893", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8e99ee02ef2c42a1e7a9757c4d357e59a0c3cfb3c8f3a0b2990391f78c6c1204", "prompt_hash": "3debcae45dc0fbecd103102b50d30ea29fa66ff12a1b63a95f2be036352dd29d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 44, "doc": {"id": "6e433471d0e2590b8c73ceef275022b1", "question": "James knew that he shouldn't have been buying beer for minors.  He didn't even get paid for it.  Why was this bad?", "question_concept": "buying beer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lose money", "fun", "have no money", "broken law", "relaxation"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: James knew that he shouldn't have been buying beer for minors.  He didn't even get paid for it.  Why was this bad?\nA. lose money\nB. fun\nC. have no money\nD. broken law\nE. relaxation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James knew that he shouldn't have been buying beer for minors.  He didn't even get paid for it.  Why was this bad?\nA. lose money\nB. fun\nC. have no money\nD. broken law\nE. relaxation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James knew that he shouldn't have been buying beer for minors.  He didn't even get paid for it.  Why was this bad?\nA. lose money\nB. fun\nC. have no money\nD. broken law\nE. relaxation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James knew that he shouldn't have been buying beer for minors.  He didn't even get paid for it.  Why was this bad?\nA. lose money\nB. fun\nC. have no money\nD. broken law\nE. relaxation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James knew that he shouldn't have been buying beer for minors.  He didn't even get paid for it.  Why was this bad?\nA. lose money\nB. fun\nC. have no money\nD. broken law\nE. relaxation\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.838578701019287", "False"]], [["-6.588578701019287", "False"]], [["-8.088578224182129", "False"]], [["-1.3385785818099976", "False"]], [["-9.588578224182129", "False"]]], "filtered_resps": [["-6.838578701019287", "False"], ["-6.588578701019287", "False"], ["-8.088578224182129", "False"], ["-1.3385785818099976", "False"], ["-9.588578224182129", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ce990ef7e0d1c2979a176143c0211a407d34c8a96782a9bf0583d8098d17a882", "prompt_hash": "f097a5cee59ee1de3c4a6b5a3e05b50bf15948bbf21abce1a8a8da62bda36622", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 45, "doc": {"id": "1bc986f8aea88d6927d8a45367855a94", "question": "What is the result of applying for  job?", "question_concept": "applying for job", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["anxiety and fear", "increased workload", "praise", "less sleep", "being employed"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is the result of applying for  job?\nA. anxiety and fear\nB. increased workload\nC. praise\nD. less sleep\nE. being employed\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the result of applying for  job?\nA. anxiety and fear\nB. increased workload\nC. praise\nD. less sleep\nE. being employed\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the result of applying for  job?\nA. anxiety and fear\nB. increased workload\nC. praise\nD. less sleep\nE. being employed\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the result of applying for  job?\nA. anxiety and fear\nB. increased workload\nC. praise\nD. less sleep\nE. being employed\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the result of applying for  job?\nA. anxiety and fear\nB. increased workload\nC. praise\nD. less sleep\nE. being employed\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.53080940246582", "False"]], [["-5.53080940246582", "False"]], [["-7.28080940246582", "False"]], [["-7.78080940246582", "False"]], [["-1.5308096408843994", "False"]]], "filtered_resps": [["-5.53080940246582", "False"], ["-5.53080940246582", "False"], ["-7.28080940246582", "False"], ["-7.78080940246582", "False"], ["-1.5308096408843994", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "614645959f6c13936d49625bde2d19178f642c7f9f260f7b5223fbf1bfc96a12", "prompt_hash": "b7d004cd6fbe679e9742426b493c41b78e62af0b2e8ba49d9b758dfd6b7a3b4d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 46, "doc": {"id": "8d1563697d751a364d688d6701ebdb39", "question": "What must someone do before they shop?", "question_concept": "shop", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get money", "have money", "bring cash", "go to market", "bring cash"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What must someone do before they shop?\nA. get money\nB. have money\nC. bring cash\nD. go to market\nE. bring cash\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What must someone do before they shop?\nA. get money\nB. have money\nC. bring cash\nD. go to market\nE. bring cash\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What must someone do before they shop?\nA. get money\nB. have money\nC. bring cash\nD. go to market\nE. bring cash\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What must someone do before they shop?\nA. get money\nB. have money\nC. bring cash\nD. go to market\nE. bring cash\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What must someone do before they shop?\nA. get money\nB. have money\nC. bring cash\nD. go to market\nE. bring cash\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9129388332366943", "False"]], [["-1.9129388332366943", "True"]], [["-6.162939071655273", "False"]], [["-7.162939071655273", "False"]], [["-5.162939071655273", "False"]]], "filtered_resps": [["-3.9129388332366943", "False"], ["-1.9129388332366943", "True"], ["-6.162939071655273", "False"], ["-7.162939071655273", "False"], ["-5.162939071655273", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7a377852f18347c881af1374a803ac020f6763dccbc1cfd002abfe1869513287", "prompt_hash": "f560b18aa381af2f564bfaf1b39fcb0e2329ce4a9407e8d8753f6f1b74ab9b11", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 47, "doc": {"id": "91f512273a2da7ae796919069b20d6cf", "question": "Because John was first violin, he had to bring something important to work ever day. What did he need to bring to work?", "question_concept": "first violin", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["music store", "obesity", "symphony orchestra", "ochestra", "violin case"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Because John was first violin, he had to bring something important to work ever day. What did he need to bring to work?\nA. music store\nB. obesity\nC. symphony orchestra\nD. ochestra\nE. violin case\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Because John was first violin, he had to bring something important to work ever day. What did he need to bring to work?\nA. music store\nB. obesity\nC. symphony orchestra\nD. ochestra\nE. violin case\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Because John was first violin, he had to bring something important to work ever day. What did he need to bring to work?\nA. music store\nB. obesity\nC. symphony orchestra\nD. ochestra\nE. violin case\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Because John was first violin, he had to bring something important to work ever day. What did he need to bring to work?\nA. music store\nB. obesity\nC. symphony orchestra\nD. ochestra\nE. violin case\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Because John was first violin, he had to bring something important to work ever day. What did he need to bring to work?\nA. music store\nB. obesity\nC. symphony orchestra\nD. ochestra\nE. violin case\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.045058727264404", "False"]], [["-5.795058727264404", "False"]], [["-7.045058727264404", "False"]], [["-4.545058727264404", "False"]], [["-0.7950586080551147", "True"]]], "filtered_resps": [["-4.045058727264404", "False"], ["-5.795058727264404", "False"], ["-7.045058727264404", "False"], ["-4.545058727264404", "False"], ["-0.7950586080551147", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "356151e12354f2cc71368a4eb6794b401a328c30defd6cd29107b2541a1de71b", "prompt_hash": "f58a5c01094f9e7388a2c4790e36fd86b8a761dc040074cc21ca36369d0a2a53", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 48, "doc": {"id": "49cda7eedbf63b3f38e59ba72f1ee1f9", "question": "What is a place that usually does not have an elevator and that sometimes has a telephone book?", "question_concept": "telephone book", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["at hotel", "kitchen", "library", "telephone booth", "house"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is a place that usually does not have an elevator and that sometimes has a telephone book?\nA. at hotel\nB. kitchen\nC. library\nD. telephone booth\nE. house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a place that usually does not have an elevator and that sometimes has a telephone book?\nA. at hotel\nB. kitchen\nC. library\nD. telephone booth\nE. house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a place that usually does not have an elevator and that sometimes has a telephone book?\nA. at hotel\nB. kitchen\nC. library\nD. telephone booth\nE. house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a place that usually does not have an elevator and that sometimes has a telephone book?\nA. at hotel\nB. kitchen\nC. library\nD. telephone booth\nE. house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a place that usually does not have an elevator and that sometimes has a telephone book?\nA. at hotel\nB. kitchen\nC. library\nD. telephone booth\nE. house\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6815226078033447", "True"]], [["-6.681522369384766", "False"]], [["-4.181522369384766", "False"]], [["-2.9315226078033447", "False"]], [["-5.931522369384766", "False"]]], "filtered_resps": [["-1.6815226078033447", "True"], ["-6.681522369384766", "False"], ["-4.181522369384766", "False"], ["-2.9315226078033447", "False"], ["-5.931522369384766", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c453366155da733e1f4432a3c40f254591ae5b914111caab8350e2164da0bdee", "prompt_hash": "f41ead0a173cfc6527852e33bcd979ce29613fabf965235745ab59a9c413a9b5", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 49, "doc": {"id": "a588407ecaecf0f30c2241c30b470fe2", "question": "Who is likely to be excited about a crab?", "question_concept": "crab", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fish market", "pet shop", "fishmongers", "intertidal zone", "obesity"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Who is likely to be excited about a crab?\nA. fish market\nB. pet shop\nC. fishmongers\nD. intertidal zone\nE. obesity\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Who is likely to be excited about a crab?\nA. fish market\nB. pet shop\nC. fishmongers\nD. intertidal zone\nE. obesity\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Who is likely to be excited about a crab?\nA. fish market\nB. pet shop\nC. fishmongers\nD. intertidal zone\nE. obesity\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Who is likely to be excited about a crab?\nA. fish market\nB. pet shop\nC. fishmongers\nD. intertidal zone\nE. obesity\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Who is likely to be excited about a crab?\nA. fish market\nB. pet shop\nC. fishmongers\nD. intertidal zone\nE. obesity\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.268838882446289", "False"]], [["-3.268838882446289", "False"]], [["-1.518838882446289", "True"]], [["-2.268838882446289", "False"]], [["-6.768838882446289", "False"]]], "filtered_resps": [["-2.268838882446289", "False"], ["-3.268838882446289", "False"], ["-1.518838882446289", "True"], ["-2.268838882446289", "False"], ["-6.768838882446289", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1af0cc017d984554f30163e34a67a771c1702352622159b3ca440920f895bc99", "prompt_hash": "3086dd2d4cd7023e45756a15f8d90ff0a96ab3b76b863b0ef8233ea5a83ca63f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 50, "doc": {"id": "011096bcfff30fd38046cf9db3a411c5", "question": "Where can a human find clothes that aren't pants?", "question_concept": "human", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pants shop", "on planet earth", "dress shop", "school", "train wreck"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where can a human find clothes that aren't pants?\nA. pants shop\nB. on planet earth\nC. dress shop\nD. school\nE. train wreck\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can a human find clothes that aren't pants?\nA. pants shop\nB. on planet earth\nC. dress shop\nD. school\nE. train wreck\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can a human find clothes that aren't pants?\nA. pants shop\nB. on planet earth\nC. dress shop\nD. school\nE. train wreck\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can a human find clothes that aren't pants?\nA. pants shop\nB. on planet earth\nC. dress shop\nD. school\nE. train wreck\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can a human find clothes that aren't pants?\nA. pants shop\nB. on planet earth\nC. dress shop\nD. school\nE. train wreck\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7245564460754395", "False"]], [["-3.4745564460754395", "False"]], [["-1.474556565284729", "False"]], [["-7.7245564460754395", "False"]], [["-9.224556922912598", "False"]]], "filtered_resps": [["-2.7245564460754395", "False"], ["-3.4745564460754395", "False"], ["-1.474556565284729", "False"], ["-7.7245564460754395", "False"], ["-9.224556922912598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8fd98dc458ac98b46c133a4462a49f4f0b2ae8f093a3e16248984958c06bc81b", "prompt_hash": "e3451878a7acf0100ba7457ec5cb309acf59de3505e6807eadc2c87203978730", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 51, "doc": {"id": "435a728f45d32faa4b3c4553c966fd6b", "question": "If I was getting drunk, and people couldn't understand me, what might I be having?", "question_concept": "getting drunk", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["a seizure", "slurred speech", "death", "forgetfulness", "pass out"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If I was getting drunk, and people couldn't understand me, what might I be having?\nA. a seizure\nB. slurred speech\nC. death\nD. forgetfulness\nE. pass out\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I was getting drunk, and people couldn't understand me, what might I be having?\nA. a seizure\nB. slurred speech\nC. death\nD. forgetfulness\nE. pass out\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I was getting drunk, and people couldn't understand me, what might I be having?\nA. a seizure\nB. slurred speech\nC. death\nD. forgetfulness\nE. pass out\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I was getting drunk, and people couldn't understand me, what might I be having?\nA. a seizure\nB. slurred speech\nC. death\nD. forgetfulness\nE. pass out\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I was getting drunk, and people couldn't understand me, what might I be having?\nA. a seizure\nB. slurred speech\nC. death\nD. forgetfulness\nE. pass out\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.532281398773193", "False"]], [["-1.0322813987731934", "True"]], [["-7.282281398773193", "False"]], [["-7.032281398773193", "False"]], [["-9.032281875610352", "False"]]], "filtered_resps": [["-5.532281398773193", "False"], ["-1.0322813987731934", "True"], ["-7.282281398773193", "False"], ["-7.032281398773193", "False"], ["-9.032281875610352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6e9f000cef241c3e6727eee5a58c8f4446686156270c1c79236994470ebd5670", "prompt_hash": "2ef637c1c84231c7502efbd7c4374ce6f39b5f1276dab2600d798c786b3500b5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 52, "doc": {"id": "e953dee48c70159ad879143a319ec607", "question": "When a person is beginning work, what are they building?", "question_concept": "beginning work", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["time", "accomplishing", "working", "momentum", "tiredness"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When a person is beginning work, what are they building?\nA. time\nB. accomplishing\nC. working\nD. momentum\nE. tiredness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When a person is beginning work, what are they building?\nA. time\nB. accomplishing\nC. working\nD. momentum\nE. tiredness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When a person is beginning work, what are they building?\nA. time\nB. accomplishing\nC. working\nD. momentum\nE. tiredness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When a person is beginning work, what are they building?\nA. time\nB. accomplishing\nC. working\nD. momentum\nE. tiredness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When a person is beginning work, what are they building?\nA. time\nB. accomplishing\nC. working\nD. momentum\nE. tiredness\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.209885835647583", "False"]], [["-2.709885835647583", "False"]], [["-3.209885835647583", "False"]], [["-2.209885835647583", "False"]], [["-5.97340726852417", "False"]]], "filtered_resps": [["-3.209885835647583", "False"], ["-2.709885835647583", "False"], ["-3.209885835647583", "False"], ["-2.209885835647583", "False"], ["-5.97340726852417", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e0c933a61eec6ccecce183dbef30475a7f64dc8e39bfd7ee5bb305668fc1bfc4", "prompt_hash": "3d4f35b366ec72106de3fffed4b2ed1a62eb2145e560007e479aefd8acbb9d90", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 53, "doc": {"id": "9c784727afd7176b54764055df7a7927", "question": "A child wants to play, what would they likely want?", "question_concept": "child", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fall down", "breathe", "play tag", "be dismembered by a chainsaw", "become adult"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: A child wants to play, what would they likely want?\nA. fall down\nB. breathe\nC. play tag\nD. be dismembered by a chainsaw\nE. become adult\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A child wants to play, what would they likely want?\nA. fall down\nB. breathe\nC. play tag\nD. be dismembered by a chainsaw\nE. become adult\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A child wants to play, what would they likely want?\nA. fall down\nB. breathe\nC. play tag\nD. be dismembered by a chainsaw\nE. become adult\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A child wants to play, what would they likely want?\nA. fall down\nB. breathe\nC. play tag\nD. be dismembered by a chainsaw\nE. become adult\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A child wants to play, what would they likely want?\nA. fall down\nB. breathe\nC. play tag\nD. be dismembered by a chainsaw\nE. become adult\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.904585361480713", "False"]], [["-5.654585361480713", "False"]], [["-0.6545853614807129", "True"]], [["-7.404585361480713", "False"]], [["-6.904585361480713", "False"]]], "filtered_resps": [["-3.904585361480713", "False"], ["-5.654585361480713", "False"], ["-0.6545853614807129", "True"], ["-7.404585361480713", "False"], ["-6.904585361480713", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2dfc6b674438bc83d83f275cb3a5b2cd7750b329868413cb35bd41d12e759469", "prompt_hash": "f9f1192a49b2ec00277dfedc40d08105dca1c3ff4dcd17ab692a89be33db3929", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 54, "doc": {"id": "b47d912136e3304cb5e5890b6b879551", "question": "Talking to the same person about the same thing over and over again is something someone can what?", "question_concept": "talking to", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["social life", "friendship", "eye contact", "get tired of", "learn lessons from"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Talking to the same person about the same thing over and over again is something someone can what?\nA. social life\nB. friendship\nC. eye contact\nD. get tired of\nE. learn lessons from\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Talking to the same person about the same thing over and over again is something someone can what?\nA. social life\nB. friendship\nC. eye contact\nD. get tired of\nE. learn lessons from\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Talking to the same person about the same thing over and over again is something someone can what?\nA. social life\nB. friendship\nC. eye contact\nD. get tired of\nE. learn lessons from\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Talking to the same person about the same thing over and over again is something someone can what?\nA. social life\nB. friendship\nC. eye contact\nD. get tired of\nE. learn lessons from\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Talking to the same person about the same thing over and over again is something someone can what?\nA. social life\nB. friendship\nC. eye contact\nD. get tired of\nE. learn lessons from\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.743642330169678", "False"]], [["-8.49364185333252", "False"]], [["-7.993642330169678", "False"]], [["-1.9936422109603882", "False"]], [["-8.99364185333252", "False"]]], "filtered_resps": [["-5.743642330169678", "False"], ["-8.49364185333252", "False"], ["-7.993642330169678", "False"], ["-1.9936422109603882", "False"], ["-8.99364185333252", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b789689f0902a8d137e8f74cc2c63a4f036a7b414b768ecb1bebd13b24e39d9f", "prompt_hash": "7b1821bf1d7c7f7c1292ed725e86da399c8ae5e8f4b5e99ad092560ea176c088", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 55, "doc": {"id": "49b4c9e1bd7946a819e173ce8fa4c7c9", "question": "The teacher doesn't tolerate noise during a test in their what?", "question_concept": "noise", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["movie theatre", "bowling alley", "factory", "store", "classroom"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The teacher doesn't tolerate noise during a test in their what?\nA. movie theatre\nB. bowling alley\nC. factory\nD. store\nE. classroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The teacher doesn't tolerate noise during a test in their what?\nA. movie theatre\nB. bowling alley\nC. factory\nD. store\nE. classroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The teacher doesn't tolerate noise during a test in their what?\nA. movie theatre\nB. bowling alley\nC. factory\nD. store\nE. classroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The teacher doesn't tolerate noise during a test in their what?\nA. movie theatre\nB. bowling alley\nC. factory\nD. store\nE. classroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The teacher doesn't tolerate noise during a test in their what?\nA. movie theatre\nB. bowling alley\nC. factory\nD. store\nE. classroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.566030025482178", "False"]], [["-5.566030025482178", "False"]], [["-8.81602954864502", "False"]], [["-7.816030025482178", "False"]], [["-0.566029965877533", "True"]]], "filtered_resps": [["-4.566030025482178", "False"], ["-5.566030025482178", "False"], ["-8.81602954864502", "False"], ["-7.816030025482178", "False"], ["-0.566029965877533", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "53d6730ae3aad9c123d86e68791512b89f350656d461aa7f44dcfafa842d6315", "prompt_hash": "b92cc2c183b9fb924d8da3b67cabc11d5a642cd429b7dec1f4ef347918663886", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 56, "doc": {"id": "950af0b765c298960ce3dada66df8db1", "question": "The freeway had no traffic and few buildings, where is it?", "question_concept": "freeway", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["california", "countryside", "big town", "florida", "america"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The freeway had no traffic and few buildings, where is it?\nA. california\nB. countryside\nC. big town\nD. florida\nE. america\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The freeway had no traffic and few buildings, where is it?\nA. california\nB. countryside\nC. big town\nD. florida\nE. america\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The freeway had no traffic and few buildings, where is it?\nA. california\nB. countryside\nC. big town\nD. florida\nE. america\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The freeway had no traffic and few buildings, where is it?\nA. california\nB. countryside\nC. big town\nD. florida\nE. america\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The freeway had no traffic and few buildings, where is it?\nA. california\nB. countryside\nC. big town\nD. florida\nE. america\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.275312423706055", "False"]], [["-1.0253125429153442", "True"]], [["-7.025312423706055", "False"]], [["-8.275312423706055", "False"]], [["-9.775312423706055", "False"]]], "filtered_resps": [["-4.275312423706055", "False"], ["-1.0253125429153442", "True"], ["-7.025312423706055", "False"], ["-8.275312423706055", "False"], ["-9.775312423706055", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3cb9fb78750affca9e663dd19288b4a2546632cf27e7eca7e71da5083a48de23", "prompt_hash": "5a9f1fca18048ddcf761db153b615477abd811731c769dc3b463268863ab941c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 57, "doc": {"id": "63cf1adb5fe302b9867ead8bc8103d0b", "question": "Where would you go if you wanted to have fun with a few people?", "question_concept": "fun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["watching television", "good", "cinema", "friend's house", "fairgrounds"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you go if you wanted to have fun with a few people?\nA. watching television\nB. good\nC. cinema\nD. friend's house\nE. fairgrounds\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you go if you wanted to have fun with a few people?\nA. watching television\nB. good\nC. cinema\nD. friend's house\nE. fairgrounds\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you go if you wanted to have fun with a few people?\nA. watching television\nB. good\nC. cinema\nD. friend's house\nE. fairgrounds\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you go if you wanted to have fun with a few people?\nA. watching television\nB. good\nC. cinema\nD. friend's house\nE. fairgrounds\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you go if you wanted to have fun with a few people?\nA. watching television\nB. good\nC. cinema\nD. friend's house\nE. fairgrounds\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.808597564697266", "False"]], [["-6.308597564697266", "False"]], [["-6.058597564697266", "False"]], [["-1.8085978031158447", "False"]], [["-2.0585978031158447", "False"]]], "filtered_resps": [["-4.808597564697266", "False"], ["-6.308597564697266", "False"], ["-6.058597564697266", "False"], ["-1.8085978031158447", "False"], ["-2.0585978031158447", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e8eae8d35c4eae0f4a62ebf2579353d80bccff9a516a2ce28d59618ea1c35150", "prompt_hash": "bbb8a70cb0af1bb9bcdc4b3b8785f18c1acd8264349f2240bc54f1943f6832d1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 58, "doc": {"id": "ede4d302fc2ffe07703158f83c1493f2", "question": "If there is a place that is hot and arid, what could it be?", "question_concept": "hot", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bland", "lifeless", "sandy", "neutral", "freezing"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If there is a place that is hot and arid, what could it be?\nA. bland\nB. lifeless\nC. sandy\nD. neutral\nE. freezing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If there is a place that is hot and arid, what could it be?\nA. bland\nB. lifeless\nC. sandy\nD. neutral\nE. freezing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If there is a place that is hot and arid, what could it be?\nA. bland\nB. lifeless\nC. sandy\nD. neutral\nE. freezing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If there is a place that is hot and arid, what could it be?\nA. bland\nB. lifeless\nC. sandy\nD. neutral\nE. freezing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If there is a place that is hot and arid, what could it be?\nA. bland\nB. lifeless\nC. sandy\nD. neutral\nE. freezing\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.742154121398926", "False"]], [["-6.242154121398926", "False"]], [["-0.7421540021896362", "True"]], [["-7.992154121398926", "False"]], [["-8.242154121398926", "False"]]], "filtered_resps": [["-3.742154121398926", "False"], ["-6.242154121398926", "False"], ["-0.7421540021896362", "True"], ["-7.992154121398926", "False"], ["-8.242154121398926", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65bcce00838b0725ac59f3c6bc99d9718aee2db63d2f39abd97772088fd84b2f", "prompt_hash": "fa6b980f1641d5de9a1d7a4a9712aee0a477c67da41dbd9abf000b492b7e71f4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 59, "doc": {"id": "74ad13a03634e79c85382f1b90969b74", "question": "What is likely to satisfy someone's curiosity?", "question_concept": "curiosity", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hear news", "read book", "see favorite show", "comedy show", "go somewhere"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is likely to satisfy someone's curiosity?\nA. hear news\nB. read book\nC. see favorite show\nD. comedy show\nE. go somewhere\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is likely to satisfy someone's curiosity?\nA. hear news\nB. read book\nC. see favorite show\nD. comedy show\nE. go somewhere\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is likely to satisfy someone's curiosity?\nA. hear news\nB. read book\nC. see favorite show\nD. comedy show\nE. go somewhere\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is likely to satisfy someone's curiosity?\nA. hear news\nB. read book\nC. see favorite show\nD. comedy show\nE. go somewhere\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is likely to satisfy someone's curiosity?\nA. hear news\nB. read book\nC. see favorite show\nD. comedy show\nE. go somewhere\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.214139938354492", "False"]], [["-2.714139938354492", "False"]], [["-3.214139938354492", "False"]], [["-4.964139938354492", "False"]], [["-3.464139938354492", "False"]]], "filtered_resps": [["-3.214139938354492", "False"], ["-2.714139938354492", "False"], ["-3.214139938354492", "False"], ["-4.964139938354492", "False"], ["-3.464139938354492", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0243e9f703a1dd48bbe3130f8baddf33df6aad5e6ba0c6fba3e6382a8ffc71c3", "prompt_hash": "d65cda2a4e8a59ab4fdf178afcb5c936c5d5e0789eec0b734933fab9e06fbcca", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 60, "doc": {"id": "49e466b1782aa4837dae53ff891fcdee", "question": "If you are in a bar in a glove shaped state where are you?", "question_concept": "bar", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["in my pocket", "michigan", "new york city", "restaurant", "public house"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you are in a bar in a glove shaped state where are you?\nA. in my pocket\nB. michigan\nC. new york city\nD. restaurant\nE. public house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you are in a bar in a glove shaped state where are you?\nA. in my pocket\nB. michigan\nC. new york city\nD. restaurant\nE. public house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you are in a bar in a glove shaped state where are you?\nA. in my pocket\nB. michigan\nC. new york city\nD. restaurant\nE. public house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you are in a bar in a glove shaped state where are you?\nA. in my pocket\nB. michigan\nC. new york city\nD. restaurant\nE. public house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you are in a bar in a glove shaped state where are you?\nA. in my pocket\nB. michigan\nC. new york city\nD. restaurant\nE. public house\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.892668724060059", "False"]], [["-1.1426684856414795", "True"]], [["-5.642668724060059", "False"]], [["-6.892668724060059", "False"]], [["-5.392668724060059", "False"]]], "filtered_resps": [["-4.892668724060059", "False"], ["-1.1426684856414795", "True"], ["-5.642668724060059", "False"], ["-6.892668724060059", "False"], ["-5.392668724060059", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "960759cde16ea6924d8618e8b557ac8ffe4f13a1223e2bc5432ca9e79a374b12", "prompt_hash": "163e0ab860fbca8b4aecc60352805acadb852b83f8f476987fcef793ad7fd1a4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 61, "doc": {"id": "a8a8ae7792901c7179ff5538c701af1f", "question": "Where would a computer user be using their own computer?", "question_concept": "computer user", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hell", "school", "indoors", "internet cafe", "house"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would a computer user be using their own computer?\nA. hell\nB. school\nC. indoors\nD. internet cafe\nE. house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would a computer user be using their own computer?\nA. hell\nB. school\nC. indoors\nD. internet cafe\nE. house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would a computer user be using their own computer?\nA. hell\nB. school\nC. indoors\nD. internet cafe\nE. house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would a computer user be using their own computer?\nA. hell\nB. school\nC. indoors\nD. internet cafe\nE. house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would a computer user be using their own computer?\nA. hell\nB. school\nC. indoors\nD. internet cafe\nE. house\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6414210796356201", "False"]], [["-6.641421318054199", "False"]], [["-4.891421318054199", "False"]], [["-8.1414213180542", "False"]], [["-1.6414210796356201", "False"]]], "filtered_resps": [["-1.6414210796356201", "False"], ["-6.641421318054199", "False"], ["-4.891421318054199", "False"], ["-8.1414213180542", "False"], ["-1.6414210796356201", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "15cca33c29529ddeca6572f779070e2184204da113c718255c51d8ca83b8b4f1", "prompt_hash": "3d9cd1dc1f3bfb87eee3e6ff6fa822bc3a54ae6bf64f88a07af8d628af007f2e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 62, "doc": {"id": "2ffa3808ce26181926990b454e429c85", "question": "Crabs live in what sort of environment?", "question_concept": "crab", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["maritime", "bodies of water", "saltwater", "galapagos", "fish market"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Crabs live in what sort of environment?\nA. maritime\nB. bodies of water\nC. saltwater\nD. galapagos\nE. fish market\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Crabs live in what sort of environment?\nA. maritime\nB. bodies of water\nC. saltwater\nD. galapagos\nE. fish market\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Crabs live in what sort of environment?\nA. maritime\nB. bodies of water\nC. saltwater\nD. galapagos\nE. fish market\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Crabs live in what sort of environment?\nA. maritime\nB. bodies of water\nC. saltwater\nD. galapagos\nE. fish market\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Crabs live in what sort of environment?\nA. maritime\nB. bodies of water\nC. saltwater\nD. galapagos\nE. fish market\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7574759721755981", "False"]], [["-5.507475852966309", "False"]], [["-0.7574759721755981", "True"]], [["-6.007475852966309", "False"]], [["-7.757475852966309", "False"]]], "filtered_resps": [["-1.7574759721755981", "False"], ["-5.507475852966309", "False"], ["-0.7574759721755981", "True"], ["-6.007475852966309", "False"], ["-7.757475852966309", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e4c70a026e8589a207c8c8c849d78db06056821f5f2adc1de88745dd8369a850", "prompt_hash": "a815364976dae4d427e3dc18cc16205399edaaaf0d0bc71d900b3938320bdd27", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 63, "doc": {"id": "4319eaa36d256a92b72445c0392f9c94", "question": "Where can you find a snake in tall grass?", "question_concept": "snake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tree", "in a jar", "pet shops", "feild", "tropical forest"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you find a snake in tall grass?\nA. tree\nB. in a jar\nC. pet shops\nD. feild\nE. tropical forest\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you find a snake in tall grass?\nA. tree\nB. in a jar\nC. pet shops\nD. feild\nE. tropical forest\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you find a snake in tall grass?\nA. tree\nB. in a jar\nC. pet shops\nD. feild\nE. tropical forest\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you find a snake in tall grass?\nA. tree\nB. in a jar\nC. pet shops\nD. feild\nE. tropical forest\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you find a snake in tall grass?\nA. tree\nB. in a jar\nC. pet shops\nD. feild\nE. tropical forest\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.474658966064453", "False"]], [["-6.224658966064453", "False"]], [["-6.974658966064453", "False"]], [["-2.224658966064453", "False"]], [["-2.974658966064453", "False"]]], "filtered_resps": [["-3.474658966064453", "False"], ["-6.224658966064453", "False"], ["-6.974658966064453", "False"], ["-2.224658966064453", "False"], ["-2.974658966064453", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fd962405250764ea58692435c82affca49873933aae5768f75e5d2b65b1947a3", "prompt_hash": "7c56273a5693327f51028f4119e9d17416b66f368873aa32c259d8d57f19521b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 64, "doc": {"id": "ec79ef747bb89281923edb89ba26786d", "question": "What is a place that has a bench nestled in trees?", "question_concept": "bench", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["state park", "bus stop", "bus depot", "statue", "train station"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is a place that has a bench nestled in trees?\nA. state park\nB. bus stop\nC. bus depot\nD. statue\nE. train station\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a place that has a bench nestled in trees?\nA. state park\nB. bus stop\nC. bus depot\nD. statue\nE. train station\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a place that has a bench nestled in trees?\nA. state park\nB. bus stop\nC. bus depot\nD. statue\nE. train station\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a place that has a bench nestled in trees?\nA. state park\nB. bus stop\nC. bus depot\nD. statue\nE. train station\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a place that has a bench nestled in trees?\nA. state park\nB. bus stop\nC. bus depot\nD. statue\nE. train station\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6858230829238892", "True"]], [["-6.6858229637146", "False"]], [["-6.6858229637146", "False"]], [["-7.1858229637146", "False"]], [["-9.185823440551758", "False"]]], "filtered_resps": [["-0.6858230829238892", "True"], ["-6.6858229637146", "False"], ["-6.6858229637146", "False"], ["-7.1858229637146", "False"], ["-9.185823440551758", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f810a89b498d88587204b1cf190f88dd0e5feb607e0a9c82cb3b5f8b01eb3bd0", "prompt_hash": "f4c27e5926ed1d0ee1fb7891a9f3485af3f4f845526090603004040026e2043a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 65, "doc": {"id": "2d33cde5e3987adc8fa2bca0af4dd3dd", "question": "Where is a human likely to go as a result of being hungry?", "question_concept": "being hungry", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat in restaurant", "make bread", "have lunch", "cook dinner", "friends house"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a human likely to go as a result of being hungry?\nA. eat in restaurant\nB. make bread\nC. have lunch\nD. cook dinner\nE. friends house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a human likely to go as a result of being hungry?\nA. eat in restaurant\nB. make bread\nC. have lunch\nD. cook dinner\nE. friends house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a human likely to go as a result of being hungry?\nA. eat in restaurant\nB. make bread\nC. have lunch\nD. cook dinner\nE. friends house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a human likely to go as a result of being hungry?\nA. eat in restaurant\nB. make bread\nC. have lunch\nD. cook dinner\nE. friends house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a human likely to go as a result of being hungry?\nA. eat in restaurant\nB. make bread\nC. have lunch\nD. cook dinner\nE. friends house\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.93384850025177", "False"]], [["-6.1838483810424805", "False"]], [["-1.68384850025177", "True"]], [["-7.4338483810424805", "False"]], [["-6.6838483810424805", "False"]]], "filtered_resps": [["-1.93384850025177", "False"], ["-6.1838483810424805", "False"], ["-1.68384850025177", "True"], ["-7.4338483810424805", "False"], ["-6.6838483810424805", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b129657a883e3cc5d90894cb39846974ab22fa34a79c53ca8f22ba9af47f65ae", "prompt_hash": "c5df78c5ded1ca8355be41d642c7a5d08e50953ac4c30379980a36887c7f5680", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 66, "doc": {"id": "cc46d936bf69d69a3863b0cb85d75c17", "question": "He was beginning to regret taking the fight when he saw how what his opponent was?", "question_concept": "regret", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fun", "joy", "satisfaction", "confident", "pride"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He was beginning to regret taking the fight when he saw how what his opponent was?\nA. fun\nB. joy\nC. satisfaction\nD. confident\nE. pride\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was beginning to regret taking the fight when he saw how what his opponent was?\nA. fun\nB. joy\nC. satisfaction\nD. confident\nE. pride\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was beginning to regret taking the fight when he saw how what his opponent was?\nA. fun\nB. joy\nC. satisfaction\nD. confident\nE. pride\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was beginning to regret taking the fight when he saw how what his opponent was?\nA. fun\nB. joy\nC. satisfaction\nD. confident\nE. pride\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was beginning to regret taking the fight when he saw how what his opponent was?\nA. fun\nB. joy\nC. satisfaction\nD. confident\nE. pride\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.35650110244751", "False"]], [["-6.60650110244751", "False"]], [["-6.60650110244751", "False"]], [["-1.1065012216567993", "True"]], [["-4.85650110244751", "False"]]], "filtered_resps": [["-4.35650110244751", "False"], ["-6.60650110244751", "False"], ["-6.60650110244751", "False"], ["-1.1065012216567993", "True"], ["-4.85650110244751", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "521cb9e8a7c49e310e1c8f720177cbc36055266fdf33289689720fa13293b321", "prompt_hash": "f3e3c62f13234145aff297086380eccb3ccfa51dc8efe7c334ce72613a1985c1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 67, "doc": {"id": "46bc1a50eeead10509a43a048e01194e", "question": "Where would you find a single shower curtain being used?", "question_concept": "shower curtain", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bathtub", "washing area", "hotel", "shower stall", "department store"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find a single shower curtain being used?\nA. bathtub\nB. washing area\nC. hotel\nD. shower stall\nE. department store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find a single shower curtain being used?\nA. bathtub\nB. washing area\nC. hotel\nD. shower stall\nE. department store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find a single shower curtain being used?\nA. bathtub\nB. washing area\nC. hotel\nD. shower stall\nE. department store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find a single shower curtain being used?\nA. bathtub\nB. washing area\nC. hotel\nD. shower stall\nE. department store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find a single shower curtain being used?\nA. bathtub\nB. washing area\nC. hotel\nD. shower stall\nE. department store\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.12125301361084", "False"]], [["-5.87125301361084", "False"]], [["-4.12125301361084", "False"]], [["-1.3712530136108398", "False"]], [["-5.62125301361084", "False"]]], "filtered_resps": [["-2.12125301361084", "False"], ["-5.87125301361084", "False"], ["-4.12125301361084", "False"], ["-1.3712530136108398", "False"], ["-5.62125301361084", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "27bdd36b411f6aeb8a0d074dbfd353b6c0a992de3945fc8705a196e638b23d7e", "prompt_hash": "e181377c1d4953e53b5e2e8c64aefc5056fcc02b1dcdd01be3f1083d530cc705", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 68, "doc": {"id": "4336a8c55b7cb17275d1c60206cd2f18", "question": "Where is a good idea but not required to have a fire extinguisher?", "question_concept": "fire extinguisher", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["school bus", "boat", "house", "hospital", "school"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a good idea but not required to have a fire extinguisher?\nA. school bus\nB. boat\nC. house\nD. hospital\nE. school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a good idea but not required to have a fire extinguisher?\nA. school bus\nB. boat\nC. house\nD. hospital\nE. school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a good idea but not required to have a fire extinguisher?\nA. school bus\nB. boat\nC. house\nD. hospital\nE. school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a good idea but not required to have a fire extinguisher?\nA. school bus\nB. boat\nC. house\nD. hospital\nE. school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a good idea but not required to have a fire extinguisher?\nA. school bus\nB. boat\nC. house\nD. hospital\nE. school\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8851252794265747", "True"]], [["-2.135125160217285", "False"]], [["-3.135125160217285", "False"]], [["-4.635125160217285", "False"]], [["-2.635125160217285", "False"]]], "filtered_resps": [["-1.8851252794265747", "True"], ["-2.135125160217285", "False"], ["-3.135125160217285", "False"], ["-4.635125160217285", "False"], ["-2.635125160217285", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5415af7aa16b4f090d90958f73d74a7f9cbf3e1fc23f4e14cc303f62a14ded8b", "prompt_hash": "a4416d4bcac9b231ee07cbaa4f48e10fa007de766cb7f77e08b352914dcc3b8f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 69, "doc": {"id": "a287575d3ba4b9f958536fc14a1f5b5a", "question": "What continent has the most castles?", "question_concept": "castle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fairy tale", "edinburgh", "germany", "europe", "antarctica"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What continent has the most castles?\nA. fairy tale\nB. edinburgh\nC. germany\nD. europe\nE. antarctica\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What continent has the most castles?\nA. fairy tale\nB. edinburgh\nC. germany\nD. europe\nE. antarctica\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What continent has the most castles?\nA. fairy tale\nB. edinburgh\nC. germany\nD. europe\nE. antarctica\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What continent has the most castles?\nA. fairy tale\nB. edinburgh\nC. germany\nD. europe\nE. antarctica\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What continent has the most castles?\nA. fairy tale\nB. edinburgh\nC. germany\nD. europe\nE. antarctica\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.171584606170654", "False"]], [["-7.171584606170654", "False"]], [["-5.171584606170654", "False"]], [["-1.9215847253799438", "False"]], [["-9.671585083007812", "False"]]], "filtered_resps": [["-6.171584606170654", "False"], ["-7.171584606170654", "False"], ["-5.171584606170654", "False"], ["-1.9215847253799438", "False"], ["-9.671585083007812", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "59bf1be8896037f3c392e3817bd654b7a21978e00626d17f93646aa25f4a2d19", "prompt_hash": "4c636e89573e68ef5d815c52b60652c87841720a1952e6c9f22efd800cb67065", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 70, "doc": {"id": "f481dc35b0a97a20dc5cdfe1a59746e2", "question": "If you have to read a book that is very dry and long you may become what?", "question_concept": "read book", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have time", "boring", "learn new", "enjoyable", "bored"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If you have to read a book that is very dry and long you may become what?\nA. have time\nB. boring\nC. learn new\nD. enjoyable\nE. bored\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you have to read a book that is very dry and long you may become what?\nA. have time\nB. boring\nC. learn new\nD. enjoyable\nE. bored\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you have to read a book that is very dry and long you may become what?\nA. have time\nB. boring\nC. learn new\nD. enjoyable\nE. bored\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you have to read a book that is very dry and long you may become what?\nA. have time\nB. boring\nC. learn new\nD. enjoyable\nE. bored\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you have to read a book that is very dry and long you may become what?\nA. have time\nB. boring\nC. learn new\nD. enjoyable\nE. bored\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.473723411560059", "False"]], [["-2.7237231731414795", "False"]], [["-8.723723411560059", "False"]], [["-7.223723411560059", "False"]], [["-1.2237231731414795", "True"]]], "filtered_resps": [["-5.473723411560059", "False"], ["-2.7237231731414795", "False"], ["-8.723723411560059", "False"], ["-7.223723411560059", "False"], ["-1.2237231731414795", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d55e009bd0f34e1c46c1d86b238a7951f862682297c2a08fe372e3edf29bb4ca", "prompt_hash": "ccbdc7426c4a1cfb0b86ba06d7662540cdf9551a92f3203ea68e787746312d8e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 71, "doc": {"id": "c1c7a9efa379b8a7024a71cf364a144c", "question": "Sally used a clipboard to hold her papers while she read off names at the beginning of the day.  Where might she work?", "question_concept": "clipboard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desk", "windows 95", "office supply store", "see work", "school"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Sally used a clipboard to hold her papers while she read off names at the beginning of the day.  Where might she work?\nA. desk\nB. windows 95\nC. office supply store\nD. see work\nE. school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sally used a clipboard to hold her papers while she read off names at the beginning of the day.  Where might she work?\nA. desk\nB. windows 95\nC. office supply store\nD. see work\nE. school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sally used a clipboard to hold her papers while she read off names at the beginning of the day.  Where might she work?\nA. desk\nB. windows 95\nC. office supply store\nD. see work\nE. school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sally used a clipboard to hold her papers while she read off names at the beginning of the day.  Where might she work?\nA. desk\nB. windows 95\nC. office supply store\nD. see work\nE. school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sally used a clipboard to hold her papers while she read off names at the beginning of the day.  Where might she work?\nA. desk\nB. windows 95\nC. office supply store\nD. see work\nE. school\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.407854080200195", "False"]], [["-5.907854080200195", "False"]], [["-7.907854080200195", "False"]], [["-8.657854080200195", "False"]], [["-1.4078543186187744", "True"]]], "filtered_resps": [["-4.407854080200195", "False"], ["-5.907854080200195", "False"], ["-7.907854080200195", "False"], ["-8.657854080200195", "False"], ["-1.4078543186187744", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "016e5a8456577667cd95359c0f9ad0a20ae7eadce58fc0f46e3e797b87e2fa57", "prompt_hash": "6aabaa14ac557be157eeeb808d82e62bfda5d42f167bc51031fe7dac09d320f4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 72, "doc": {"id": "821b32d39f57396979069b948030afe9", "question": "The kids didn't clean up after they had done what?", "question_concept": "kids", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["learn things", "play games", "disneyland", "play with toys", "talking"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The kids didn't clean up after they had done what?\nA. learn things\nB. play games\nC. disneyland\nD. play with toys\nE. talking\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The kids didn't clean up after they had done what?\nA. learn things\nB. play games\nC. disneyland\nD. play with toys\nE. talking\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The kids didn't clean up after they had done what?\nA. learn things\nB. play games\nC. disneyland\nD. play with toys\nE. talking\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The kids didn't clean up after they had done what?\nA. learn things\nB. play games\nC. disneyland\nD. play with toys\nE. talking\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The kids didn't clean up after they had done what?\nA. learn things\nB. play games\nC. disneyland\nD. play with toys\nE. talking\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.752231597900391", "False"]], [["-2.5022318363189697", "False"]], [["-6.002231597900391", "False"]], [["-1.5022318363189697", "True"]], [["-8.50223159790039", "False"]]], "filtered_resps": [["-4.752231597900391", "False"], ["-2.5022318363189697", "False"], ["-6.002231597900391", "False"], ["-1.5022318363189697", "True"], ["-8.50223159790039", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "90a3769ebebeefee94a901a677e136bd7c4473ee994a6070977ef9a3f6ee9df0", "prompt_hash": "2fbcf508b1416b571ebca37e6f7dea7a32d318d624e6ff5d50a23ef43396b2b0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 73, "doc": {"id": "c68b4082a6872cf8198502651d0f3352", "question": "Despite the name a pawn can be quite versatile, all the parts are important in a what?", "question_concept": "pawn", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["chess game", "scheme", "chess set", "checkers", "north carolina"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Despite the name a pawn can be quite versatile, all the parts are important in a what?\nA. chess game\nB. scheme\nC. chess set\nD. checkers\nE. north carolina\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Despite the name a pawn can be quite versatile, all the parts are important in a what?\nA. chess game\nB. scheme\nC. chess set\nD. checkers\nE. north carolina\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Despite the name a pawn can be quite versatile, all the parts are important in a what?\nA. chess game\nB. scheme\nC. chess set\nD. checkers\nE. north carolina\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Despite the name a pawn can be quite versatile, all the parts are important in a what?\nA. chess game\nB. scheme\nC. chess set\nD. checkers\nE. north carolina\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Despite the name a pawn can be quite versatile, all the parts are important in a what?\nA. chess game\nB. scheme\nC. chess set\nD. checkers\nE. north carolina\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9278382062911987", "True"]], [["-6.177838325500488", "False"]], [["-4.427838325500488", "False"]], [["-7.70967435836792", "False"]], [["-10.177838325500488", "False"]]], "filtered_resps": [["-0.9278382062911987", "True"], ["-6.177838325500488", "False"], ["-4.427838325500488", "False"], ["-7.70967435836792", "False"], ["-10.177838325500488", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2f1383b893375eaf18edd8a3bee8fb217558b877d595211985f1c48b0b078bc4", "prompt_hash": "f595bf68a30ba737f8c102331e40ae1c78294f391492bb747eae22107cbe7ebe", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 74, "doc": {"id": "dd11fea36d89aa09f9a6069545ba4c9c", "question": "What would not be true about a basketball if it had a hole in it but it did not lose its general shape?", "question_concept": "basketball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["punctured", "popular in america", "full of air", "gone", "round"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What would not be true about a basketball if it had a hole in it but it did not lose its general shape?\nA. punctured\nB. popular in america\nC. full of air\nD. gone\nE. round\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would not be true about a basketball if it had a hole in it but it did not lose its general shape?\nA. punctured\nB. popular in america\nC. full of air\nD. gone\nE. round\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would not be true about a basketball if it had a hole in it but it did not lose its general shape?\nA. punctured\nB. popular in america\nC. full of air\nD. gone\nE. round\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would not be true about a basketball if it had a hole in it but it did not lose its general shape?\nA. punctured\nB. popular in america\nC. full of air\nD. gone\nE. round\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would not be true about a basketball if it had a hole in it but it did not lose its general shape?\nA. punctured\nB. popular in america\nC. full of air\nD. gone\nE. round\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4993791580200195", "True"]], [["-3.9993791580200195", "False"]], [["-3.7493791580200195", "False"]], [["-2.2493791580200195", "False"]], [["-3.9993791580200195", "False"]]], "filtered_resps": [["-1.4993791580200195", "True"], ["-3.9993791580200195", "False"], ["-3.7493791580200195", "False"], ["-2.2493791580200195", "False"], ["-3.9993791580200195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "57aae5eecf484863c06235c0f9660124f68e40e380c25c32e6eb59196bbe33e7", "prompt_hash": "38de517386a1b61bd8c9b84229c4d0a2a73e0b8bdcbfdc20f5082c7bbb9fe452", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 75, "doc": {"id": "7792b2c6518ecf9775efba6d41253312", "question": "If you are awaking multiple times throughout the night because a lot is on your mind, what is a likely cause?", "question_concept": "awaking", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["irritability", "depression", "getting out of bed", "happiness", "discomfort"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you are awaking multiple times throughout the night because a lot is on your mind, what is a likely cause?\nA. irritability\nB. depression\nC. getting out of bed\nD. happiness\nE. discomfort\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you are awaking multiple times throughout the night because a lot is on your mind, what is a likely cause?\nA. irritability\nB. depression\nC. getting out of bed\nD. happiness\nE. discomfort\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you are awaking multiple times throughout the night because a lot is on your mind, what is a likely cause?\nA. irritability\nB. depression\nC. getting out of bed\nD. happiness\nE. discomfort\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you are awaking multiple times throughout the night because a lot is on your mind, what is a likely cause?\nA. irritability\nB. depression\nC. getting out of bed\nD. happiness\nE. discomfort\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you are awaking multiple times throughout the night because a lot is on your mind, what is a likely cause?\nA. irritability\nB. depression\nC. getting out of bed\nD. happiness\nE. discomfort\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.060417890548706", "False"]], [["-1.310417890548706", "True"]], [["-5.810418128967285", "False"]], [["-5.310418128967285", "False"]], [["-4.060418128967285", "False"]]], "filtered_resps": [["-2.060417890548706", "False"], ["-1.310417890548706", "True"], ["-5.810418128967285", "False"], ["-5.310418128967285", "False"], ["-4.060418128967285", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "892a5051e2d26ade7856319a9db095c801d28c10eb7b6b3e0c2b1636cf84b90f", "prompt_hash": "21925b2375836eb3db77aed42cfdec5b66f5716150d5422cada3b645b4d279e0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 76, "doc": {"id": "1feb4c2a0e8ed638259f5d27b16eae9a", "question": "Where does a wild bird usually live?", "question_concept": "bird", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cage", "sky", "countryside", "desert", "windowsill"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where does a wild bird usually live?\nA. cage\nB. sky\nC. countryside\nD. desert\nE. windowsill\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where does a wild bird usually live?\nA. cage\nB. sky\nC. countryside\nD. desert\nE. windowsill\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where does a wild bird usually live?\nA. cage\nB. sky\nC. countryside\nD. desert\nE. windowsill\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where does a wild bird usually live?\nA. cage\nB. sky\nC. countryside\nD. desert\nE. windowsill\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where does a wild bird usually live?\nA. cage\nB. sky\nC. countryside\nD. desert\nE. windowsill\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.170004367828369", "False"]], [["-2.170004367828369", "False"]], [["-1.6700044870376587", "True"]], [["-6.920004367828369", "False"]], [["-8.920004844665527", "False"]]], "filtered_resps": [["-3.170004367828369", "False"], ["-2.170004367828369", "False"], ["-1.6700044870376587", "True"], ["-6.920004367828369", "False"], ["-8.920004844665527", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6ccdfa0171faa614d34a7e2334af44b9badb2576ffb71a8ffc45e6c7da3064b3", "prompt_hash": "ae34ab29c1d997af91a3233e185192c3723acf5b695abb2ef8f64387a89e87aa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 77, "doc": {"id": "2de08c7a518b7c226e19bdc8fc10ef1d", "question": "Where would you expect to find white mice?", "question_concept": "mice", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bell cat", "bush", "attic", "countryside", "laboratory"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you expect to find white mice?\nA. bell cat\nB. bush\nC. attic\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you expect to find white mice?\nA. bell cat\nB. bush\nC. attic\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you expect to find white mice?\nA. bell cat\nB. bush\nC. attic\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you expect to find white mice?\nA. bell cat\nB. bush\nC. attic\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you expect to find white mice?\nA. bell cat\nB. bush\nC. attic\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.404669761657715", "False"]], [["-7.654669761657715", "False"]], [["-6.654669761657715", "False"]], [["-7.404669761657715", "False"]], [["-1.4046698808670044", "False"]]], "filtered_resps": [["-6.404669761657715", "False"], ["-7.654669761657715", "False"], ["-6.654669761657715", "False"], ["-7.404669761657715", "False"], ["-1.4046698808670044", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "80c0f70b4522db8979fdfdfc8148b85c4724c91e912ef2bcb420511823079bf9", "prompt_hash": "684d869524f47caca028f94b9be2393b159abb726868ff707791d71a3532bb51", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 78, "doc": {"id": "ea8664e77205224154f8519f922220e1", "question": "John felt that his actions were fate.   Harry said that he could have always made a different what?", "question_concept": "fate", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["free will", "choice", "will", "alcohol", "freedom"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John felt that his actions were fate.   Harry said that he could have always made a different what?\nA. free will\nB. choice\nC. will\nD. alcohol\nE. freedom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John felt that his actions were fate.   Harry said that he could have always made a different what?\nA. free will\nB. choice\nC. will\nD. alcohol\nE. freedom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John felt that his actions were fate.   Harry said that he could have always made a different what?\nA. free will\nB. choice\nC. will\nD. alcohol\nE. freedom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John felt that his actions were fate.   Harry said that he could have always made a different what?\nA. free will\nB. choice\nC. will\nD. alcohol\nE. freedom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John felt that his actions were fate.   Harry said that he could have always made a different what?\nA. free will\nB. choice\nC. will\nD. alcohol\nE. freedom\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.564918041229248", "False"]], [["-1.3149179220199585", "True"]], [["-6.814918041229248", "False"]], [["-8.81491756439209", "False"]], [["-9.56491756439209", "False"]]], "filtered_resps": [["-3.564918041229248", "False"], ["-1.3149179220199585", "True"], ["-6.814918041229248", "False"], ["-8.81491756439209", "False"], ["-9.56491756439209", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9f32d0ff88ed5ab5de1d0d6a63c301cb07755d891fef1a3fbc479bc4a5c4a265", "prompt_hash": "d7a3a1ae293382ef51c9e00c3ef0363893791a662993792cadbca3b4f332e35f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 79, "doc": {"id": "a64d45cecde84fdcf5f0a79805a0c6fe", "question": "What could committing murder prevent someone from doing?", "question_concept": "committing murder", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["go to jail", "cry", "find god", "guilty conscience", "problems"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What could committing murder prevent someone from doing?\nA. go to jail\nB. cry\nC. find god\nD. guilty conscience\nE. problems\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could committing murder prevent someone from doing?\nA. go to jail\nB. cry\nC. find god\nD. guilty conscience\nE. problems\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could committing murder prevent someone from doing?\nA. go to jail\nB. cry\nC. find god\nD. guilty conscience\nE. problems\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could committing murder prevent someone from doing?\nA. go to jail\nB. cry\nC. find god\nD. guilty conscience\nE. problems\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could committing murder prevent someone from doing?\nA. go to jail\nB. cry\nC. find god\nD. guilty conscience\nE. problems\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1292028427124023", "False"]], [["-4.129202842712402", "False"]], [["-3.3792028427124023", "False"]], [["-2.6292028427124023", "False"]], [["-6.129202842712402", "False"]]], "filtered_resps": [["-2.1292028427124023", "False"], ["-4.129202842712402", "False"], ["-3.3792028427124023", "False"], ["-2.6292028427124023", "False"], ["-6.129202842712402", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5d140297a918c72fee89ab0298fda89b645c3d060e394b0f8035a4f2aa58e20d", "prompt_hash": "3a179f898bcb76e43930f1bfccff94c9a600c946e2cf0ce92a727656f2aa39a7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 80, "doc": {"id": "60e92cd2f35c345872d1a898e1718d55", "question": "George didn't have a car, but he still had his two feet.   His socks were smelly and his soles were blistered, but that didn't matter.  He could still do what?", "question_concept": "feet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["michigan", "walk", "stay still", "stink", "hands"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: George didn't have a car, but he still had his two feet.   His socks were smelly and his soles were blistered, but that didn't matter.  He could still do what?\nA. michigan\nB. walk\nC. stay still\nD. stink\nE. hands\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: George didn't have a car, but he still had his two feet.   His socks were smelly and his soles were blistered, but that didn't matter.  He could still do what?\nA. michigan\nB. walk\nC. stay still\nD. stink\nE. hands\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: George didn't have a car, but he still had his two feet.   His socks were smelly and his soles were blistered, but that didn't matter.  He could still do what?\nA. michigan\nB. walk\nC. stay still\nD. stink\nE. hands\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: George didn't have a car, but he still had his two feet.   His socks were smelly and his soles were blistered, but that didn't matter.  He could still do what?\nA. michigan\nB. walk\nC. stay still\nD. stink\nE. hands\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: George didn't have a car, but he still had his two feet.   His socks were smelly and his soles were blistered, but that didn't matter.  He could still do what?\nA. michigan\nB. walk\nC. stay still\nD. stink\nE. hands\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.303286075592041", "False"]], [["-0.8032859563827515", "True"]], [["-8.803285598754883", "False"]], [["-9.053285598754883", "False"]], [["-9.553285598754883", "False"]]], "filtered_resps": [["-6.303286075592041", "False"], ["-0.8032859563827515", "True"], ["-8.803285598754883", "False"], ["-9.053285598754883", "False"], ["-9.553285598754883", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f696e6ee729fc6f792362cc5cb080a9de9492171077bc9d1b22efbe856cf0390", "prompt_hash": "151bd30f0d9bcc0ec0a53619872895a824b75b41619ab46289cec68a0ad61e41", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 81, "doc": {"id": "08f3c187908646997b9080c7e9ea7da4", "question": "A crane uses many a steel cable when working a what?", "question_concept": "steel cable", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["abaft", "ship", "winch", "construction site", "building"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: A crane uses many a steel cable when working a what?\nA. abaft\nB. ship\nC. winch\nD. construction site\nE. building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A crane uses many a steel cable when working a what?\nA. abaft\nB. ship\nC. winch\nD. construction site\nE. building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A crane uses many a steel cable when working a what?\nA. abaft\nB. ship\nC. winch\nD. construction site\nE. building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A crane uses many a steel cable when working a what?\nA. abaft\nB. ship\nC. winch\nD. construction site\nE. building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A crane uses many a steel cable when working a what?\nA. abaft\nB. ship\nC. winch\nD. construction site\nE. building\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1362619400024414", "True"]], [["-3.6362619400024414", "False"]], [["-1.8862619400024414", "False"]], [["-2.8862619400024414", "False"]], [["-2.3862619400024414", "False"]]], "filtered_resps": [["-1.1362619400024414", "True"], ["-3.6362619400024414", "False"], ["-1.8862619400024414", "False"], ["-2.8862619400024414", "False"], ["-2.3862619400024414", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d25e85f7c8b9237cfaef362b719f11d6c99f778f480cf425899a6d807bf7ef53", "prompt_hash": "8002c3072d14a0be8a8d5b7c2e4fd6a405483fdda57e2a21e019a6c66d870753", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 82, "doc": {"id": "9aff72f0c480c2b4edde45bd2e7e4870", "question": "What is the main purpose of farmers?", "question_concept": "farmers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["raise cattle", "grow corn", "farm land", "drive tractors", "supply food"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is the main purpose of farmers?\nA. raise cattle\nB. grow corn\nC. farm land\nD. drive tractors\nE. supply food\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the main purpose of farmers?\nA. raise cattle\nB. grow corn\nC. farm land\nD. drive tractors\nE. supply food\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the main purpose of farmers?\nA. raise cattle\nB. grow corn\nC. farm land\nD. drive tractors\nE. supply food\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the main purpose of farmers?\nA. raise cattle\nB. grow corn\nC. farm land\nD. drive tractors\nE. supply food\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the main purpose of farmers?\nA. raise cattle\nB. grow corn\nC. farm land\nD. drive tractors\nE. supply food\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.697276592254639", "False"]], [["-5.697276592254639", "False"]], [["-3.9472765922546387", "False"]], [["-7.447276592254639", "False"]], [["-1.4472767114639282", "True"]]], "filtered_resps": [["-5.697276592254639", "False"], ["-5.697276592254639", "False"], ["-3.9472765922546387", "False"], ["-7.447276592254639", "False"], ["-1.4472767114639282", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "01c7fab0786dda505edf7d093a086f067e75d758b9829384584f63b75440768a", "prompt_hash": "4fe5515121379844a0944d4fe35b1a047cf64eae61137047f42f772c8b1a85af", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 83, "doc": {"id": "fd243c96edec5b1b8520d5bfeddc6622", "question": "Where can I put this penny to save for later?", "question_concept": "penny", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["piggy bank", "wallet", "toy", "ground", "pocket"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where can I put this penny to save for later?\nA. piggy bank\nB. wallet\nC. toy\nD. ground\nE. pocket\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can I put this penny to save for later?\nA. piggy bank\nB. wallet\nC. toy\nD. ground\nE. pocket\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can I put this penny to save for later?\nA. piggy bank\nB. wallet\nC. toy\nD. ground\nE. pocket\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can I put this penny to save for later?\nA. piggy bank\nB. wallet\nC. toy\nD. ground\nE. pocket\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can I put this penny to save for later?\nA. piggy bank\nB. wallet\nC. toy\nD. ground\nE. pocket\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2375500202178955", "False"]], [["-7.237550258636475", "False"]], [["-9.237549781799316", "False"]], [["-8.487549781799316", "False"]], [["-8.737549781799316", "False"]]], "filtered_resps": [["-1.2375500202178955", "False"], ["-7.237550258636475", "False"], ["-9.237549781799316", "False"], ["-8.487549781799316", "False"], ["-8.737549781799316", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d275905dc9a265402bb5f852523ffd9b2e2eb0190cd8ad56c5ac2698553c5078", "prompt_hash": "5447f2d13d5ee2ac712b64358ed4e40426cded4ba0666dc3b61a4eb9a956f4ae", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 84, "doc": {"id": "f5ec4fdfd0e37e733bfc1606b986f1e2", "question": "Where would you put uncooked crab meat?", "question_concept": "crab", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wharf", "red lobster", "tidepools", "boss's office", "stew pot"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put uncooked crab meat?\nA. wharf\nB. red lobster\nC. tidepools\nD. boss's office\nE. stew pot\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put uncooked crab meat?\nA. wharf\nB. red lobster\nC. tidepools\nD. boss's office\nE. stew pot\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put uncooked crab meat?\nA. wharf\nB. red lobster\nC. tidepools\nD. boss's office\nE. stew pot\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put uncooked crab meat?\nA. wharf\nB. red lobster\nC. tidepools\nD. boss's office\nE. stew pot\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put uncooked crab meat?\nA. wharf\nB. red lobster\nC. tidepools\nD. boss's office\nE. stew pot\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.748263359069824", "False"]], [["-3.998263359069824", "False"]], [["-4.998263359069824", "False"]], [["-7.748263359069824", "False"]], [["-0.7482634782791138", "True"]]], "filtered_resps": [["-4.748263359069824", "False"], ["-3.998263359069824", "False"], ["-4.998263359069824", "False"], ["-7.748263359069824", "False"], ["-0.7482634782791138", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c31e703a4a7f35715e60f2ddc59830c0337f27416f5f0babf2f9b1353be5dba3", "prompt_hash": "71fcc82a51d3e460fb39c4fd46e3913e121865045949d16ffbc7c2fa4dc16f1e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 85, "doc": {"id": "e3c6d147f8a727d314046e70e9579ba0", "question": "The man had a fear of illness, so he never visited friends who were a what?", "question_concept": "illness", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sick person", "hospital", "elderly person", "graveyard", "doctor's office"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The man had a fear of illness, so he never visited friends who were a what?\nA. sick person\nB. hospital\nC. elderly person\nD. graveyard\nE. doctor's office\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man had a fear of illness, so he never visited friends who were a what?\nA. sick person\nB. hospital\nC. elderly person\nD. graveyard\nE. doctor's office\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man had a fear of illness, so he never visited friends who were a what?\nA. sick person\nB. hospital\nC. elderly person\nD. graveyard\nE. doctor's office\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man had a fear of illness, so he never visited friends who were a what?\nA. sick person\nB. hospital\nC. elderly person\nD. graveyard\nE. doctor's office\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man had a fear of illness, so he never visited friends who were a what?\nA. sick person\nB. hospital\nC. elderly person\nD. graveyard\nE. doctor's office\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4965774416923523", "True"]], [["-5.996577262878418", "False"]], [["-7.496577262878418", "False"]], [["-6.996577262878418", "False"]], [["-5.746577262878418", "False"]]], "filtered_resps": [["-0.4965774416923523", "True"], ["-5.996577262878418", "False"], ["-7.496577262878418", "False"], ["-6.996577262878418", "False"], ["-5.746577262878418", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "37f983af67850631caea88d73bddc07e8f7b08c0692840d985b315a0b170541a", "prompt_hash": "5b0627aa3011365fce9f8f894d5a0f4ad1d3be85ec1ac9f35cce2082fad73214", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 86, "doc": {"id": "8ce13c6e08bf38d4cd4af756b661e47c", "question": "Where would you put pans if you want to bring them with you?", "question_concept": "pans", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cooking", "cook food", "kitchen", "backpack", "drawer"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put pans if you want to bring them with you?\nA. cooking\nB. cook food\nC. kitchen\nD. backpack\nE. drawer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put pans if you want to bring them with you?\nA. cooking\nB. cook food\nC. kitchen\nD. backpack\nE. drawer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put pans if you want to bring them with you?\nA. cooking\nB. cook food\nC. kitchen\nD. backpack\nE. drawer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put pans if you want to bring them with you?\nA. cooking\nB. cook food\nC. kitchen\nD. backpack\nE. drawer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put pans if you want to bring them with you?\nA. cooking\nB. cook food\nC. kitchen\nD. backpack\nE. drawer\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.649293899536133", "False"]], [["-5.399293899536133", "False"]], [["-7.149293899536133", "False"]], [["-0.8992939591407776", "True"]], [["-8.899293899536133", "False"]]], "filtered_resps": [["-3.649293899536133", "False"], ["-5.399293899536133", "False"], ["-7.149293899536133", "False"], ["-0.8992939591407776", "True"], ["-8.899293899536133", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eec85e269415ce2b18aa779ffca047e2db287b846631fee4d7057925f39c3c32", "prompt_hash": "3993d8f075d539da3cd7618e1ba1ca22857ddfedb1edbbd3bc7cf51afc3ba4ef", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 87, "doc": {"id": "0f4159e80f8dbf682819215bbf0f5b5a_1", "question": "If you're remembering something, it's because of your what of it to begin with?", "question_concept": "remembering", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["knowledge", "knowing", "forgetful", "pleasure", "depression"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you're remembering something, it's because of your what of it to begin with?\nA. knowledge\nB. knowing\nC. forgetful\nD. pleasure\nE. depression\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're remembering something, it's because of your what of it to begin with?\nA. knowledge\nB. knowing\nC. forgetful\nD. pleasure\nE. depression\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're remembering something, it's because of your what of it to begin with?\nA. knowledge\nB. knowing\nC. forgetful\nD. pleasure\nE. depression\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're remembering something, it's because of your what of it to begin with?\nA. knowledge\nB. knowing\nC. forgetful\nD. pleasure\nE. depression\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're remembering something, it's because of your what of it to begin with?\nA. knowledge\nB. knowing\nC. forgetful\nD. pleasure\nE. depression\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2499631643295288", "True"]], [["-2.4999632835388184", "False"]], [["-8.24996280670166", "False"]], [["-7.999963283538818", "False"]], [["-8.74996280670166", "False"]]], "filtered_resps": [["-1.2499631643295288", "True"], ["-2.4999632835388184", "False"], ["-8.24996280670166", "False"], ["-7.999963283538818", "False"], ["-8.74996280670166", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6c3d36ffb08d9d72f5b64ede5474868a0960a2289a08bd73d8403fea9f778c6a", "prompt_hash": "8acd20fe10a1c7674498b60be6f591dfb822823d6495d0eba73139d886155d7c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 88, "doc": {"id": "1a8b3c2a46efabcbd506f9cf70886ed0", "question": "Which large land mass is home to the most monkeys?", "question_concept": "monkey", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["amazon basin", "friend's house", "lift number 3", "research laboratory", "african continent"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Which large land mass is home to the most monkeys?\nA. amazon basin\nB. friend's house\nC. lift number 3\nD. research laboratory\nE. african continent\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Which large land mass is home to the most monkeys?\nA. amazon basin\nB. friend's house\nC. lift number 3\nD. research laboratory\nE. african continent\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Which large land mass is home to the most monkeys?\nA. amazon basin\nB. friend's house\nC. lift number 3\nD. research laboratory\nE. african continent\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Which large land mass is home to the most monkeys?\nA. amazon basin\nB. friend's house\nC. lift number 3\nD. research laboratory\nE. african continent\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Which large land mass is home to the most monkeys?\nA. amazon basin\nB. friend's house\nC. lift number 3\nD. research laboratory\nE. african continent\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5563864707946777", "True"]], [["-6.806386470794678", "False"]], [["-7.806386470794678", "False"]], [["-8.806386947631836", "False"]], [["-1.8063864707946777", "False"]]], "filtered_resps": [["-1.5563864707946777", "True"], ["-6.806386470794678", "False"], ["-7.806386470794678", "False"], ["-8.806386947631836", "False"], ["-1.8063864707946777", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1081b1adde558cc293e0ad5dfac3fafdea22a6866705bccd56025e5f2b57a8a9", "prompt_hash": "d4e19cd585f829dafb2497ab963ea29c283d0b33bd789cb40924141c80a4e9d8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 89, "doc": {"id": "db0cfd52ca6b2bbfcf26d1a898fd929b", "question": "Friday was James's 5th Anniversary.  They planned on going to bed early so that they could spend a long time doing what?", "question_concept": "going to bed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rest", "insomnia", "making love", "sleeping in", "texting"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Friday was James's 5th Anniversary.  They planned on going to bed early so that they could spend a long time doing what?\nA. rest\nB. insomnia\nC. making love\nD. sleeping in\nE. texting\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Friday was James's 5th Anniversary.  They planned on going to bed early so that they could spend a long time doing what?\nA. rest\nB. insomnia\nC. making love\nD. sleeping in\nE. texting\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Friday was James's 5th Anniversary.  They planned on going to bed early so that they could spend a long time doing what?\nA. rest\nB. insomnia\nC. making love\nD. sleeping in\nE. texting\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Friday was James's 5th Anniversary.  They planned on going to bed early so that they could spend a long time doing what?\nA. rest\nB. insomnia\nC. making love\nD. sleeping in\nE. texting\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Friday was James's 5th Anniversary.  They planned on going to bed early so that they could spend a long time doing what?\nA. rest\nB. insomnia\nC. making love\nD. sleeping in\nE. texting\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.997941017150879", "False"]], [["-5.247941017150879", "False"]], [["-1.997941017150879", "False"]], [["-5.997941017150879", "False"]], [["-7.997941017150879", "False"]]], "filtered_resps": [["-3.997941017150879", "False"], ["-5.247941017150879", "False"], ["-1.997941017150879", "False"], ["-5.997941017150879", "False"], ["-7.997941017150879", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c1a6e12ad98462dc4066d8d6f9d435c5c1c24360b6cb19532148bbe358f38ceb", "prompt_hash": "1b6d89351b0eb68e3d1827ddbca5d66218c33b0c3a9fc8a9ff2a269261e42e32", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 90, "doc": {"id": "400fb2e196e71abb70e5b3f9aab4b9ee", "question": "The teens were trying to hide that they get drink, but when they walked in the door their what gave it away?", "question_concept": "get drunk", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["health", "fall down", "stagger", "get arrested", "vomit"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The teens were trying to hide that they get drink, but when they walked in the door their what gave it away?\nA. health\nB. fall down\nC. stagger\nD. get arrested\nE. vomit\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The teens were trying to hide that they get drink, but when they walked in the door their what gave it away?\nA. health\nB. fall down\nC. stagger\nD. get arrested\nE. vomit\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The teens were trying to hide that they get drink, but when they walked in the door their what gave it away?\nA. health\nB. fall down\nC. stagger\nD. get arrested\nE. vomit\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The teens were trying to hide that they get drink, but when they walked in the door their what gave it away?\nA. health\nB. fall down\nC. stagger\nD. get arrested\nE. vomit\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The teens were trying to hide that they get drink, but when they walked in the door their what gave it away?\nA. health\nB. fall down\nC. stagger\nD. get arrested\nE. vomit\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.382358074188232", "False"]], [["-5.882358074188232", "False"]], [["-1.132358193397522", "True"]], [["-7.132358074188232", "False"]], [["-4.882358074188232", "False"]]], "filtered_resps": [["-5.382358074188232", "False"], ["-5.882358074188232", "False"], ["-1.132358193397522", "True"], ["-7.132358074188232", "False"], ["-4.882358074188232", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "27ce458c31c5e8eac24eb533ae20d4b978bd4d9f2a494b6cf9f898b76276e826", "prompt_hash": "fe8def031e0617f8762d6d7a0d62a0920c1642ad382840e5f7fc8ee0403c9e86", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 91, "doc": {"id": "3fb36127a61903029a363911a1d2b1e9_1", "question": "You'll find a landing at the top of what?", "question_concept": "landing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ocean", "apartment building", "stairwell", "airport", "room"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: You'll find a landing at the top of what?\nA. ocean\nB. apartment building\nC. stairwell\nD. airport\nE. room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: You'll find a landing at the top of what?\nA. ocean\nB. apartment building\nC. stairwell\nD. airport\nE. room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: You'll find a landing at the top of what?\nA. ocean\nB. apartment building\nC. stairwell\nD. airport\nE. room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: You'll find a landing at the top of what?\nA. ocean\nB. apartment building\nC. stairwell\nD. airport\nE. room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: You'll find a landing at the top of what?\nA. ocean\nB. apartment building\nC. stairwell\nD. airport\nE. room\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.330603837966919", "False"]], [["-3.580603837966919", "False"]], [["-3.830603837966919", "False"]], [["-1.080603837966919", "True"]], [["-7.33060359954834", "False"]]], "filtered_resps": [["-3.330603837966919", "False"], ["-3.580603837966919", "False"], ["-3.830603837966919", "False"], ["-1.080603837966919", "True"], ["-7.33060359954834", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "57e88e9fbf7d67d43f24ecd9b50d6c6fe38e1ea68accf83157163ba6ecbba989", "prompt_hash": "4faa5a655d8b4609c4c507f13628c449675dd915b7fe1f16f46863aae1868589", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 92, "doc": {"id": "8494b0b95533dcedbd76ae2916c481d4", "question": "Anybody could be hired in the kitchen, what was needed of them?", "question_concept": "anybody", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["forget", "oil squeaky hinge", "question authority", "wash dishes", "oik squeaky hinge"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Anybody could be hired in the kitchen, what was needed of them?\nA. forget\nB. oil squeaky hinge\nC. question authority\nD. wash dishes\nE. oik squeaky hinge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Anybody could be hired in the kitchen, what was needed of them?\nA. forget\nB. oil squeaky hinge\nC. question authority\nD. wash dishes\nE. oik squeaky hinge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Anybody could be hired in the kitchen, what was needed of them?\nA. forget\nB. oil squeaky hinge\nC. question authority\nD. wash dishes\nE. oik squeaky hinge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Anybody could be hired in the kitchen, what was needed of them?\nA. forget\nB. oil squeaky hinge\nC. question authority\nD. wash dishes\nE. oik squeaky hinge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Anybody could be hired in the kitchen, what was needed of them?\nA. forget\nB. oil squeaky hinge\nC. question authority\nD. wash dishes\nE. oik squeaky hinge\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.902671813964844", "False"]], [["-5.902671813964844", "False"]], [["-7.652671813964844", "False"]], [["-0.9026715755462646", "True"]], [["-9.402671813964844", "False"]]], "filtered_resps": [["-4.902671813964844", "False"], ["-5.902671813964844", "False"], ["-7.652671813964844", "False"], ["-0.9026715755462646", "True"], ["-9.402671813964844", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "820ab4c3531cdf69a8eef46cb97bb63c8030401ad77c27190a9a7d4c32bd8084", "prompt_hash": "2a2523ccb2589e1c602d1c6346514e250cc0ed865d5e187fe4a117e0443d6729", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 93, "doc": {"id": "1531f1523f5fd24bbdb42c311dbf90e8", "question": "Where can you find a number of wind instruments together in public?", "question_concept": "wind instrument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["music store", "create music", "zoo", "music room", "symphony"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you find a number of wind instruments together in public?\nA. music store\nB. create music\nC. zoo\nD. music room\nE. symphony\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you find a number of wind instruments together in public?\nA. music store\nB. create music\nC. zoo\nD. music room\nE. symphony\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you find a number of wind instruments together in public?\nA. music store\nB. create music\nC. zoo\nD. music room\nE. symphony\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you find a number of wind instruments together in public?\nA. music store\nB. create music\nC. zoo\nD. music room\nE. symphony\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you find a number of wind instruments together in public?\nA. music store\nB. create music\nC. zoo\nD. music room\nE. symphony\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.410153388977051", "False"]], [["-6.660153388977051", "False"]], [["-6.910153388977051", "False"]], [["-6.660153388977051", "False"]], [["-1.9101532697677612", "False"]]], "filtered_resps": [["-2.410153388977051", "False"], ["-6.660153388977051", "False"], ["-6.910153388977051", "False"], ["-6.660153388977051", "False"], ["-1.9101532697677612", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a9bfb976787145078d9de1a65352a8db528cf6f487f3ee01d5eca35557d560da", "prompt_hash": "56429a119eee3afe4af3a9e1761b9dba1813226afea3d2795469be50016a518b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 94, "doc": {"id": "716ce4404a84b42dd64e561390c4b53b", "question": "A mountie got off at a subway stop.  What city might he be in?", "question_concept": "subway stop", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["urban area", "metropolis", "chicago", "new york city", "toronto"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: A mountie got off at a subway stop.  What city might he be in?\nA. urban area\nB. metropolis\nC. chicago\nD. new york city\nE. toronto\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A mountie got off at a subway stop.  What city might he be in?\nA. urban area\nB. metropolis\nC. chicago\nD. new york city\nE. toronto\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A mountie got off at a subway stop.  What city might he be in?\nA. urban area\nB. metropolis\nC. chicago\nD. new york city\nE. toronto\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A mountie got off at a subway stop.  What city might he be in?\nA. urban area\nB. metropolis\nC. chicago\nD. new york city\nE. toronto\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A mountie got off at a subway stop.  What city might he be in?\nA. urban area\nB. metropolis\nC. chicago\nD. new york city\nE. toronto\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.45584774017334", "False"]], [["-4.70584774017334", "False"]], [["-4.45584774017334", "False"]], [["-4.20584774017334", "False"]], [["-1.9558476209640503", "False"]]], "filtered_resps": [["-2.45584774017334", "False"], ["-4.70584774017334", "False"], ["-4.45584774017334", "False"], ["-4.20584774017334", "False"], ["-1.9558476209640503", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "44aff19a3314f92c02a62e8a10fa3be0c89eaafa57d6a616b7aabaeac849fd7d", "prompt_hash": "a00f2a79186685a3e837866d0e61254b0ebe123a27223453455db0b27c430c9a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 95, "doc": {"id": "5169f7ae0781b15161551de3a189ebef", "question": "What do you want someone to do when you illustrate point?", "question_concept": "illustrate point", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["did not understand", "accepting", "make clear", "understood", "understanding"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do you want someone to do when you illustrate point?\nA. did not understand\nB. accepting\nC. make clear\nD. understood\nE. understanding\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you want someone to do when you illustrate point?\nA. did not understand\nB. accepting\nC. make clear\nD. understood\nE. understanding\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you want someone to do when you illustrate point?\nA. did not understand\nB. accepting\nC. make clear\nD. understood\nE. understanding\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you want someone to do when you illustrate point?\nA. did not understand\nB. accepting\nC. make clear\nD. understood\nE. understanding\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you want someone to do when you illustrate point?\nA. did not understand\nB. accepting\nC. make clear\nD. understood\nE. understanding\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.992269039154053", "False"]], [["-4.992269039154053", "False"]], [["-1.9922690391540527", "False"]], [["-6.742269039154053", "False"]], [["-6.492269039154053", "False"]]], "filtered_resps": [["-5.992269039154053", "False"], ["-4.992269039154053", "False"], ["-1.9922690391540527", "False"], ["-6.742269039154053", "False"], ["-6.492269039154053", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "50e68a5799e8583bcf4700f79c5a4fe61d4740022021ffd54599d0f956c0997b", "prompt_hash": "ed94445bb2e4d325f9c865dbb3c2aa1884f66a342445508ef4537a8734b804fa", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 96, "doc": {"id": "ef22ef7aeec70aaa688720f805c1cf38", "question": "Billy set aside a block of time for having fun after work. Why might he do this?", "question_concept": "having fun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["happiness", "stress relief", "pleasure", "ocean", "may laugh"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Billy set aside a block of time for having fun after work. Why might he do this?\nA. happiness\nB. stress relief\nC. pleasure\nD. ocean\nE. may laugh\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Billy set aside a block of time for having fun after work. Why might he do this?\nA. happiness\nB. stress relief\nC. pleasure\nD. ocean\nE. may laugh\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Billy set aside a block of time for having fun after work. Why might he do this?\nA. happiness\nB. stress relief\nC. pleasure\nD. ocean\nE. may laugh\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Billy set aside a block of time for having fun after work. Why might he do this?\nA. happiness\nB. stress relief\nC. pleasure\nD. ocean\nE. may laugh\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Billy set aside a block of time for having fun after work. Why might he do this?\nA. happiness\nB. stress relief\nC. pleasure\nD. ocean\nE. may laugh\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.644974708557129", "False"]], [["-0.8949748277664185", "True"]], [["-4.144974708557129", "False"]], [["-5.894974708557129", "False"]], [["-6.894974708557129", "False"]]], "filtered_resps": [["-4.644974708557129", "False"], ["-0.8949748277664185", "True"], ["-4.144974708557129", "False"], ["-5.894974708557129", "False"], ["-6.894974708557129", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "72dbfa43ae0dd8a6b065445da16199135d169c8994b76cfe811ee088e1f0b909", "prompt_hash": "afe9fba67515747cb0a1c92646c840822190f0dcef7cabcc3a8e5360b221ad1d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 97, "doc": {"id": "514310637fb43a252bfadc8cbf79b277", "question": "The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?", "question_concept": "lazy", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["restless", "active", "lazybutt", "productive", "hard work"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?\nA. restless\nB. active\nC. lazybutt\nD. productive\nE. hard work\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?\nA. restless\nB. active\nC. lazybutt\nD. productive\nE. hard work\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?\nA. restless\nB. active\nC. lazybutt\nD. productive\nE. hard work\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?\nA. restless\nB. active\nC. lazybutt\nD. productive\nE. hard work\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?\nA. restless\nB. active\nC. lazybutt\nD. productive\nE. hard work\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.8508806228637695", "False"]], [["-2.3508806228637695", "False"]], [["-7.6008806228637695", "False"]], [["-1.85088050365448", "False"]], [["-9.85088062286377", "False"]]], "filtered_resps": [["-4.8508806228637695", "False"], ["-2.3508806228637695", "False"], ["-7.6008806228637695", "False"], ["-1.85088050365448", "False"], ["-9.85088062286377", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a78d310fc0c35db738cc363e0f227c37aa17b7ec4fc4d8c2ee8d94f8f00f3bf7", "prompt_hash": "eb11b8aeeb38331f08c8b1ea4d0e77268ae5870f386e8f91256ca2d124fd9973", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 98, "doc": {"id": "9370b2b0897b796dec4a40f107854c8d", "question": "What would you be unable to do if you have too much greed?", "question_concept": "greed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["keep things", "make friends", "play poker", "conquer opponent", "lie"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What would you be unable to do if you have too much greed?\nA. keep things\nB. make friends\nC. play poker\nD. conquer opponent\nE. lie\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you be unable to do if you have too much greed?\nA. keep things\nB. make friends\nC. play poker\nD. conquer opponent\nE. lie\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you be unable to do if you have too much greed?\nA. keep things\nB. make friends\nC. play poker\nD. conquer opponent\nE. lie\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you be unable to do if you have too much greed?\nA. keep things\nB. make friends\nC. play poker\nD. conquer opponent\nE. lie\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you be unable to do if you have too much greed?\nA. keep things\nB. make friends\nC. play poker\nD. conquer opponent\nE. lie\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.697178363800049", "False"]], [["-1.1971784830093384", "True"]], [["-3.697178363800049", "False"]], [["-4.697178363800049", "False"]], [["-4.697178363800049", "False"]]], "filtered_resps": [["-2.697178363800049", "False"], ["-1.1971784830093384", "True"], ["-3.697178363800049", "False"], ["-4.697178363800049", "False"], ["-4.697178363800049", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b95ae05c87acf02cde07dd6069ba672a6ca14a2f0fc9a9d41533587f269ba88d", "prompt_hash": "8712798b2a18a8b3bc25966b366073babbd984f8b7ff9cb4736eee5e77b57823", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 99, "doc": {"id": "49902e768c45aa41a0f9f95be81114e5", "question": "It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?", "question_concept": "hotel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bed away from home", "wwii bunker", "resort", "las vegas", "city"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?\nA. bed away from home\nB. wwii bunker\nC. resort\nD. las vegas\nE. city\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?\nA. bed away from home\nB. wwii bunker\nC. resort\nD. las vegas\nE. city\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?\nA. bed away from home\nB. wwii bunker\nC. resort\nD. las vegas\nE. city\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?\nA. bed away from home\nB. wwii bunker\nC. resort\nD. las vegas\nE. city\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?\nA. bed away from home\nB. wwii bunker\nC. resort\nD. las vegas\nE. city\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5019445419311523", "False"]], [["-6.001944541931152", "False"]], [["-3.5019445419311523", "False"]], [["-6.001944541931152", "False"]], [["-2.0019445419311523", "False"]]], "filtered_resps": [["-2.5019445419311523", "False"], ["-6.001944541931152", "False"], ["-3.5019445419311523", "False"], ["-6.001944541931152", "False"], ["-2.0019445419311523", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1d9643e9731a49e042e83cc90620bf9de219ee642d0484b3954928c45d9160a7", "prompt_hash": "a1a6011cf7ba8919cac37a27d0e0b226071553c239182fc2f4687a6b0becaf40", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 100, "doc": {"id": "e1f90cd664a6b150291e6d8444d85c54", "question": "I did not need a servant.  I was not a what?", "question_concept": "servant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["freedom", "rich person", "hired help", "in charge", "busy"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: I did not need a servant.  I was not a what?\nA. freedom\nB. rich person\nC. hired help\nD. in charge\nE. busy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I did not need a servant.  I was not a what?\nA. freedom\nB. rich person\nC. hired help\nD. in charge\nE. busy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I did not need a servant.  I was not a what?\nA. freedom\nB. rich person\nC. hired help\nD. in charge\nE. busy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I did not need a servant.  I was not a what?\nA. freedom\nB. rich person\nC. hired help\nD. in charge\nE. busy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I did not need a servant.  I was not a what?\nA. freedom\nB. rich person\nC. hired help\nD. in charge\nE. busy\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.008480072021484", "False"]], [["-3.0084800720214844", "False"]], [["-1.2584799528121948", "True"]], [["-5.758480072021484", "False"]], [["-6.008480072021484", "False"]]], "filtered_resps": [["-4.008480072021484", "False"], ["-3.0084800720214844", "False"], ["-1.2584799528121948", "True"], ["-5.758480072021484", "False"], ["-6.008480072021484", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4bca664b5e61a8bd7cc018eea6a26743ddb621dccb0845426c39a0678e4abc2a", "prompt_hash": "b0dfc6e67edba5a4c2b0b777b4d7dad268786ab7933d906b1122073477dd85c1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 101, "doc": {"id": "320ec9b68fdefe13d59cc8b628083790", "question": "How would you get from one side of a canal to another?", "question_concept": "canal", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["michigan", "amsterdam", "venice", "bridge", "barges to travel on"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: How would you get from one side of a canal to another?\nA. michigan\nB. amsterdam\nC. venice\nD. bridge\nE. barges to travel on\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How would you get from one side of a canal to another?\nA. michigan\nB. amsterdam\nC. venice\nD. bridge\nE. barges to travel on\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How would you get from one side of a canal to another?\nA. michigan\nB. amsterdam\nC. venice\nD. bridge\nE. barges to travel on\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How would you get from one side of a canal to another?\nA. michigan\nB. amsterdam\nC. venice\nD. bridge\nE. barges to travel on\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How would you get from one side of a canal to another?\nA. michigan\nB. amsterdam\nC. venice\nD. bridge\nE. barges to travel on\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.521495819091797", "False"]], [["-3.271495819091797", "False"]], [["-2.521495819091797", "False"]], [["-1.7714958190917969", "True"]], [["-4.271495819091797", "False"]]], "filtered_resps": [["-3.521495819091797", "False"], ["-3.271495819091797", "False"], ["-2.521495819091797", "False"], ["-1.7714958190917969", "True"], ["-4.271495819091797", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "89552e15f27a81550fba747fdf9a497004827e773825b09c626f07c714edf28e", "prompt_hash": "7106ff3efdfd7f209f39e3c7d75f48ac238d198eaa18a11779343c8a016048ca", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 102, "doc": {"id": "964185aed0e381853332bca1a4d91f46", "question": "When learning about the world and different cultures, what is important if you are committed to eliminating preconceived notions", "question_concept": "learning about world", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["newness", "loss of innocence", "enlightenment", "open mind", "smartness"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When learning about the world and different cultures, what is important if you are committed to eliminating preconceived notions\nA. newness\nB. loss of innocence\nC. enlightenment\nD. open mind\nE. smartness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When learning about the world and different cultures, what is important if you are committed to eliminating preconceived notions\nA. newness\nB. loss of innocence\nC. enlightenment\nD. open mind\nE. smartness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When learning about the world and different cultures, what is important if you are committed to eliminating preconceived notions\nA. newness\nB. loss of innocence\nC. enlightenment\nD. open mind\nE. smartness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When learning about the world and different cultures, what is important if you are committed to eliminating preconceived notions\nA. newness\nB. loss of innocence\nC. enlightenment\nD. open mind\nE. smartness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When learning about the world and different cultures, what is important if you are committed to eliminating preconceived notions\nA. newness\nB. loss of innocence\nC. enlightenment\nD. open mind\nE. smartness\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.762510776519775", "False"]], [["-4.262510776519775", "False"]], [["-4.512510776519775", "False"]], [["-2.2625107765197754", "False"]], [["-6.012510776519775", "False"]]], "filtered_resps": [["-4.762510776519775", "False"], ["-4.262510776519775", "False"], ["-4.512510776519775", "False"], ["-2.2625107765197754", "False"], ["-6.012510776519775", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b71c2b416124e92fc0371767f32c7bb98365d596722140739755250dd013190c", "prompt_hash": "d0226082e9c52fd8ecb74034c01000a44b78d171841003ae778e7d28ac7708dd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 103, "doc": {"id": "db8e010754c532d78635e5b7cf81a147", "question": "An underrated thing about computers is how they manage workflow, at one time it was a big deal when they could first do what?", "question_concept": "computers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["share files", "do arithmetic", "turn on", "cost money", "multitask"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: An underrated thing about computers is how they manage workflow, at one time it was a big deal when they could first do what?\nA. share files\nB. do arithmetic\nC. turn on\nD. cost money\nE. multitask\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: An underrated thing about computers is how they manage workflow, at one time it was a big deal when they could first do what?\nA. share files\nB. do arithmetic\nC. turn on\nD. cost money\nE. multitask\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: An underrated thing about computers is how they manage workflow, at one time it was a big deal when they could first do what?\nA. share files\nB. do arithmetic\nC. turn on\nD. cost money\nE. multitask\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: An underrated thing about computers is how they manage workflow, at one time it was a big deal when they could first do what?\nA. share files\nB. do arithmetic\nC. turn on\nD. cost money\nE. multitask\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: An underrated thing about computers is how they manage workflow, at one time it was a big deal when they could first do what?\nA. share files\nB. do arithmetic\nC. turn on\nD. cost money\nE. multitask\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.689143657684326", "False"]], [["-2.439143657684326", "False"]], [["-5.439143657684326", "False"]], [["-4.939143657684326", "False"]], [["-1.1891437768936157", "True"]]], "filtered_resps": [["-3.689143657684326", "False"], ["-2.439143657684326", "False"], ["-5.439143657684326", "False"], ["-4.939143657684326", "False"], ["-1.1891437768936157", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6b2abf6d423e2ba79fe0325e6a712736fe3bc7c2557a1208e35e09168f04a9b6", "prompt_hash": "c241076c1c5b0ac00a4e705a755615c59c954c7688b1e3ac7cd9496e01557c44", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 104, "doc": {"id": "998381f854f51da2a6ccde45909e5168", "question": "Obstructing justice is sometimes an excuse used for police brutality which causes what in people?", "question_concept": "obstructing justice", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["committing perjury", "prosecution", "attack", "getting hurt", "riot"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Obstructing justice is sometimes an excuse used for police brutality which causes what in people?\nA. committing perjury\nB. prosecution\nC. attack\nD. getting hurt\nE. riot\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Obstructing justice is sometimes an excuse used for police brutality which causes what in people?\nA. committing perjury\nB. prosecution\nC. attack\nD. getting hurt\nE. riot\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Obstructing justice is sometimes an excuse used for police brutality which causes what in people?\nA. committing perjury\nB. prosecution\nC. attack\nD. getting hurt\nE. riot\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Obstructing justice is sometimes an excuse used for police brutality which causes what in people?\nA. committing perjury\nB. prosecution\nC. attack\nD. getting hurt\nE. riot\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Obstructing justice is sometimes an excuse used for police brutality which causes what in people?\nA. committing perjury\nB. prosecution\nC. attack\nD. getting hurt\nE. riot\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.211564540863037", "False"]], [["-5.211564540863037", "False"]], [["-5.711564540863037", "False"]], [["-2.461564779281616", "False"]], [["-8.461565017700195", "False"]]], "filtered_resps": [["-6.211564540863037", "False"], ["-5.211564540863037", "False"], ["-5.711564540863037", "False"], ["-2.461564779281616", "False"], ["-8.461565017700195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c76fe13fed79db8ef70df5d863e5a509fda4d416a860415bbd93205f428dbce7", "prompt_hash": "3dcf0ea7f90a1564c85a3d3c18cad63448e8fc8672eb1851478e45df2bbe1e6e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 105, "doc": {"id": "bc38ad28e99cff7a65771233f734a007", "question": "While washing clothes they became what when caught on the sharp object?", "question_concept": "washing clothes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["damaged", "wet clothes", "wear out", "torn", "have fun"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: While washing clothes they became what when caught on the sharp object?\nA. damaged\nB. wet clothes\nC. wear out\nD. torn\nE. have fun\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: While washing clothes they became what when caught on the sharp object?\nA. damaged\nB. wet clothes\nC. wear out\nD. torn\nE. have fun\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: While washing clothes they became what when caught on the sharp object?\nA. damaged\nB. wet clothes\nC. wear out\nD. torn\nE. have fun\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: While washing clothes they became what when caught on the sharp object?\nA. damaged\nB. wet clothes\nC. wear out\nD. torn\nE. have fun\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: While washing clothes they became what when caught on the sharp object?\nA. damaged\nB. wet clothes\nC. wear out\nD. torn\nE. have fun\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0316901206970215", "False"]], [["-6.2816901206970215", "False"]], [["-5.5316901206970215", "False"]], [["-1.531690001487732", "False"]], [["-9.781689643859863", "False"]]], "filtered_resps": [["-3.0316901206970215", "False"], ["-6.2816901206970215", "False"], ["-5.5316901206970215", "False"], ["-1.531690001487732", "False"], ["-9.781689643859863", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5f31f5124a9d9d798ca20b3bfcc2c1014fb693963b707fb1b5a2752610845a29", "prompt_hash": "0fc39f0c6df6ab7c9b2b892c9649860fddc5c437ab1a5c7eb7582d85e29c6a47", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 106, "doc": {"id": "e3949997bf9d02048cfa5d8dd0f287aa", "question": "Seafood restaurants are used to draw tourists where?", "question_concept": "seafood restaurant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["maine", "shoe shop", "city", "boston", "coastal cities"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Seafood restaurants are used to draw tourists where?\nA. maine\nB. shoe shop\nC. city\nD. boston\nE. coastal cities\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Seafood restaurants are used to draw tourists where?\nA. maine\nB. shoe shop\nC. city\nD. boston\nE. coastal cities\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Seafood restaurants are used to draw tourists where?\nA. maine\nB. shoe shop\nC. city\nD. boston\nE. coastal cities\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Seafood restaurants are used to draw tourists where?\nA. maine\nB. shoe shop\nC. city\nD. boston\nE. coastal cities\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Seafood restaurants are used to draw tourists where?\nA. maine\nB. shoe shop\nC. city\nD. boston\nE. coastal cities\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.247466564178467", "False"]], [["-6.747466564178467", "False"]], [["-6.497466564178467", "False"]], [["-7.247466564178467", "False"]], [["-1.4974665641784668", "False"]]], "filtered_resps": [["-4.247466564178467", "False"], ["-6.747466564178467", "False"], ["-6.497466564178467", "False"], ["-7.247466564178467", "False"], ["-1.4974665641784668", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5508a67fda3a3a1ed71a9d80e9769b89daad0c85bae9745997d9890c1a94de0d", "prompt_hash": "ff6e65935bc4ac717ecf9f7f796557ed1e119c0d03a94db937c1aa5abcd6db36", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 107, "doc": {"id": "a7d51b753c2113d8b2dbd0ebb5375855", "question": "James's nice asked him about her grandfather. She was interested in learning about what?", "question_concept": "niece", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["family tree", "family reunion", "babysitting", "brother's house", "heirlooms"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James's nice asked him about her grandfather. She was interested in learning about what?\nA. family tree\nB. family reunion\nC. babysitting\nD. brother's house\nE. heirlooms\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James's nice asked him about her grandfather. She was interested in learning about what?\nA. family tree\nB. family reunion\nC. babysitting\nD. brother's house\nE. heirlooms\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James's nice asked him about her grandfather. She was interested in learning about what?\nA. family tree\nB. family reunion\nC. babysitting\nD. brother's house\nE. heirlooms\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James's nice asked him about her grandfather. She was interested in learning about what?\nA. family tree\nB. family reunion\nC. babysitting\nD. brother's house\nE. heirlooms\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James's nice asked him about her grandfather. She was interested in learning about what?\nA. family tree\nB. family reunion\nC. babysitting\nD. brother's house\nE. heirlooms\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9825550317764282", "True"]], [["-5.482554912567139", "False"]], [["-6.482554912567139", "False"]], [["-7.732554912567139", "False"]], [["-4.232554912567139", "False"]]], "filtered_resps": [["-0.9825550317764282", "True"], ["-5.482554912567139", "False"], ["-6.482554912567139", "False"], ["-7.732554912567139", "False"], ["-4.232554912567139", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7432b16d17257d3102ff8f5274b847937f811f0623019661a651bb0176d1bb5f", "prompt_hash": "8bc045a62ee4b2e2043c79d19e5dd0eb784209c19df2b7fb846d8048a14b68d4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 108, "doc": {"id": "3e4b326aff96e9adbb52ba18cfa877b2", "question": "James looked up and saw the start twinkling in the black yonder.  He marveled the sheer number of them and the size of what?", "question_concept": "stars", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["universe", "orbit", "night sky", "outer space", "his wallet"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James looked up and saw the start twinkling in the black yonder.  He marveled the sheer number of them and the size of what?\nA. universe\nB. orbit\nC. night sky\nD. outer space\nE. his wallet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James looked up and saw the start twinkling in the black yonder.  He marveled the sheer number of them and the size of what?\nA. universe\nB. orbit\nC. night sky\nD. outer space\nE. his wallet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James looked up and saw the start twinkling in the black yonder.  He marveled the sheer number of them and the size of what?\nA. universe\nB. orbit\nC. night sky\nD. outer space\nE. his wallet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James looked up and saw the start twinkling in the black yonder.  He marveled the sheer number of them and the size of what?\nA. universe\nB. orbit\nC. night sky\nD. outer space\nE. his wallet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James looked up and saw the start twinkling in the black yonder.  He marveled the sheer number of them and the size of what?\nA. universe\nB. orbit\nC. night sky\nD. outer space\nE. his wallet\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.579501152038574", "False"]], [["-5.829501152038574", "False"]], [["-1.3295012712478638", "True"]], [["-4.829501152038574", "False"]], [["-7.579501152038574", "False"]]], "filtered_resps": [["-3.579501152038574", "False"], ["-5.829501152038574", "False"], ["-1.3295012712478638", "True"], ["-4.829501152038574", "False"], ["-7.579501152038574", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e3c2df9f9d505198b4e6a7d74eecc855107bf94f2a102ce22bc36fab132be697", "prompt_hash": "ba4e0bdcad2ae9ca1f377af9aa626a73cf230d468c2de723d7f3eee93c0abd9a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 109, "doc": {"id": "5ac83e9e6fa9851ad3cccb0d57c1d88f", "question": "What would encourage someone to continue playing tennis?", "question_concept": "playing tennis", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["becoming tired", "tennis elbow", "exercise", "hunger", "victory"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What would encourage someone to continue playing tennis?\nA. becoming tired\nB. tennis elbow\nC. exercise\nD. hunger\nE. victory\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would encourage someone to continue playing tennis?\nA. becoming tired\nB. tennis elbow\nC. exercise\nD. hunger\nE. victory\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would encourage someone to continue playing tennis?\nA. becoming tired\nB. tennis elbow\nC. exercise\nD. hunger\nE. victory\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would encourage someone to continue playing tennis?\nA. becoming tired\nB. tennis elbow\nC. exercise\nD. hunger\nE. victory\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would encourage someone to continue playing tennis?\nA. becoming tired\nB. tennis elbow\nC. exercise\nD. hunger\nE. victory\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.603081703186035", "False"]], [["-5.603081703186035", "False"]], [["-2.603081703186035", "False"]], [["-7.103081703186035", "False"]], [["-0.8530817031860352", "True"]]], "filtered_resps": [["-4.603081703186035", "False"], ["-5.603081703186035", "False"], ["-2.603081703186035", "False"], ["-7.103081703186035", "False"], ["-0.8530817031860352", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b05241499bbbdb3d137df55f8058dbe04ce87f9b40114c00b30c1c83a89cc0f6", "prompt_hash": "765a4e1c8f5587bf3e5839cf0f936d7f7162f88fd04f8559493efb7a7aa2affb", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 110, "doc": {"id": "2c0030cc14a27be2401dcfdaa501f0fc", "question": "James found the sound relaxing.   It was so relaxing he almost did what despite his efforts?", "question_concept": "relaxing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["deep breathing", "worried", "fall asleep", "invigorating", "feeling good"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: James found the sound relaxing.   It was so relaxing he almost did what despite his efforts?\nA. deep breathing\nB. worried\nC. fall asleep\nD. invigorating\nE. feeling good\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James found the sound relaxing.   It was so relaxing he almost did what despite his efforts?\nA. deep breathing\nB. worried\nC. fall asleep\nD. invigorating\nE. feeling good\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James found the sound relaxing.   It was so relaxing he almost did what despite his efforts?\nA. deep breathing\nB. worried\nC. fall asleep\nD. invigorating\nE. feeling good\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James found the sound relaxing.   It was so relaxing he almost did what despite his efforts?\nA. deep breathing\nB. worried\nC. fall asleep\nD. invigorating\nE. feeling good\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James found the sound relaxing.   It was so relaxing he almost did what despite his efforts?\nA. deep breathing\nB. worried\nC. fall asleep\nD. invigorating\nE. feeling good\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.500373840332031", "False"]], [["-5.000373840332031", "False"]], [["-1.0003737211227417", "True"]], [["-7.000373840332031", "False"]], [["-9.750373840332031", "False"]]], "filtered_resps": [["-5.500373840332031", "False"], ["-5.000373840332031", "False"], ["-1.0003737211227417", "True"], ["-7.000373840332031", "False"], ["-9.750373840332031", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "714a087854295fdb73241b0ab967ec8b4949ce101913f27f11696f5101587faf", "prompt_hash": "c2c17a3424703920b95ee03fc972c6fda3490264fd027aae9b2b83e3bf49514f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 111, "doc": {"id": "feb83263e6be392351db0794004efc3f", "question": "What regions of a town would you have found a dime store?", "question_concept": "dime store", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["commercial building", "old movie", "small neighborhood", "past", "mall"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What regions of a town would you have found a dime store?\nA. commercial building\nB. old movie\nC. small neighborhood\nD. past\nE. mall\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What regions of a town would you have found a dime store?\nA. commercial building\nB. old movie\nC. small neighborhood\nD. past\nE. mall\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What regions of a town would you have found a dime store?\nA. commercial building\nB. old movie\nC. small neighborhood\nD. past\nE. mall\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What regions of a town would you have found a dime store?\nA. commercial building\nB. old movie\nC. small neighborhood\nD. past\nE. mall\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What regions of a town would you have found a dime store?\nA. commercial building\nB. old movie\nC. small neighborhood\nD. past\nE. mall\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7270374298095703", "True"]], [["-6.47703742980957", "False"]], [["-2.7270374298095703", "False"]], [["-3.9770374298095703", "False"]], [["-5.72703742980957", "False"]]], "filtered_resps": [["-0.7270374298095703", "True"], ["-6.47703742980957", "False"], ["-2.7270374298095703", "False"], ["-3.9770374298095703", "False"], ["-5.72703742980957", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7edc28c6f3104d74b5c84555ddda3eeb3d62ea3d255e1c5dfb6d5671b1dc0eb8", "prompt_hash": "ce2adaf4b600079ce4b6d9a88424f0e8b6755e3ccd755fd639c195c136daba85", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 112, "doc": {"id": "80697d599280d994d8a584c95824ef1f", "question": "Where might an unused chess set be stored?", "question_concept": "chess set", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["toy store", "michigan", "living room", "attic", "cupboard"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where might an unused chess set be stored?\nA. toy store\nB. michigan\nC. living room\nD. attic\nE. cupboard\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where might an unused chess set be stored?\nA. toy store\nB. michigan\nC. living room\nD. attic\nE. cupboard\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where might an unused chess set be stored?\nA. toy store\nB. michigan\nC. living room\nD. attic\nE. cupboard\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where might an unused chess set be stored?\nA. toy store\nB. michigan\nC. living room\nD. attic\nE. cupboard\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where might an unused chess set be stored?\nA. toy store\nB. michigan\nC. living room\nD. attic\nE. cupboard\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5742757320404053", "False"]], [["-6.324275970458984", "False"]], [["-4.574275970458984", "False"]], [["-2.3242757320404053", "False"]], [["-3.0742757320404053", "False"]]], "filtered_resps": [["-3.5742757320404053", "False"], ["-6.324275970458984", "False"], ["-4.574275970458984", "False"], ["-2.3242757320404053", "False"], ["-3.0742757320404053", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3e5d3efd65b1f1c175044c36d0bbfaa71d09644130b32df0bbb98b16e7006e55", "prompt_hash": "24c3660f9050a72af7812c5cac0c2f97da3c718609494ae773920e35946750ea", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 113, "doc": {"id": "3c1800e7dd96d37fdd3c51b9fe502342", "question": "james told his son to settle down and be careful.  There were many frogs mating in the area, and James didn't want his son to do what to them?", "question_concept": "settle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wander", "migrate", "scare", "disturb", "agitate"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: james told his son to settle down and be careful.  There were many frogs mating in the area, and James didn't want his son to do what to them?\nA. wander\nB. migrate\nC. scare\nD. disturb\nE. agitate\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: james told his son to settle down and be careful.  There were many frogs mating in the area, and James didn't want his son to do what to them?\nA. wander\nB. migrate\nC. scare\nD. disturb\nE. agitate\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: james told his son to settle down and be careful.  There were many frogs mating in the area, and James didn't want his son to do what to them?\nA. wander\nB. migrate\nC. scare\nD. disturb\nE. agitate\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: james told his son to settle down and be careful.  There were many frogs mating in the area, and James didn't want his son to do what to them?\nA. wander\nB. migrate\nC. scare\nD. disturb\nE. agitate\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: james told his son to settle down and be careful.  There were many frogs mating in the area, and James didn't want his son to do what to them?\nA. wander\nB. migrate\nC. scare\nD. disturb\nE. agitate\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.914989471435547", "False"]], [["-7.414989471435547", "False"]], [["-4.414989471435547", "False"]], [["-2.164989471435547", "False"]], [["-10.414989471435547", "False"]]], "filtered_resps": [["-4.914989471435547", "False"], ["-7.414989471435547", "False"], ["-4.414989471435547", "False"], ["-2.164989471435547", "False"], ["-10.414989471435547", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2909a5fad5e693e6c9338cb2e68745439e708c7f99956eba50e27e65afb58b1c", "prompt_hash": "8706a64140cbe4a868f58cba29c6c80b14a970a20014143255a02bc239b7aaf2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 114, "doc": {"id": "4da33e6f4b789776acb1bc10195baa83", "question": "A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?", "question_concept": "air conditioning", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["car", "house", "offices", "park", "movie theatre"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nA. car\nB. house\nC. offices\nD. park\nE. movie theatre\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nA. car\nB. house\nC. offices\nD. park\nE. movie theatre\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nA. car\nB. house\nC. offices\nD. park\nE. movie theatre\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nA. car\nB. house\nC. offices\nD. park\nE. movie theatre\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nA. car\nB. house\nC. offices\nD. park\nE. movie theatre\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.121099948883057", "False"]], [["-1.3710999488830566", "False"]], [["-7.121099948883057", "False"]], [["-8.371099472045898", "False"]], [["-2.1210999488830566", "False"]]], "filtered_resps": [["-4.121099948883057", "False"], ["-1.3710999488830566", "False"], ["-7.121099948883057", "False"], ["-8.371099472045898", "False"], ["-2.1210999488830566", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4cfc10b80ec8b0b389ba544be8f0b852d02a3764fbcccc6f9d68c647a0309410", "prompt_hash": "42ab1320a2187e0e34adff39939ea3e90eeef43fac3b5b22982c4062a53f1b81", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 115, "doc": {"id": "ae038e9af9d5a511ada7456b5e73b15e", "question": "What could be playing a balailaika?", "question_concept": "balalaika", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["movie dr", "orchestra", "music store", "cat", "symphony"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What could be playing a balailaika?\nA. movie dr\nB. orchestra\nC. music store\nD. cat\nE. symphony\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could be playing a balailaika?\nA. movie dr\nB. orchestra\nC. music store\nD. cat\nE. symphony\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could be playing a balailaika?\nA. movie dr\nB. orchestra\nC. music store\nD. cat\nE. symphony\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could be playing a balailaika?\nA. movie dr\nB. orchestra\nC. music store\nD. cat\nE. symphony\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could be playing a balailaika?\nA. movie dr\nB. orchestra\nC. music store\nD. cat\nE. symphony\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0438069105148315", "True"]], [["-1.5438069105148315", "False"]], [["-5.793807029724121", "False"]], [["-6.543807029724121", "False"]], [["-3.293807029724121", "False"]]], "filtered_resps": [["-1.0438069105148315", "True"], ["-1.5438069105148315", "False"], ["-5.793807029724121", "False"], ["-6.543807029724121", "False"], ["-3.293807029724121", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "67d008bda454a084292375bb90d2efb3466520e5ede629f8082a86798e585292", "prompt_hash": "a1239d152a3b5d0636bbc24cb35bf789dd269e8964bccee32c9ee9ca57e309ff", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 116, "doc": {"id": "a400b9fd1e319f901471c4b42d401c52", "question": "Sailors drive many different types of boats, what type of boat involves their namesake.", "question_concept": "sailor", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["coming home", "row boat", "board ship", "inflatable raft", "sail boat"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Sailors drive many different types of boats, what type of boat involves their namesake.\nA. coming home\nB. row boat\nC. board ship\nD. inflatable raft\nE. sail boat\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sailors drive many different types of boats, what type of boat involves their namesake.\nA. coming home\nB. row boat\nC. board ship\nD. inflatable raft\nE. sail boat\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sailors drive many different types of boats, what type of boat involves their namesake.\nA. coming home\nB. row boat\nC. board ship\nD. inflatable raft\nE. sail boat\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sailors drive many different types of boats, what type of boat involves their namesake.\nA. coming home\nB. row boat\nC. board ship\nD. inflatable raft\nE. sail boat\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sailors drive many different types of boats, what type of boat involves their namesake.\nA. coming home\nB. row boat\nC. board ship\nD. inflatable raft\nE. sail boat\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.959099531173706", "False"]], [["-6.209099769592285", "False"]], [["-6.209099769592285", "False"]], [["-9.209099769592285", "False"]], [["-0.959099531173706", "True"]]], "filtered_resps": [["-3.959099531173706", "False"], ["-6.209099769592285", "False"], ["-6.209099769592285", "False"], ["-9.209099769592285", "False"], ["-0.959099531173706", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e275709175205b72807223c86c04862c164d173f9b853cc7fda6985f94dd63f3", "prompt_hash": "da7f10b87f40ba8727d51b44ae8915a92bc151e1f66d0303c97c245affb55137", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 117, "doc": {"id": "9dffd2021771e0ecddb19031acf3701b", "question": "Where could a person avoid the rain?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bus stop", "tunnel", "synagogue", "fairy tale", "street corner"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where could a person avoid the rain?\nA. bus stop\nB. tunnel\nC. synagogue\nD. fairy tale\nE. street corner\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could a person avoid the rain?\nA. bus stop\nB. tunnel\nC. synagogue\nD. fairy tale\nE. street corner\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could a person avoid the rain?\nA. bus stop\nB. tunnel\nC. synagogue\nD. fairy tale\nE. street corner\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could a person avoid the rain?\nA. bus stop\nB. tunnel\nC. synagogue\nD. fairy tale\nE. street corner\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could a person avoid the rain?\nA. bus stop\nB. tunnel\nC. synagogue\nD. fairy tale\nE. street corner\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.03456974029541", "False"]], [["-1.0345698595046997", "True"]], [["-6.53456974029541", "False"]], [["-7.28456974029541", "False"]], [["-9.53456974029541", "False"]]], "filtered_resps": [["-2.03456974029541", "False"], ["-1.0345698595046997", "True"], ["-6.53456974029541", "False"], ["-7.28456974029541", "False"], ["-9.53456974029541", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "67c8e543ba048fb900530e4f5d55f27e550b7b1511b09520994853507b245ae7", "prompt_hash": "8fffe4261cf7b331a129fbba840f80e3b43741f58458cd10d784cd2b76c27f30", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 118, "doc": {"id": "3730c646fdf54472ab873aac9ff7852e", "question": "Why would a person like to have a large house?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have choice", "mentally challenged", "own house", "obesity", "lots of space"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Why would a person like to have a large house?\nA. have choice\nB. mentally challenged\nC. own house\nD. obesity\nE. lots of space\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would a person like to have a large house?\nA. have choice\nB. mentally challenged\nC. own house\nD. obesity\nE. lots of space\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would a person like to have a large house?\nA. have choice\nB. mentally challenged\nC. own house\nD. obesity\nE. lots of space\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would a person like to have a large house?\nA. have choice\nB. mentally challenged\nC. own house\nD. obesity\nE. lots of space\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would a person like to have a large house?\nA. have choice\nB. mentally challenged\nC. own house\nD. obesity\nE. lots of space\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6819605827331543", "False"]], [["-7.431960582733154", "False"]], [["-5.931960582733154", "False"]], [["-7.931960582733154", "False"]], [["-1.1819605827331543", "True"]]], "filtered_resps": [["-1.6819605827331543", "False"], ["-7.431960582733154", "False"], ["-5.931960582733154", "False"], ["-7.931960582733154", "False"], ["-1.1819605827331543", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9225380e63ac87e52f6ae88e16d440bac91b2b68e90a732497776e20fd363c05", "prompt_hash": "bec04a45b03e26ff91a4a6874160f804efe31f8d9aede24ea7d8ab9a5a42dbe4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 119, "doc": {"id": "175e7dcdded13d5adafaebf2264c3abd", "question": "Where will a cheap book be found?", "question_concept": "book", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bookstore", "classroom", "discount store", "school room", "bedside table"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where will a cheap book be found?\nA. bookstore\nB. classroom\nC. discount store\nD. school room\nE. bedside table\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where will a cheap book be found?\nA. bookstore\nB. classroom\nC. discount store\nD. school room\nE. bedside table\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where will a cheap book be found?\nA. bookstore\nB. classroom\nC. discount store\nD. school room\nE. bedside table\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where will a cheap book be found?\nA. bookstore\nB. classroom\nC. discount store\nD. school room\nE. bedside table\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where will a cheap book be found?\nA. bookstore\nB. classroom\nC. discount store\nD. school room\nE. bedside table\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.011453151702881", "False"]], [["-6.261453151702881", "False"]], [["-1.5114530324935913", "False"]], [["-8.761452674865723", "False"]], [["-5.261453151702881", "False"]]], "filtered_resps": [["-2.011453151702881", "False"], ["-6.261453151702881", "False"], ["-1.5114530324935913", "False"], ["-8.761452674865723", "False"], ["-5.261453151702881", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c52accbba1ecaaac4f3b97cf350b4580dff4e84aea5cb20e635f247a39ecba38", "prompt_hash": "0b03bf4e787bdf211e85623be50ab5694a0c2d7d89061b74384b259fb37473c7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 120, "doc": {"id": "11d7db1d8e1cff2f40d4184f15cf7ae7", "question": "John and James are idiots. They bought two tickets to the Falcons vs the Jets even though neither wanted to see the what?", "question_concept": "idiots", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["internet cafe", "sporting event", "pressing wrong buttons", "obesity", "hockey game"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John and James are idiots. They bought two tickets to the Falcons vs the Jets even though neither wanted to see the what?\nA. internet cafe\nB. sporting event\nC. pressing wrong buttons\nD. obesity\nE. hockey game\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John and James are idiots. They bought two tickets to the Falcons vs the Jets even though neither wanted to see the what?\nA. internet cafe\nB. sporting event\nC. pressing wrong buttons\nD. obesity\nE. hockey game\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John and James are idiots. They bought two tickets to the Falcons vs the Jets even though neither wanted to see the what?\nA. internet cafe\nB. sporting event\nC. pressing wrong buttons\nD. obesity\nE. hockey game\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John and James are idiots. They bought two tickets to the Falcons vs the Jets even though neither wanted to see the what?\nA. internet cafe\nB. sporting event\nC. pressing wrong buttons\nD. obesity\nE. hockey game\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John and James are idiots. They bought two tickets to the Falcons vs the Jets even though neither wanted to see the what?\nA. internet cafe\nB. sporting event\nC. pressing wrong buttons\nD. obesity\nE. hockey game\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.270941257476807", "False"]], [["-1.2709412574768066", "False"]], [["-8.270940780639648", "False"]], [["-8.270940780639648", "False"]], [["-7.270941257476807", "False"]]], "filtered_resps": [["-6.270941257476807", "False"], ["-1.2709412574768066", "False"], ["-8.270940780639648", "False"], ["-8.270940780639648", "False"], ["-7.270941257476807", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ac489aed6b9513cae3fc79fb61826f6327ccf8461758126e50e2124d7b84d8cd", "prompt_hash": "a6d44b17653f0a98f9a70211b787eccbde8b99d16dced891f28a02567b8d69a0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 121, "doc": {"id": "08db69edf0ec5848c1a53dca8fc1601a", "question": "James noticed that his penis was bigger. .  How might he act toward his plastic surgeon?", "question_concept": "bigger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["accidental", "detestable", "effusive", "enabled", "apathetic"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: James noticed that his penis was bigger. .  How might he act toward his plastic surgeon?\nA. accidental\nB. detestable\nC. effusive\nD. enabled\nE. apathetic\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James noticed that his penis was bigger. .  How might he act toward his plastic surgeon?\nA. accidental\nB. detestable\nC. effusive\nD. enabled\nE. apathetic\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James noticed that his penis was bigger. .  How might he act toward his plastic surgeon?\nA. accidental\nB. detestable\nC. effusive\nD. enabled\nE. apathetic\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James noticed that his penis was bigger. .  How might he act toward his plastic surgeon?\nA. accidental\nB. detestable\nC. effusive\nD. enabled\nE. apathetic\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James noticed that his penis was bigger. .  How might he act toward his plastic surgeon?\nA. accidental\nB. detestable\nC. effusive\nD. enabled\nE. apathetic\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.707825660705566", "False"]], [["-6.457825660705566", "False"]], [["-1.7078256607055664", "False"]], [["-7.707825660705566", "False"]], [["-5.457825660705566", "False"]]], "filtered_resps": [["-4.707825660705566", "False"], ["-6.457825660705566", "False"], ["-1.7078256607055664", "False"], ["-7.707825660705566", "False"], ["-5.457825660705566", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bb61350ac91d79281ad2e6a31e4080baf2b8da24460b9111c252613f55d625e5", "prompt_hash": "584f09d42af86be34d3316cb51fdacfd682d50cbda7a8f1f9d63d95fc733d783", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 122, "doc": {"id": "855ab6ba47f6311104c4d29e24ef0234", "question": "Who do professors work with?", "question_concept": "professors", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["methods of facts", "teach courses", "wear wrinkled tweed jackets", "school students", "state facts"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Who do professors work with?\nA. methods of facts\nB. teach courses\nC. wear wrinkled tweed jackets\nD. school students\nE. state facts\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Who do professors work with?\nA. methods of facts\nB. teach courses\nC. wear wrinkled tweed jackets\nD. school students\nE. state facts\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Who do professors work with?\nA. methods of facts\nB. teach courses\nC. wear wrinkled tweed jackets\nD. school students\nE. state facts\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Who do professors work with?\nA. methods of facts\nB. teach courses\nC. wear wrinkled tweed jackets\nD. school students\nE. state facts\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Who do professors work with?\nA. methods of facts\nB. teach courses\nC. wear wrinkled tweed jackets\nD. school students\nE. state facts\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.445826530456543", "False"]], [["-3.695826530456543", "False"]], [["-7.695826530456543", "False"]], [["-1.445826530456543", "True"]], [["-10.195826530456543", "False"]]], "filtered_resps": [["-5.445826530456543", "False"], ["-3.695826530456543", "False"], ["-7.695826530456543", "False"], ["-1.445826530456543", "True"], ["-10.195826530456543", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "71c1be4e40a53223a7950e0428f4733d3781e85019bd11d0e12b5db531addeba", "prompt_hash": "b6b7164ea85a79f8180344e6b3d6f50a17d0dd2e975f770834806f88f837374c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 123, "doc": {"id": "7ec11eeca4221795c117943ca2639e86", "question": "Colorful anemone look somewhat like what object you find on window sills?", "question_concept": "anemone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["intertidal zone", "coral sea", "under water", "flower bed", "florida keys"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Colorful anemone look somewhat like what object you find on window sills?\nA. intertidal zone\nB. coral sea\nC. under water\nD. flower bed\nE. florida keys\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Colorful anemone look somewhat like what object you find on window sills?\nA. intertidal zone\nB. coral sea\nC. under water\nD. flower bed\nE. florida keys\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Colorful anemone look somewhat like what object you find on window sills?\nA. intertidal zone\nB. coral sea\nC. under water\nD. flower bed\nE. florida keys\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Colorful anemone look somewhat like what object you find on window sills?\nA. intertidal zone\nB. coral sea\nC. under water\nD. flower bed\nE. florida keys\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Colorful anemone look somewhat like what object you find on window sills?\nA. intertidal zone\nB. coral sea\nC. under water\nD. flower bed\nE. florida keys\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3804614543914795", "False"]], [["-5.630461692810059", "False"]], [["-4.630461692810059", "False"]], [["-2.1304614543914795", "False"]], [["-7.880461692810059", "False"]]], "filtered_resps": [["-3.3804614543914795", "False"], ["-5.630461692810059", "False"], ["-4.630461692810059", "False"], ["-2.1304614543914795", "False"], ["-7.880461692810059", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b1c27a8af562eb94ef9a09af4fb03c7c9c4ecc1e50107e02a880af4c524ef7a8", "prompt_hash": "b5cec01c4b38eb1c99ab810c9f98163b3c63a3142bcef508aabfcb492860904f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 124, "doc": {"id": "e9389b08fdd17f14b148d498d6ff4dfe", "question": "From where do aliens arrive?", "question_concept": "aliens", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["outer space", "weekly world news", "roswell", "universe", "mars"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: From where do aliens arrive?\nA. outer space\nB. weekly world news\nC. roswell\nD. universe\nE. mars\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: From where do aliens arrive?\nA. outer space\nB. weekly world news\nC. roswell\nD. universe\nE. mars\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: From where do aliens arrive?\nA. outer space\nB. weekly world news\nC. roswell\nD. universe\nE. mars\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: From where do aliens arrive?\nA. outer space\nB. weekly world news\nC. roswell\nD. universe\nE. mars\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: From where do aliens arrive?\nA. outer space\nB. weekly world news\nC. roswell\nD. universe\nE. mars\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1609712839126587", "True"]], [["-6.910971164703369", "False"]], [["-6.410971164703369", "False"]], [["-6.160971164703369", "False"]], [["-9.410971641540527", "False"]]], "filtered_resps": [["-1.1609712839126587", "True"], ["-6.910971164703369", "False"], ["-6.410971164703369", "False"], ["-6.160971164703369", "False"], ["-9.410971641540527", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9d976c36b32b85bd9f3eefca227a1863903281a31ac7f4b3312d594910e7ffbe", "prompt_hash": "06c65d44a60c144cf7af7dedde9eddc517ac660ed7274a57ff1f279775b6623b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 125, "doc": {"id": "afa2899cc21e204fa64e63e7839e8c1e", "question": "The hikers stopped to have a drink, simply put they what?", "question_concept": "drink", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["had a party", "were thirsty", "refreshment", "getting drunk", "celebrating"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The hikers stopped to have a drink, simply put they what?\nA. had a party\nB. were thirsty\nC. refreshment\nD. getting drunk\nE. celebrating\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The hikers stopped to have a drink, simply put they what?\nA. had a party\nB. were thirsty\nC. refreshment\nD. getting drunk\nE. celebrating\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The hikers stopped to have a drink, simply put they what?\nA. had a party\nB. were thirsty\nC. refreshment\nD. getting drunk\nE. celebrating\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The hikers stopped to have a drink, simply put they what?\nA. had a party\nB. were thirsty\nC. refreshment\nD. getting drunk\nE. celebrating\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The hikers stopped to have a drink, simply put they what?\nA. had a party\nB. were thirsty\nC. refreshment\nD. getting drunk\nE. celebrating\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.230768203735352", "False"]], [["-1.4807679653167725", "False"]], [["-3.4807679653167725", "False"]], [["-7.480768203735352", "False"]], [["-8.730768203735352", "False"]]], "filtered_resps": [["-6.230768203735352", "False"], ["-1.4807679653167725", "False"], ["-3.4807679653167725", "False"], ["-7.480768203735352", "False"], ["-8.730768203735352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "97dffc4a5a9cbdb37afd7ae6c5895e9ac17d562a72479bc221f974a526458c13", "prompt_hash": "7c96b4a5437a988eba32af1ed48055eefd6a463adb00eff50bda3740f54e8459", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 126, "doc": {"id": "f898eb5b789d2dc6804edba269f051f0", "question": "When you get up in the morning before you begin work you should do what?", "question_concept": "begin work", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["apply for job", "sleep", "concentrate", "shower", "just do"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When you get up in the morning before you begin work you should do what?\nA. apply for job\nB. sleep\nC. concentrate\nD. shower\nE. just do\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you get up in the morning before you begin work you should do what?\nA. apply for job\nB. sleep\nC. concentrate\nD. shower\nE. just do\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you get up in the morning before you begin work you should do what?\nA. apply for job\nB. sleep\nC. concentrate\nD. shower\nE. just do\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you get up in the morning before you begin work you should do what?\nA. apply for job\nB. sleep\nC. concentrate\nD. shower\nE. just do\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you get up in the morning before you begin work you should do what?\nA. apply for job\nB. sleep\nC. concentrate\nD. shower\nE. just do\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.66716194152832", "False"]], [["-2.9171619415283203", "False"]], [["-5.41716194152832", "False"]], [["-1.9171618223190308", "False"]], [["-4.91716194152832", "False"]]], "filtered_resps": [["-4.66716194152832", "False"], ["-2.9171619415283203", "False"], ["-5.41716194152832", "False"], ["-1.9171618223190308", "False"], ["-4.91716194152832", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b97bc0034250bd760811bc0bd96caf0971b0afab2300cdf9dc2c64095555cfb0", "prompt_hash": "075f940f0aca546fee8292378d57bf1914e6525ea146b498197d8767365c7542", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 127, "doc": {"id": "7ed7379fc51fd35a47be022f6c56ce51", "question": "The kitten had nothing to dig it's claws into, so when it tried to stop it slid across what?", "question_concept": "kitten", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["living room", "floor", "warm place", "carpet", "farmhouse"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The kitten had nothing to dig it's claws into, so when it tried to stop it slid across what?\nA. living room\nB. floor\nC. warm place\nD. carpet\nE. farmhouse\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The kitten had nothing to dig it's claws into, so when it tried to stop it slid across what?\nA. living room\nB. floor\nC. warm place\nD. carpet\nE. farmhouse\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The kitten had nothing to dig it's claws into, so when it tried to stop it slid across what?\nA. living room\nB. floor\nC. warm place\nD. carpet\nE. farmhouse\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The kitten had nothing to dig it's claws into, so when it tried to stop it slid across what?\nA. living room\nB. floor\nC. warm place\nD. carpet\nE. farmhouse\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The kitten had nothing to dig it's claws into, so when it tried to stop it slid across what?\nA. living room\nB. floor\nC. warm place\nD. carpet\nE. farmhouse\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.299929618835449", "False"]], [["-1.7999298572540283", "True"]], [["-7.049929618835449", "False"]], [["-3.0499298572540283", "False"]], [["-9.79992961883545", "False"]]], "filtered_resps": [["-4.299929618835449", "False"], ["-1.7999298572540283", "True"], ["-7.049929618835449", "False"], ["-3.0499298572540283", "False"], ["-9.79992961883545", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f85fdd71c4fd611a24d302071c69ea439b3643e6a28971d0bc50235dde83c3d4", "prompt_hash": "f476f85e2b24c0a108b0d0fe4592cc68dd279463f79b1bd72c9b9858a5d11c1d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 128, "doc": {"id": "15798a23ee6952fedd6d202064069126", "question": "If a person is trying to keep something in their hand what should they do?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["complete collection", "own house", "procrastinate", "explode", "have to hold"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If a person is trying to keep something in their hand what should they do?\nA. complete collection\nB. own house\nC. procrastinate\nD. explode\nE. have to hold\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a person is trying to keep something in their hand what should they do?\nA. complete collection\nB. own house\nC. procrastinate\nD. explode\nE. have to hold\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a person is trying to keep something in their hand what should they do?\nA. complete collection\nB. own house\nC. procrastinate\nD. explode\nE. have to hold\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a person is trying to keep something in their hand what should they do?\nA. complete collection\nB. own house\nC. procrastinate\nD. explode\nE. have to hold\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a person is trying to keep something in their hand what should they do?\nA. complete collection\nB. own house\nC. procrastinate\nD. explode\nE. have to hold\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.551067352294922", "False"]], [["-5.551067352294922", "False"]], [["-7.301067352294922", "False"]], [["-7.551067352294922", "False"]], [["-1.5510673522949219", "True"]]], "filtered_resps": [["-2.551067352294922", "False"], ["-5.551067352294922", "False"], ["-7.301067352294922", "False"], ["-7.551067352294922", "False"], ["-1.5510673522949219", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "205a4a028b22b1f0d533f2dd807038240fb8731b3ac267f16e435d34d9922c20", "prompt_hash": "eeababb721a837ec66e8e1f0a0e554cfc77c59f80a4e6b1a2b393ca1afaf17b0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 129, "doc": {"id": "273d0134e8ce53d4ebcf41ca7fde02af", "question": "Where could you find hundreds of thousands of home?", "question_concept": "home", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["field", "neighborhood", "star can", "city or town", "apartment building"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where could you find hundreds of thousands of home?\nA. field\nB. neighborhood\nC. star can\nD. city or town\nE. apartment building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could you find hundreds of thousands of home?\nA. field\nB. neighborhood\nC. star can\nD. city or town\nE. apartment building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could you find hundreds of thousands of home?\nA. field\nB. neighborhood\nC. star can\nD. city or town\nE. apartment building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could you find hundreds of thousands of home?\nA. field\nB. neighborhood\nC. star can\nD. city or town\nE. apartment building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could you find hundreds of thousands of home?\nA. field\nB. neighborhood\nC. star can\nD. city or town\nE. apartment building\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.251474618911743", "False"]], [["-4.501474380493164", "False"]], [["-6.001474380493164", "False"]], [["-1.2514746189117432", "True"]], [["-4.501474380493164", "False"]]], "filtered_resps": [["-3.251474618911743", "False"], ["-4.501474380493164", "False"], ["-6.001474380493164", "False"], ["-1.2514746189117432", "True"], ["-4.501474380493164", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6723791115b7ff3fc804d69e592af0fd9303d87096eff879cda6eeec5cb6b79b", "prompt_hash": "97c7fb54b3fcf9dfc30735326c607177963aba3d76da13e5dc430bf30320f1ef", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 130, "doc": {"id": "2f0931adc3d0d422d9ab6264395e89d8", "question": "Playing baseball is a lot like any other sport, there is always a risk of what?", "question_concept": "playing baseball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sore muscles", "errors", "happiness", "injury", "fun"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Playing baseball is a lot like any other sport, there is always a risk of what?\nA. sore muscles\nB. errors\nC. happiness\nD. injury\nE. fun\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Playing baseball is a lot like any other sport, there is always a risk of what?\nA. sore muscles\nB. errors\nC. happiness\nD. injury\nE. fun\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Playing baseball is a lot like any other sport, there is always a risk of what?\nA. sore muscles\nB. errors\nC. happiness\nD. injury\nE. fun\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Playing baseball is a lot like any other sport, there is always a risk of what?\nA. sore muscles\nB. errors\nC. happiness\nD. injury\nE. fun\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Playing baseball is a lot like any other sport, there is always a risk of what?\nA. sore muscles\nB. errors\nC. happiness\nD. injury\nE. fun\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.482561111450195", "False"]], [["-3.232560873031616", "False"]], [["-7.482561111450195", "False"]], [["-0.7325608730316162", "True"]], [["-8.732561111450195", "False"]]], "filtered_resps": [["-5.482561111450195", "False"], ["-3.232560873031616", "False"], ["-7.482561111450195", "False"], ["-0.7325608730316162", "True"], ["-8.732561111450195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "473666ba29a205584c78bb1a9e477e0df71a745193315431e65ab82c7ccb0879", "prompt_hash": "c36f749d0742cb34de79477a90cb4e55bd23fd4dcc8518403432c944df313806", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 131, "doc": {"id": "d00d3ba777cb3889a45799d72fca0a50", "question": "If I want to watch a movie without leaving my home what might I use?", "question_concept": "movie", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["drive in movie", "drive in movie", "television", "video store", "show"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If I want to watch a movie without leaving my home what might I use?\nA. drive in movie\nB. drive in movie\nC. television\nD. video store\nE. show\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I want to watch a movie without leaving my home what might I use?\nA. drive in movie\nB. drive in movie\nC. television\nD. video store\nE. show\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I want to watch a movie without leaving my home what might I use?\nA. drive in movie\nB. drive in movie\nC. television\nD. video store\nE. show\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I want to watch a movie without leaving my home what might I use?\nA. drive in movie\nB. drive in movie\nC. television\nD. video store\nE. show\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I want to watch a movie without leaving my home what might I use?\nA. drive in movie\nB. drive in movie\nC. television\nD. video store\nE. show\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.637659072875977", "False"]], [["-6.387659072875977", "False"]], [["-1.8876593112945557", "False"]], [["-7.387659072875977", "False"]], [["-3.8876593112945557", "False"]]], "filtered_resps": [["-4.637659072875977", "False"], ["-6.387659072875977", "False"], ["-1.8876593112945557", "False"], ["-7.387659072875977", "False"], ["-3.8876593112945557", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1fd65af7ab0378551f15a79acddd2f8f0a36c79bd3b70a2264a368f0a7343d8e", "prompt_hash": "1505a2e44d9ede3c9f6547d6f2c76da5735349125c1481fcfee3533dfa9125f6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 132, "doc": {"id": "b1f36d1c8ab7e5a28783cb38e8709c27", "question": "The victim was to take stand today, they were going to do what?", "question_concept": "take stand", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["testify", "runaway", "witness", "tell truth", "go home"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The victim was to take stand today, they were going to do what?\nA. testify\nB. runaway\nC. witness\nD. tell truth\nE. go home\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The victim was to take stand today, they were going to do what?\nA. testify\nB. runaway\nC. witness\nD. tell truth\nE. go home\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The victim was to take stand today, they were going to do what?\nA. testify\nB. runaway\nC. witness\nD. tell truth\nE. go home\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The victim was to take stand today, they were going to do what?\nA. testify\nB. runaway\nC. witness\nD. tell truth\nE. go home\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The victim was to take stand today, they were going to do what?\nA. testify\nB. runaway\nC. witness\nD. tell truth\nE. go home\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7238990664482117", "True"]], [["-8.723898887634277", "False"]], [["-7.223898887634277", "False"]], [["-8.723898887634277", "False"]], [["-10.223898887634277", "False"]]], "filtered_resps": [["-0.7238990664482117", "True"], ["-8.723898887634277", "False"], ["-7.223898887634277", "False"], ["-8.723898887634277", "False"], ["-10.223898887634277", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "203d2cf8c204f3561aea13ea0c85193db498a382a7a7cd1f6588bff8c6298209", "prompt_hash": "cc61c9e22347d35db6317ec9f91be6cdf56ad5d58fbd125ea8e784535d5562d4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 133, "doc": {"id": "a5e76dd088aab4f89e2fe93f6de6e46d", "question": "What does a successful dog grooming session likely to make a owner feel?", "question_concept": "grooming", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cleanliness", "mistakes", "growth", "satisfaction", "late"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What does a successful dog grooming session likely to make a owner feel?\nA. cleanliness\nB. mistakes\nC. growth\nD. satisfaction\nE. late\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a successful dog grooming session likely to make a owner feel?\nA. cleanliness\nB. mistakes\nC. growth\nD. satisfaction\nE. late\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a successful dog grooming session likely to make a owner feel?\nA. cleanliness\nB. mistakes\nC. growth\nD. satisfaction\nE. late\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a successful dog grooming session likely to make a owner feel?\nA. cleanliness\nB. mistakes\nC. growth\nD. satisfaction\nE. late\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a successful dog grooming session likely to make a owner feel?\nA. cleanliness\nB. mistakes\nC. growth\nD. satisfaction\nE. late\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6951338052749634", "False"]], [["-7.695133686065674", "False"]], [["-6.195133686065674", "False"]], [["-1.1951338052749634", "True"]], [["-9.695134162902832", "False"]]], "filtered_resps": [["-1.6951338052749634", "False"], ["-7.695133686065674", "False"], ["-6.195133686065674", "False"], ["-1.1951338052749634", "True"], ["-9.695134162902832", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c4b99cb68111da7e10c5e066ff709ef79ef97b53f81cd11d3d2f7a43d0bfae3a", "prompt_hash": "06a1160f1cf4bccd89e9eb1d02bb5bc93a51b2e437283f28dcf1f408c196840d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 134, "doc": {"id": "ac6f0e24dd6203cda43e1089dcf081d6", "question": "The runner was in third place, but he pushed harder and thought he might be able to reach second.  What was beginning to do?", "question_concept": "runner", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["near finish line", "finish", "get tired", "gain ground", "trip over"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The runner was in third place, but he pushed harder and thought he might be able to reach second.  What was beginning to do?\nA. near finish line\nB. finish\nC. get tired\nD. gain ground\nE. trip over\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The runner was in third place, but he pushed harder and thought he might be able to reach second.  What was beginning to do?\nA. near finish line\nB. finish\nC. get tired\nD. gain ground\nE. trip over\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The runner was in third place, but he pushed harder and thought he might be able to reach second.  What was beginning to do?\nA. near finish line\nB. finish\nC. get tired\nD. gain ground\nE. trip over\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The runner was in third place, but he pushed harder and thought he might be able to reach second.  What was beginning to do?\nA. near finish line\nB. finish\nC. get tired\nD. gain ground\nE. trip over\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The runner was in third place, but he pushed harder and thought he might be able to reach second.  What was beginning to do?\nA. near finish line\nB. finish\nC. get tired\nD. gain ground\nE. trip over\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.579892635345459", "False"]], [["-7.329892635345459", "False"]], [["-7.329892635345459", "False"]], [["-1.079892635345459", "True"]], [["-11.079893112182617", "False"]]], "filtered_resps": [["-4.579892635345459", "False"], ["-7.329892635345459", "False"], ["-7.329892635345459", "False"], ["-1.079892635345459", "True"], ["-11.079893112182617", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5619dab29372fc6a4a798cb9b2ea5a06e5222b0921caf5915375c98ee28db8cf", "prompt_hash": "d3a0fcce5364ed3d15ed0dfddf72d3f0d490594bdcad08654ea6392d47666210", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 135, "doc": {"id": "1ab746bcd100ccf513055fe93c61010b", "question": "The tourist entered Mammoth cave, what state were they in?", "question_concept": "cave", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["west virginia", "kentucky", "rocky hills", "scotland", "canyon"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The tourist entered Mammoth cave, what state were they in?\nA. west virginia\nB. kentucky\nC. rocky hills\nD. scotland\nE. canyon\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The tourist entered Mammoth cave, what state were they in?\nA. west virginia\nB. kentucky\nC. rocky hills\nD. scotland\nE. canyon\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The tourist entered Mammoth cave, what state were they in?\nA. west virginia\nB. kentucky\nC. rocky hills\nD. scotland\nE. canyon\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The tourist entered Mammoth cave, what state were they in?\nA. west virginia\nB. kentucky\nC. rocky hills\nD. scotland\nE. canyon\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The tourist entered Mammoth cave, what state were they in?\nA. west virginia\nB. kentucky\nC. rocky hills\nD. scotland\nE. canyon\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9383230209350586", "False"]], [["-1.1883231401443481", "True"]], [["-6.188323020935059", "False"]], [["-8.938323020935059", "False"]], [["-9.938323020935059", "False"]]], "filtered_resps": [["-2.9383230209350586", "False"], ["-1.1883231401443481", "True"], ["-6.188323020935059", "False"], ["-8.938323020935059", "False"], ["-9.938323020935059", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4a2a8b8298abe126d30f51d7a26e71200511d47957c0d1f3c735793ffaedced2", "prompt_hash": "329a2ddf38a2b775a9c29a4d7c047727abcd0d1fa6d98abcd296f2909b1438e0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 136, "doc": {"id": "af836abc58e0daf36df1d8d6830b70c5", "question": "What does someone typically feel when applying for a job?", "question_concept": "applying for job", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["horror", "anxiety and fear", "rejection", "increased workload", "being employed"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What does someone typically feel when applying for a job?\nA. horror\nB. anxiety and fear\nC. rejection\nD. increased workload\nE. being employed\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does someone typically feel when applying for a job?\nA. horror\nB. anxiety and fear\nC. rejection\nD. increased workload\nE. being employed\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does someone typically feel when applying for a job?\nA. horror\nB. anxiety and fear\nC. rejection\nD. increased workload\nE. being employed\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does someone typically feel when applying for a job?\nA. horror\nB. anxiety and fear\nC. rejection\nD. increased workload\nE. being employed\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does someone typically feel when applying for a job?\nA. horror\nB. anxiety and fear\nC. rejection\nD. increased workload\nE. being employed\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.231235027313232", "False"]], [["-1.4812349081039429", "False"]], [["-6.731235027313232", "False"]], [["-7.731235027313232", "False"]], [["-6.981235027313232", "False"]]], "filtered_resps": [["-5.231235027313232", "False"], ["-1.4812349081039429", "False"], ["-6.731235027313232", "False"], ["-7.731235027313232", "False"], ["-6.981235027313232", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "696ef7050b926de8acba1fdb7ccdb278a9f6b03fe74f4053a999753e6bc9c153", "prompt_hash": "9f7e6247f931dcff9d44873eaf436e4f00f3d9327fc64413338917003bfb1126", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 137, "doc": {"id": "2ed66cfd206723a006b37599b516ad6e", "question": "He was on trial for obstructing justice, during which he made a questionable comment and was also found guilty of what?", "question_concept": "obstructing justice", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["prosecution", "getting hurt", "sweat", "steam", "committing perjury"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: He was on trial for obstructing justice, during which he made a questionable comment and was also found guilty of what?\nA. prosecution\nB. getting hurt\nC. sweat\nD. steam\nE. committing perjury\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was on trial for obstructing justice, during which he made a questionable comment and was also found guilty of what?\nA. prosecution\nB. getting hurt\nC. sweat\nD. steam\nE. committing perjury\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was on trial for obstructing justice, during which he made a questionable comment and was also found guilty of what?\nA. prosecution\nB. getting hurt\nC. sweat\nD. steam\nE. committing perjury\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was on trial for obstructing justice, during which he made a questionable comment and was also found guilty of what?\nA. prosecution\nB. getting hurt\nC. sweat\nD. steam\nE. committing perjury\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was on trial for obstructing justice, during which he made a questionable comment and was also found guilty of what?\nA. prosecution\nB. getting hurt\nC. sweat\nD. steam\nE. committing perjury\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.39669942855835", "False"]], [["-7.89669942855835", "False"]], [["-6.14669942855835", "False"]], [["-6.39669942855835", "False"]], [["-0.8966993093490601", "True"]]], "filtered_resps": [["-4.39669942855835", "False"], ["-7.89669942855835", "False"], ["-6.14669942855835", "False"], ["-6.39669942855835", "False"], ["-0.8966993093490601", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "75870561359255b9bde8955d5a899877a7f6637f2b2466c164188e4dfa99d290", "prompt_hash": "239f61c4ac466ccf36b25e21faf16765fef0519086c071900a2104f24bd57fe7", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 138, "doc": {"id": "e89a2762d578cb7bc2cc0a5b2a16d933", "question": "What kind of feelings does buying presents for others create?", "question_concept": "buy presents for others", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tears", "please", "like", "thank", "make happy"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of feelings does buying presents for others create?\nA. tears\nB. please\nC. like\nD. thank\nE. make happy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of feelings does buying presents for others create?\nA. tears\nB. please\nC. like\nD. thank\nE. make happy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of feelings does buying presents for others create?\nA. tears\nB. please\nC. like\nD. thank\nE. make happy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of feelings does buying presents for others create?\nA. tears\nB. please\nC. like\nD. thank\nE. make happy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of feelings does buying presents for others create?\nA. tears\nB. please\nC. like\nD. thank\nE. make happy\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.236798286437988", "False"]], [["-2.2367982864379883", "False"]], [["-4.986798286437988", "False"]], [["-6.736798286437988", "False"]], [["-1.4867982864379883", "True"]]], "filtered_resps": [["-5.236798286437988", "False"], ["-2.2367982864379883", "False"], ["-4.986798286437988", "False"], ["-6.736798286437988", "False"], ["-1.4867982864379883", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c2033bf752785614e689e26a4ec3acbcf7f3ee5adff214e6e4f7dffb40483c8a", "prompt_hash": "3a8e45e32a0f12b6a635362b89a10dc80cbfc836903293fbb8c5b117ef1d9415", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 139, "doc": {"id": "43cec0fff43a976fade9112d02b66021", "question": "What green area is a marmot likely to be found in?", "question_concept": "marmot", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["countryside", "great plains", "encyclopedia", "jungle", "north america"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What green area is a marmot likely to be found in?\nA. countryside\nB. great plains\nC. encyclopedia\nD. jungle\nE. north america\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What green area is a marmot likely to be found in?\nA. countryside\nB. great plains\nC. encyclopedia\nD. jungle\nE. north america\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What green area is a marmot likely to be found in?\nA. countryside\nB. great plains\nC. encyclopedia\nD. jungle\nE. north america\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What green area is a marmot likely to be found in?\nA. countryside\nB. great plains\nC. encyclopedia\nD. jungle\nE. north america\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What green area is a marmot likely to be found in?\nA. countryside\nB. great plains\nC. encyclopedia\nD. jungle\nE. north america\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1139016151428223", "True"]], [["-5.363901615142822", "False"]], [["-6.113901615142822", "False"]], [["-6.363901615142822", "False"]], [["-6.363901615142822", "False"]]], "filtered_resps": [["-1.1139016151428223", "True"], ["-5.363901615142822", "False"], ["-6.113901615142822", "False"], ["-6.363901615142822", "False"], ["-6.363901615142822", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5cfac582d6b3a98c9461b0a0f3b2a90aef215f9211d4ba8e07d93bdebcf0f653", "prompt_hash": "72ea4c6a09fbf771406c28e882ec120fde9c592bdf76da0aeeb64f5f46200f39", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 140, "doc": {"id": "30e66db11e0257a14a17108b90cd69fb", "question": "Jan tested the current, and noticed that it was high.  He thought that the wires might have too much what?", "question_concept": "current", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["later", "updated", "still", "resistance", "now"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Jan tested the current, and noticed that it was high.  He thought that the wires might have too much what?\nA. later\nB. updated\nC. still\nD. resistance\nE. now\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Jan tested the current, and noticed that it was high.  He thought that the wires might have too much what?\nA. later\nB. updated\nC. still\nD. resistance\nE. now\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Jan tested the current, and noticed that it was high.  He thought that the wires might have too much what?\nA. later\nB. updated\nC. still\nD. resistance\nE. now\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Jan tested the current, and noticed that it was high.  He thought that the wires might have too much what?\nA. later\nB. updated\nC. still\nD. resistance\nE. now\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Jan tested the current, and noticed that it was high.  He thought that the wires might have too much what?\nA. later\nB. updated\nC. still\nD. resistance\nE. now\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.592877388000488", "False"]], [["-6.092877388000488", "False"]], [["-5.842877388000488", "False"]], [["-1.3428771495819092", "True"]], [["-9.842877388000488", "False"]]], "filtered_resps": [["-5.592877388000488", "False"], ["-6.092877388000488", "False"], ["-5.842877388000488", "False"], ["-1.3428771495819092", "True"], ["-9.842877388000488", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dc40771f29d351932a43746905da6080d5385e2eb3a16094c5940a9f11ec2a00", "prompt_hash": "7b96498ad64e4431dcfb54f63d692737cdaa48a8d3138baa8d5360b96cf0edbe", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 141, "doc": {"id": "f21ef67b31bd36a3174b6b4c7b4bbc7b", "question": "What does a kindergarten teacher do before nap time?", "question_concept": "teacher", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lower expectations", "encourage", "fear", "time test", "tell story"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What does a kindergarten teacher do before nap time?\nA. lower expectations\nB. encourage\nC. fear\nD. time test\nE. tell story\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a kindergarten teacher do before nap time?\nA. lower expectations\nB. encourage\nC. fear\nD. time test\nE. tell story\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a kindergarten teacher do before nap time?\nA. lower expectations\nB. encourage\nC. fear\nD. time test\nE. tell story\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a kindergarten teacher do before nap time?\nA. lower expectations\nB. encourage\nC. fear\nD. time test\nE. tell story\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a kindergarten teacher do before nap time?\nA. lower expectations\nB. encourage\nC. fear\nD. time test\nE. tell story\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1773414611816406", "False"]], [["-2.1773414611816406", "False"]], [["-6.927341461181641", "False"]], [["-8.17734146118164", "False"]], [["-1.927341341972351", "False"]]], "filtered_resps": [["-2.1773414611816406", "False"], ["-2.1773414611816406", "False"], ["-6.927341461181641", "False"], ["-8.17734146118164", "False"], ["-1.927341341972351", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "12d830fbf533044380346ad4ad7adea32627671804cfbc5b94d47e0c08793f58", "prompt_hash": "51923264b6ec84a3a574c5a2d8f203ea010591376cf93363ee2b1d1bd6d20c1b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 142, "doc": {"id": "e476e2c8c278eaecfe1a8b884b6aeb8e", "question": "Sam was a stranger.  Even so, Mark treated him like what?", "question_concept": "stranger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["friend", "family", "known person", "park", "outsider"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Sam was a stranger.  Even so, Mark treated him like what?\nA. friend\nB. family\nC. known person\nD. park\nE. outsider\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sam was a stranger.  Even so, Mark treated him like what?\nA. friend\nB. family\nC. known person\nD. park\nE. outsider\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sam was a stranger.  Even so, Mark treated him like what?\nA. friend\nB. family\nC. known person\nD. park\nE. outsider\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sam was a stranger.  Even so, Mark treated him like what?\nA. friend\nB. family\nC. known person\nD. park\nE. outsider\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sam was a stranger.  Even so, Mark treated him like what?\nA. friend\nB. family\nC. known person\nD. park\nE. outsider\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0879642963409424", "False"]], [["-1.8379642963409424", "False"]], [["-7.3379645347595215", "False"]], [["-7.5879645347595215", "False"]], [["-7.8379645347595215", "False"]]], "filtered_resps": [["-3.0879642963409424", "False"], ["-1.8379642963409424", "False"], ["-7.3379645347595215", "False"], ["-7.5879645347595215", "False"], ["-7.8379645347595215", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4b60b3f0d85da54477559d0a3b514b970fd596852aa6a0b9051e13a5d10f763e", "prompt_hash": "91ee5a78abec4713ca1b71b755534a9438574dce23de9a02fb3a4da6f2aad35c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 143, "doc": {"id": "191e3c676f05a11d6b2565d8c27d2001", "question": "Bob's only light source was a small bulb.  There were four walls, if there was a door he couldn't see it.  What was Bob in?", "question_concept": "light source", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["closed room", "sky", "dard", "his grave", "house"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Bob's only light source was a small bulb.  There were four walls, if there was a door he couldn't see it.  What was Bob in?\nA. closed room\nB. sky\nC. dard\nD. his grave\nE. house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Bob's only light source was a small bulb.  There were four walls, if there was a door he couldn't see it.  What was Bob in?\nA. closed room\nB. sky\nC. dard\nD. his grave\nE. house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Bob's only light source was a small bulb.  There were four walls, if there was a door he couldn't see it.  What was Bob in?\nA. closed room\nB. sky\nC. dard\nD. his grave\nE. house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Bob's only light source was a small bulb.  There were four walls, if there was a door he couldn't see it.  What was Bob in?\nA. closed room\nB. sky\nC. dard\nD. his grave\nE. house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Bob's only light source was a small bulb.  There were four walls, if there was a door he couldn't see it.  What was Bob in?\nA. closed room\nB. sky\nC. dard\nD. his grave\nE. house\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6073087453842163", "True"]], [["-3.857308864593506", "False"]], [["-6.607308864593506", "False"]], [["-5.857308864593506", "False"]], [["-2.857308864593506", "False"]]], "filtered_resps": [["-1.6073087453842163", "True"], ["-3.857308864593506", "False"], ["-6.607308864593506", "False"], ["-5.857308864593506", "False"], ["-2.857308864593506", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "61325dd7cea99f820d64904628df8bcf71a9f90f28d340057c501b853c97320a", "prompt_hash": "43155033e845e9c66d087dc1920a2d705673cd70ebf7669c4c65537c9d2a5298", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 144, "doc": {"id": "99098375c7b651d524eebac72e358238", "question": "James thought of criminal justice like a computer program.  It need to work right.   What ideas might James not like?", "question_concept": "computer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["manual", "process information", "power down", "control model", "reason exists"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: James thought of criminal justice like a computer program.  It need to work right.   What ideas might James not like?\nA. manual\nB. process information\nC. power down\nD. control model\nE. reason exists\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James thought of criminal justice like a computer program.  It need to work right.   What ideas might James not like?\nA. manual\nB. process information\nC. power down\nD. control model\nE. reason exists\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James thought of criminal justice like a computer program.  It need to work right.   What ideas might James not like?\nA. manual\nB. process information\nC. power down\nD. control model\nE. reason exists\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James thought of criminal justice like a computer program.  It need to work right.   What ideas might James not like?\nA. manual\nB. process information\nC. power down\nD. control model\nE. reason exists\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James thought of criminal justice like a computer program.  It need to work right.   What ideas might James not like?\nA. manual\nB. process information\nC. power down\nD. control model\nE. reason exists\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6077030897140503", "False"]], [["-4.85770320892334", "False"]], [["-4.10770320892334", "False"]], [["-4.10770320892334", "False"]], [["-7.60770320892334", "False"]]], "filtered_resps": [["-1.6077030897140503", "False"], ["-4.85770320892334", "False"], ["-4.10770320892334", "False"], ["-4.10770320892334", "False"], ["-7.60770320892334", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e0d09b77c4713ed10aab1ff3f3e9666b0c453bec2392ddb99a99781a288e2477", "prompt_hash": "92b1e051e3c786600bf05ef767f37432f816fd8a739824aa304a0c1b65299bc5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 145, "doc": {"id": "290fac9f881a83d8bfb34355f8e71044", "question": "With the card slot lit up he knew how to get started finding his balance with what?", "question_concept": "card slot", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["slot machine", "ticket machine", "bank machine", "telephone", "automated teller"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: With the card slot lit up he knew how to get started finding his balance with what?\nA. slot machine\nB. ticket machine\nC. bank machine\nD. telephone\nE. automated teller\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: With the card slot lit up he knew how to get started finding his balance with what?\nA. slot machine\nB. ticket machine\nC. bank machine\nD. telephone\nE. automated teller\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: With the card slot lit up he knew how to get started finding his balance with what?\nA. slot machine\nB. ticket machine\nC. bank machine\nD. telephone\nE. automated teller\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: With the card slot lit up he knew how to get started finding his balance with what?\nA. slot machine\nB. ticket machine\nC. bank machine\nD. telephone\nE. automated teller\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: With the card slot lit up he knew how to get started finding his balance with what?\nA. slot machine\nB. ticket machine\nC. bank machine\nD. telephone\nE. automated teller\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.357083797454834", "False"]], [["-3.857083797454834", "False"]], [["-3.607083797454834", "False"]], [["-5.857083797454834", "False"]], [["-2.357083797454834", "False"]]], "filtered_resps": [["-2.357083797454834", "False"], ["-3.857083797454834", "False"], ["-3.607083797454834", "False"], ["-5.857083797454834", "False"], ["-2.357083797454834", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0d013cae09dbdc6f1cbcbc302aeba23f9b9fab57a804060b568c7916a77c4ee3", "prompt_hash": "35e83c97915f89a30aeaa1fb09e726d51a8e4df2ecb0977b8f669c6684343169", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 146, "doc": {"id": "6c36226b23377a0dd0188bf56840e22a", "question": "To play sports professionally you must do what very often?", "question_concept": "play sports", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wash your clothes", "get in shape", "practice", "take off uniform", "stretch"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: To play sports professionally you must do what very often?\nA. wash your clothes\nB. get in shape\nC. practice\nD. take off uniform\nE. stretch\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: To play sports professionally you must do what very often?\nA. wash your clothes\nB. get in shape\nC. practice\nD. take off uniform\nE. stretch\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: To play sports professionally you must do what very often?\nA. wash your clothes\nB. get in shape\nC. practice\nD. take off uniform\nE. stretch\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: To play sports professionally you must do what very often?\nA. wash your clothes\nB. get in shape\nC. practice\nD. take off uniform\nE. stretch\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: To play sports professionally you must do what very often?\nA. wash your clothes\nB. get in shape\nC. practice\nD. take off uniform\nE. stretch\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.490368843078613", "False"]], [["-2.7403688430786133", "False"]], [["-1.7403688430786133", "False"]], [["-7.990368843078613", "False"]], [["-8.240368843078613", "False"]]], "filtered_resps": [["-6.490368843078613", "False"], ["-2.7403688430786133", "False"], ["-1.7403688430786133", "False"], ["-7.990368843078613", "False"], ["-8.240368843078613", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3472bbcb8ac311e9b516a5396c428f7240d7d467b6809147f6052f7b3e25cb62", "prompt_hash": "a3ab044e965f41c20bcd250e5bfb251cb84a3f157fc869babb634b7cb35d4d3f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 147, "doc": {"id": "aa5aa36557a5fbb93391506182f1025c", "question": "Some people prefer releasing energy through work while others prefer to release it through what?", "question_concept": "releasing energy", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["motion", "stretch", "exercise", "movement", "muscles"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Some people prefer releasing energy through work while others prefer to release it through what?\nA. motion\nB. stretch\nC. exercise\nD. movement\nE. muscles\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Some people prefer releasing energy through work while others prefer to release it through what?\nA. motion\nB. stretch\nC. exercise\nD. movement\nE. muscles\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Some people prefer releasing energy through work while others prefer to release it through what?\nA. motion\nB. stretch\nC. exercise\nD. movement\nE. muscles\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Some people prefer releasing energy through work while others prefer to release it through what?\nA. motion\nB. stretch\nC. exercise\nD. movement\nE. muscles\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Some people prefer releasing energy through work while others prefer to release it through what?\nA. motion\nB. stretch\nC. exercise\nD. movement\nE. muscles\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3791122436523438", "False"]], [["-3.8791122436523438", "False"]], [["-1.8791121244430542", "True"]], [["-5.129112243652344", "False"]], [["-4.379112243652344", "False"]]], "filtered_resps": [["-3.3791122436523438", "False"], ["-3.8791122436523438", "False"], ["-1.8791121244430542", "True"], ["-5.129112243652344", "False"], ["-4.379112243652344", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65cc52656e5ef79596f04f255e4690d5481567aaafd51fbfb359c4fb0ab5a1e4", "prompt_hash": "1efa9d6086e71bc41e56873542cdcbb4414841f312b6e8ac20dc802a46eb353c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 148, "doc": {"id": "a38df3e750b1edd30f905e17af803c61", "question": "What will a person going for a jog likely be wearing?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["grope", "acknowledgment", "comfortable clothes", "ipod", "passionate kisses"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What will a person going for a jog likely be wearing?\nA. grope\nB. acknowledgment\nC. comfortable clothes\nD. ipod\nE. passionate kisses\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will a person going for a jog likely be wearing?\nA. grope\nB. acknowledgment\nC. comfortable clothes\nD. ipod\nE. passionate kisses\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will a person going for a jog likely be wearing?\nA. grope\nB. acknowledgment\nC. comfortable clothes\nD. ipod\nE. passionate kisses\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will a person going for a jog likely be wearing?\nA. grope\nB. acknowledgment\nC. comfortable clothes\nD. ipod\nE. passionate kisses\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will a person going for a jog likely be wearing?\nA. grope\nB. acknowledgment\nC. comfortable clothes\nD. ipod\nE. passionate kisses\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.651505470275879", "False"]], [["-8.401505470275879", "False"]], [["-0.6515054702758789", "True"]], [["-8.401505470275879", "False"]], [["-10.901505470275879", "False"]]], "filtered_resps": [["-2.651505470275879", "False"], ["-8.401505470275879", "False"], ["-0.6515054702758789", "True"], ["-8.401505470275879", "False"], ["-10.901505470275879", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "138dde592406d5771e42a078898246485527d9b4d76eec56e5b4b12c9abbf3a6", "prompt_hash": "5b5ebbbb0661dd68929acbc512e3efaea4394c72019dfba275e8283e75b01a38", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 149, "doc": {"id": "dba51270f789c75a2e38a5201b124d99", "question": "The child pretended he was reading newspaper, he couldn't actually do it without what?", "question_concept": "reading newspaper", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["patience", "falling down", "literacy", "buying", "knowing how to read"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The child pretended he was reading newspaper, he couldn't actually do it without what?\nA. patience\nB. falling down\nC. literacy\nD. buying\nE. knowing how to read\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The child pretended he was reading newspaper, he couldn't actually do it without what?\nA. patience\nB. falling down\nC. literacy\nD. buying\nE. knowing how to read\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The child pretended he was reading newspaper, he couldn't actually do it without what?\nA. patience\nB. falling down\nC. literacy\nD. buying\nE. knowing how to read\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The child pretended he was reading newspaper, he couldn't actually do it without what?\nA. patience\nB. falling down\nC. literacy\nD. buying\nE. knowing how to read\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The child pretended he was reading newspaper, he couldn't actually do it without what?\nA. patience\nB. falling down\nC. literacy\nD. buying\nE. knowing how to read\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.077970504760742", "False"]], [["-7.827970504760742", "False"]], [["-3.827970504760742", "False"]], [["-10.327970504760742", "False"]], [["-1.8279703855514526", "False"]]], "filtered_resps": [["-4.077970504760742", "False"], ["-7.827970504760742", "False"], ["-3.827970504760742", "False"], ["-10.327970504760742", "False"], ["-1.8279703855514526", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "70fb81b22c556303488f16c06d08fd1d85beedf2e04c0f4e1500dbdfd59b027e", "prompt_hash": "2c1fff9041b3f756076b59efeaa50ca72f68d1bb9ed06031fe364258c053e7c8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 150, "doc": {"id": "1be8ec824eb0c7218b6bc160fd191428", "question": "Jenny enjoyed helping people.  It brought her a great deal of what?", "question_concept": "helping", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["satisfaction", "complications", "train", "feel good about yourself", "enjoyment"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Jenny enjoyed helping people.  It brought her a great deal of what?\nA. satisfaction\nB. complications\nC. train\nD. feel good about yourself\nE. enjoyment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Jenny enjoyed helping people.  It brought her a great deal of what?\nA. satisfaction\nB. complications\nC. train\nD. feel good about yourself\nE. enjoyment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Jenny enjoyed helping people.  It brought her a great deal of what?\nA. satisfaction\nB. complications\nC. train\nD. feel good about yourself\nE. enjoyment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Jenny enjoyed helping people.  It brought her a great deal of what?\nA. satisfaction\nB. complications\nC. train\nD. feel good about yourself\nE. enjoyment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Jenny enjoyed helping people.  It brought her a great deal of what?\nA. satisfaction\nB. complications\nC. train\nD. feel good about yourself\nE. enjoyment\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0538561344146729", "True"]], [["-5.553855895996094", "False"]], [["-7.053855895996094", "False"]], [["-2.803856134414673", "False"]], [["-3.303856134414673", "False"]]], "filtered_resps": [["-1.0538561344146729", "True"], ["-5.553855895996094", "False"], ["-7.053855895996094", "False"], ["-2.803856134414673", "False"], ["-3.303856134414673", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b296a7c21ddb66c14202ab338463e9e21ade3c53b9831e8de987a6c02b7da6ec", "prompt_hash": "8ab876512ada0a31bb381525037e0ae445c310672fdd1e990d1a311d1fb71195", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 151, "doc": {"id": "0e80f2afe5c4f652e8720b52d7c06c87", "question": "What might someone believe in if they are cleaning clothes?", "question_concept": "cleaning clothes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feminism", "sanitation", "ruined", "wrinkles", "buttons to fall off"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What might someone believe in if they are cleaning clothes?\nA. feminism\nB. sanitation\nC. ruined\nD. wrinkles\nE. buttons to fall off\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might someone believe in if they are cleaning clothes?\nA. feminism\nB. sanitation\nC. ruined\nD. wrinkles\nE. buttons to fall off\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might someone believe in if they are cleaning clothes?\nA. feminism\nB. sanitation\nC. ruined\nD. wrinkles\nE. buttons to fall off\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might someone believe in if they are cleaning clothes?\nA. feminism\nB. sanitation\nC. ruined\nD. wrinkles\nE. buttons to fall off\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might someone believe in if they are cleaning clothes?\nA. feminism\nB. sanitation\nC. ruined\nD. wrinkles\nE. buttons to fall off\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.1320695877075195", "False"]], [["-0.8820698261260986", "True"]], [["-4.3820695877075195", "False"]], [["-5.3820695877075195", "False"]], [["-7.1320695877075195", "False"]]], "filtered_resps": [["-4.1320695877075195", "False"], ["-0.8820698261260986", "True"], ["-4.3820695877075195", "False"], ["-5.3820695877075195", "False"], ["-7.1320695877075195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f712a42b0041c84e2a28d6fec105180b6d5e652eb72fdb3f6b4daa9633f4e039", "prompt_hash": "27d2eaadb6e6c8702c6223d77ba07d325575985feef959fe81c7b514449ae77f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 152, "doc": {"id": "b67971747e95ba425a5b81e0ba8d0b28", "question": "Where would you find a basement that can be accessed with an elevator?", "question_concept": "basement", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat cake", "closet", "church", "office building", "own house"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find a basement that can be accessed with an elevator?\nA. eat cake\nB. closet\nC. church\nD. office building\nE. own house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find a basement that can be accessed with an elevator?\nA. eat cake\nB. closet\nC. church\nD. office building\nE. own house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find a basement that can be accessed with an elevator?\nA. eat cake\nB. closet\nC. church\nD. office building\nE. own house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find a basement that can be accessed with an elevator?\nA. eat cake\nB. closet\nC. church\nD. office building\nE. own house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find a basement that can be accessed with an elevator?\nA. eat cake\nB. closet\nC. church\nD. office building\nE. own house\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7668724060058594", "False"]], [["-4.766872406005859", "False"]], [["-5.016872406005859", "False"]], [["-1.0168724060058594", "True"]], [["-3.7668724060058594", "False"]]], "filtered_resps": [["-2.7668724060058594", "False"], ["-4.766872406005859", "False"], ["-5.016872406005859", "False"], ["-1.0168724060058594", "True"], ["-3.7668724060058594", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9d0154ccf9d07dad8f41daad0bc78ee3c8062dd5646f625e55392d4b22d5c03f", "prompt_hash": "ad2bf5502b2443f0adfc73fa70c1ba65aed5c5a5bf343139042260c1d24e8615", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 153, "doc": {"id": "fcd39cfa321728fea069a6ae4285b06f", "question": "In order to learn to program from another person you can do what?", "question_concept": "program", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["learn how to", "have a friend", "knowledge", "take class", "have computer"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: In order to learn to program from another person you can do what?\nA. learn how to\nB. have a friend\nC. knowledge\nD. take class\nE. have computer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: In order to learn to program from another person you can do what?\nA. learn how to\nB. have a friend\nC. knowledge\nD. take class\nE. have computer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: In order to learn to program from another person you can do what?\nA. learn how to\nB. have a friend\nC. knowledge\nD. take class\nE. have computer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: In order to learn to program from another person you can do what?\nA. learn how to\nB. have a friend\nC. knowledge\nD. take class\nE. have computer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: In order to learn to program from another person you can do what?\nA. learn how to\nB. have a friend\nC. knowledge\nD. take class\nE. have computer\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.0584583282470703", "True"]], [["-3.0584583282470703", "False"]], [["-4.55845832824707", "False"]], [["-2.3084583282470703", "False"]], [["-6.80845832824707", "False"]]], "filtered_resps": [["-2.0584583282470703", "True"], ["-3.0584583282470703", "False"], ["-4.55845832824707", "False"], ["-2.3084583282470703", "False"], ["-6.80845832824707", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8968b31074de58c4392e576ada2b30687a533ca7f5431122f9627e1d0a3f4666", "prompt_hash": "f4c230a288b32df03ebdb4bfde9d6803dbb7349c9d5073ff92854b7a5a02885d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 154, "doc": {"id": "cb6766fb25daee911fc8e9816b98938c", "question": "He was at the gym trying to build muscle, what is it called that he is trying to build muscle on?", "question_concept": "muscle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["body of animal", "arm", "bodybuilder", "body of dog", "human body"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: He was at the gym trying to build muscle, what is it called that he is trying to build muscle on?\nA. body of animal\nB. arm\nC. bodybuilder\nD. body of dog\nE. human body\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was at the gym trying to build muscle, what is it called that he is trying to build muscle on?\nA. body of animal\nB. arm\nC. bodybuilder\nD. body of dog\nE. human body\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was at the gym trying to build muscle, what is it called that he is trying to build muscle on?\nA. body of animal\nB. arm\nC. bodybuilder\nD. body of dog\nE. human body\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was at the gym trying to build muscle, what is it called that he is trying to build muscle on?\nA. body of animal\nB. arm\nC. bodybuilder\nD. body of dog\nE. human body\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was at the gym trying to build muscle, what is it called that he is trying to build muscle on?\nA. body of animal\nB. arm\nC. bodybuilder\nD. body of dog\nE. human body\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.167785167694092", "False"]], [["-5.417785167694092", "False"]], [["-6.667785167694092", "False"]], [["-10.167784690856934", "False"]], [["-1.4177849292755127", "False"]]], "filtered_resps": [["-5.167785167694092", "False"], ["-5.417785167694092", "False"], ["-6.667785167694092", "False"], ["-10.167784690856934", "False"], ["-1.4177849292755127", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "065a2e9c107362e70809289a8b4276ef09e0ef456245d55c987a05d8ca5042a9", "prompt_hash": "74a7fd03df622805b3021973bf81c52a7332cd3805478f4457741c940404c13a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 155, "doc": {"id": "54231f875bb7fe4d3e4afb6eae64387c", "question": "What part of plants is pretty?", "question_concept": "plants", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dirt", "no neurons in", "millions of cells", "flowers on", "roots"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What part of plants is pretty?\nA. dirt\nB. no neurons in\nC. millions of cells\nD. flowers on\nE. roots\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What part of plants is pretty?\nA. dirt\nB. no neurons in\nC. millions of cells\nD. flowers on\nE. roots\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What part of plants is pretty?\nA. dirt\nB. no neurons in\nC. millions of cells\nD. flowers on\nE. roots\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What part of plants is pretty?\nA. dirt\nB. no neurons in\nC. millions of cells\nD. flowers on\nE. roots\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What part of plants is pretty?\nA. dirt\nB. no neurons in\nC. millions of cells\nD. flowers on\nE. roots\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.085702896118164", "False"]], [["-4.835702896118164", "False"]], [["-3.585702896118164", "False"]], [["-1.3357027769088745", "True"]], [["-6.835702896118164", "False"]]], "filtered_resps": [["-4.085702896118164", "False"], ["-4.835702896118164", "False"], ["-3.585702896118164", "False"], ["-1.3357027769088745", "True"], ["-6.835702896118164", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "96e831fd9a4a4ec4018a5acaa3424f1bb355e8f929f2604b9468cebc30473e7c", "prompt_hash": "f1285a0388d371c570636c94e92abf492f14a512755eef350c24420b8b9abfb7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 156, "doc": {"id": "7d7f7d7a8ae3b20ca9fc0da6efe467b4", "question": "The man was going fishing instead of work, what is he seeking?", "question_concept": "going fishing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["food", "relaxation", "killing", "missing morning cartoons", "boredom"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The man was going fishing instead of work, what is he seeking?\nA. food\nB. relaxation\nC. killing\nD. missing morning cartoons\nE. boredom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man was going fishing instead of work, what is he seeking?\nA. food\nB. relaxation\nC. killing\nD. missing morning cartoons\nE. boredom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man was going fishing instead of work, what is he seeking?\nA. food\nB. relaxation\nC. killing\nD. missing morning cartoons\nE. boredom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man was going fishing instead of work, what is he seeking?\nA. food\nB. relaxation\nC. killing\nD. missing morning cartoons\nE. boredom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man was going fishing instead of work, what is he seeking?\nA. food\nB. relaxation\nC. killing\nD. missing morning cartoons\nE. boredom\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3899093866348267", "True"]], [["-1.8899093866348267", "False"]], [["-6.639909267425537", "False"]], [["-7.639909267425537", "False"]], [["-8.639909744262695", "False"]]], "filtered_resps": [["-1.3899093866348267", "True"], ["-1.8899093866348267", "False"], ["-6.639909267425537", "False"], ["-7.639909267425537", "False"], ["-8.639909744262695", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "07b832fd1d2b43a8c55629f3267d3fd21c769a662a1429526f18feddbef32ab0", "prompt_hash": "e5b47dc474eb6acb9bece23ae08a863af0f893c2e81d1ac3c85c4ab0fd390417", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 157, "doc": {"id": "31b72d4e4ae7c672c20e27e42499ec79", "question": "What could you get an unsmooth pit from?", "question_concept": "pit", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["backyard", "rock", "mine", "cherry", "peach"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What could you get an unsmooth pit from?\nA. backyard\nB. rock\nC. mine\nD. cherry\nE. peach\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could you get an unsmooth pit from?\nA. backyard\nB. rock\nC. mine\nD. cherry\nE. peach\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could you get an unsmooth pit from?\nA. backyard\nB. rock\nC. mine\nD. cherry\nE. peach\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could you get an unsmooth pit from?\nA. backyard\nB. rock\nC. mine\nD. cherry\nE. peach\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could you get an unsmooth pit from?\nA. backyard\nB. rock\nC. mine\nD. cherry\nE. peach\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.711843967437744", "False"]], [["-1.2118439674377441", "True"]], [["-2.461843967437744", "False"]], [["-4.961843967437744", "False"]], [["-4.211843967437744", "False"]]], "filtered_resps": [["-2.711843967437744", "False"], ["-1.2118439674377441", "True"], ["-2.461843967437744", "False"], ["-4.961843967437744", "False"], ["-4.211843967437744", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8a45152f930b8f7c673c5961117cf5e2ed3241ebf917dd839fbfaef402ab9c9e", "prompt_hash": "d8299e92785cc67507260368df1fd7fea1eb3a88398689dad33fa4293af48901", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 158, "doc": {"id": "26ce83b8e9a263079aa8cdbd5258d667", "question": "The man tried to reply to the woman, but he had difficulty keeping track of conversations that he didn't do what to?", "question_concept": "reply", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["initiate", "ignore", "question", "answer", "ask"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The man tried to reply to the woman, but he had difficulty keeping track of conversations that he didn't do what to?\nA. initiate\nB. ignore\nC. question\nD. answer\nE. ask\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man tried to reply to the woman, but he had difficulty keeping track of conversations that he didn't do what to?\nA. initiate\nB. ignore\nC. question\nD. answer\nE. ask\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man tried to reply to the woman, but he had difficulty keeping track of conversations that he didn't do what to?\nA. initiate\nB. ignore\nC. question\nD. answer\nE. ask\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man tried to reply to the woman, but he had difficulty keeping track of conversations that he didn't do what to?\nA. initiate\nB. ignore\nC. question\nD. answer\nE. ask\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man tried to reply to the woman, but he had difficulty keeping track of conversations that he didn't do what to?\nA. initiate\nB. ignore\nC. question\nD. answer\nE. ask\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6656511425971985", "True"]], [["-6.165651321411133", "False"]], [["-7.665651321411133", "False"]], [["-5.665651321411133", "False"]], [["-8.415651321411133", "False"]]], "filtered_resps": [["-0.6656511425971985", "True"], ["-6.165651321411133", "False"], ["-7.665651321411133", "False"], ["-5.665651321411133", "False"], ["-8.415651321411133", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "24e63229cc018ea3356ccf6f633f63dd1ac3d998b957589b6762b9402d9a40d3", "prompt_hash": "adc9fbaf9688491d13fd0b5ca0642d7cb50a40aa963b2dd3dbc98b595d32d1cc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 159, "doc": {"id": "30138608d4934a75cf0911a06b021374", "question": "I couldn't find anybody who recalled the event, what were they adroit at doing?", "question_concept": "anybody", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["question authority", "act fool", "wash dishes", "act innocent", "forget"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: I couldn't find anybody who recalled the event, what were they adroit at doing?\nA. question authority\nB. act fool\nC. wash dishes\nD. act innocent\nE. forget\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I couldn't find anybody who recalled the event, what were they adroit at doing?\nA. question authority\nB. act fool\nC. wash dishes\nD. act innocent\nE. forget\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I couldn't find anybody who recalled the event, what were they adroit at doing?\nA. question authority\nB. act fool\nC. wash dishes\nD. act innocent\nE. forget\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I couldn't find anybody who recalled the event, what were they adroit at doing?\nA. question authority\nB. act fool\nC. wash dishes\nD. act innocent\nE. forget\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I couldn't find anybody who recalled the event, what were they adroit at doing?\nA. question authority\nB. act fool\nC. wash dishes\nD. act innocent\nE. forget\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9334356784820557", "False"]], [["-4.183435440063477", "False"]], [["-4.933435440063477", "False"]], [["-2.9334356784820557", "False"]], [["-1.9334356784820557", "False"]]], "filtered_resps": [["-3.9334356784820557", "False"], ["-4.183435440063477", "False"], ["-4.933435440063477", "False"], ["-2.9334356784820557", "False"], ["-1.9334356784820557", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cbec7d0433d74784dbd2fbb894d485e69bd8af1149bbf2e68520b91e83da6219", "prompt_hash": "09e12f896b7aa913da7eea940b16ec98e378e3933415375704d15884b667b050", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 160, "doc": {"id": "01abce8c4964371d85a5be2019f75827", "question": "Where would you find a large dining room containing a fancy chandelier?", "question_concept": "dining room", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mansion", "every house", "own home", "table", "restaurant"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find a large dining room containing a fancy chandelier?\nA. mansion\nB. every house\nC. own home\nD. table\nE. restaurant\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find a large dining room containing a fancy chandelier?\nA. mansion\nB. every house\nC. own home\nD. table\nE. restaurant\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find a large dining room containing a fancy chandelier?\nA. mansion\nB. every house\nC. own home\nD. table\nE. restaurant\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find a large dining room containing a fancy chandelier?\nA. mansion\nB. every house\nC. own home\nD. table\nE. restaurant\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find a large dining room containing a fancy chandelier?\nA. mansion\nB. every house\nC. own home\nD. table\nE. restaurant\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.847558856010437", "True"]], [["-7.597558975219727", "False"]], [["-6.847558975219727", "False"]], [["-8.097558975219727", "False"]], [["-3.5975589752197266", "False"]]], "filtered_resps": [["-0.847558856010437", "True"], ["-7.597558975219727", "False"], ["-6.847558975219727", "False"], ["-8.097558975219727", "False"], ["-3.5975589752197266", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0acc8ece8f328657a8233126c2c0a6c5a55e0b3cbd41443a917cd52c287f1005", "prompt_hash": "0b68a4ad1fcd5325f88f33da0f84ecc733dd86332f3af15e44ed78cdd90f3076", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 161, "doc": {"id": "3e2222c99e11fca2ad4af2d470eb8ea2_1", "question": "The extremely large cargo plane could only land at a specialized runway, these were only located at a what?", "question_concept": "runway", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["back yard", "bowling alley", "city", "military base", "fashion show"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The extremely large cargo plane could only land at a specialized runway, these were only located at a what?\nA. back yard\nB. bowling alley\nC. city\nD. military base\nE. fashion show\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The extremely large cargo plane could only land at a specialized runway, these were only located at a what?\nA. back yard\nB. bowling alley\nC. city\nD. military base\nE. fashion show\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The extremely large cargo plane could only land at a specialized runway, these were only located at a what?\nA. back yard\nB. bowling alley\nC. city\nD. military base\nE. fashion show\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The extremely large cargo plane could only land at a specialized runway, these were only located at a what?\nA. back yard\nB. bowling alley\nC. city\nD. military base\nE. fashion show\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The extremely large cargo plane could only land at a specialized runway, these were only located at a what?\nA. back yard\nB. bowling alley\nC. city\nD. military base\nE. fashion show\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0698277950286865", "False"]], [["-6.319827556610107", "False"]], [["-4.069827556610107", "False"]], [["-0.8198277354240417", "True"]], [["-10.069828033447266", "False"]]], "filtered_resps": [["-3.0698277950286865", "False"], ["-6.319827556610107", "False"], ["-4.069827556610107", "False"], ["-0.8198277354240417", "True"], ["-10.069828033447266", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2c0109095baab7b53d5c30689fc8f033423c0a11454a10288741bd52591a6fe2", "prompt_hash": "42fbf95216371e1b9132404d4dab8c9a7565509d7a2a0564b842699bdaea22d4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 162, "doc": {"id": "847dbf5b73c3e8d49bb9a36491d95e79", "question": "The carpet was smelly and discouraged the league from playing there, where was this smelly carpet installed?", "question_concept": "carpet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bedroom", "movie theater", "bowling alley", "church", "office"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The carpet was smelly and discouraged the league from playing there, where was this smelly carpet installed?\nA. bedroom\nB. movie theater\nC. bowling alley\nD. church\nE. office\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The carpet was smelly and discouraged the league from playing there, where was this smelly carpet installed?\nA. bedroom\nB. movie theater\nC. bowling alley\nD. church\nE. office\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The carpet was smelly and discouraged the league from playing there, where was this smelly carpet installed?\nA. bedroom\nB. movie theater\nC. bowling alley\nD. church\nE. office\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The carpet was smelly and discouraged the league from playing there, where was this smelly carpet installed?\nA. bedroom\nB. movie theater\nC. bowling alley\nD. church\nE. office\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The carpet was smelly and discouraged the league from playing there, where was this smelly carpet installed?\nA. bedroom\nB. movie theater\nC. bowling alley\nD. church\nE. office\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5895962715148926", "False"]], [["-3.0895962715148926", "False"]], [["-2.3395962715148926", "False"]], [["-3.0895962715148926", "False"]], [["-4.589596271514893", "False"]]], "filtered_resps": [["-3.5895962715148926", "False"], ["-3.0895962715148926", "False"], ["-2.3395962715148926", "False"], ["-3.0895962715148926", "False"], ["-4.589596271514893", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5a8d26ce49854d616fa13f5cd95a00ced5eee6748c55919a1128dbb9373c0afd", "prompt_hash": "6127927adf2efcfefb55193d0a80c64a49f7b5ef6db6c326a93bf36b64771b04", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 163, "doc": {"id": "fa031cff8e11e75c68d6a99ef0e5ca3a", "question": "How can someone be let into a brownstone?", "question_concept": "brownstone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["brooklyn", "ring", "subdivision", "bricks", "new york city"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: How can someone be let into a brownstone?\nA. brooklyn\nB. ring\nC. subdivision\nD. bricks\nE. new york city\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How can someone be let into a brownstone?\nA. brooklyn\nB. ring\nC. subdivision\nD. bricks\nE. new york city\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How can someone be let into a brownstone?\nA. brooklyn\nB. ring\nC. subdivision\nD. bricks\nE. new york city\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How can someone be let into a brownstone?\nA. brooklyn\nB. ring\nC. subdivision\nD. bricks\nE. new york city\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How can someone be let into a brownstone?\nA. brooklyn\nB. ring\nC. subdivision\nD. bricks\nE. new york city\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.14703369140625", "False"]], [["-1.14703369140625", "True"]], [["-6.39703369140625", "False"]], [["-7.39703369140625", "False"]], [["-4.89703369140625", "False"]]], "filtered_resps": [["-3.14703369140625", "False"], ["-1.14703369140625", "True"], ["-6.39703369140625", "False"], ["-7.39703369140625", "False"], ["-4.89703369140625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "627bf184276357c991a4ee9a4d850855d3ea0492972156623721b855d2a1c566", "prompt_hash": "cdb56d445025ba48e2f8bc03188a6704cf90cdb3862abc26092db0b1fba66c45", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 164, "doc": {"id": "c592258c88295756833e9796e881057b", "question": "Where would someone purchase an upright piano?", "question_concept": "upright piano", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["music class", "college", "music store", "music room", "music band"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would someone purchase an upright piano?\nA. music class\nB. college\nC. music store\nD. music room\nE. music band\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would someone purchase an upright piano?\nA. music class\nB. college\nC. music store\nD. music room\nE. music band\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would someone purchase an upright piano?\nA. music class\nB. college\nC. music store\nD. music room\nE. music band\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would someone purchase an upright piano?\nA. music class\nB. college\nC. music store\nD. music room\nE. music band\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would someone purchase an upright piano?\nA. music class\nB. college\nC. music store\nD. music room\nE. music band\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.964808225631714", "False"]], [["-7.214808464050293", "False"]], [["-1.2148082256317139", "False"]], [["-9.464808464050293", "False"]], [["-10.214808464050293", "False"]]], "filtered_resps": [["-3.964808225631714", "False"], ["-7.214808464050293", "False"], ["-1.2148082256317139", "False"], ["-9.464808464050293", "False"], ["-10.214808464050293", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "833954cf23b5759d36642769871f3b2a85e11b2721a3d03026373d2c6448e396", "prompt_hash": "ef6a7c075ab37140489f49338359ac437d418a33ce3ea17a75a4ca6f9916cf03", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 165, "doc": {"id": "e1403a7c581bc263aea2ed8d179826d1", "question": "Where would you keep an ottoman near your front door?", "question_concept": "ottoman", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["living room", "parlor", "furniture store", "basement", "kitchen"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you keep an ottoman near your front door?\nA. living room\nB. parlor\nC. furniture store\nD. basement\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you keep an ottoman near your front door?\nA. living room\nB. parlor\nC. furniture store\nD. basement\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you keep an ottoman near your front door?\nA. living room\nB. parlor\nC. furniture store\nD. basement\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you keep an ottoman near your front door?\nA. living room\nB. parlor\nC. furniture store\nD. basement\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you keep an ottoman near your front door?\nA. living room\nB. parlor\nC. furniture store\nD. basement\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.524056077003479", "True"]], [["-4.2740559577941895", "False"]], [["-6.5240559577941895", "False"]], [["-6.5240559577941895", "False"]], [["-7.7740559577941895", "False"]]], "filtered_resps": [["-0.524056077003479", "True"], ["-4.2740559577941895", "False"], ["-6.5240559577941895", "False"], ["-6.5240559577941895", "False"], ["-7.7740559577941895", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "54710f27f27b7bd196fb5434ea058035419e98ffba61119f59a939a6997bc9d4", "prompt_hash": "4c406560a98be46299d6356f63ceaf6bb6104c7abe34cacc7cd84b8ae8d447ab", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 166, "doc": {"id": "15c38f66e811d6ed68cde931bc31d93c", "question": "Diving into backyard pools can be very dangerous and can lead to serious head and what?", "question_concept": "diving", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["going somewhere", "splats", "cancer", "getting wet", "spinal injuries"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Diving into backyard pools can be very dangerous and can lead to serious head and what?\nA. going somewhere\nB. splats\nC. cancer\nD. getting wet\nE. spinal injuries\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Diving into backyard pools can be very dangerous and can lead to serious head and what?\nA. going somewhere\nB. splats\nC. cancer\nD. getting wet\nE. spinal injuries\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Diving into backyard pools can be very dangerous and can lead to serious head and what?\nA. going somewhere\nB. splats\nC. cancer\nD. getting wet\nE. spinal injuries\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Diving into backyard pools can be very dangerous and can lead to serious head and what?\nA. going somewhere\nB. splats\nC. cancer\nD. getting wet\nE. spinal injuries\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Diving into backyard pools can be very dangerous and can lead to serious head and what?\nA. going somewhere\nB. splats\nC. cancer\nD. getting wet\nE. spinal injuries\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.076492786407471", "False"]], [["-4.826492786407471", "False"]], [["-3.8264927864074707", "False"]], [["-1.3264927864074707", "True"]], [["-1.5764927864074707", "False"]]], "filtered_resps": [["-5.076492786407471", "False"], ["-4.826492786407471", "False"], ["-3.8264927864074707", "False"], ["-1.3264927864074707", "True"], ["-1.5764927864074707", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "02a59242afccdb3066bff56ec6965894d26c7fc36bc9f858ad467cd0951b58e2", "prompt_hash": "4579369d4052d40abeaba31c4e16c162f9e84538de0951ec7fb60fb8860e3ec0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 167, "doc": {"id": "1ac54dbf6b67f27daa3d456416047584", "question": "Where would one find a snake in a cage?", "question_concept": "snake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tropical forest", "oregon", "woods", "pet store", "louisiana"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would one find a snake in a cage?\nA. tropical forest\nB. oregon\nC. woods\nD. pet store\nE. louisiana\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would one find a snake in a cage?\nA. tropical forest\nB. oregon\nC. woods\nD. pet store\nE. louisiana\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would one find a snake in a cage?\nA. tropical forest\nB. oregon\nC. woods\nD. pet store\nE. louisiana\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would one find a snake in a cage?\nA. tropical forest\nB. oregon\nC. woods\nD. pet store\nE. louisiana\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would one find a snake in a cage?\nA. tropical forest\nB. oregon\nC. woods\nD. pet store\nE. louisiana\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7216105461120605", "False"]], [["-6.7216105461120605", "False"]], [["-8.471610069274902", "False"]], [["-1.471610426902771", "False"]], [["-9.471610069274902", "False"]]], "filtered_resps": [["-2.7216105461120605", "False"], ["-6.7216105461120605", "False"], ["-8.471610069274902", "False"], ["-1.471610426902771", "False"], ["-9.471610069274902", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a2ccb796f4faecc25f3e2009cc3295ad285c683620f218de10833135cbd62071", "prompt_hash": "17b5e8608bc4c0e030edacba078bc83519a028ccc02b649880713ea8fa2d93a1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 168, "doc": {"id": "21763a65765b5405c9a54484c2e54a72", "question": "Where are people likely to become impatient?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["end of line", "buildings", "apartment", "neighbor's house", "address"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where are people likely to become impatient?\nA. end of line\nB. buildings\nC. apartment\nD. neighbor's house\nE. address\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are people likely to become impatient?\nA. end of line\nB. buildings\nC. apartment\nD. neighbor's house\nE. address\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are people likely to become impatient?\nA. end of line\nB. buildings\nC. apartment\nD. neighbor's house\nE. address\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are people likely to become impatient?\nA. end of line\nB. buildings\nC. apartment\nD. neighbor's house\nE. address\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are people likely to become impatient?\nA. end of line\nB. buildings\nC. apartment\nD. neighbor's house\nE. address\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9659048318862915", "True"]], [["-6.965904712677002", "False"]], [["-8.21590518951416", "False"]], [["-8.21590518951416", "False"]], [["-9.46590518951416", "False"]]], "filtered_resps": [["-0.9659048318862915", "True"], ["-6.965904712677002", "False"], ["-8.21590518951416", "False"], ["-8.21590518951416", "False"], ["-9.46590518951416", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "996967c261372774ebee2b5635a23b2caea5c8d1113177bb75bad7853f815e44", "prompt_hash": "093601c625ad30362682d3758e55d4b7ddb30b3861ad66350c903240222151c3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 169, "doc": {"id": "c492b8b9754a181c924c1df19998cbc7", "question": "When you fail to finish something, you failed at doing what to it", "question_concept": "fail", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["winning", "passing", "completing", "do well", "succeeding"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: When you fail to finish something, you failed at doing what to it\nA. winning\nB. passing\nC. completing\nD. do well\nE. succeeding\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you fail to finish something, you failed at doing what to it\nA. winning\nB. passing\nC. completing\nD. do well\nE. succeeding\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you fail to finish something, you failed at doing what to it\nA. winning\nB. passing\nC. completing\nD. do well\nE. succeeding\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you fail to finish something, you failed at doing what to it\nA. winning\nB. passing\nC. completing\nD. do well\nE. succeeding\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you fail to finish something, you failed at doing what to it\nA. winning\nB. passing\nC. completing\nD. do well\nE. succeeding\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.146847724914551", "False"]], [["-5.146847724914551", "False"]], [["-2.146847724914551", "False"]], [["-7.396847724914551", "False"]], [["-5.146847724914551", "False"]]], "filtered_resps": [["-6.146847724914551", "False"], ["-5.146847724914551", "False"], ["-2.146847724914551", "False"], ["-7.396847724914551", "False"], ["-5.146847724914551", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f189bec1fc0633a125f0ca001d62a48e67faee6b8c4e91310a56aaa1459ba694", "prompt_hash": "89d3f0e44545e6a24e2db52e725f1bf2d59deee71c7864dc4f7551c8dd03b7ac", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 170, "doc": {"id": "fff554fffa1a0adc64b8d1e21d55534b", "question": "John didn't care about style.  He felt that form was less important than what?", "question_concept": "form", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shapeless", "quality", "function", "change shape", "chaos"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: John didn't care about style.  He felt that form was less important than what?\nA. shapeless\nB. quality\nC. function\nD. change shape\nE. chaos\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John didn't care about style.  He felt that form was less important than what?\nA. shapeless\nB. quality\nC. function\nD. change shape\nE. chaos\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John didn't care about style.  He felt that form was less important than what?\nA. shapeless\nB. quality\nC. function\nD. change shape\nE. chaos\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John didn't care about style.  He felt that form was less important than what?\nA. shapeless\nB. quality\nC. function\nD. change shape\nE. chaos\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John didn't care about style.  He felt that form was less important than what?\nA. shapeless\nB. quality\nC. function\nD. change shape\nE. chaos\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.8241167068481445", "False"]], [["-3.5741167068481445", "False"]], [["-1.0741167068481445", "True"]], [["-7.8241167068481445", "False"]], [["-10.574116706848145", "False"]]], "filtered_resps": [["-6.8241167068481445", "False"], ["-3.5741167068481445", "False"], ["-1.0741167068481445", "True"], ["-7.8241167068481445", "False"], ["-10.574116706848145", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1c9ec8f38ebcf95a8c5bd6964c483e109e934d3d6d886c744380e5d223a4fe7d", "prompt_hash": "c1bf74ca4e01b11e881791130aeae83e7fc39fabbf4ae8c7d0b312629a59ec8b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 171, "doc": {"id": "8ea5720718c0e122efa6277edb511569", "question": "When you get together with friends to watch film, you might do plenty of this?", "question_concept": "watch film", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["see what happens", "enjoy stories", "pass time", "have fun", "interesting"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When you get together with friends to watch film, you might do plenty of this?\nA. see what happens\nB. enjoy stories\nC. pass time\nD. have fun\nE. interesting\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you get together with friends to watch film, you might do plenty of this?\nA. see what happens\nB. enjoy stories\nC. pass time\nD. have fun\nE. interesting\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you get together with friends to watch film, you might do plenty of this?\nA. see what happens\nB. enjoy stories\nC. pass time\nD. have fun\nE. interesting\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you get together with friends to watch film, you might do plenty of this?\nA. see what happens\nB. enjoy stories\nC. pass time\nD. have fun\nE. interesting\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you get together with friends to watch film, you might do plenty of this?\nA. see what happens\nB. enjoy stories\nC. pass time\nD. have fun\nE. interesting\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.729255199432373", "False"]], [["-2.229255199432373", "False"]], [["-4.979255199432373", "False"]], [["-1.9792553186416626", "False"]], [["-7.479255199432373", "False"]]], "filtered_resps": [["-3.729255199432373", "False"], ["-2.229255199432373", "False"], ["-4.979255199432373", "False"], ["-1.9792553186416626", "False"], ["-7.479255199432373", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0740fdb6850f1ff5d0ddf4daa6c89bdba783a8bda2a750eea18df9231a55626f", "prompt_hash": "1aedc119fd46199ee13795c65fd759c867d925f222f7171db5401e10a84e22a9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 172, "doc": {"id": "23e4257a49972efd8a97672f060be1c1", "question": "A supermarket is uncommon in what type of collection of shops?", "question_concept": "supermarket", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["strip mall", "city or town", "shoppingcentre", "boutique", "vermont"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A supermarket is uncommon in what type of collection of shops?\nA. strip mall\nB. city or town\nC. shoppingcentre\nD. boutique\nE. vermont\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A supermarket is uncommon in what type of collection of shops?\nA. strip mall\nB. city or town\nC. shoppingcentre\nD. boutique\nE. vermont\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A supermarket is uncommon in what type of collection of shops?\nA. strip mall\nB. city or town\nC. shoppingcentre\nD. boutique\nE. vermont\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A supermarket is uncommon in what type of collection of shops?\nA. strip mall\nB. city or town\nC. shoppingcentre\nD. boutique\nE. vermont\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A supermarket is uncommon in what type of collection of shops?\nA. strip mall\nB. city or town\nC. shoppingcentre\nD. boutique\nE. vermont\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4181547164916992", "True"]], [["-3.418154716491699", "False"]], [["-5.918154716491699", "False"]], [["-1.9181547164916992", "False"]], [["-3.668154716491699", "False"]]], "filtered_resps": [["-1.4181547164916992", "True"], ["-3.418154716491699", "False"], ["-5.918154716491699", "False"], ["-1.9181547164916992", "False"], ["-3.668154716491699", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1ae32bb540e44fa8a489af58757d42864c087e1d046897d8079dad51e64afb1c", "prompt_hash": "af1702335a380ac1893181dfc4cd9282489c38f3362fd91682c091cb427ce454", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 173, "doc": {"id": "a018d65a74b9e77d81014fd8f6d78f77", "question": "Bill puts meat on the scale, where does Bill work?", "question_concept": "scale", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["music store", "assay office", "tidal wave", "butcher shop", "bathroom"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Bill puts meat on the scale, where does Bill work?\nA. music store\nB. assay office\nC. tidal wave\nD. butcher shop\nE. bathroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Bill puts meat on the scale, where does Bill work?\nA. music store\nB. assay office\nC. tidal wave\nD. butcher shop\nE. bathroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Bill puts meat on the scale, where does Bill work?\nA. music store\nB. assay office\nC. tidal wave\nD. butcher shop\nE. bathroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Bill puts meat on the scale, where does Bill work?\nA. music store\nB. assay office\nC. tidal wave\nD. butcher shop\nE. bathroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Bill puts meat on the scale, where does Bill work?\nA. music store\nB. assay office\nC. tidal wave\nD. butcher shop\nE. bathroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.615056037902832", "False"]], [["-5.115056037902832", "False"]], [["-9.865056037902832", "False"]], [["-2.365056276321411", "False"]], [["-12.115056037902832", "False"]]], "filtered_resps": [["-6.615056037902832", "False"], ["-5.115056037902832", "False"], ["-9.865056037902832", "False"], ["-2.365056276321411", "False"], ["-12.115056037902832", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9fa7a60f5d37156a9f7a158f9b07c58bf8dc3f9a4d16718adf53981b349f98a3", "prompt_hash": "ae72982e4456588084b144820653f8c836bb4840ab2bba056b7d5083414c8f29", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 174, "doc": {"id": "24ceaf5c10863e73919b5f1b0f2db38e", "question": "I'm having some food at my party, what will I need to serve it?", "question_concept": "food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["zoo", "pan", "bowl", "kitchen", "spoon"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: I'm having some food at my party, what will I need to serve it?\nA. zoo\nB. pan\nC. bowl\nD. kitchen\nE. spoon\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I'm having some food at my party, what will I need to serve it?\nA. zoo\nB. pan\nC. bowl\nD. kitchen\nE. spoon\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I'm having some food at my party, what will I need to serve it?\nA. zoo\nB. pan\nC. bowl\nD. kitchen\nE. spoon\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I'm having some food at my party, what will I need to serve it?\nA. zoo\nB. pan\nC. bowl\nD. kitchen\nE. spoon\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I'm having some food at my party, what will I need to serve it?\nA. zoo\nB. pan\nC. bowl\nD. kitchen\nE. spoon\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.781940221786499", "False"]], [["-2.281940221786499", "False"]], [["-3.031940221786499", "False"]], [["-6.031940460205078", "False"]], [["-2.781940221786499", "False"]]], "filtered_resps": [["-3.781940221786499", "False"], ["-2.281940221786499", "False"], ["-3.031940221786499", "False"], ["-6.031940460205078", "False"], ["-2.781940221786499", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e59a6bceb0fa69f1ad627edb390c4db39697e454dec03b00d960369faf5da4d9", "prompt_hash": "de016f7a45a75e10e7400bb24258f18d4a118e04e50b9f12308a0fd72113e239", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 175, "doc": {"id": "900492bd731f8f615ed7c08155737d44", "question": "Before racers start to run they must do what at the starting line?", "question_concept": "run", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["learn to walk", "walking", "walk slowly", "breathe", "stand still"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Before racers start to run they must do what at the starting line?\nA. learn to walk\nB. walking\nC. walk slowly\nD. breathe\nE. stand still\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Before racers start to run they must do what at the starting line?\nA. learn to walk\nB. walking\nC. walk slowly\nD. breathe\nE. stand still\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Before racers start to run they must do what at the starting line?\nA. learn to walk\nB. walking\nC. walk slowly\nD. breathe\nE. stand still\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Before racers start to run they must do what at the starting line?\nA. learn to walk\nB. walking\nC. walk slowly\nD. breathe\nE. stand still\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Before racers start to run they must do what at the starting line?\nA. learn to walk\nB. walking\nC. walk slowly\nD. breathe\nE. stand still\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.006229400634766", "False"]], [["-5.256229400634766", "False"]], [["-7.756229400634766", "False"]], [["-3.7562294006347656", "False"]], [["-1.5062294006347656", "False"]]], "filtered_resps": [["-6.006229400634766", "False"], ["-5.256229400634766", "False"], ["-7.756229400634766", "False"], ["-3.7562294006347656", "False"], ["-1.5062294006347656", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a93e9be8c64ea00076f2c91b7f41a9d67e2ff960b93801b8a568675cd81689da", "prompt_hash": "9398cc85cd695480809d5b80e8d9b89d8ceece52c1da23ff2e037623ce9befad", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 176, "doc": {"id": "4e3f85dc92eaad4ae6bc6529d62e382c", "question": "What does an actor do when they are bored of their roles?", "question_concept": "actor", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mask", "branch out", "wear costume", "pretend", "sing songs"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What does an actor do when they are bored of their roles?\nA. mask\nB. branch out\nC. wear costume\nD. pretend\nE. sing songs\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does an actor do when they are bored of their roles?\nA. mask\nB. branch out\nC. wear costume\nD. pretend\nE. sing songs\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does an actor do when they are bored of their roles?\nA. mask\nB. branch out\nC. wear costume\nD. pretend\nE. sing songs\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does an actor do when they are bored of their roles?\nA. mask\nB. branch out\nC. wear costume\nD. pretend\nE. sing songs\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does an actor do when they are bored of their roles?\nA. mask\nB. branch out\nC. wear costume\nD. pretend\nE. sing songs\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.378701210021973", "False"]], [["-1.878701090812683", "False"]], [["-8.128701210021973", "False"]], [["-7.628701210021973", "False"]], [["-9.378701210021973", "False"]]], "filtered_resps": [["-4.378701210021973", "False"], ["-1.878701090812683", "False"], ["-8.128701210021973", "False"], ["-7.628701210021973", "False"], ["-9.378701210021973", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b85ae68f7559e7c7855906f18db042f9ba5e9d77ff0d30b5a71f7b85d04dcd89", "prompt_hash": "477dbb7b220077890dbf1b05e231f3fb5f2df1d40b72129465008d8ac922da28", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 177, "doc": {"id": "fa1f17ca535c7e875f4f58510dc2f430", "question": "What is a person called who doesn't have immortality?", "question_concept": "immortality", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mortal", "dying", "death", "dead", "mortal"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is a person called who doesn't have immortality?\nA. mortal\nB. dying\nC. death\nD. dead\nE. mortal\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a person called who doesn't have immortality?\nA. mortal\nB. dying\nC. death\nD. dead\nE. mortal\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a person called who doesn't have immortality?\nA. mortal\nB. dying\nC. death\nD. dead\nE. mortal\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a person called who doesn't have immortality?\nA. mortal\nB. dying\nC. death\nD. dead\nE. mortal\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a person called who doesn't have immortality?\nA. mortal\nB. dying\nC. death\nD. dead\nE. mortal\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0523836612701416", "False"]], [["-8.052383422851562", "False"]], [["-10.552383422851562", "False"]], [["-10.552383422851562", "False"]], [["-5.8023834228515625", "False"]]], "filtered_resps": [["-1.0523836612701416", "False"], ["-8.052383422851562", "False"], ["-10.552383422851562", "False"], ["-10.552383422851562", "False"], ["-5.8023834228515625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "81fff87520139bc72ecb780283aa6cbc99827671aae8298df3f4d690bb06ce31", "prompt_hash": "b05d783b91bfee035974e35eb1fa28acb9b8547f42dc702c60a260502acc10d0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 178, "doc": {"id": "76b6f0765a3b2fba71021f902142edc0", "question": "Why would you be watching tv instead of doing something else?", "question_concept": "watching tv", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["headache", "laughter", "laziness", "erections", "wasting time"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Why would you be watching tv instead of doing something else?\nA. headache\nB. laughter\nC. laziness\nD. erections\nE. wasting time\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would you be watching tv instead of doing something else?\nA. headache\nB. laughter\nC. laziness\nD. erections\nE. wasting time\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would you be watching tv instead of doing something else?\nA. headache\nB. laughter\nC. laziness\nD. erections\nE. wasting time\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would you be watching tv instead of doing something else?\nA. headache\nB. laughter\nC. laziness\nD. erections\nE. wasting time\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would you be watching tv instead of doing something else?\nA. headache\nB. laughter\nC. laziness\nD. erections\nE. wasting time\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.46608567237854", "False"]], [["-3.21608567237854", "False"]], [["-1.71608567237854", "True"]], [["-5.966085433959961", "False"]], [["-1.96608567237854", "False"]]], "filtered_resps": [["-3.46608567237854", "False"], ["-3.21608567237854", "False"], ["-1.71608567237854", "True"], ["-5.966085433959961", "False"], ["-1.96608567237854", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c8ff94271c0ff6a9b3cf03b18dc1fffcb3c1fcb1c93944313c72c8ba0cffbf4f", "prompt_hash": "f0b94af447bd9b60e85d10a38e0df1eb7e8821e7cd0dadc5142094176a16937c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 179, "doc": {"id": "f1368ab1d4ee05d72d555474fcd737d7", "question": "If chewing food is difficult for you, what is a possible reason?", "question_concept": "chewing food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["broken jaw", "sore mouth", "eating", "good digestion", "avoiding choking"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If chewing food is difficult for you, what is a possible reason?\nA. broken jaw\nB. sore mouth\nC. eating\nD. good digestion\nE. avoiding choking\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If chewing food is difficult for you, what is a possible reason?\nA. broken jaw\nB. sore mouth\nC. eating\nD. good digestion\nE. avoiding choking\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If chewing food is difficult for you, what is a possible reason?\nA. broken jaw\nB. sore mouth\nC. eating\nD. good digestion\nE. avoiding choking\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If chewing food is difficult for you, what is a possible reason?\nA. broken jaw\nB. sore mouth\nC. eating\nD. good digestion\nE. avoiding choking\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If chewing food is difficult for you, what is a possible reason?\nA. broken jaw\nB. sore mouth\nC. eating\nD. good digestion\nE. avoiding choking\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6922260522842407", "False"]], [["-1.9422260522842407", "False"]], [["-4.192225933074951", "False"]], [["-4.942225933074951", "False"]], [["-6.442225933074951", "False"]]], "filtered_resps": [["-1.6922260522842407", "False"], ["-1.9422260522842407", "False"], ["-4.192225933074951", "False"], ["-4.942225933074951", "False"], ["-6.442225933074951", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9919a435d4fe48a221ce9ebceb9fb76878e71e59de4a64dc83ffed4bc033933a", "prompt_hash": "cde02ca34926e61390c5f5ed1f2fc6f52585b24304b58bebda1cf86143cbcaa8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 180, "doc": {"id": "3dee8fc7f0a3fbf4de111b6686fca157", "question": "He had to wear a tuxedo while playing the keyboard instrument, so did the other hundred members of the what?", "question_concept": "keyboard instrument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["music store", "band", "medium", "orchestra", "piano store"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He had to wear a tuxedo while playing the keyboard instrument, so did the other hundred members of the what?\nA. music store\nB. band\nC. medium\nD. orchestra\nE. piano store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He had to wear a tuxedo while playing the keyboard instrument, so did the other hundred members of the what?\nA. music store\nB. band\nC. medium\nD. orchestra\nE. piano store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He had to wear a tuxedo while playing the keyboard instrument, so did the other hundred members of the what?\nA. music store\nB. band\nC. medium\nD. orchestra\nE. piano store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He had to wear a tuxedo while playing the keyboard instrument, so did the other hundred members of the what?\nA. music store\nB. band\nC. medium\nD. orchestra\nE. piano store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He had to wear a tuxedo while playing the keyboard instrument, so did the other hundred members of the what?\nA. music store\nB. band\nC. medium\nD. orchestra\nE. piano store\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.666041374206543", "False"]], [["-2.666041374206543", "False"]], [["-8.916041374206543", "False"]], [["-2.166041374206543", "False"]], [["-10.416041374206543", "False"]]], "filtered_resps": [["-3.666041374206543", "False"], ["-2.666041374206543", "False"], ["-8.916041374206543", "False"], ["-2.166041374206543", "False"], ["-10.416041374206543", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b8fab95170110c86b8f5db8f59b5985e4386c64ca8a045f391aeab78380d2be8", "prompt_hash": "aeeeb2645167cf62546e84c70ef97b12848b6921602344026450bba266f27e62", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 181, "doc": {"id": "ea0e7771afd86a59fd9f7764b77e3fa4", "question": "Where do you find the most amount of leafs?", "question_concept": "leaf", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["floral arrangement", "ground", "forrest", "field", "compost pile"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you find the most amount of leafs?\nA. floral arrangement\nB. ground\nC. forrest\nD. field\nE. compost pile\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you find the most amount of leafs?\nA. floral arrangement\nB. ground\nC. forrest\nD. field\nE. compost pile\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you find the most amount of leafs?\nA. floral arrangement\nB. ground\nC. forrest\nD. field\nE. compost pile\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you find the most amount of leafs?\nA. floral arrangement\nB. ground\nC. forrest\nD. field\nE. compost pile\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you find the most amount of leafs?\nA. floral arrangement\nB. ground\nC. forrest\nD. field\nE. compost pile\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.365224361419678", "False"]], [["-5.615224361419678", "False"]], [["-1.3652244806289673", "False"]], [["-6.115224361419678", "False"]], [["-4.365224361419678", "False"]]], "filtered_resps": [["-4.365224361419678", "False"], ["-5.615224361419678", "False"], ["-1.3652244806289673", "False"], ["-6.115224361419678", "False"], ["-4.365224361419678", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2004a5830d66c8ec31abc6cfa8eaa4119b146d3bb8ecf16c02b507505b2c32c4", "prompt_hash": "f1ec36c74832df4ca50c84337ab6889562f3744aa7ad38f48f7d9e4edc57398f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 182, "doc": {"id": "2c845646032bbf27fb3904330d59d324", "question": "Where can children play with animals?", "question_concept": "animals", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["meadow", "play room", "surface of earth", "zoos", "fairgrounds"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where can children play with animals?\nA. meadow\nB. play room\nC. surface of earth\nD. zoos\nE. fairgrounds\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can children play with animals?\nA. meadow\nB. play room\nC. surface of earth\nD. zoos\nE. fairgrounds\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can children play with animals?\nA. meadow\nB. play room\nC. surface of earth\nD. zoos\nE. fairgrounds\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can children play with animals?\nA. meadow\nB. play room\nC. surface of earth\nD. zoos\nE. fairgrounds\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can children play with animals?\nA. meadow\nB. play room\nC. surface of earth\nD. zoos\nE. fairgrounds\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.386940956115723", "False"]], [["-4.136940956115723", "False"]], [["-4.636940956115723", "False"]], [["-1.3869409561157227", "True"]], [["-7.886940956115723", "False"]]], "filtered_resps": [["-4.386940956115723", "False"], ["-4.136940956115723", "False"], ["-4.636940956115723", "False"], ["-1.3869409561157227", "True"], ["-7.886940956115723", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "801f09fc08cc52b9ae8d9c33e228877d7a89b9bd287480057834d5693bc8290c", "prompt_hash": "ec6315baabf3f6c39bdead66942610e8bd3293b45c5f0286f2b840597624af7d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 183, "doc": {"id": "bc08c354e5bead6863ea4a29cb8fa359", "question": "What kind of tale might feature a talking weasel?", "question_concept": "weasel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mulberry bush", "animated film", "chicken coop", "history book", "children's story"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of tale might feature a talking weasel?\nA. mulberry bush\nB. animated film\nC. chicken coop\nD. history book\nE. children's story\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of tale might feature a talking weasel?\nA. mulberry bush\nB. animated film\nC. chicken coop\nD. history book\nE. children's story\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of tale might feature a talking weasel?\nA. mulberry bush\nB. animated film\nC. chicken coop\nD. history book\nE. children's story\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of tale might feature a talking weasel?\nA. mulberry bush\nB. animated film\nC. chicken coop\nD. history book\nE. children's story\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of tale might feature a talking weasel?\nA. mulberry bush\nB. animated film\nC. chicken coop\nD. history book\nE. children's story\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3957531452178955", "False"]], [["-2.1457531452178955", "False"]], [["-7.145753383636475", "False"]], [["-9.145752906799316", "False"]], [["-0.6457532048225403", "True"]]], "filtered_resps": [["-2.3957531452178955", "False"], ["-2.1457531452178955", "False"], ["-7.145753383636475", "False"], ["-9.145752906799316", "False"], ["-0.6457532048225403", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c1935058966860b6111fbe86be048173b230a9ec0e27967a122086147c77fe83", "prompt_hash": "2f251351996c32451aaaa91ad3c58d8a330b2f5b9e31e4d8f6d82c19700112dc", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 184, "doc": {"id": "fb35c7aa5694bab2cde4b7257bfae003", "question": "What kind of status is the bald eagle given?", "question_concept": "bald eagle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["outside", "world", "protection", "colorado", "america"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of status is the bald eagle given?\nA. outside\nB. world\nC. protection\nD. colorado\nE. america\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of status is the bald eagle given?\nA. outside\nB. world\nC. protection\nD. colorado\nE. america\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of status is the bald eagle given?\nA. outside\nB. world\nC. protection\nD. colorado\nE. america\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of status is the bald eagle given?\nA. outside\nB. world\nC. protection\nD. colorado\nE. america\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of status is the bald eagle given?\nA. outside\nB. world\nC. protection\nD. colorado\nE. america\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.459360122680664", "False"]], [["-5.459360122680664", "False"]], [["-1.4593602418899536", "False"]], [["-8.459360122680664", "False"]], [["-7.459360122680664", "False"]]], "filtered_resps": [["-5.459360122680664", "False"], ["-5.459360122680664", "False"], ["-1.4593602418899536", "False"], ["-8.459360122680664", "False"], ["-7.459360122680664", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a98e7c5c7170c34b40ca10b00b6f3824eb73cd97536b9b95b2cc7f63e8f0ba2c", "prompt_hash": "1f8727dbcbf67b5da7b2d1443d4743aa974ae80ee835738f5bcc8cd176b32be8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 185, "doc": {"id": "e2a9f0041d17a9944377a91bef5e0d0d", "question": "Why do most people take a quick rest during the day?", "question_concept": "rest", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["need to", "hungry", "feel more energetic", "weak", "regenerate"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Why do most people take a quick rest during the day?\nA. need to\nB. hungry\nC. feel more energetic\nD. weak\nE. regenerate\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why do most people take a quick rest during the day?\nA. need to\nB. hungry\nC. feel more energetic\nD. weak\nE. regenerate\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why do most people take a quick rest during the day?\nA. need to\nB. hungry\nC. feel more energetic\nD. weak\nE. regenerate\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why do most people take a quick rest during the day?\nA. need to\nB. hungry\nC. feel more energetic\nD. weak\nE. regenerate\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why do most people take a quick rest during the day?\nA. need to\nB. hungry\nC. feel more energetic\nD. weak\nE. regenerate\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1221179962158203", "False"]], [["-6.37211799621582", "False"]], [["-6.62211799621582", "False"]], [["-5.37211799621582", "False"]], [["-2.1221179962158203", "False"]]], "filtered_resps": [["-2.1221179962158203", "False"], ["-6.37211799621582", "False"], ["-6.62211799621582", "False"], ["-5.37211799621582", "False"], ["-2.1221179962158203", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ce97eea95e77f0772adb48417f07bb0eef7fa8bd84b4d023cdb29fd9c8b29c53", "prompt_hash": "495f6b30dbb29375e1c9cec0040537931de9da7a4c96ee880f1825c101cae250", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 186, "doc": {"id": "ae56eff01d05422ddbcb26be7181356a", "question": "What could suddenly stop someone when he or she is running?", "question_concept": "running", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mushroom", "falling down", "sweating", "exhaustion", "getting tired"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What could suddenly stop someone when he or she is running?\nA. mushroom\nB. falling down\nC. sweating\nD. exhaustion\nE. getting tired\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could suddenly stop someone when he or she is running?\nA. mushroom\nB. falling down\nC. sweating\nD. exhaustion\nE. getting tired\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could suddenly stop someone when he or she is running?\nA. mushroom\nB. falling down\nC. sweating\nD. exhaustion\nE. getting tired\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could suddenly stop someone when he or she is running?\nA. mushroom\nB. falling down\nC. sweating\nD. exhaustion\nE. getting tired\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could suddenly stop someone when he or she is running?\nA. mushroom\nB. falling down\nC. sweating\nD. exhaustion\nE. getting tired\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.493686676025391", "False"]], [["-1.2436866760253906", "True"]], [["-5.743686676025391", "False"]], [["-2.4936866760253906", "False"]], [["-5.493686676025391", "False"]]], "filtered_resps": [["-4.493686676025391", "False"], ["-1.2436866760253906", "True"], ["-5.743686676025391", "False"], ["-2.4936866760253906", "False"], ["-5.493686676025391", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "190c91f1a9e8368c0dbecceb92733fed7dc00a407656ce176f18ea2cc94052e6", "prompt_hash": "b7b3cd79bc7b1b5dd114828684f2ec06dc6cd8d85e32901d5061f85f66ebd8db", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 187, "doc": {"id": "895aa97bb84d874d71b2aed572cebfdd", "question": "Where would you find a monkey in the wild?", "question_concept": "monkey", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["zoo", "barrel", "research laboratory", "captivity", "thailand"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find a monkey in the wild?\nA. zoo\nB. barrel\nC. research laboratory\nD. captivity\nE. thailand\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find a monkey in the wild?\nA. zoo\nB. barrel\nC. research laboratory\nD. captivity\nE. thailand\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find a monkey in the wild?\nA. zoo\nB. barrel\nC. research laboratory\nD. captivity\nE. thailand\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find a monkey in the wild?\nA. zoo\nB. barrel\nC. research laboratory\nD. captivity\nE. thailand\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find a monkey in the wild?\nA. zoo\nB. barrel\nC. research laboratory\nD. captivity\nE. thailand\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7457938194274902", "False"]], [["-6.99579381942749", "False"]], [["-6.49579381942749", "False"]], [["-8.245793342590332", "False"]], [["-0.7457937002182007", "True"]]], "filtered_resps": [["-3.7457938194274902", "False"], ["-6.99579381942749", "False"], ["-6.49579381942749", "False"], ["-8.245793342590332", "False"], ["-0.7457937002182007", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "99b6838ce6560d9daa854afdb2d65b1ea2183319801e1780a28c00f43cf864c6", "prompt_hash": "23a49f5c9c8a23ab0b4185f914f6e39f4048f1204a0cc1d3954035ab213abdef", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 188, "doc": {"id": "9d625e948e9c3777e7cc54ed8ffea135", "question": "Where could a sloth live?", "question_concept": "sloth", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tropical jungle", "manual", "work", "transit", "countryside"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where could a sloth live?\nA. tropical jungle\nB. manual\nC. work\nD. transit\nE. countryside\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could a sloth live?\nA. tropical jungle\nB. manual\nC. work\nD. transit\nE. countryside\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could a sloth live?\nA. tropical jungle\nB. manual\nC. work\nD. transit\nE. countryside\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could a sloth live?\nA. tropical jungle\nB. manual\nC. work\nD. transit\nE. countryside\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could a sloth live?\nA. tropical jungle\nB. manual\nC. work\nD. transit\nE. countryside\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8556829690933228", "True"]], [["-8.355683326721191", "False"]], [["-8.855683326721191", "False"]], [["-9.855683326721191", "False"]], [["-8.355683326721191", "False"]]], "filtered_resps": [["-0.8556829690933228", "True"], ["-8.355683326721191", "False"], ["-8.855683326721191", "False"], ["-9.855683326721191", "False"], ["-8.355683326721191", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "57261cf79a0ff3907f22a0905645b9aa58ce62dc84ac895d677a4ad60057e2b9", "prompt_hash": "32061f6290c3078b7a5a4e39ec3cf9006b06c3453312caf268239c9315cbd8df", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 189, "doc": {"id": "d107d67d525a686fbd8282314d2ea33c", "question": "A gentleman is carrying equipment for golf, what is he likely to have?", "question_concept": "gentleman", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["club", "assembly hall", "meditation center", "meeting", "church"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A gentleman is carrying equipment for golf, what is he likely to have?\nA. club\nB. assembly hall\nC. meditation center\nD. meeting\nE. church\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A gentleman is carrying equipment for golf, what is he likely to have?\nA. club\nB. assembly hall\nC. meditation center\nD. meeting\nE. church\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A gentleman is carrying equipment for golf, what is he likely to have?\nA. club\nB. assembly hall\nC. meditation center\nD. meeting\nE. church\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A gentleman is carrying equipment for golf, what is he likely to have?\nA. club\nB. assembly hall\nC. meditation center\nD. meeting\nE. church\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A gentleman is carrying equipment for golf, what is he likely to have?\nA. club\nB. assembly hall\nC. meditation center\nD. meeting\nE. church\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4865689277648926", "True"]], [["-6.986568927764893", "False"]], [["-7.736568927764893", "False"]], [["-9.236568450927734", "False"]], [["-10.986568450927734", "False"]]], "filtered_resps": [["-0.4865689277648926", "True"], ["-6.986568927764893", "False"], ["-7.736568927764893", "False"], ["-9.236568450927734", "False"], ["-10.986568450927734", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "678b59ed52bc65da9a7ad369d1159fe83ff6d97602be36680e229d25bd6793d5", "prompt_hash": "024177c5f8d49708028d69f9c3682e3b0734afb9f71e6c5aead71cf28206918c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 190, "doc": {"id": "fee5ff19811750ad019665af7b36b3c4", "question": "If you have a home with a courtyard, what's one thing you probably don't have to care for any longer?", "question_concept": "courtyard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lawn", "kids", "asshole", "spain", "office complex"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If you have a home with a courtyard, what's one thing you probably don't have to care for any longer?\nA. lawn\nB. kids\nC. asshole\nD. spain\nE. office complex\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you have a home with a courtyard, what's one thing you probably don't have to care for any longer?\nA. lawn\nB. kids\nC. asshole\nD. spain\nE. office complex\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you have a home with a courtyard, what's one thing you probably don't have to care for any longer?\nA. lawn\nB. kids\nC. asshole\nD. spain\nE. office complex\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you have a home with a courtyard, what's one thing you probably don't have to care for any longer?\nA. lawn\nB. kids\nC. asshole\nD. spain\nE. office complex\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you have a home with a courtyard, what's one thing you probably don't have to care for any longer?\nA. lawn\nB. kids\nC. asshole\nD. spain\nE. office complex\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3224587440490723", "True"]], [["-2.8224587440490723", "False"]], [["-2.8224587440490723", "False"]], [["-4.322458744049072", "False"]], [["-3.5724587440490723", "False"]]], "filtered_resps": [["-1.3224587440490723", "True"], ["-2.8224587440490723", "False"], ["-2.8224587440490723", "False"], ["-4.322458744049072", "False"], ["-3.5724587440490723", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c959cfcfa740fe98fa0172f853570ee5df4e3ef9e53cb3823764e6b2c4c45b78", "prompt_hash": "4d2d8673705d27abab08f27f65fe1020d8a27ba775e6c6b9e9af6dc1d742f9d5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 191, "doc": {"id": "e69da59cbcf2a302e4523571eba8186b", "question": "The computer was difficult for he to understand at the store, so what did she sign up for to learn more?", "question_concept": "computer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["classroom", "facebook", "school", "apartment", "demonstration"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The computer was difficult for he to understand at the store, so what did she sign up for to learn more?\nA. classroom\nB. facebook\nC. school\nD. apartment\nE. demonstration\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The computer was difficult for he to understand at the store, so what did she sign up for to learn more?\nA. classroom\nB. facebook\nC. school\nD. apartment\nE. demonstration\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The computer was difficult for he to understand at the store, so what did she sign up for to learn more?\nA. classroom\nB. facebook\nC. school\nD. apartment\nE. demonstration\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The computer was difficult for he to understand at the store, so what did she sign up for to learn more?\nA. classroom\nB. facebook\nC. school\nD. apartment\nE. demonstration\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The computer was difficult for he to understand at the store, so what did she sign up for to learn more?\nA. classroom\nB. facebook\nC. school\nD. apartment\nE. demonstration\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7929348945617676", "True"]], [["-5.542934894561768", "False"]], [["-3.0429348945617676", "False"]], [["-6.542934894561768", "False"]], [["-2.0429348945617676", "False"]]], "filtered_resps": [["-1.7929348945617676", "True"], ["-5.542934894561768", "False"], ["-3.0429348945617676", "False"], ["-6.542934894561768", "False"], ["-2.0429348945617676", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b14132b93224d2010e8025c53b030d7f744b31b4e1a9d083d1372eb0e84c8232", "prompt_hash": "d45479e0a387408e545798b7b1bae39e2fab60e0a97fdc9a09c482194c2f06a0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 192, "doc": {"id": "2dd138a63b5895cf737ced793cc668e7", "question": "If you take the risk buying a used car, you still hope it can what?", "question_concept": "car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["go fast", "start running", "going too fast", "look good", "last several years"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If you take the risk buying a used car, you still hope it can what?\nA. go fast\nB. start running\nC. going too fast\nD. look good\nE. last several years\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you take the risk buying a used car, you still hope it can what?\nA. go fast\nB. start running\nC. going too fast\nD. look good\nE. last several years\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you take the risk buying a used car, you still hope it can what?\nA. go fast\nB. start running\nC. going too fast\nD. look good\nE. last several years\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you take the risk buying a used car, you still hope it can what?\nA. go fast\nB. start running\nC. going too fast\nD. look good\nE. last several years\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you take the risk buying a used car, you still hope it can what?\nA. go fast\nB. start running\nC. going too fast\nD. look good\nE. last several years\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.434764385223389", "False"]], [["-3.4347643852233887", "False"]], [["-6.684764385223389", "False"]], [["-7.434764385223389", "False"]], [["-1.4347645044326782", "True"]]], "filtered_resps": [["-4.434764385223389", "False"], ["-3.4347643852233887", "False"], ["-6.684764385223389", "False"], ["-7.434764385223389", "False"], ["-1.4347645044326782", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5cb1d4ca5d117ba3bc612077018e546fadfd9f18d99bf50397489de658b8f609", "prompt_hash": "5684df2d6283f3c8ec779eaf6cb452549f961e4095858a86beecbc158c1d10d3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 193, "doc": {"id": "b33047f46db680a9b630c13e8ca115cc", "question": "Dan was ditting quietly on the couch with a book in his hand.  Laurie thought that he was just focused on what he was doing, but he actually did what?", "question_concept": "sitting quietly", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat", "think", "reading", "meditate", "fall asleep"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Dan was ditting quietly on the couch with a book in his hand.  Laurie thought that he was just focused on what he was doing, but he actually did what?\nA. eat\nB. think\nC. reading\nD. meditate\nE. fall asleep\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Dan was ditting quietly on the couch with a book in his hand.  Laurie thought that he was just focused on what he was doing, but he actually did what?\nA. eat\nB. think\nC. reading\nD. meditate\nE. fall asleep\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Dan was ditting quietly on the couch with a book in his hand.  Laurie thought that he was just focused on what he was doing, but he actually did what?\nA. eat\nB. think\nC. reading\nD. meditate\nE. fall asleep\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Dan was ditting quietly on the couch with a book in his hand.  Laurie thought that he was just focused on what he was doing, but he actually did what?\nA. eat\nB. think\nC. reading\nD. meditate\nE. fall asleep\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Dan was ditting quietly on the couch with a book in his hand.  Laurie thought that he was just focused on what he was doing, but he actually did what?\nA. eat\nB. think\nC. reading\nD. meditate\nE. fall asleep\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.54633903503418", "False"]], [["-4.29633903503418", "False"]], [["-1.7963392734527588", "False"]], [["-4.79633903503418", "False"]], [["-6.54633903503418", "False"]]], "filtered_resps": [["-5.54633903503418", "False"], ["-4.29633903503418", "False"], ["-1.7963392734527588", "False"], ["-4.79633903503418", "False"], ["-6.54633903503418", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bc7d67a3baefe1449c730cb6287dc55c790ef125e0e93aa65754ba68af74e027", "prompt_hash": "1d5e1defc30a6cb92a8b65a93c8d1a76bd152eabe3b625fe8a2f581f46cbf2cb", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 194, "doc": {"id": "f20d40bc4af588223e880e0bb58b27b8", "question": "What is the primary purpose of cars?", "question_concept": "cars", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cost money", "slow down", "move people", "turn right", "get girls"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is the primary purpose of cars?\nA. cost money\nB. slow down\nC. move people\nD. turn right\nE. get girls\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the primary purpose of cars?\nA. cost money\nB. slow down\nC. move people\nD. turn right\nE. get girls\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the primary purpose of cars?\nA. cost money\nB. slow down\nC. move people\nD. turn right\nE. get girls\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the primary purpose of cars?\nA. cost money\nB. slow down\nC. move people\nD. turn right\nE. get girls\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the primary purpose of cars?\nA. cost money\nB. slow down\nC. move people\nD. turn right\nE. get girls\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.431499481201172", "False"]], [["-8.681499481201172", "False"]], [["-1.1814994812011719", "True"]], [["-7.931499481201172", "False"]], [["-9.431499481201172", "False"]]], "filtered_resps": [["-6.431499481201172", "False"], ["-8.681499481201172", "False"], ["-1.1814994812011719", "True"], ["-7.931499481201172", "False"], ["-9.431499481201172", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5f38c41ff9bdb4b04fb444ce65a5b2f12867a2e497eac1a88b324c5ef29e9e6d", "prompt_hash": "544347fb8b833e22f1046c04db62d097a27be69bef0b31c66aa3f082343bf89b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 195, "doc": {"id": "b6b66d4519a84b8331ea55f84767e9df", "question": "Alabama is full of different people, but they are all citizens of what?", "question_concept": "alabama", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["united states", "deep south", "floribama", "gulf states", "florabama"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Alabama is full of different people, but they are all citizens of what?\nA. united states\nB. deep south\nC. floribama\nD. gulf states\nE. florabama\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Alabama is full of different people, but they are all citizens of what?\nA. united states\nB. deep south\nC. floribama\nD. gulf states\nE. florabama\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Alabama is full of different people, but they are all citizens of what?\nA. united states\nB. deep south\nC. floribama\nD. gulf states\nE. florabama\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Alabama is full of different people, but they are all citizens of what?\nA. united states\nB. deep south\nC. floribama\nD. gulf states\nE. florabama\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Alabama is full of different people, but they are all citizens of what?\nA. united states\nB. deep south\nC. floribama\nD. gulf states\nE. florabama\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1032280921936035", "True"]], [["-5.6032280921936035", "False"]], [["-4.8532280921936035", "False"]], [["-7.3532280921936035", "False"]], [["-5.3532280921936035", "False"]]], "filtered_resps": [["-1.1032280921936035", "True"], ["-5.6032280921936035", "False"], ["-4.8532280921936035", "False"], ["-7.3532280921936035", "False"], ["-5.3532280921936035", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7b6e0ee129de4588d6702779db363f167c49df19f67b27a95d6579a8340b958e", "prompt_hash": "1cdcd026a0b4b8c6e648db8ea7253929d618a24d7122e61ee789ba8e49b0ca17", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 196, "doc": {"id": "952cf4b2f7a434b2eeae9f4c7ed89c0a", "question": "They were hoping their campaign would create a rise in awareness of the problem and hopefully do what to its effect?", "question_concept": "rise", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["set", "fall", "park", "descend", "reduce"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: They were hoping their campaign would create a rise in awareness of the problem and hopefully do what to its effect?\nA. set\nB. fall\nC. park\nD. descend\nE. reduce\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They were hoping their campaign would create a rise in awareness of the problem and hopefully do what to its effect?\nA. set\nB. fall\nC. park\nD. descend\nE. reduce\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They were hoping their campaign would create a rise in awareness of the problem and hopefully do what to its effect?\nA. set\nB. fall\nC. park\nD. descend\nE. reduce\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They were hoping their campaign would create a rise in awareness of the problem and hopefully do what to its effect?\nA. set\nB. fall\nC. park\nD. descend\nE. reduce\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They were hoping their campaign would create a rise in awareness of the problem and hopefully do what to its effect?\nA. set\nB. fall\nC. park\nD. descend\nE. reduce\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7049503326416016", "False"]], [["-7.454950332641602", "False"]], [["-8.454950332641602", "False"]], [["-8.454950332641602", "False"]], [["-0.9549503326416016", "True"]]], "filtered_resps": [["-3.7049503326416016", "False"], ["-7.454950332641602", "False"], ["-8.454950332641602", "False"], ["-8.454950332641602", "False"], ["-0.9549503326416016", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4bfa6d5643f3c3185296d1d0d6f873251ed2aea7a4b6638dc0d4aaca262bd7ea", "prompt_hash": "d3dea431f595cb75caf52bdc4126a084cbb63aa6729a5aae9eb1335e8066d037", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 197, "doc": {"id": "b63e5cd88bfe75d29ff9fdc6dd97fed6", "question": "What do airplanes do as they are arriving at the gate?", "question_concept": "airplanes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["slow down", "crash", "speed up", "land", "carry people"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do airplanes do as they are arriving at the gate?\nA. slow down\nB. crash\nC. speed up\nD. land\nE. carry people\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do airplanes do as they are arriving at the gate?\nA. slow down\nB. crash\nC. speed up\nD. land\nE. carry people\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do airplanes do as they are arriving at the gate?\nA. slow down\nB. crash\nC. speed up\nD. land\nE. carry people\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do airplanes do as they are arriving at the gate?\nA. slow down\nB. crash\nC. speed up\nD. land\nE. carry people\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do airplanes do as they are arriving at the gate?\nA. slow down\nB. crash\nC. speed up\nD. land\nE. carry people\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.254184603691101", "True"]], [["-7.004184722900391", "False"]], [["-6.254184722900391", "False"]], [["-4.754184722900391", "False"]], [["-9.75418472290039", "False"]]], "filtered_resps": [["-1.254184603691101", "True"], ["-7.004184722900391", "False"], ["-6.254184722900391", "False"], ["-4.754184722900391", "False"], ["-9.75418472290039", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bbb5249de5ca77e252e147f51cd22210483529f31fffc6954de719a961365f85", "prompt_hash": "0a26634dcc034851fc88b2c4223b404137ba68a31466382257be896b04a7b576", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 198, "doc": {"id": "ec5a336080e37fbe95d72ad5f9c65ba7", "question": "If a person with mental illness stops treatment what will likely happen?", "question_concept": "mental illness", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["managed", "dancing", "recur", "effectively treated", "cause suffering"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If a person with mental illness stops treatment what will likely happen?\nA. managed\nB. dancing\nC. recur\nD. effectively treated\nE. cause suffering\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a person with mental illness stops treatment what will likely happen?\nA. managed\nB. dancing\nC. recur\nD. effectively treated\nE. cause suffering\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a person with mental illness stops treatment what will likely happen?\nA. managed\nB. dancing\nC. recur\nD. effectively treated\nE. cause suffering\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a person with mental illness stops treatment what will likely happen?\nA. managed\nB. dancing\nC. recur\nD. effectively treated\nE. cause suffering\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a person with mental illness stops treatment what will likely happen?\nA. managed\nB. dancing\nC. recur\nD. effectively treated\nE. cause suffering\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.927206039428711", "False"]], [["-6.427206039428711", "False"]], [["-1.9272061586380005", "False"]], [["-7.177206039428711", "False"]], [["-4.177206039428711", "False"]]], "filtered_resps": [["-3.927206039428711", "False"], ["-6.427206039428711", "False"], ["-1.9272061586380005", "False"], ["-7.177206039428711", "False"], ["-4.177206039428711", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ec8add41541fb9494355a45b026c16717bd18ae2c94af7733da33b572086c0aa", "prompt_hash": "605d50ff9598c9e0e6d4ce9ae79c79c8d9bf8a2e9bb9a4607b9bce58e6b203f1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 199, "doc": {"id": "6386bcf080633bc3eeb3317a5435b7b7", "question": "The gimmicky low brow TV show was about animals when they what?", "question_concept": "animals", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sick", "mammals", "males", "bite", "attack"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The gimmicky low brow TV show was about animals when they what?\nA. sick\nB. mammals\nC. males\nD. bite\nE. attack\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The gimmicky low brow TV show was about animals when they what?\nA. sick\nB. mammals\nC. males\nD. bite\nE. attack\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The gimmicky low brow TV show was about animals when they what?\nA. sick\nB. mammals\nC. males\nD. bite\nE. attack\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The gimmicky low brow TV show was about animals when they what?\nA. sick\nB. mammals\nC. males\nD. bite\nE. attack\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The gimmicky low brow TV show was about animals when they what?\nA. sick\nB. mammals\nC. males\nD. bite\nE. attack\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1016578674316406", "False"]], [["-2.1016578674316406", "False"]], [["-4.351657867431641", "False"]], [["-4.101657867431641", "False"]], [["-2.3516578674316406", "False"]]], "filtered_resps": [["-3.1016578674316406", "False"], ["-2.1016578674316406", "False"], ["-4.351657867431641", "False"], ["-4.101657867431641", "False"], ["-2.3516578674316406", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ae2cfe682fddfe937eac1d6fff23068fb1434271b86b4258021139b4fe6aa5db", "prompt_hash": "31f9da4b7450519cb05c3217021a17171d67ffde772443f1530ab5bf4ed40d7d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 200, "doc": {"id": "43ab0ff711e60d51f943bbd2cdd6515a", "question": "A loud machine is irritating, but many are expected where?", "question_concept": "machine", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["museum", "house", "laboratory", "library", "industrial area"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: A loud machine is irritating, but many are expected where?\nA. museum\nB. house\nC. laboratory\nD. library\nE. industrial area\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A loud machine is irritating, but many are expected where?\nA. museum\nB. house\nC. laboratory\nD. library\nE. industrial area\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A loud machine is irritating, but many are expected where?\nA. museum\nB. house\nC. laboratory\nD. library\nE. industrial area\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A loud machine is irritating, but many are expected where?\nA. museum\nB. house\nC. laboratory\nD. library\nE. industrial area\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A loud machine is irritating, but many are expected where?\nA. museum\nB. house\nC. laboratory\nD. library\nE. industrial area\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.9566200971603394", "False"]], [["-4.956620216369629", "False"]], [["-4.956620216369629", "False"]], [["-4.456620216369629", "False"]], [["-0.7066200971603394", "True"]]], "filtered_resps": [["-1.9566200971603394", "False"], ["-4.956620216369629", "False"], ["-4.956620216369629", "False"], ["-4.456620216369629", "False"], ["-0.7066200971603394", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3c1c91380c70881e60f5be3c541e02fcd3d17f229f1bebb3fca5c83a9bdef6a3", "prompt_hash": "080ee9a99185f6e3454fa3bf5c11746cbbd903bf0b25bf0073c6ef2fb085d1a9", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 201, "doc": {"id": "11c4c78d61e8212f0984fd07eb22b669", "question": "What part of a table would you put a ruler in?", "question_concept": "ruler", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["drawer", "desk", "the backside", "office", "measure distance"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What part of a table would you put a ruler in?\nA. drawer\nB. desk\nC. the backside\nD. office\nE. measure distance\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What part of a table would you put a ruler in?\nA. drawer\nB. desk\nC. the backside\nD. office\nE. measure distance\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What part of a table would you put a ruler in?\nA. drawer\nB. desk\nC. the backside\nD. office\nE. measure distance\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What part of a table would you put a ruler in?\nA. drawer\nB. desk\nC. the backside\nD. office\nE. measure distance\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What part of a table would you put a ruler in?\nA. drawer\nB. desk\nC. the backside\nD. office\nE. measure distance\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.0237460136413574", "False"]], [["-1.7737458944320679", "True"]], [["-6.273746013641357", "False"]], [["-7.023746013641357", "False"]], [["-1.7737458944320679", "True"]]], "filtered_resps": [["-2.0237460136413574", "False"], ["-1.7737458944320679", "True"], ["-6.273746013641357", "False"], ["-7.023746013641357", "False"], ["-1.7737458944320679", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d86e52a0d680af1cc8325d77d257cc5c9a884248192c2a5799815c8a8bd5f817", "prompt_hash": "f3ea164cd1e7eaa487610f90129885d51c36d2f6902061585976dbc36465f89f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 202, "doc": {"id": "e61891746aa94ab57aaa754614034aef", "question": "What happens if someone kisses too long?", "question_concept": "kissing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["strong feelings", "herpes", "shortness of breath", "excitement", "arousal"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What happens if someone kisses too long?\nA. strong feelings\nB. herpes\nC. shortness of breath\nD. excitement\nE. arousal\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens if someone kisses too long?\nA. strong feelings\nB. herpes\nC. shortness of breath\nD. excitement\nE. arousal\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens if someone kisses too long?\nA. strong feelings\nB. herpes\nC. shortness of breath\nD. excitement\nE. arousal\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens if someone kisses too long?\nA. strong feelings\nB. herpes\nC. shortness of breath\nD. excitement\nE. arousal\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens if someone kisses too long?\nA. strong feelings\nB. herpes\nC. shortness of breath\nD. excitement\nE. arousal\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.2387943267822266", "False"]], [["-4.488794326782227", "False"]], [["-3.9887943267822266", "False"]], [["-4.988794326782227", "False"]], [["-3.9887943267822266", "False"]]], "filtered_resps": [["-2.2387943267822266", "False"], ["-4.488794326782227", "False"], ["-3.9887943267822266", "False"], ["-4.988794326782227", "False"], ["-3.9887943267822266", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2392c48b72c5a2fd3ce9f577b6a21f179a8579a15229c43dbeff0370815cbbb8", "prompt_hash": "1b64ea8b4225944a4b79de13bdc83a4a38537aa948c665e4d7ba09eb0a8e62eb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 203, "doc": {"id": "97da9aa4ea4b22744ec51cba49f35bfc", "question": "If I have a modern light source in my living room, what is it likely to be?", "question_concept": "light source", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sky", "house", "lamp", "match", "candle"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If I have a modern light source in my living room, what is it likely to be?\nA. sky\nB. house\nC. lamp\nD. match\nE. candle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I have a modern light source in my living room, what is it likely to be?\nA. sky\nB. house\nC. lamp\nD. match\nE. candle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I have a modern light source in my living room, what is it likely to be?\nA. sky\nB. house\nC. lamp\nD. match\nE. candle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I have a modern light source in my living room, what is it likely to be?\nA. sky\nB. house\nC. lamp\nD. match\nE. candle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I have a modern light source in my living room, what is it likely to be?\nA. sky\nB. house\nC. lamp\nD. match\nE. candle\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.758535861968994", "False"]], [["-7.008535861968994", "False"]], [["-1.2585357427597046", "False"]], [["-9.008535385131836", "False"]], [["-8.758535385131836", "False"]]], "filtered_resps": [["-2.758535861968994", "False"], ["-7.008535861968994", "False"], ["-1.2585357427597046", "False"], ["-9.008535385131836", "False"], ["-8.758535385131836", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a37178ef4845468b13126d14c49c37bb2b0e2c5f197d2791d8e3118a2263debf", "prompt_hash": "ef90e07460a0e350cfcf4dac091e580ffeb5bbee9e000595315cb82e664ecc5a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 204, "doc": {"id": "46241bc83e8d81196ae5783b2b9854a4", "question": "The person saw the mess his children made, what was his following reaction?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["smell smoke", "cross street", "cry", "bank savings", "look angry"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The person saw the mess his children made, what was his following reaction?\nA. smell smoke\nB. cross street\nC. cry\nD. bank savings\nE. look angry\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The person saw the mess his children made, what was his following reaction?\nA. smell smoke\nB. cross street\nC. cry\nD. bank savings\nE. look angry\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The person saw the mess his children made, what was his following reaction?\nA. smell smoke\nB. cross street\nC. cry\nD. bank savings\nE. look angry\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The person saw the mess his children made, what was his following reaction?\nA. smell smoke\nB. cross street\nC. cry\nD. bank savings\nE. look angry\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The person saw the mess his children made, what was his following reaction?\nA. smell smoke\nB. cross street\nC. cry\nD. bank savings\nE. look angry\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.982516288757324", "False"]], [["-6.982516288757324", "False"]], [["-5.732516288757324", "False"]], [["-8.232516288757324", "False"]], [["-1.4825165271759033", "False"]]], "filtered_resps": [["-4.982516288757324", "False"], ["-6.982516288757324", "False"], ["-5.732516288757324", "False"], ["-8.232516288757324", "False"], ["-1.4825165271759033", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6df84b7303de4abf7d08e67e9865240fdbc22647ba7485663e2cbdc679897990", "prompt_hash": "0e9b9297233b190407df33dd7f578baea96326ee21f098d353305ebb5a91a24e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 205, "doc": {"id": "18844d3aa4e52b331b5382c8244cf4db", "question": "Who might wear dark glasses indoors?", "question_concept": "dark glasses", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["blind person", "glove box", "movie studio", "ray charles", "glove compartment"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Who might wear dark glasses indoors?\nA. blind person\nB. glove box\nC. movie studio\nD. ray charles\nE. glove compartment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Who might wear dark glasses indoors?\nA. blind person\nB. glove box\nC. movie studio\nD. ray charles\nE. glove compartment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Who might wear dark glasses indoors?\nA. blind person\nB. glove box\nC. movie studio\nD. ray charles\nE. glove compartment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Who might wear dark glasses indoors?\nA. blind person\nB. glove box\nC. movie studio\nD. ray charles\nE. glove compartment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Who might wear dark glasses indoors?\nA. blind person\nB. glove box\nC. movie studio\nD. ray charles\nE. glove compartment\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5358412265777588", "True"]], [["-4.53584098815918", "False"]], [["-5.28584098815918", "False"]], [["-1.7858412265777588", "False"]], [["-8.28584098815918", "False"]]], "filtered_resps": [["-1.5358412265777588", "True"], ["-4.53584098815918", "False"], ["-5.28584098815918", "False"], ["-1.7858412265777588", "False"], ["-8.28584098815918", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ddd2a89c66b2306aaf6f788e6f123655b48731fc9d01ff7e2251dd72e1fb6868", "prompt_hash": "80b429a6b1a86a4feb70ae10fee4df55ecefd5380cb6b58ade318bfd9422db66", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 206, "doc": {"id": "056b33c7050c167b0d4348d40d169358", "question": "Where would stones not be arranged in a path?", "question_concept": "stones", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["quarries", "field", "park", "bridge", "made from rocks"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where would stones not be arranged in a path?\nA. quarries\nB. field\nC. park\nD. bridge\nE. made from rocks\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would stones not be arranged in a path?\nA. quarries\nB. field\nC. park\nD. bridge\nE. made from rocks\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would stones not be arranged in a path?\nA. quarries\nB. field\nC. park\nD. bridge\nE. made from rocks\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would stones not be arranged in a path?\nA. quarries\nB. field\nC. park\nD. bridge\nE. made from rocks\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would stones not be arranged in a path?\nA. quarries\nB. field\nC. park\nD. bridge\nE. made from rocks\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.685943841934204", "True"]], [["-4.685943603515625", "False"]], [["-5.435943603515625", "False"]], [["-2.435943841934204", "False"]], [["-2.935943841934204", "False"]]], "filtered_resps": [["-1.685943841934204", "True"], ["-4.685943603515625", "False"], ["-5.435943603515625", "False"], ["-2.435943841934204", "False"], ["-2.935943841934204", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c4e0e602f3d221f4fbbbd1d1d82400b0104b16b1af5974cd46682f786633d09c", "prompt_hash": "682c6d16f4892657a75df1e8ec550c4e6d90299a72a7854f75fd1aa08e3aa177", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 207, "doc": {"id": "31d7dd1d00aabe411568df3e72d5b5e0", "question": "A bald eagle is likely to be found on what kind of work?", "question_concept": "bald eagle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rural area", "book", "canada", "painting", "aviary"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: A bald eagle is likely to be found on what kind of work?\nA. rural area\nB. book\nC. canada\nD. painting\nE. aviary\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A bald eagle is likely to be found on what kind of work?\nA. rural area\nB. book\nC. canada\nD. painting\nE. aviary\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A bald eagle is likely to be found on what kind of work?\nA. rural area\nB. book\nC. canada\nD. painting\nE. aviary\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A bald eagle is likely to be found on what kind of work?\nA. rural area\nB. book\nC. canada\nD. painting\nE. aviary\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A bald eagle is likely to be found on what kind of work?\nA. rural area\nB. book\nC. canada\nD. painting\nE. aviary\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6061313152313232", "True"]], [["-3.8561313152313232", "False"]], [["-5.856131553649902", "False"]], [["-5.606131553649902", "False"]], [["-4.856131553649902", "False"]]], "filtered_resps": [["-0.6061313152313232", "True"], ["-3.8561313152313232", "False"], ["-5.856131553649902", "False"], ["-5.606131553649902", "False"], ["-4.856131553649902", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c8db0152f338fd2e32c5e60201294a54719f0771307f769d992b6d82ad716afd", "prompt_hash": "b57a294a2f888c827ebf096213453fbbecafcfc1e0ef4a057bb5b518f5011ecb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 208, "doc": {"id": "cbf3dd48b4d591fc872a53cd4b9dd3af", "question": "The hostess was good at her job, she always had a smile when she would what?", "question_concept": "hostess", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["group people", "ready parlor for guests", "welcome guests", "work room", "park"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The hostess was good at her job, she always had a smile when she would what?\nA. group people\nB. ready parlor for guests\nC. welcome guests\nD. work room\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The hostess was good at her job, she always had a smile when she would what?\nA. group people\nB. ready parlor for guests\nC. welcome guests\nD. work room\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The hostess was good at her job, she always had a smile when she would what?\nA. group people\nB. ready parlor for guests\nC. welcome guests\nD. work room\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The hostess was good at her job, she always had a smile when she would what?\nA. group people\nB. ready parlor for guests\nC. welcome guests\nD. work room\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The hostess was good at her job, she always had a smile when she would what?\nA. group people\nB. ready parlor for guests\nC. welcome guests\nD. work room\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.610381603240967", "False"]], [["-5.610381603240967", "False"]], [["-0.8603814840316772", "True"]], [["-7.610381603240967", "False"]], [["-9.110381126403809", "False"]]], "filtered_resps": [["-4.610381603240967", "False"], ["-5.610381603240967", "False"], ["-0.8603814840316772", "True"], ["-7.610381603240967", "False"], ["-9.110381126403809", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "082eba17d47ac2d8a9a4f21fc8291027dd1054985708e271315edf1f5524cc91", "prompt_hash": "7feadb21b99d32a1567972b7d01e7fc7868f9a70fb1a3b293fffff58cfbd66da", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 209, "doc": {"id": "60e8f1a86d4063895f340cd1e3c55f50", "question": "What is likely to happen to someone who is learning?", "question_concept": "learning", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["overconfidence", "effectiveness", "knowing more", "head grows larger", "growth"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is likely to happen to someone who is learning?\nA. overconfidence\nB. effectiveness\nC. knowing more\nD. head grows larger\nE. growth\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is likely to happen to someone who is learning?\nA. overconfidence\nB. effectiveness\nC. knowing more\nD. head grows larger\nE. growth\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is likely to happen to someone who is learning?\nA. overconfidence\nB. effectiveness\nC. knowing more\nD. head grows larger\nE. growth\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is likely to happen to someone who is learning?\nA. overconfidence\nB. effectiveness\nC. knowing more\nD. head grows larger\nE. growth\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is likely to happen to someone who is learning?\nA. overconfidence\nB. effectiveness\nC. knowing more\nD. head grows larger\nE. growth\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.407792091369629", "False"]], [["-3.407792091369629", "False"]], [["-2.657792091369629", "False"]], [["-5.407792091369629", "False"]], [["-1.9077919721603394", "True"]]], "filtered_resps": [["-3.407792091369629", "False"], ["-3.407792091369629", "False"], ["-2.657792091369629", "False"], ["-5.407792091369629", "False"], ["-1.9077919721603394", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7ddd324359062216018df5ea6e737ddf8fe9dc82da718c07f9f0e355772f7ac8", "prompt_hash": "859dda016bd832983b96617b069545f87b6f426cd9701d0883d232e027217c37", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 210, "doc": {"id": "eee8cb7a0d806a62d2de24831f82e3e1", "question": "The inspector was agreeing with the factory protocols, what was the status of the factory?", "question_concept": "agreeing with", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["compliance", "eligible", "contract", "harmony", "friendship"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The inspector was agreeing with the factory protocols, what was the status of the factory?\nA. compliance\nB. eligible\nC. contract\nD. harmony\nE. friendship\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The inspector was agreeing with the factory protocols, what was the status of the factory?\nA. compliance\nB. eligible\nC. contract\nD. harmony\nE. friendship\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The inspector was agreeing with the factory protocols, what was the status of the factory?\nA. compliance\nB. eligible\nC. contract\nD. harmony\nE. friendship\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The inspector was agreeing with the factory protocols, what was the status of the factory?\nA. compliance\nB. eligible\nC. contract\nD. harmony\nE. friendship\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The inspector was agreeing with the factory protocols, what was the status of the factory?\nA. compliance\nB. eligible\nC. contract\nD. harmony\nE. friendship\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9640578627586365", "True"]], [["-7.464057922363281", "False"]], [["-7.714057922363281", "False"]], [["-4.214057922363281", "False"]], [["-9.464057922363281", "False"]]], "filtered_resps": [["-0.9640578627586365", "True"], ["-7.464057922363281", "False"], ["-7.714057922363281", "False"], ["-4.214057922363281", "False"], ["-9.464057922363281", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e671e03acac35a722b81eff050db40a18e46d7212da0a5f43a4c4d63b262e20c", "prompt_hash": "3f6cad994d9c80d48d91bef80427e69ecbc43402b2776d85e9c137001b2abd26", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 211, "doc": {"id": "9a23a7f04e63bf9f4c7dfe50c58abfd2", "question": "After standing up I had to sit right back down, why would I feel like this?", "question_concept": "standing up", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["train", "effort", "balance", "feet", "muscles"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: After standing up I had to sit right back down, why would I feel like this?\nA. train\nB. effort\nC. balance\nD. feet\nE. muscles\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: After standing up I had to sit right back down, why would I feel like this?\nA. train\nB. effort\nC. balance\nD. feet\nE. muscles\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: After standing up I had to sit right back down, why would I feel like this?\nA. train\nB. effort\nC. balance\nD. feet\nE. muscles\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: After standing up I had to sit right back down, why would I feel like this?\nA. train\nB. effort\nC. balance\nD. feet\nE. muscles\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: After standing up I had to sit right back down, why would I feel like this?\nA. train\nB. effort\nC. balance\nD. feet\nE. muscles\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.904409885406494", "False"]], [["-3.404409885406494", "False"]], [["-1.6544098854064941", "True"]], [["-5.154409885406494", "False"]], [["-2.904409885406494", "False"]]], "filtered_resps": [["-3.904409885406494", "False"], ["-3.404409885406494", "False"], ["-1.6544098854064941", "True"], ["-5.154409885406494", "False"], ["-2.904409885406494", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "66b49c5e2b05a612c9aab320f65fdf627b707028407e299ce4cbe01448af1519", "prompt_hash": "8683e4eff47338deb10d583a6940b27fdad15f1dbf160a270cb6b0ad77962d52", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 212, "doc": {"id": "e3426e4f60c142aa3d813479f79d6305", "question": "Where do you go on a night out before going to the bar?", "question_concept": "bar", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["new york city", "las vegas", "restaurant", "nightclub", "park"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you go on a night out before going to the bar?\nA. new york city\nB. las vegas\nC. restaurant\nD. nightclub\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you go on a night out before going to the bar?\nA. new york city\nB. las vegas\nC. restaurant\nD. nightclub\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you go on a night out before going to the bar?\nA. new york city\nB. las vegas\nC. restaurant\nD. nightclub\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you go on a night out before going to the bar?\nA. new york city\nB. las vegas\nC. restaurant\nD. nightclub\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you go on a night out before going to the bar?\nA. new york city\nB. las vegas\nC. restaurant\nD. nightclub\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2584877014160156", "False"]], [["-4.508487701416016", "False"]], [["-1.2584877014160156", "True"]], [["-4.758487701416016", "False"]], [["-8.758487701416016", "False"]]], "filtered_resps": [["-3.2584877014160156", "False"], ["-4.508487701416016", "False"], ["-1.2584877014160156", "True"], ["-4.758487701416016", "False"], ["-8.758487701416016", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1d832ed98ac2209dc7341c06cb3dfde265769f2b66f15893454d018238ec7ed4", "prompt_hash": "dd33243fb8928847067e61d32d2d35c849df451915410c0221dfd07cd2945e3f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 213, "doc": {"id": "3526550b02d9594abd4fc43553010fc6", "question": "The dad wanted to protect his house, where did he put his gun?", "question_concept": "gun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["police station", "crime scene", "restroom", "drawer", "holster"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The dad wanted to protect his house, where did he put his gun?\nA. police station\nB. crime scene\nC. restroom\nD. drawer\nE. holster\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The dad wanted to protect his house, where did he put his gun?\nA. police station\nB. crime scene\nC. restroom\nD. drawer\nE. holster\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The dad wanted to protect his house, where did he put his gun?\nA. police station\nB. crime scene\nC. restroom\nD. drawer\nE. holster\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The dad wanted to protect his house, where did he put his gun?\nA. police station\nB. crime scene\nC. restroom\nD. drawer\nE. holster\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The dad wanted to protect his house, where did he put his gun?\nA. police station\nB. crime scene\nC. restroom\nD. drawer\nE. holster\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.361252784729004", "False"]], [["-5.861252784729004", "False"]], [["-5.861252784729004", "False"]], [["-1.6112526655197144", "True"]], [["-3.361252784729004", "False"]]], "filtered_resps": [["-3.361252784729004", "False"], ["-5.861252784729004", "False"], ["-5.861252784729004", "False"], ["-1.6112526655197144", "True"], ["-3.361252784729004", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1a52f49f5670973f4d80902f296ec03237b39dfc99feedf74913ed01f047f9ff", "prompt_hash": "7c0fe43fa62b65ec99e1dae93c717818bdf38ed58e6e5da3b82240963909820a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 214, "doc": {"id": "e567c94d88829fb07a30e3d46c02e664", "question": "What instrument can be played with an air of happiness?", "question_concept": "happiness", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["jump up and down", "jump up and down", "sing", "play games", "fiddle"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What instrument can be played with an air of happiness?\nA. jump up and down\nB. jump up and down\nC. sing\nD. play games\nE. fiddle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What instrument can be played with an air of happiness?\nA. jump up and down\nB. jump up and down\nC. sing\nD. play games\nE. fiddle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What instrument can be played with an air of happiness?\nA. jump up and down\nB. jump up and down\nC. sing\nD. play games\nE. fiddle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What instrument can be played with an air of happiness?\nA. jump up and down\nB. jump up and down\nC. sing\nD. play games\nE. fiddle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What instrument can be played with an air of happiness?\nA. jump up and down\nB. jump up and down\nC. sing\nD. play games\nE. fiddle\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.847763776779175", "False"]], [["-5.347764015197754", "False"]], [["-1.8477637767791748", "True"]], [["-7.847764015197754", "False"]], [["-2.347763776779175", "False"]]], "filtered_resps": [["-3.847763776779175", "False"], ["-5.347764015197754", "False"], ["-1.8477637767791748", "True"], ["-7.847764015197754", "False"], ["-2.347763776779175", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "02e9853b962dde61c83c99058cc06d68e9f6c2c8dd87778fbc2d730a2957c02c", "prompt_hash": "80e7076b4032dbbcdfa6ddb3d0ac89795abb8c9b9a877dc34c56fca13377954e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 215, "doc": {"id": "cf5a710c931779fb3dde198e0ace3b6a", "question": "What to kids do for boredom on a ramp?", "question_concept": "boredom", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["watch film", "fire game", "hang out at bar", "go skiing", "skateboard"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What to kids do for boredom on a ramp?\nA. watch film\nB. fire game\nC. hang out at bar\nD. go skiing\nE. skateboard\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What to kids do for boredom on a ramp?\nA. watch film\nB. fire game\nC. hang out at bar\nD. go skiing\nE. skateboard\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What to kids do for boredom on a ramp?\nA. watch film\nB. fire game\nC. hang out at bar\nD. go skiing\nE. skateboard\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What to kids do for boredom on a ramp?\nA. watch film\nB. fire game\nC. hang out at bar\nD. go skiing\nE. skateboard\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What to kids do for boredom on a ramp?\nA. watch film\nB. fire game\nC. hang out at bar\nD. go skiing\nE. skateboard\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6818742752075195", "False"]], [["-2.6818742752075195", "False"]], [["-4.6818742752075195", "False"]], [["-3.6818742752075195", "False"]], [["-1.4318742752075195", "True"]]], "filtered_resps": [["-3.6818742752075195", "False"], ["-2.6818742752075195", "False"], ["-4.6818742752075195", "False"], ["-3.6818742752075195", "False"], ["-1.4318742752075195", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "66c028fb54059c8a86c4df9db8ab13bfb13f751793c513e1dfe636385127effe", "prompt_hash": "65a3ba1f062871d0c6066074d842a58ae8493ee274ffb20e1d3a0e3e698d3d73", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 216, "doc": {"id": "0f2377604e628c55ba588366139396b9", "question": "What animal has quills all over it?", "question_concept": "quill", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feather", "chicken", "calligraphy", "porcupine", "hedgehog"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What animal has quills all over it?\nA. feather\nB. chicken\nC. calligraphy\nD. porcupine\nE. hedgehog\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What animal has quills all over it?\nA. feather\nB. chicken\nC. calligraphy\nD. porcupine\nE. hedgehog\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What animal has quills all over it?\nA. feather\nB. chicken\nC. calligraphy\nD. porcupine\nE. hedgehog\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What animal has quills all over it?\nA. feather\nB. chicken\nC. calligraphy\nD. porcupine\nE. hedgehog\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What animal has quills all over it?\nA. feather\nB. chicken\nC. calligraphy\nD. porcupine\nE. hedgehog\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.109224319458008", "False"]], [["-6.109224319458008", "False"]], [["-8.859224319458008", "False"]], [["-1.3592242002487183", "False"]], [["-4.359224319458008", "False"]]], "filtered_resps": [["-3.109224319458008", "False"], ["-6.109224319458008", "False"], ["-8.859224319458008", "False"], ["-1.3592242002487183", "False"], ["-4.359224319458008", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1ca332a8f13a32da267a90ae6fc01c45923ef3e65ab84c85b08f23162cb00dba", "prompt_hash": "4255d499beb1f7d7e80cc6583251df9fcf01bb4eca743734dc90f2bde42428d9", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 217, "doc": {"id": "ada088b7c97de80336ad043757c2db16", "question": "Why would you go to an office?", "question_concept": "office", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["work", "school building", "paper", "city", "habit"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Why would you go to an office?\nA. work\nB. school building\nC. paper\nD. city\nE. habit\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would you go to an office?\nA. work\nB. school building\nC. paper\nD. city\nE. habit\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would you go to an office?\nA. work\nB. school building\nC. paper\nD. city\nE. habit\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would you go to an office?\nA. work\nB. school building\nC. paper\nD. city\nE. habit\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would you go to an office?\nA. work\nB. school building\nC. paper\nD. city\nE. habit\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5322710275650024", "True"]], [["-7.282270908355713", "False"]], [["-9.532271385192871", "False"]], [["-9.282271385192871", "False"]], [["-7.782270908355713", "False"]]], "filtered_resps": [["-0.5322710275650024", "True"], ["-7.282270908355713", "False"], ["-9.532271385192871", "False"], ["-9.282271385192871", "False"], ["-7.782270908355713", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "872235cb4ba8a144ef64ac93ff3070e3f1bd9953e8cdacc79beccbaf607d5205", "prompt_hash": "27fc63c89bd126728abd1fecd1e527566c0762c6c3e78227261f3172f5ba844a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 218, "doc": {"id": "beef0aa2058297904bb4acc1dc340c85", "question": "When is the worst time for having food?", "question_concept": "having food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["digesting", "not hungry", "gas", "weight gain", "feeling of fullness"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: When is the worst time for having food?\nA. digesting\nB. not hungry\nC. gas\nD. weight gain\nE. feeling of fullness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When is the worst time for having food?\nA. digesting\nB. not hungry\nC. gas\nD. weight gain\nE. feeling of fullness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When is the worst time for having food?\nA. digesting\nB. not hungry\nC. gas\nD. weight gain\nE. feeling of fullness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When is the worst time for having food?\nA. digesting\nB. not hungry\nC. gas\nD. weight gain\nE. feeling of fullness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When is the worst time for having food?\nA. digesting\nB. not hungry\nC. gas\nD. weight gain\nE. feeling of fullness\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.400810718536377", "False"]], [["-2.150810718536377", "False"]], [["-3.650810718536377", "False"]], [["-4.900810718536377", "False"]], [["-2.650810718536377", "False"]]], "filtered_resps": [["-2.400810718536377", "False"], ["-2.150810718536377", "False"], ["-3.650810718536377", "False"], ["-4.900810718536377", "False"], ["-2.650810718536377", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2735dcf23f48e5ecc301e09e351bf4f906318072add07ee8aadc1d88921eeae8", "prompt_hash": "d3e5acdffa7fa8e31a9f9c5b4811a68029134b2af2dce4f76bb26550adfb0c51", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 219, "doc": {"id": "ba9a05bd2086c0d37733e26479d6630f", "question": "If you spend all your time buying and not saving what is is likely to happen?", "question_concept": "buying", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["using money", "feel better", "ocean", "losing money", "go broke"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If you spend all your time buying and not saving what is is likely to happen?\nA. using money\nB. feel better\nC. ocean\nD. losing money\nE. go broke\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you spend all your time buying and not saving what is is likely to happen?\nA. using money\nB. feel better\nC. ocean\nD. losing money\nE. go broke\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you spend all your time buying and not saving what is is likely to happen?\nA. using money\nB. feel better\nC. ocean\nD. losing money\nE. go broke\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you spend all your time buying and not saving what is is likely to happen?\nA. using money\nB. feel better\nC. ocean\nD. losing money\nE. go broke\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you spend all your time buying and not saving what is is likely to happen?\nA. using money\nB. feel better\nC. ocean\nD. losing money\nE. go broke\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.833751678466797", "False"]], [["-5.583751678466797", "False"]], [["-7.333751678466797", "False"]], [["-2.3337514400482178", "False"]], [["-2.0837514400482178", "False"]]], "filtered_resps": [["-5.833751678466797", "False"], ["-5.583751678466797", "False"], ["-7.333751678466797", "False"], ["-2.3337514400482178", "False"], ["-2.0837514400482178", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "48948281f2dcd72bf306bd6e8f69b84de9ecb71e9ece6b805fdd9515fae9b3d0", "prompt_hash": "153331e6acf68e610db639ab91bf98834281d62eecaeb8036f3ea1824ce861ef", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 220, "doc": {"id": "6b0bf501aa68b06ddc5ad72ac5ff68fc", "question": "Though a mouse might prefer your house, you might also see him where?", "question_concept": "mouse", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tin", "department store", "garden", "small hole", "cupboard"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Though a mouse might prefer your house, you might also see him where?\nA. tin\nB. department store\nC. garden\nD. small hole\nE. cupboard\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Though a mouse might prefer your house, you might also see him where?\nA. tin\nB. department store\nC. garden\nD. small hole\nE. cupboard\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Though a mouse might prefer your house, you might also see him where?\nA. tin\nB. department store\nC. garden\nD. small hole\nE. cupboard\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Though a mouse might prefer your house, you might also see him where?\nA. tin\nB. department store\nC. garden\nD. small hole\nE. cupboard\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Though a mouse might prefer your house, you might also see him where?\nA. tin\nB. department store\nC. garden\nD. small hole\nE. cupboard\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.417527675628662", "False"]], [["-5.917527675628662", "False"]], [["-2.667527675628662", "False"]], [["-2.417527675628662", "False"]], [["-6.917527675628662", "False"]]], "filtered_resps": [["-2.417527675628662", "False"], ["-5.917527675628662", "False"], ["-2.667527675628662", "False"], ["-2.417527675628662", "False"], ["-6.917527675628662", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "60e765dd48bd65bd69f5d14d621b59fb764243ed8917bee1b77587595b24860e", "prompt_hash": "7f3d3e5afc74b44fd5c16ba5270777c0b05cdc5eb42100765bebdd1d23bfb00c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 221, "doc": {"id": "926298bbdd03ce96acfeb4408b888b61", "question": "What is performing a type of?", "question_concept": "performing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["singing", "act", "feat", "smile", "acting"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is performing a type of?\nA. singing\nB. act\nC. feat\nD. smile\nE. acting\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is performing a type of?\nA. singing\nB. act\nC. feat\nD. smile\nE. acting\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is performing a type of?\nA. singing\nB. act\nC. feat\nD. smile\nE. acting\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is performing a type of?\nA. singing\nB. act\nC. feat\nD. smile\nE. acting\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is performing a type of?\nA. singing\nB. act\nC. feat\nD. smile\nE. acting\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3171660900115967", "False"]], [["-3.3171660900115967", "False"]], [["-2.5671660900115967", "False"]], [["-6.567166328430176", "False"]], [["-1.3171660900115967", "True"]]], "filtered_resps": [["-3.3171660900115967", "False"], ["-3.3171660900115967", "False"], ["-2.5671660900115967", "False"], ["-6.567166328430176", "False"], ["-1.3171660900115967", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2b764869fdd78abd7d99a6bf63739290d896b00718349cf1a711de21aa6984e9", "prompt_hash": "9f81ffba491a1a9e952bfc37d9ec619ce591cf1f91f2c60b7b22861e8a0131f8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 222, "doc": {"id": "faa0aa438b94c19be8ff52ee80d9e298", "question": "The car was going from Alabama to New York, what was its goal?", "question_concept": "car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["head north", "speed up", "heading north", "go fast", "headed south"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The car was going from Alabama to New York, what was its goal?\nA. head north\nB. speed up\nC. heading north\nD. go fast\nE. headed south\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The car was going from Alabama to New York, what was its goal?\nA. head north\nB. speed up\nC. heading north\nD. go fast\nE. headed south\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The car was going from Alabama to New York, what was its goal?\nA. head north\nB. speed up\nC. heading north\nD. go fast\nE. headed south\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The car was going from Alabama to New York, what was its goal?\nA. head north\nB. speed up\nC. heading north\nD. go fast\nE. headed south\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The car was going from Alabama to New York, what was its goal?\nA. head north\nB. speed up\nC. heading north\nD. go fast\nE. headed south\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8552978038787842", "False"]], [["-7.105298042297363", "False"]], [["-2.355297803878784", "False"]], [["-8.605298042297363", "False"]], [["-7.355298042297363", "False"]]], "filtered_resps": [["-1.8552978038787842", "False"], ["-7.105298042297363", "False"], ["-2.355297803878784", "False"], ["-8.605298042297363", "False"], ["-7.355298042297363", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "40fd4c027d9f910dd10cac3b73a625c1ae147561a0b19c51c85a28eb94064c26", "prompt_hash": "3e9eb504fce1fa312b406424c0a0e291ae6cb2f1f7ed1f51d99378076d113c4c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 223, "doc": {"id": "9310c39a0752f28640c3a05cba1d5ca7", "question": "What do they call the trash in Australia?", "question_concept": "trash", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dirt", "subway", "state park", "container", "dustbin"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do they call the trash in Australia?\nA. dirt\nB. subway\nC. state park\nD. container\nE. dustbin\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do they call the trash in Australia?\nA. dirt\nB. subway\nC. state park\nD. container\nE. dustbin\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do they call the trash in Australia?\nA. dirt\nB. subway\nC. state park\nD. container\nE. dustbin\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do they call the trash in Australia?\nA. dirt\nB. subway\nC. state park\nD. container\nE. dustbin\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do they call the trash in Australia?\nA. dirt\nB. subway\nC. state park\nD. container\nE. dustbin\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.749497413635254", "False"]], [["-6.749497413635254", "False"]], [["-7.749497413635254", "False"]], [["-3.999497413635254", "False"]], [["-2.249497413635254", "False"]]], "filtered_resps": [["-3.749497413635254", "False"], ["-6.749497413635254", "False"], ["-7.749497413635254", "False"], ["-3.999497413635254", "False"], ["-2.249497413635254", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b0497cb3a908a49e9cc549ede5ee389955c91a86e3eb6d5500712f1825b3ed93", "prompt_hash": "a1c651195ab5728619cec753b835df48ba9c65b7f9be9d7dfb442853216bc984", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 224, "doc": {"id": "fee5f4e9d8e37f0183e36eb9b8dbcbb9", "question": "Joan wants to cook a potato, where should she place it?", "question_concept": "potato", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boiling water", "paper bag", "restaurants", "underground", "cupboard"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Joan wants to cook a potato, where should she place it?\nA. boiling water\nB. paper bag\nC. restaurants\nD. underground\nE. cupboard\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joan wants to cook a potato, where should she place it?\nA. boiling water\nB. paper bag\nC. restaurants\nD. underground\nE. cupboard\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joan wants to cook a potato, where should she place it?\nA. boiling water\nB. paper bag\nC. restaurants\nD. underground\nE. cupboard\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joan wants to cook a potato, where should she place it?\nA. boiling water\nB. paper bag\nC. restaurants\nD. underground\nE. cupboard\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joan wants to cook a potato, where should she place it?\nA. boiling water\nB. paper bag\nC. restaurants\nD. underground\nE. cupboard\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3399975299835205", "False"]], [["-6.589997291564941", "False"]], [["-8.339997291564941", "False"]], [["-8.089997291564941", "False"]], [["-10.339997291564941", "False"]]], "filtered_resps": [["-1.3399975299835205", "False"], ["-6.589997291564941", "False"], ["-8.339997291564941", "False"], ["-8.089997291564941", "False"], ["-10.339997291564941", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8f640242221444b7e1a0dbce0048d718f834d3a48e859dda9606f9a447f9d0f9", "prompt_hash": "9b847c526738855d81cd035a69d2017633ca354944dc21a4b506da634feeef20", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 225, "doc": {"id": "5392af3f1c4665e95ff3354e5115de42", "question": "Writers with a great what can amass a large fortune?", "question_concept": "fortune", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cookie", "bank", "real estate", "imagination", "bank roll"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Writers with a great what can amass a large fortune?\nA. cookie\nB. bank\nC. real estate\nD. imagination\nE. bank roll\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Writers with a great what can amass a large fortune?\nA. cookie\nB. bank\nC. real estate\nD. imagination\nE. bank roll\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Writers with a great what can amass a large fortune?\nA. cookie\nB. bank\nC. real estate\nD. imagination\nE. bank roll\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Writers with a great what can amass a large fortune?\nA. cookie\nB. bank\nC. real estate\nD. imagination\nE. bank roll\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Writers with a great what can amass a large fortune?\nA. cookie\nB. bank\nC. real estate\nD. imagination\nE. bank roll\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.841169595718384", "False"]], [["-6.091169357299805", "False"]], [["-6.341169357299805", "False"]], [["-1.0911695957183838", "True"]], [["-5.591169357299805", "False"]]], "filtered_resps": [["-3.841169595718384", "False"], ["-6.091169357299805", "False"], ["-6.341169357299805", "False"], ["-1.0911695957183838", "True"], ["-5.591169357299805", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4537c6a11ea75515dcfc66eb35499ab9411969cad557780bba7d37c8d860bb6d", "prompt_hash": "23a4677a213fd815d5fc042622a0e5d64aba3b652444f7ede61d4cd347a91e72", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 226, "doc": {"id": "4c5c74b3287492d6ddb2da4c8c0fd51a", "question": "Where do all animals live?", "question_concept": "animals", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["the moon", "fairgrounds", "surface of earth", "meadow", "zoos"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where do all animals live?\nA. the moon\nB. fairgrounds\nC. surface of earth\nD. meadow\nE. zoos\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do all animals live?\nA. the moon\nB. fairgrounds\nC. surface of earth\nD. meadow\nE. zoos\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do all animals live?\nA. the moon\nB. fairgrounds\nC. surface of earth\nD. meadow\nE. zoos\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do all animals live?\nA. the moon\nB. fairgrounds\nC. surface of earth\nD. meadow\nE. zoos\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do all animals live?\nA. the moon\nB. fairgrounds\nC. surface of earth\nD. meadow\nE. zoos\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.223759174346924", "False"]], [["-7.473759174346924", "False"]], [["-1.2237592935562134", "True"]], [["-5.973759174346924", "False"]], [["-5.723759174346924", "False"]]], "filtered_resps": [["-4.223759174346924", "False"], ["-7.473759174346924", "False"], ["-1.2237592935562134", "True"], ["-5.973759174346924", "False"], ["-5.723759174346924", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7f758bb2c77bcaac7da60c057342f4b11575ae3b4155212e52e145b534bada4b", "prompt_hash": "253e7256b2268b6542208b3d379188fa657060e453a62641b6469d3b15c09d7e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 227, "doc": {"id": "52f3eb6c9a6b9671050fc769d465ed03", "question": "How are the conditions for someone who is living in a homeless shelter?", "question_concept": "living", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sometimes bad", "happy", "respiration", "growing older", "death"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: How are the conditions for someone who is living in a homeless shelter?\nA. sometimes bad\nB. happy\nC. respiration\nD. growing older\nE. death\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How are the conditions for someone who is living in a homeless shelter?\nA. sometimes bad\nB. happy\nC. respiration\nD. growing older\nE. death\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How are the conditions for someone who is living in a homeless shelter?\nA. sometimes bad\nB. happy\nC. respiration\nD. growing older\nE. death\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How are the conditions for someone who is living in a homeless shelter?\nA. sometimes bad\nB. happy\nC. respiration\nD. growing older\nE. death\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How are the conditions for someone who is living in a homeless shelter?\nA. sometimes bad\nB. happy\nC. respiration\nD. growing older\nE. death\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2661360502243042", "True"]], [["-7.016136169433594", "False"]], [["-7.016136169433594", "False"]], [["-6.016136169433594", "False"]], [["-7.266136169433594", "False"]]], "filtered_resps": [["-1.2661360502243042", "True"], ["-7.016136169433594", "False"], ["-7.016136169433594", "False"], ["-6.016136169433594", "False"], ["-7.266136169433594", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ef1f60acbbb3b0fba02e4a0b35b3d87200ae7143f7fc32ee204cc165cc8080d6", "prompt_hash": "97d75ee699b124678dbc80425f071597483e9831be5e458931b00aaf097ba2aa", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 228, "doc": {"id": "03ee30b5801b61aee791a551a9d9a49f", "question": "You can do knitting to get the feeling of what?", "question_concept": "knitting", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["relaxation", "arthritis", "adrenaline", "your", "sweater may produced"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: You can do knitting to get the feeling of what?\nA. relaxation\nB. arthritis\nC. adrenaline\nD. your\nE. sweater may produced\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: You can do knitting to get the feeling of what?\nA. relaxation\nB. arthritis\nC. adrenaline\nD. your\nE. sweater may produced\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: You can do knitting to get the feeling of what?\nA. relaxation\nB. arthritis\nC. adrenaline\nD. your\nE. sweater may produced\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: You can do knitting to get the feeling of what?\nA. relaxation\nB. arthritis\nC. adrenaline\nD. your\nE. sweater may produced\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: You can do knitting to get the feeling of what?\nA. relaxation\nB. arthritis\nC. adrenaline\nD. your\nE. sweater may produced\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9229850172996521", "True"]], [["-6.672985076904297", "False"]], [["-7.672985076904297", "False"]], [["-6.422985076904297", "False"]], [["-8.422985076904297", "False"]]], "filtered_resps": [["-0.9229850172996521", "True"], ["-6.672985076904297", "False"], ["-7.672985076904297", "False"], ["-6.422985076904297", "False"], ["-8.422985076904297", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f4f516ce5cfa2bd62568132a3aa6745dc0a1dd7f1a42fca27383c7212db2b2ee", "prompt_hash": "a9e37678231093b7c1aa805b1f3d0ff567a5a4dc6b7b3ab3698a290daa7498ff", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 229, "doc": {"id": "6d1d483745bc0aae0f4dd04e851ceffb", "question": "What might a very large table be?", "question_concept": "table", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dining room", "conference", "kitchen", "in a lake", "demonstration"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What might a very large table be?\nA. dining room\nB. conference\nC. kitchen\nD. in a lake\nE. demonstration\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might a very large table be?\nA. dining room\nB. conference\nC. kitchen\nD. in a lake\nE. demonstration\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might a very large table be?\nA. dining room\nB. conference\nC. kitchen\nD. in a lake\nE. demonstration\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might a very large table be?\nA. dining room\nB. conference\nC. kitchen\nD. in a lake\nE. demonstration\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might a very large table be?\nA. dining room\nB. conference\nC. kitchen\nD. in a lake\nE. demonstration\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0806084871292114", "True"]], [["-2.080608367919922", "False"]], [["-5.080608367919922", "False"]], [["-3.580608367919922", "False"]], [["-3.330608367919922", "False"]]], "filtered_resps": [["-1.0806084871292114", "True"], ["-2.080608367919922", "False"], ["-5.080608367919922", "False"], ["-3.580608367919922", "False"], ["-3.330608367919922", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eb4c5e720256d7e873c66b658fdee42d74307bc053c445ae958c6c9778b0d789", "prompt_hash": "0a122eb6dc26095edb081f670ee697f429576849f8f2ac044f079543d66e07b5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 230, "doc": {"id": "bf10bfda7328c8671e15adf8546b64d7", "question": "John got his tax refund back.  He treated it like it was what?", "question_concept": "tax", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["candy", "death and", "free money", "discount", "credit"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: John got his tax refund back.  He treated it like it was what?\nA. candy\nB. death and\nC. free money\nD. discount\nE. credit\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John got his tax refund back.  He treated it like it was what?\nA. candy\nB. death and\nC. free money\nD. discount\nE. credit\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John got his tax refund back.  He treated it like it was what?\nA. candy\nB. death and\nC. free money\nD. discount\nE. credit\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John got his tax refund back.  He treated it like it was what?\nA. candy\nB. death and\nC. free money\nD. discount\nE. credit\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John got his tax refund back.  He treated it like it was what?\nA. candy\nB. death and\nC. free money\nD. discount\nE. credit\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.921696186065674", "False"]], [["-3.421696186065674", "False"]], [["-1.1716963052749634", "True"]], [["-6.671696186065674", "False"]], [["-7.421696186065674", "False"]]], "filtered_resps": [["-3.921696186065674", "False"], ["-3.421696186065674", "False"], ["-1.1716963052749634", "True"], ["-6.671696186065674", "False"], ["-7.421696186065674", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "30507e44b0863225cadf1f6127c0bec9bb1a55b2e8377b03e397e101de20f850", "prompt_hash": "36438a63dfb5bd4e35604ca76e52f1d8699f8c3dab8e6421b1da56615bf99b0c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 231, "doc": {"id": "0b3a3ee40dd25be9735ac5e3342ca4dd", "question": "A person with an allergy might be doing what if they awake suddenly?", "question_concept": "awake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have fun", "enjoy with friends", "stretch", "yawn", "sneezing"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: A person with an allergy might be doing what if they awake suddenly?\nA. have fun\nB. enjoy with friends\nC. stretch\nD. yawn\nE. sneezing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A person with an allergy might be doing what if they awake suddenly?\nA. have fun\nB. enjoy with friends\nC. stretch\nD. yawn\nE. sneezing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A person with an allergy might be doing what if they awake suddenly?\nA. have fun\nB. enjoy with friends\nC. stretch\nD. yawn\nE. sneezing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A person with an allergy might be doing what if they awake suddenly?\nA. have fun\nB. enjoy with friends\nC. stretch\nD. yawn\nE. sneezing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A person with an allergy might be doing what if they awake suddenly?\nA. have fun\nB. enjoy with friends\nC. stretch\nD. yawn\nE. sneezing\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.2381751537323", "False"]], [["-8.488175392150879", "False"]], [["-7.238175392150879", "False"]], [["-7.238175392150879", "False"]], [["-0.7381751537322998", "True"]]], "filtered_resps": [["-2.2381751537323", "False"], ["-8.488175392150879", "False"], ["-7.238175392150879", "False"], ["-7.238175392150879", "False"], ["-0.7381751537322998", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "52b5ff9b3b891eba788a73e9d2a0d74e2972bab4c97c902f21b31decdf004292", "prompt_hash": "d151affc4b251c53b8de4a6b8fc8911690f9d6df9bd0f528c7495d3829241da8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 232, "doc": {"id": "77e2a0b469b56bea81921a4a945ffcb5", "question": "Where is a ferret unlikely to be?", "question_concept": "ferret", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["classroom", "outdoors", "aquarium", "north carolina", "great britain"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a ferret unlikely to be?\nA. classroom\nB. outdoors\nC. aquarium\nD. north carolina\nE. great britain\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a ferret unlikely to be?\nA. classroom\nB. outdoors\nC. aquarium\nD. north carolina\nE. great britain\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a ferret unlikely to be?\nA. classroom\nB. outdoors\nC. aquarium\nD. north carolina\nE. great britain\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a ferret unlikely to be?\nA. classroom\nB. outdoors\nC. aquarium\nD. north carolina\nE. great britain\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a ferret unlikely to be?\nA. classroom\nB. outdoors\nC. aquarium\nD. north carolina\nE. great britain\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.56072199344635", "True"]], [["-2.5607218742370605", "False"]], [["-6.0607218742370605", "False"]], [["-3.0607218742370605", "False"]], [["-2.3107218742370605", "False"]]], "filtered_resps": [["-1.56072199344635", "True"], ["-2.5607218742370605", "False"], ["-6.0607218742370605", "False"], ["-3.0607218742370605", "False"], ["-2.3107218742370605", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4a813602e98e750d7e3c9c6294aa7df010ca47a1d40b6ce3574d78c4bc634649", "prompt_hash": "37338b880106fcbe1d0d75948fc21ddd51dc5b07ab0288555a4a9a9ed34aa6fd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 233, "doc": {"id": "dc964e4f6df6b70815e81e466d0ff717", "question": "If you jump in any of the oceans you will get?", "question_concept": "oceans", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tanned", "wet", "wide", "very deep", "fish"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you jump in any of the oceans you will get?\nA. tanned\nB. wet\nC. wide\nD. very deep\nE. fish\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you jump in any of the oceans you will get?\nA. tanned\nB. wet\nC. wide\nD. very deep\nE. fish\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you jump in any of the oceans you will get?\nA. tanned\nB. wet\nC. wide\nD. very deep\nE. fish\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you jump in any of the oceans you will get?\nA. tanned\nB. wet\nC. wide\nD. very deep\nE. fish\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you jump in any of the oceans you will get?\nA. tanned\nB. wet\nC. wide\nD. very deep\nE. fish\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.312134742736816", "False"]], [["-1.0621349811553955", "True"]], [["-7.062134742736816", "False"]], [["-6.062134742736816", "False"]], [["-8.312134742736816", "False"]]], "filtered_resps": [["-6.312134742736816", "False"], ["-1.0621349811553955", "True"], ["-7.062134742736816", "False"], ["-6.062134742736816", "False"], ["-8.312134742736816", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "17960fc2a4098a2fe8a574985f5b50f3bc975a8832154257d3032a83fac7cfe0", "prompt_hash": "9b67ded457518b9f3f61a1efefc3a01ac421889e111f6717a49cf56452c20192", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 234, "doc": {"id": "6b9221c1af583ffb43580857d6fde38a", "question": "Immediately after peeing, a person's bladder is what?", "question_concept": "bladder", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["collapsed", "empty", "full", "filled", "stretchable"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Immediately after peeing, a person's bladder is what?\nA. collapsed\nB. empty\nC. full\nD. filled\nE. stretchable\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Immediately after peeing, a person's bladder is what?\nA. collapsed\nB. empty\nC. full\nD. filled\nE. stretchable\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Immediately after peeing, a person's bladder is what?\nA. collapsed\nB. empty\nC. full\nD. filled\nE. stretchable\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Immediately after peeing, a person's bladder is what?\nA. collapsed\nB. empty\nC. full\nD. filled\nE. stretchable\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Immediately after peeing, a person's bladder is what?\nA. collapsed\nB. empty\nC. full\nD. filled\nE. stretchable\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8615403175354004", "False"]], [["-1.61154043674469", "True"]], [["-2.6115403175354004", "False"]], [["-2.1115403175354004", "False"]], [["-4.6115403175354", "False"]]], "filtered_resps": [["-3.8615403175354004", "False"], ["-1.61154043674469", "True"], ["-2.6115403175354004", "False"], ["-2.1115403175354004", "False"], ["-4.6115403175354", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7c2976dc5d9e7ea81e94c9918b6ec51b611c3ab24fe94b02fe76d93128cc0c40", "prompt_hash": "b8d943d925ee951d30e63db2e3edeaa4d2be2f51a13a57f91c71e25d43726f50", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 235, "doc": {"id": "4dc2c4596b08e9bfd893174e67bff40a", "question": "The lady would eat and eat, and because of mental issues would then make herself what?", "question_concept": "eat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wash dishes", "throwing up", "drinking", "throw up", "turn inside out"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The lady would eat and eat, and because of mental issues would then make herself what?\nA. wash dishes\nB. throwing up\nC. drinking\nD. throw up\nE. turn inside out\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The lady would eat and eat, and because of mental issues would then make herself what?\nA. wash dishes\nB. throwing up\nC. drinking\nD. throw up\nE. turn inside out\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The lady would eat and eat, and because of mental issues would then make herself what?\nA. wash dishes\nB. throwing up\nC. drinking\nD. throw up\nE. turn inside out\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The lady would eat and eat, and because of mental issues would then make herself what?\nA. wash dishes\nB. throwing up\nC. drinking\nD. throw up\nE. turn inside out\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The lady would eat and eat, and because of mental issues would then make herself what?\nA. wash dishes\nB. throwing up\nC. drinking\nD. throw up\nE. turn inside out\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.354433059692383", "False"]], [["-2.104433059692383", "False"]], [["-6.854433059692383", "False"]], [["-2.104433059692383", "False"]], [["-6.354433059692383", "False"]]], "filtered_resps": [["-5.354433059692383", "False"], ["-2.104433059692383", "False"], ["-6.854433059692383", "False"], ["-2.104433059692383", "False"], ["-6.354433059692383", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4b16bcb2476dee1cba58cd8100c76866d8bece16523646fb36b9313cbc94ad42", "prompt_hash": "266f972c85f702cd941c7b24e3ea89e01b7d6c3a1a5bd000287d32f0be2a02b2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 236, "doc": {"id": "8ae24d3ff199077a59e0d970feb665b7", "question": "A car was hailed to chauffeur someone to the opera house, where was it heading?", "question_concept": "car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["go downtown", "appear suddenly", "go fast", "bottom out", "east"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A car was hailed to chauffeur someone to the opera house, where was it heading?\nA. go downtown\nB. appear suddenly\nC. go fast\nD. bottom out\nE. east\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A car was hailed to chauffeur someone to the opera house, where was it heading?\nA. go downtown\nB. appear suddenly\nC. go fast\nD. bottom out\nE. east\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A car was hailed to chauffeur someone to the opera house, where was it heading?\nA. go downtown\nB. appear suddenly\nC. go fast\nD. bottom out\nE. east\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A car was hailed to chauffeur someone to the opera house, where was it heading?\nA. go downtown\nB. appear suddenly\nC. go fast\nD. bottom out\nE. east\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A car was hailed to chauffeur someone to the opera house, where was it heading?\nA. go downtown\nB. appear suddenly\nC. go fast\nD. bottom out\nE. east\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7413678169250488", "False"]], [["-6.991367816925049", "False"]], [["-6.491367816925049", "False"]], [["-7.491367816925049", "False"]], [["-5.491367816925049", "False"]]], "filtered_resps": [["-1.7413678169250488", "False"], ["-6.991367816925049", "False"], ["-6.491367816925049", "False"], ["-7.491367816925049", "False"], ["-5.491367816925049", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "58550c29d52ab1110dab26461eb878c473f28abf6fbd136d4771d0a58a9101c0", "prompt_hash": "c23493d24ea939c2f21f64f576876b251a8eb58ead64b0f07b20dc1178769fc8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 237, "doc": {"id": "d64a676e9d22e7edd12e7f4ce267a9f0", "question": "What do you go to see for live entertainment?", "question_concept": "entertainment", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["movie", "show", "concert venue", "casino", "theatre"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What do you go to see for live entertainment?\nA. movie\nB. show\nC. concert venue\nD. casino\nE. theatre\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you go to see for live entertainment?\nA. movie\nB. show\nC. concert venue\nD. casino\nE. theatre\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you go to see for live entertainment?\nA. movie\nB. show\nC. concert venue\nD. casino\nE. theatre\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you go to see for live entertainment?\nA. movie\nB. show\nC. concert venue\nD. casino\nE. theatre\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you go to see for live entertainment?\nA. movie\nB. show\nC. concert venue\nD. casino\nE. theatre\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3194942474365234", "False"]], [["-1.8194942474365234", "True"]], [["-2.0694942474365234", "False"]], [["-6.819494247436523", "False"]], [["-2.3194942474365234", "False"]]], "filtered_resps": [["-3.3194942474365234", "False"], ["-1.8194942474365234", "True"], ["-2.0694942474365234", "False"], ["-6.819494247436523", "False"], ["-2.3194942474365234", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "62908f929c625b9a193434b26fc407d89796823af5143ccfea2a2ddb528c5bdc", "prompt_hash": "d107904c5b585ffd2ef97e286d2c09f132dca5d0927361f9a3e7e1f121f7748e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 238, "doc": {"id": "54ecb521df1d0f5b130a393c42b4126d", "question": "The teacher thought that a ferret can be very mischievous and probably wouldn't make a great pet for the entire what?", "question_concept": "ferret", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bad mood", "hutch", "classroom", "pair of trousers", "year"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The teacher thought that a ferret can be very mischievous and probably wouldn't make a great pet for the entire what?\nA. bad mood\nB. hutch\nC. classroom\nD. pair of trousers\nE. year\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The teacher thought that a ferret can be very mischievous and probably wouldn't make a great pet for the entire what?\nA. bad mood\nB. hutch\nC. classroom\nD. pair of trousers\nE. year\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The teacher thought that a ferret can be very mischievous and probably wouldn't make a great pet for the entire what?\nA. bad mood\nB. hutch\nC. classroom\nD. pair of trousers\nE. year\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The teacher thought that a ferret can be very mischievous and probably wouldn't make a great pet for the entire what?\nA. bad mood\nB. hutch\nC. classroom\nD. pair of trousers\nE. year\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The teacher thought that a ferret can be very mischievous and probably wouldn't make a great pet for the entire what?\nA. bad mood\nB. hutch\nC. classroom\nD. pair of trousers\nE. year\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.4080374240875244", "False"]], [["-4.408037185668945", "False"]], [["-2.9080374240875244", "False"]], [["-6.658037185668945", "False"]], [["-1.9080374240875244", "True"]]], "filtered_resps": [["-3.4080374240875244", "False"], ["-4.408037185668945", "False"], ["-2.9080374240875244", "False"], ["-6.658037185668945", "False"], ["-1.9080374240875244", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eebb30867e79570ab7a52d06bdf66f7a2c6b82de075b18a032e5882a044a07e6", "prompt_hash": "8288de1abfa25932c117ae4cf16d4d565f1392bb3af2a43c243b3085c8fd647e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 239, "doc": {"id": "b7276bb9139ec25c98c7e3822404eb6c", "question": "A creek is a body of water found in what low land?", "question_concept": "creek", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["forest", "valley", "outdoors", "countryside", "woods"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: A creek is a body of water found in what low land?\nA. forest\nB. valley\nC. outdoors\nD. countryside\nE. woods\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A creek is a body of water found in what low land?\nA. forest\nB. valley\nC. outdoors\nD. countryside\nE. woods\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A creek is a body of water found in what low land?\nA. forest\nB. valley\nC. outdoors\nD. countryside\nE. woods\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A creek is a body of water found in what low land?\nA. forest\nB. valley\nC. outdoors\nD. countryside\nE. woods\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A creek is a body of water found in what low land?\nA. forest\nB. valley\nC. outdoors\nD. countryside\nE. woods\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8135396838188171", "True"]], [["-2.063539743423462", "False"]], [["-3.813539743423462", "False"]], [["-7.813539505004883", "False"]], [["-9.813539505004883", "False"]]], "filtered_resps": [["-0.8135396838188171", "True"], ["-2.063539743423462", "False"], ["-3.813539743423462", "False"], ["-7.813539505004883", "False"], ["-9.813539505004883", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "afe053d996034e24d52f43bae9be7088598999281b6b828b68f72e8fa3c650ca", "prompt_hash": "982c048e8741f4960fd702588084fd5cf58102a658c677587212c48baf8022a2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 240, "doc": {"id": "ecb8758b0d088f9aedc182a516dd1190", "question": "If I have a pet bird, what does it likely live in?", "question_concept": "bird", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["forest", "bathroom", "windowsill", "countryside", "cage"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If I have a pet bird, what does it likely live in?\nA. forest\nB. bathroom\nC. windowsill\nD. countryside\nE. cage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I have a pet bird, what does it likely live in?\nA. forest\nB. bathroom\nC. windowsill\nD. countryside\nE. cage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I have a pet bird, what does it likely live in?\nA. forest\nB. bathroom\nC. windowsill\nD. countryside\nE. cage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I have a pet bird, what does it likely live in?\nA. forest\nB. bathroom\nC. windowsill\nD. countryside\nE. cage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I have a pet bird, what does it likely live in?\nA. forest\nB. bathroom\nC. windowsill\nD. countryside\nE. cage\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.0549116134643555", "False"]], [["-4.8049116134643555", "False"]], [["-5.8049116134643555", "False"]], [["-10.054911613464355", "False"]], [["-0.5549114346504211", "True"]]], "filtered_resps": [["-4.0549116134643555", "False"], ["-4.8049116134643555", "False"], ["-5.8049116134643555", "False"], ["-10.054911613464355", "False"], ["-0.5549114346504211", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2b4758ad3be2fa0ce2812c98f6faa68af7823d96d79633ace72fa49f1a3b08de", "prompt_hash": "f72081e88f1c9ffbae12477056658914fe74717627e7cda10f56256f1ac84ca9", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 241, "doc": {"id": "f2645d0ee8662b6553954cee7e77979e", "question": "Joe and Mac were playing basketball. They did it every day in their back yard.  Why were they playing basketball?", "question_concept": "playing basketball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["study", "have fun", "pain", "cheers", "knee injury"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Joe and Mac were playing basketball. They did it every day in their back yard.  Why were they playing basketball?\nA. study\nB. have fun\nC. pain\nD. cheers\nE. knee injury\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe and Mac were playing basketball. They did it every day in their back yard.  Why were they playing basketball?\nA. study\nB. have fun\nC. pain\nD. cheers\nE. knee injury\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe and Mac were playing basketball. They did it every day in their back yard.  Why were they playing basketball?\nA. study\nB. have fun\nC. pain\nD. cheers\nE. knee injury\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe and Mac were playing basketball. They did it every day in their back yard.  Why were they playing basketball?\nA. study\nB. have fun\nC. pain\nD. cheers\nE. knee injury\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe and Mac were playing basketball. They did it every day in their back yard.  Why were they playing basketball?\nA. study\nB. have fun\nC. pain\nD. cheers\nE. knee injury\nAnswer:", "arg_1": " E"}}, "resps": [[["-8.347338676452637", "False"]], [["-1.5973390340805054", "False"]], [["-10.597338676452637", "False"]], [["-10.597338676452637", "False"]], [["-12.097338676452637", "False"]]], "filtered_resps": [["-8.347338676452637", "False"], ["-1.5973390340805054", "False"], ["-10.597338676452637", "False"], ["-10.597338676452637", "False"], ["-12.097338676452637", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d5b6de9f4b4d11dbaf7f224681b98de9ce1c0623d95506f3edba2f7dceb7a3a4", "prompt_hash": "9a85b1a9eeab3f7c6e553ac48cf4216a7d347c571a72a57abb9ee87c5e4e0b85", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 242, "doc": {"id": "ea6d1a739ea841be282e13789270651e", "question": "What makes someone a nomad?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["unpleasant things", "hangnail", "have no home", "have no car", "schizophrenia"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What makes someone a nomad?\nA. unpleasant things\nB. hangnail\nC. have no home\nD. have no car\nE. schizophrenia\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What makes someone a nomad?\nA. unpleasant things\nB. hangnail\nC. have no home\nD. have no car\nE. schizophrenia\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What makes someone a nomad?\nA. unpleasant things\nB. hangnail\nC. have no home\nD. have no car\nE. schizophrenia\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What makes someone a nomad?\nA. unpleasant things\nB. hangnail\nC. have no home\nD. have no car\nE. schizophrenia\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What makes someone a nomad?\nA. unpleasant things\nB. hangnail\nC. have no home\nD. have no car\nE. schizophrenia\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.38301420211792", "False"]], [["-5.88301420211792", "False"]], [["-1.63301420211792", "False"]], [["-7.63301420211792", "False"]], [["-8.883014678955078", "False"]]], "filtered_resps": [["-2.38301420211792", "False"], ["-5.88301420211792", "False"], ["-1.63301420211792", "False"], ["-7.63301420211792", "False"], ["-8.883014678955078", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9a154014908e8d4c01c9871cad0ef019b13c212d73d3cd95e7d01051e4c2fda3", "prompt_hash": "21f3490e4fc1dca07cb54c1b7456174a5ae5844e1b8b059922087ff4338b4b54", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 243, "doc": {"id": "c82ed0c2a2e115452b4d596c5faafbcf", "question": "What is a treat that you dog will enjoy?", "question_concept": "dog", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["salad", "petted", "affection", "bone", "lots of attention"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is a treat that you dog will enjoy?\nA. salad\nB. petted\nC. affection\nD. bone\nE. lots of attention\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a treat that you dog will enjoy?\nA. salad\nB. petted\nC. affection\nD. bone\nE. lots of attention\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a treat that you dog will enjoy?\nA. salad\nB. petted\nC. affection\nD. bone\nE. lots of attention\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a treat that you dog will enjoy?\nA. salad\nB. petted\nC. affection\nD. bone\nE. lots of attention\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a treat that you dog will enjoy?\nA. salad\nB. petted\nC. affection\nD. bone\nE. lots of attention\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.6004443168640137", "False"]], [["-4.850444316864014", "False"]], [["-6.600444316864014", "False"]], [["-0.8504443168640137", "True"]], [["-8.600444793701172", "False"]]], "filtered_resps": [["-2.6004443168640137", "False"], ["-4.850444316864014", "False"], ["-6.600444316864014", "False"], ["-0.8504443168640137", "True"], ["-8.600444793701172", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e1ac46a096a989adc9457b2e10a96bf28fa63a9201552b2dcfd80cf8c17e158c", "prompt_hash": "18e3b01f485ee3c4c9bb6dcc5220bb47214f0f27d8f8cbc95c009f5cd26d5f7f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 244, "doc": {"id": "163d83851ecd4a4144b31b8738e4c335", "question": "Women used to be expected to wear a dress but it's now acceptable for them to wear what?", "question_concept": "dress", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["man suit", "pants", "naked", "action", "long skirt"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Women used to be expected to wear a dress but it's now acceptable for them to wear what?\nA. man suit\nB. pants\nC. naked\nD. action\nE. long skirt\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Women used to be expected to wear a dress but it's now acceptable for them to wear what?\nA. man suit\nB. pants\nC. naked\nD. action\nE. long skirt\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Women used to be expected to wear a dress but it's now acceptable for them to wear what?\nA. man suit\nB. pants\nC. naked\nD. action\nE. long skirt\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Women used to be expected to wear a dress but it's now acceptable for them to wear what?\nA. man suit\nB. pants\nC. naked\nD. action\nE. long skirt\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Women used to be expected to wear a dress but it's now acceptable for them to wear what?\nA. man suit\nB. pants\nC. naked\nD. action\nE. long skirt\nAnswer:", "arg_1": " E"}}, "resps": [[["-7.059928894042969", "False"]], [["-1.8099287748336792", "False"]], [["-8.309928894042969", "False"]], [["-8.059928894042969", "False"]], [["-10.059928894042969", "False"]]], "filtered_resps": [["-7.059928894042969", "False"], ["-1.8099287748336792", "False"], ["-8.309928894042969", "False"], ["-8.059928894042969", "False"], ["-10.059928894042969", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e1d60afe6848c3f227b838437ee0de5850ed3b68619cb24b31625301b313b9d0", "prompt_hash": "b2683c33b52ec4491e6315dccdc6a9dd41eb373c35768e8ebf264e228e4ef972", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 245, "doc": {"id": "095767956c500ca1af7cf7671556de5b", "question": "The fact that Joe was able to memorize the list in spite of his apparent  state proved that part of his brain was what?", "question_concept": "memorize", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["awake", "repeat", "sleeping", "concentrate", "read aloud"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The fact that Joe was able to memorize the list in spite of his apparent  state proved that part of his brain was what?\nA. awake\nB. repeat\nC. sleeping\nD. concentrate\nE. read aloud\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The fact that Joe was able to memorize the list in spite of his apparent  state proved that part of his brain was what?\nA. awake\nB. repeat\nC. sleeping\nD. concentrate\nE. read aloud\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The fact that Joe was able to memorize the list in spite of his apparent  state proved that part of his brain was what?\nA. awake\nB. repeat\nC. sleeping\nD. concentrate\nE. read aloud\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The fact that Joe was able to memorize the list in spite of his apparent  state proved that part of his brain was what?\nA. awake\nB. repeat\nC. sleeping\nD. concentrate\nE. read aloud\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The fact that Joe was able to memorize the list in spite of his apparent  state proved that part of his brain was what?\nA. awake\nB. repeat\nC. sleeping\nD. concentrate\nE. read aloud\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2023820877075195", "True"]], [["-4.9523820877075195", "False"]], [["-3.2023820877075195", "False"]], [["-2.9523820877075195", "False"]], [["-6.7023820877075195", "False"]]], "filtered_resps": [["-1.2023820877075195", "True"], ["-4.9523820877075195", "False"], ["-3.2023820877075195", "False"], ["-2.9523820877075195", "False"], ["-6.7023820877075195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "115088ce6c29b9d8c4a3a79d66cb07ed2fe9beaca2df6b6374ecd1d5215582a7", "prompt_hash": "908ab4c32a2962bd9481c631128a0be35b91e9b9b7b27757036baea41b45743a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 246, "doc": {"id": "d31ee38f67d1173275e120b8ad36039c", "question": "What is a wet person likely to do?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["gain weight", "thank god", "catch cold", "suicide", "cross street"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is a wet person likely to do?\nA. gain weight\nB. thank god\nC. catch cold\nD. suicide\nE. cross street\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a wet person likely to do?\nA. gain weight\nB. thank god\nC. catch cold\nD. suicide\nE. cross street\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a wet person likely to do?\nA. gain weight\nB. thank god\nC. catch cold\nD. suicide\nE. cross street\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a wet person likely to do?\nA. gain weight\nB. thank god\nC. catch cold\nD. suicide\nE. cross street\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a wet person likely to do?\nA. gain weight\nB. thank god\nC. catch cold\nD. suicide\nE. cross street\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4541219472885132", "True"]], [["-3.9541220664978027", "False"]], [["-1.4541219472885132", "True"]], [["-5.204122066497803", "False"]], [["-5.204122066497803", "False"]]], "filtered_resps": [["-1.4541219472885132", "True"], ["-3.9541220664978027", "False"], ["-1.4541219472885132", "True"], ["-5.204122066497803", "False"], ["-5.204122066497803", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6aba8e996384f2ad94648479c5d92aaed356d223a028d999ee204a43d4217235", "prompt_hash": "9f6b7d4aeafe772f54a7d27aef47f248464a7f2d263036c92bd262275eedd26e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 247, "doc": {"id": "c410a4626dfce4b4cfd3e5937602cd77", "question": "After recovering from the disease, what did the doctor call the patient?", "question_concept": "disease", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["healthy", "passing around", "cure", "wellness", "healthy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: After recovering from the disease, what did the doctor call the patient?\nA. healthy\nB. passing around\nC. cure\nD. wellness\nE. healthy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: After recovering from the disease, what did the doctor call the patient?\nA. healthy\nB. passing around\nC. cure\nD. wellness\nE. healthy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: After recovering from the disease, what did the doctor call the patient?\nA. healthy\nB. passing around\nC. cure\nD. wellness\nE. healthy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: After recovering from the disease, what did the doctor call the patient?\nA. healthy\nB. passing around\nC. cure\nD. wellness\nE. healthy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: After recovering from the disease, what did the doctor call the patient?\nA. healthy\nB. passing around\nC. cure\nD. wellness\nE. healthy\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.370032787322998", "False"]], [["-6.870032787322998", "False"]], [["-7.370032787322998", "False"]], [["-6.870032787322998", "False"]], [["-4.620032787322998", "False"]]], "filtered_resps": [["-2.370032787322998", "False"], ["-6.870032787322998", "False"], ["-7.370032787322998", "False"], ["-6.870032787322998", "False"], ["-4.620032787322998", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "321808e77386ba7892418288d59dac3863267b05bb3ae026f1b721e7dbb3f10b", "prompt_hash": "bc5bf9c804f9846a9d0cd91d64f0775a0801883b9835342a4e0b76d9989747c9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 248, "doc": {"id": "14d760e43728e9e4643c414627f2b596", "question": "The painter started to edge the room with tape, he always took extra care to make the lines clean and crisp when working with an what?", "question_concept": "edge", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["triangle", "middle", "corner", "center", "interior"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The painter started to edge the room with tape, he always took extra care to make the lines clean and crisp when working with an what?\nA. triangle\nB. middle\nC. corner\nD. center\nE. interior\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The painter started to edge the room with tape, he always took extra care to make the lines clean and crisp when working with an what?\nA. triangle\nB. middle\nC. corner\nD. center\nE. interior\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The painter started to edge the room with tape, he always took extra care to make the lines clean and crisp when working with an what?\nA. triangle\nB. middle\nC. corner\nD. center\nE. interior\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The painter started to edge the room with tape, he always took extra care to make the lines clean and crisp when working with an what?\nA. triangle\nB. middle\nC. corner\nD. center\nE. interior\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The painter started to edge the room with tape, he always took extra care to make the lines clean and crisp when working with an what?\nA. triangle\nB. middle\nC. corner\nD. center\nE. interior\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.4956774711608887", "False"]], [["-5.495677471160889", "False"]], [["-1.2456774711608887", "True"]], [["-4.995677471160889", "False"]], [["-4.245677471160889", "False"]]], "filtered_resps": [["-3.4956774711608887", "False"], ["-5.495677471160889", "False"], ["-1.2456774711608887", "True"], ["-4.995677471160889", "False"], ["-4.245677471160889", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f50454ad8329e27529aa788da731bb0168502ad11db272c03e008353808fe3a1", "prompt_hash": "548e589b1f72c13dc509571d6ea9eb2e434e444ce4c50c6e92c7da90b2c2fddf", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 249, "doc": {"id": "abcf1b550b4d44f46d4f68b8e1d98ec8", "question": "After high tide, where on the coast can you look to find a sea anemone?", "question_concept": "anemone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["nursery", "museum", "gulf of mexico", "tide pool", "intertidal zone"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: After high tide, where on the coast can you look to find a sea anemone?\nA. nursery\nB. museum\nC. gulf of mexico\nD. tide pool\nE. intertidal zone\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: After high tide, where on the coast can you look to find a sea anemone?\nA. nursery\nB. museum\nC. gulf of mexico\nD. tide pool\nE. intertidal zone\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: After high tide, where on the coast can you look to find a sea anemone?\nA. nursery\nB. museum\nC. gulf of mexico\nD. tide pool\nE. intertidal zone\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: After high tide, where on the coast can you look to find a sea anemone?\nA. nursery\nB. museum\nC. gulf of mexico\nD. tide pool\nE. intertidal zone\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: After high tide, where on the coast can you look to find a sea anemone?\nA. nursery\nB. museum\nC. gulf of mexico\nD. tide pool\nE. intertidal zone\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3473992347717285", "False"]], [["-6.3473992347717285", "False"]], [["-6.3473992347717285", "False"]], [["-3.8473992347717285", "False"]], [["-1.097399115562439", "True"]]], "filtered_resps": [["-3.3473992347717285", "False"], ["-6.3473992347717285", "False"], ["-6.3473992347717285", "False"], ["-3.8473992347717285", "False"], ["-1.097399115562439", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5406f78038df0361ec1cb72aea7dd4492ffbb8c499caea8ae42bb716cc2792a4", "prompt_hash": "6c9974fbb57485f2746d5a71052d12d5df6e71c8ad6ac550b7d04f027474b834", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 250, "doc": {"id": "5b8af6f26335dbd501b0104c71e26d9e", "question": "What could a driving car do to a pedestrian?", "question_concept": "driving car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["say hello", "wreak", "pollution", "smoke", "relaxation"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What could a driving car do to a pedestrian?\nA. say hello\nB. wreak\nC. pollution\nD. smoke\nE. relaxation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could a driving car do to a pedestrian?\nA. say hello\nB. wreak\nC. pollution\nD. smoke\nE. relaxation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could a driving car do to a pedestrian?\nA. say hello\nB. wreak\nC. pollution\nD. smoke\nE. relaxation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could a driving car do to a pedestrian?\nA. say hello\nB. wreak\nC. pollution\nD. smoke\nE. relaxation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could a driving car do to a pedestrian?\nA. say hello\nB. wreak\nC. pollution\nD. smoke\nE. relaxation\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.113753080368042", "False"]], [["-0.863753080368042", "True"]], [["-6.113753318786621", "False"]], [["-5.363753318786621", "False"]], [["-8.863753318786621", "False"]]], "filtered_resps": [["-2.113753080368042", "False"], ["-0.863753080368042", "True"], ["-6.113753318786621", "False"], ["-5.363753318786621", "False"], ["-8.863753318786621", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5634e87e4725c09bd13330686ed7aad60a9373397cb7d276a659c449bfbc1c96", "prompt_hash": "464beb1b25d6d8f2575b43b069bbe7d77efef5cc365cd4b990cc6f5c3008fc98", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 251, "doc": {"id": "4364b4b342fb7b44434bd6694bf8fd51", "question": "People do many things to alleviate boredom.  If you can't get out of the house you might decide to do what?", "question_concept": "boredom", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["play cards", "skateboard", "meet interesting people", "listen to music", "go to a concert"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: People do many things to alleviate boredom.  If you can't get out of the house you might decide to do what?\nA. play cards\nB. skateboard\nC. meet interesting people\nD. listen to music\nE. go to a concert\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: People do many things to alleviate boredom.  If you can't get out of the house you might decide to do what?\nA. play cards\nB. skateboard\nC. meet interesting people\nD. listen to music\nE. go to a concert\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: People do many things to alleviate boredom.  If you can't get out of the house you might decide to do what?\nA. play cards\nB. skateboard\nC. meet interesting people\nD. listen to music\nE. go to a concert\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: People do many things to alleviate boredom.  If you can't get out of the house you might decide to do what?\nA. play cards\nB. skateboard\nC. meet interesting people\nD. listen to music\nE. go to a concert\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: People do many things to alleviate boredom.  If you can't get out of the house you might decide to do what?\nA. play cards\nB. skateboard\nC. meet interesting people\nD. listen to music\nE. go to a concert\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7461979389190674", "False"]], [["-3.9961979389190674", "False"]], [["-5.9961981773376465", "False"]], [["-0.9961979985237122", "True"]], [["-8.746197700500488", "False"]]], "filtered_resps": [["-2.7461979389190674", "False"], ["-3.9961979389190674", "False"], ["-5.9961981773376465", "False"], ["-0.9961979985237122", "True"], ["-8.746197700500488", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1cb8d0c95edc3ac69dfbef87380accd05679cc1c300d2dcd9e64ceacc3a94b68", "prompt_hash": "097ffbfae0a28244a905fae4e5309eecb48df1dbefd356fceb644614803a1df8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 252, "doc": {"id": "3ffe67fb009529d9b0c49ccd7141ee4a", "question": "At a grocery store they sell individual potatoes, where does the grocery clerk likely put the potato?", "question_concept": "potato", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boiling water", "root cellar", "rocket ship", "paper bag", "underground"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: At a grocery store they sell individual potatoes, where does the grocery clerk likely put the potato?\nA. boiling water\nB. root cellar\nC. rocket ship\nD. paper bag\nE. underground\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: At a grocery store they sell individual potatoes, where does the grocery clerk likely put the potato?\nA. boiling water\nB. root cellar\nC. rocket ship\nD. paper bag\nE. underground\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: At a grocery store they sell individual potatoes, where does the grocery clerk likely put the potato?\nA. boiling water\nB. root cellar\nC. rocket ship\nD. paper bag\nE. underground\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: At a grocery store they sell individual potatoes, where does the grocery clerk likely put the potato?\nA. boiling water\nB. root cellar\nC. rocket ship\nD. paper bag\nE. underground\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: At a grocery store they sell individual potatoes, where does the grocery clerk likely put the potato?\nA. boiling water\nB. root cellar\nC. rocket ship\nD. paper bag\nE. underground\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6190662384033203", "False"]], [["-6.61906623840332", "False"]], [["-8.11906623840332", "False"]], [["-2.1190662384033203", "False"]], [["-7.36906623840332", "False"]]], "filtered_resps": [["-3.6190662384033203", "False"], ["-6.61906623840332", "False"], ["-8.11906623840332", "False"], ["-2.1190662384033203", "False"], ["-7.36906623840332", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6ea349c0885e92f83f5383e7bedcf618752ef86d87c0294735dea02738a67ba7", "prompt_hash": "d044903de2b21e53e28164b56ce80ccc4ff8710c0b201edf21c3cd628cd12cb5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 253, "doc": {"id": "f372587fa4c99d5bebf0d0eb987c44e2", "question": "What room is a rubber bath mat usually kept?", "question_concept": "mat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["doorway", "living room", "sand", "floors", "bathroom"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What room is a rubber bath mat usually kept?\nA. doorway\nB. living room\nC. sand\nD. floors\nE. bathroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What room is a rubber bath mat usually kept?\nA. doorway\nB. living room\nC. sand\nD. floors\nE. bathroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What room is a rubber bath mat usually kept?\nA. doorway\nB. living room\nC. sand\nD. floors\nE. bathroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What room is a rubber bath mat usually kept?\nA. doorway\nB. living room\nC. sand\nD. floors\nE. bathroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What room is a rubber bath mat usually kept?\nA. doorway\nB. living room\nC. sand\nD. floors\nE. bathroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.96176278591156", "False"]], [["-6.46176290512085", "False"]], [["-8.461762428283691", "False"]], [["-7.21176290512085", "False"]], [["-1.21176278591156", "True"]]], "filtered_resps": [["-1.96176278591156", "False"], ["-6.46176290512085", "False"], ["-8.461762428283691", "False"], ["-7.21176290512085", "False"], ["-1.21176278591156", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "794f87f0a0c02ef4c97a7b4c8e0fd0238aa543b0f09a8876ad0b22eee1bbd285", "prompt_hash": "eeafabb985cc8482e4bb689671216564918a9ff12ebcef14222fc26fbee386ca", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 254, "doc": {"id": "d35a8a3bd560fdd651ecf314878ed30f", "question": "What would you put meat on top of to cook it?", "question_concept": "meat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["oil", "freezer", "ham sandwich", "oven", "frying pan"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What would you put meat on top of to cook it?\nA. oil\nB. freezer\nC. ham sandwich\nD. oven\nE. frying pan\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you put meat on top of to cook it?\nA. oil\nB. freezer\nC. ham sandwich\nD. oven\nE. frying pan\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you put meat on top of to cook it?\nA. oil\nB. freezer\nC. ham sandwich\nD. oven\nE. frying pan\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you put meat on top of to cook it?\nA. oil\nB. freezer\nC. ham sandwich\nD. oven\nE. frying pan\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you put meat on top of to cook it?\nA. oil\nB. freezer\nC. ham sandwich\nD. oven\nE. frying pan\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0284156799316406", "False"]], [["-6.278415679931641", "False"]], [["-5.028415679931641", "False"]], [["-1.528415560722351", "True"]], [["-1.778415560722351", "False"]]], "filtered_resps": [["-3.0284156799316406", "False"], ["-6.278415679931641", "False"], ["-5.028415679931641", "False"], ["-1.528415560722351", "True"], ["-1.778415560722351", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ad20ce7557f89771ac523ca89394ab9a87dfffde03b7e64c37e5014711f67e29", "prompt_hash": "b909ef1c3a081361e40804b5d39c199c2a5cd82d5cbaf2d4bc0465090b18b3b7", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 255, "doc": {"id": "0542414710025f56b0c26e1bae5c4d06", "question": "Minerals can be obtained in what way for a person who avoids leafy greens?", "question_concept": "mineral", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["multivitamin", "farm", "michigan", "earth", "ore"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Minerals can be obtained in what way for a person who avoids leafy greens?\nA. multivitamin\nB. farm\nC. michigan\nD. earth\nE. ore\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Minerals can be obtained in what way for a person who avoids leafy greens?\nA. multivitamin\nB. farm\nC. michigan\nD. earth\nE. ore\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Minerals can be obtained in what way for a person who avoids leafy greens?\nA. multivitamin\nB. farm\nC. michigan\nD. earth\nE. ore\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Minerals can be obtained in what way for a person who avoids leafy greens?\nA. multivitamin\nB. farm\nC. michigan\nD. earth\nE. ore\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Minerals can be obtained in what way for a person who avoids leafy greens?\nA. multivitamin\nB. farm\nC. michigan\nD. earth\nE. ore\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8260912895202637", "False"]], [["-7.076091289520264", "False"]], [["-7.076091289520264", "False"]], [["-6.326091289520264", "False"]], [["-1.8260912895202637", "False"]]], "filtered_resps": [["-1.8260912895202637", "False"], ["-7.076091289520264", "False"], ["-7.076091289520264", "False"], ["-6.326091289520264", "False"], ["-1.8260912895202637", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ed3b6386a430bfa302c1c862f8abfcc94bd068c51daf0f50535096397a93f075", "prompt_hash": "1b7442ff79046a7a10e7392dd2ad8f31283cd64b8c4e978e63f56af366371f73", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 256, "doc": {"id": "1875f70cf736c68c7a9df3ef870224a1", "question": "What could you be a few hours after you finish cashing in due to your cash?", "question_concept": "cashing in", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["happy", "receiving money", "getting paid", "spending money", "selling out"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What could you be a few hours after you finish cashing in due to your cash?\nA. happy\nB. receiving money\nC. getting paid\nD. spending money\nE. selling out\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could you be a few hours after you finish cashing in due to your cash?\nA. happy\nB. receiving money\nC. getting paid\nD. spending money\nE. selling out\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could you be a few hours after you finish cashing in due to your cash?\nA. happy\nB. receiving money\nC. getting paid\nD. spending money\nE. selling out\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could you be a few hours after you finish cashing in due to your cash?\nA. happy\nB. receiving money\nC. getting paid\nD. spending money\nE. selling out\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could you be a few hours after you finish cashing in due to your cash?\nA. happy\nB. receiving money\nC. getting paid\nD. spending money\nE. selling out\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7492241859436035", "False"]], [["-1.999224066734314", "False"]], [["-1.749224066734314", "True"]], [["-3.7492241859436035", "False"]], [["-5.2492241859436035", "False"]]], "filtered_resps": [["-2.7492241859436035", "False"], ["-1.999224066734314", "False"], ["-1.749224066734314", "True"], ["-3.7492241859436035", "False"], ["-5.2492241859436035", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ed5264cc90e68e35c6c9932255f7507dded74547f9507cc6c152a6c5914166df", "prompt_hash": "020e2e92d46074439504b503b481e657d1be21cecf00d545763b3d9b23bd3250", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 257, "doc": {"id": "83250ae2dfeb2e3886ead4cde8e1290f", "question": "The smelly man was having a bath, but what is he pursuing?", "question_concept": "having bath", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hydration", "being clear", "personal cleanliness", "will drown", "use of water"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The smelly man was having a bath, but what is he pursuing?\nA. hydration\nB. being clear\nC. personal cleanliness\nD. will drown\nE. use of water\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The smelly man was having a bath, but what is he pursuing?\nA. hydration\nB. being clear\nC. personal cleanliness\nD. will drown\nE. use of water\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The smelly man was having a bath, but what is he pursuing?\nA. hydration\nB. being clear\nC. personal cleanliness\nD. will drown\nE. use of water\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The smelly man was having a bath, but what is he pursuing?\nA. hydration\nB. being clear\nC. personal cleanliness\nD. will drown\nE. use of water\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The smelly man was having a bath, but what is he pursuing?\nA. hydration\nB. being clear\nC. personal cleanliness\nD. will drown\nE. use of water\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8461151123046875", "False"]], [["-5.5961151123046875", "False"]], [["-1.346114993095398", "True"]], [["-6.0961151123046875", "False"]], [["-7.3461151123046875", "False"]]], "filtered_resps": [["-3.8461151123046875", "False"], ["-5.5961151123046875", "False"], ["-1.346114993095398", "True"], ["-6.0961151123046875", "False"], ["-7.3461151123046875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "840804d0bfe079bdeaf3686f6d76379dff8c878af4a84e20762f29f0938ca245", "prompt_hash": "00c6c74c3a758fa65e66f74b5764355f10ff9dac4ec17348532cac79ec8a62c3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 258, "doc": {"id": "70c39372c0d50566554fd72c768b75f6", "question": "What might a couple have a lot of when they are deciding on stopping being married to each other?", "question_concept": "stopping being married to", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pleasure", "detachment", "exercise", "bankruptcy", "fights"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What might a couple have a lot of when they are deciding on stopping being married to each other?\nA. pleasure\nB. detachment\nC. exercise\nD. bankruptcy\nE. fights\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might a couple have a lot of when they are deciding on stopping being married to each other?\nA. pleasure\nB. detachment\nC. exercise\nD. bankruptcy\nE. fights\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might a couple have a lot of when they are deciding on stopping being married to each other?\nA. pleasure\nB. detachment\nC. exercise\nD. bankruptcy\nE. fights\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might a couple have a lot of when they are deciding on stopping being married to each other?\nA. pleasure\nB. detachment\nC. exercise\nD. bankruptcy\nE. fights\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might a couple have a lot of when they are deciding on stopping being married to each other?\nA. pleasure\nB. detachment\nC. exercise\nD. bankruptcy\nE. fights\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.708808422088623", "False"]], [["-2.958808422088623", "False"]], [["-6.708808422088623", "False"]], [["-5.208808422088623", "False"]], [["-1.7088085412979126", "False"]]], "filtered_resps": [["-2.708808422088623", "False"], ["-2.958808422088623", "False"], ["-6.708808422088623", "False"], ["-5.208808422088623", "False"], ["-1.7088085412979126", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2f3fa08fd15135a7a7ad48c9455b55f9d401290480d116cb99ce24494d7fbf68", "prompt_hash": "5a65f947a8bf4f9f028fd6452b4b5efb44cdac03c1725c81e46bce065eee9602", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 259, "doc": {"id": "c21ec5b367f409a0288d616f626555ae", "question": "If a person is working a lot, what are they likely trying to earn?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["much money", "own house", "creativity", "new car", "caregiver"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If a person is working a lot, what are they likely trying to earn?\nA. much money\nB. own house\nC. creativity\nD. new car\nE. caregiver\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a person is working a lot, what are they likely trying to earn?\nA. much money\nB. own house\nC. creativity\nD. new car\nE. caregiver\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a person is working a lot, what are they likely trying to earn?\nA. much money\nB. own house\nC. creativity\nD. new car\nE. caregiver\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a person is working a lot, what are they likely trying to earn?\nA. much money\nB. own house\nC. creativity\nD. new car\nE. caregiver\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a person is working a lot, what are they likely trying to earn?\nA. much money\nB. own house\nC. creativity\nD. new car\nE. caregiver\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5116203427314758", "True"]], [["-6.01162052154541", "False"]], [["-6.76162052154541", "False"]], [["-7.01162052154541", "False"]], [["-8.76162052154541", "False"]]], "filtered_resps": [["-0.5116203427314758", "True"], ["-6.01162052154541", "False"], ["-6.76162052154541", "False"], ["-7.01162052154541", "False"], ["-8.76162052154541", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e0dc564765e37419de3cbc9608d37058ca107d1751222529a86544502d593076", "prompt_hash": "9718e5202bf6fa4958e9e2a558ef91cabd58bb695a927c250fdfe2421e6a01fd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 260, "doc": {"id": "a2cd03ed068f6d613e85f3a60f4db0a1", "question": "The traveling business man was glad his credit card had perks, it offset the high prices for travel from a what?", "question_concept": "high prices", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["car", "theatre", "airport", "hotel", "disneyland"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The traveling business man was glad his credit card had perks, it offset the high prices for travel from a what?\nA. car\nB. theatre\nC. airport\nD. hotel\nE. disneyland\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The traveling business man was glad his credit card had perks, it offset the high prices for travel from a what?\nA. car\nB. theatre\nC. airport\nD. hotel\nE. disneyland\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The traveling business man was glad his credit card had perks, it offset the high prices for travel from a what?\nA. car\nB. theatre\nC. airport\nD. hotel\nE. disneyland\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The traveling business man was glad his credit card had perks, it offset the high prices for travel from a what?\nA. car\nB. theatre\nC. airport\nD. hotel\nE. disneyland\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The traveling business man was glad his credit card had perks, it offset the high prices for travel from a what?\nA. car\nB. theatre\nC. airport\nD. hotel\nE. disneyland\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.467921018600464", "False"]], [["-5.967921257019043", "False"]], [["-2.717921018600464", "False"]], [["-1.4679210186004639", "True"]], [["-8.217921257019043", "False"]]], "filtered_resps": [["-2.467921018600464", "False"], ["-5.967921257019043", "False"], ["-2.717921018600464", "False"], ["-1.4679210186004639", "True"], ["-8.217921257019043", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0124a13cba3b0bf6dc2a82ab9047a4eea9eea70293b21133b46443333b913812", "prompt_hash": "3dc4cee0ef2081d1a2e7e024ee405c770868cbc9e47e60747783683cf58fb787", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 261, "doc": {"id": "d2871dc28c82471e5d7f71f79e49c257", "question": "Billy hated using other people's toilets. He was only comfortable on his own.  So whenever he needed to poop, he would go back to his what?", "question_concept": "toilet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bathroom", "motel room", "nearest public restroom", "house", "apartment"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Billy hated using other people's toilets. He was only comfortable on his own.  So whenever he needed to poop, he would go back to his what?\nA. bathroom\nB. motel room\nC. nearest public restroom\nD. house\nE. apartment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Billy hated using other people's toilets. He was only comfortable on his own.  So whenever he needed to poop, he would go back to his what?\nA. bathroom\nB. motel room\nC. nearest public restroom\nD. house\nE. apartment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Billy hated using other people's toilets. He was only comfortable on his own.  So whenever he needed to poop, he would go back to his what?\nA. bathroom\nB. motel room\nC. nearest public restroom\nD. house\nE. apartment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Billy hated using other people's toilets. He was only comfortable on his own.  So whenever he needed to poop, he would go back to his what?\nA. bathroom\nB. motel room\nC. nearest public restroom\nD. house\nE. apartment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Billy hated using other people's toilets. He was only comfortable on his own.  So whenever he needed to poop, he would go back to his what?\nA. bathroom\nB. motel room\nC. nearest public restroom\nD. house\nE. apartment\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2741291522979736", "True"]], [["-3.0241291522979736", "False"]], [["-6.2741289138793945", "False"]], [["-2.0241291522979736", "False"]], [["-8.024128913879395", "False"]]], "filtered_resps": [["-1.2741291522979736", "True"], ["-3.0241291522979736", "False"], ["-6.2741289138793945", "False"], ["-2.0241291522979736", "False"], ["-8.024128913879395", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7079d30c3110d3603f4657b4d83bd8e5bde44ecd3dcf3782fcfdbd07215210cb", "prompt_hash": "398043779deaf7aeffdfd8ab182fc6fd24789433f2550a171824dfa4fc1879a3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 262, "doc": {"id": "94770e75c4e2000e717b4218ddff19e8", "question": "The forest experienced a cold winter, where is it located?", "question_concept": "forest", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["earth", "south america", "amazon basin", "temperate zone", "national park"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The forest experienced a cold winter, where is it located?\nA. earth\nB. south america\nC. amazon basin\nD. temperate zone\nE. national park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The forest experienced a cold winter, where is it located?\nA. earth\nB. south america\nC. amazon basin\nD. temperate zone\nE. national park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The forest experienced a cold winter, where is it located?\nA. earth\nB. south america\nC. amazon basin\nD. temperate zone\nE. national park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The forest experienced a cold winter, where is it located?\nA. earth\nB. south america\nC. amazon basin\nD. temperate zone\nE. national park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The forest experienced a cold winter, where is it located?\nA. earth\nB. south america\nC. amazon basin\nD. temperate zone\nE. national park\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.119101047515869", "False"]], [["-4.869101047515869", "False"]], [["-3.619101047515869", "False"]], [["-1.3691011667251587", "True"]], [["-7.119101047515869", "False"]]], "filtered_resps": [["-3.119101047515869", "False"], ["-4.869101047515869", "False"], ["-3.619101047515869", "False"], ["-1.3691011667251587", "True"], ["-7.119101047515869", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2004245be8ef4ff73ce115eb97d178fafbcb31d52e703c01b4cb97384e7bef37", "prompt_hash": "b024a84613c31db29f501feba142f663f3ead899b32058ed889d66d6d8970d94", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 263, "doc": {"id": "08ad17d3ca1838b8724d21cf5921ec52", "question": "How can you let someone know about your anger without hurting him or her?", "question_concept": "anger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["release energy", "destroy enemy", "punch", "write letter", "lose your temper"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: How can you let someone know about your anger without hurting him or her?\nA. release energy\nB. destroy enemy\nC. punch\nD. write letter\nE. lose your temper\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How can you let someone know about your anger without hurting him or her?\nA. release energy\nB. destroy enemy\nC. punch\nD. write letter\nE. lose your temper\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How can you let someone know about your anger without hurting him or her?\nA. release energy\nB. destroy enemy\nC. punch\nD. write letter\nE. lose your temper\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How can you let someone know about your anger without hurting him or her?\nA. release energy\nB. destroy enemy\nC. punch\nD. write letter\nE. lose your temper\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How can you let someone know about your anger without hurting him or her?\nA. release energy\nB. destroy enemy\nC. punch\nD. write letter\nE. lose your temper\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8168840408325195", "True"]], [["-5.8168840408325195", "False"]], [["-5.8168840408325195", "False"]], [["-1.8168840408325195", "True"]], [["-6.3168840408325195", "False"]]], "filtered_resps": [["-1.8168840408325195", "True"], ["-5.8168840408325195", "False"], ["-5.8168840408325195", "False"], ["-1.8168840408325195", "True"], ["-6.3168840408325195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4304ab19f43c2a34e4b4a76a974cdc91c7581f8ef252129e0eac04f82aed20dc", "prompt_hash": "f06423f6264a7bd5831ead94b434c8d4cebbaa4f1541d87d2ef39610edf16c2d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 264, "doc": {"id": "21fb76bd8349628b441c76f47c33e77b", "question": "Where is one likely to find a brownstone?", "question_concept": "brownstone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["new york city", "subdivision", "ring", "hazleton", "live in"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is one likely to find a brownstone?\nA. new york city\nB. subdivision\nC. ring\nD. hazleton\nE. live in\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is one likely to find a brownstone?\nA. new york city\nB. subdivision\nC. ring\nD. hazleton\nE. live in\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is one likely to find a brownstone?\nA. new york city\nB. subdivision\nC. ring\nD. hazleton\nE. live in\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is one likely to find a brownstone?\nA. new york city\nB. subdivision\nC. ring\nD. hazleton\nE. live in\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is one likely to find a brownstone?\nA. new york city\nB. subdivision\nC. ring\nD. hazleton\nE. live in\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2734341621398926", "True"]], [["-3.7734341621398926", "False"]], [["-6.273434162139893", "False"]], [["-6.273434162139893", "False"]], [["-9.023433685302734", "False"]]], "filtered_resps": [["-1.2734341621398926", "True"], ["-3.7734341621398926", "False"], ["-6.273434162139893", "False"], ["-6.273434162139893", "False"], ["-9.023433685302734", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e8ef1dae2a3eace5d944d69ea9315177abf5da3760ce81d4d5ea4dbaa806e0a3", "prompt_hash": "bbbd43875a36895619ce1741f31ce4b94439ef2ffed5b1eec0c276348ae24229", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 265, "doc": {"id": "e151b44e0a7bf08a1dd3c861eef09161", "question": "What may I place the telephone on?", "question_concept": "telephone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bar", "friend's house", "desktop", "party", "office"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What may I place the telephone on?\nA. bar\nB. friend's house\nC. desktop\nD. party\nE. office\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What may I place the telephone on?\nA. bar\nB. friend's house\nC. desktop\nD. party\nE. office\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What may I place the telephone on?\nA. bar\nB. friend's house\nC. desktop\nD. party\nE. office\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What may I place the telephone on?\nA. bar\nB. friend's house\nC. desktop\nD. party\nE. office\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What may I place the telephone on?\nA. bar\nB. friend's house\nC. desktop\nD. party\nE. office\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.09859037399292", "False"]], [["-6.84859037399292", "False"]], [["-1.3485902547836304", "True"]], [["-8.598589897155762", "False"]], [["-4.84859037399292", "False"]]], "filtered_resps": [["-3.09859037399292", "False"], ["-6.84859037399292", "False"], ["-1.3485902547836304", "True"], ["-8.598589897155762", "False"], ["-4.84859037399292", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "36708a05377321cb55312bd3741ba0cc7e6a8112cbf5809aa66d46ef200a85d3", "prompt_hash": "ffb0bac2aa18cf4f5465856fbff6d0c83a5c33fd5b99f57284424855ae2f98c3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 266, "doc": {"id": "46351b3a6beb694c5f623583a3b1473d", "question": "What language type is someone from Iran likely to use?", "question_concept": "light source", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["books", "dard", "sky", "closed room", "television"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What language type is someone from Iran likely to use?\nA. books\nB. dard\nC. sky\nD. closed room\nE. television\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What language type is someone from Iran likely to use?\nA. books\nB. dard\nC. sky\nD. closed room\nE. television\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What language type is someone from Iran likely to use?\nA. books\nB. dard\nC. sky\nD. closed room\nE. television\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What language type is someone from Iran likely to use?\nA. books\nB. dard\nC. sky\nD. closed room\nE. television\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What language type is someone from Iran likely to use?\nA. books\nB. dard\nC. sky\nD. closed room\nE. television\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7075769901275635", "False"]], [["-2.4575769901275635", "False"]], [["-5.957576751708984", "False"]], [["-4.957576751708984", "False"]], [["-8.457576751708984", "False"]]], "filtered_resps": [["-2.7075769901275635", "False"], ["-2.4575769901275635", "False"], ["-5.957576751708984", "False"], ["-4.957576751708984", "False"], ["-8.457576751708984", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8abeefc1763647296b885babe15a2e7b52bc0df2a16a74ffdbb9fe2505835c9a", "prompt_hash": "dc6154e1087549e915c3a6120dd7d28fff2cf2587b4f82ee6a2ce938ca536c8a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 267, "doc": {"id": "db75e16788cf56d5dfb9773eaf91fe7e", "question": "John went to a party that lasted all night.  Because of this, he didn't have time for what?", "question_concept": "party", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["meeting", "blowing off steam", "stay home", "partying hard", "studying"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: John went to a party that lasted all night.  Because of this, he didn't have time for what?\nA. meeting\nB. blowing off steam\nC. stay home\nD. partying hard\nE. studying\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John went to a party that lasted all night.  Because of this, he didn't have time for what?\nA. meeting\nB. blowing off steam\nC. stay home\nD. partying hard\nE. studying\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John went to a party that lasted all night.  Because of this, he didn't have time for what?\nA. meeting\nB. blowing off steam\nC. stay home\nD. partying hard\nE. studying\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John went to a party that lasted all night.  Because of this, he didn't have time for what?\nA. meeting\nB. blowing off steam\nC. stay home\nD. partying hard\nE. studying\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John went to a party that lasted all night.  Because of this, he didn't have time for what?\nA. meeting\nB. blowing off steam\nC. stay home\nD. partying hard\nE. studying\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6966962814331055", "False"]], [["-3.4466962814331055", "False"]], [["-4.4466962814331055", "False"]], [["-5.4466962814331055", "False"]], [["-1.9466962814331055", "True"]]], "filtered_resps": [["-3.6966962814331055", "False"], ["-3.4466962814331055", "False"], ["-4.4466962814331055", "False"], ["-5.4466962814331055", "False"], ["-1.9466962814331055", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bbcdc5961bf47d9218b4d2f2b1536d3d840ec24681219197c77274fb88a58035", "prompt_hash": "4fc813ffed7b8a569c25ce2939f90893d441d933d341ce4ccaa48df849d66337", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 268, "doc": {"id": "ffd89796a9b09bef56c5803f188764c6", "question": "The child wasn't allowed in the kitchen but still wanted to help, what could it do to help in the dining room?", "question_concept": "child", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["set table", "form opinions", "make honey", "become adult", "gather flowers"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The child wasn't allowed in the kitchen but still wanted to help, what could it do to help in the dining room?\nA. set table\nB. form opinions\nC. make honey\nD. become adult\nE. gather flowers\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The child wasn't allowed in the kitchen but still wanted to help, what could it do to help in the dining room?\nA. set table\nB. form opinions\nC. make honey\nD. become adult\nE. gather flowers\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The child wasn't allowed in the kitchen but still wanted to help, what could it do to help in the dining room?\nA. set table\nB. form opinions\nC. make honey\nD. become adult\nE. gather flowers\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The child wasn't allowed in the kitchen but still wanted to help, what could it do to help in the dining room?\nA. set table\nB. form opinions\nC. make honey\nD. become adult\nE. gather flowers\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The child wasn't allowed in the kitchen but still wanted to help, what could it do to help in the dining room?\nA. set table\nB. form opinions\nC. make honey\nD. become adult\nE. gather flowers\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7197096943855286", "True"]], [["-7.719709873199463", "False"]], [["-7.719709873199463", "False"]], [["-8.469709396362305", "False"]], [["-9.719709396362305", "False"]]], "filtered_resps": [["-0.7197096943855286", "True"], ["-7.719709873199463", "False"], ["-7.719709873199463", "False"], ["-8.469709396362305", "False"], ["-9.719709396362305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ebdb6d38762ce2783ca64afc40b1bc4c64bd26a9424cad32432ba137035ca293", "prompt_hash": "d62a91d85fb0948a70309a4e0fe9a076012a0e7083ec32905e604c9a3eeb1be3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 269, "doc": {"id": "5622e49306bb82ec1cec817ad0506c60", "question": "He was having a hard time expressing himself in a healthy way, the psychologist said he was mentally what?", "question_concept": "expressing yourself", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["slow", "understood", "suffering", "embarrassment", "fun"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: He was having a hard time expressing himself in a healthy way, the psychologist said he was mentally what?\nA. slow\nB. understood\nC. suffering\nD. embarrassment\nE. fun\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was having a hard time expressing himself in a healthy way, the psychologist said he was mentally what?\nA. slow\nB. understood\nC. suffering\nD. embarrassment\nE. fun\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was having a hard time expressing himself in a healthy way, the psychologist said he was mentally what?\nA. slow\nB. understood\nC. suffering\nD. embarrassment\nE. fun\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was having a hard time expressing himself in a healthy way, the psychologist said he was mentally what?\nA. slow\nB. understood\nC. suffering\nD. embarrassment\nE. fun\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was having a hard time expressing himself in a healthy way, the psychologist said he was mentally what?\nA. slow\nB. understood\nC. suffering\nD. embarrassment\nE. fun\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.759310245513916", "False"]], [["-5.009310245513916", "False"]], [["-0.7593103051185608", "True"]], [["-6.759310245513916", "False"]], [["-8.759310722351074", "False"]]], "filtered_resps": [["-3.759310245513916", "False"], ["-5.009310245513916", "False"], ["-0.7593103051185608", "True"], ["-6.759310245513916", "False"], ["-8.759310722351074", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a04d6306189e4dbc8fb6394b613319200a4f2d79eb5ab6926e61e44b81d6fb06", "prompt_hash": "d3e08bb20d8c563bf0121935e6beb4845b504d6ccb9bc613e39ae5f519b994f9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 270, "doc": {"id": "6efaeb796307036719635242fa5ad0f3", "question": "When someone is physically competing what does their body do?", "question_concept": "competing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tension", "perform better", "releases heat", "winning or losing", "sweat"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: When someone is physically competing what does their body do?\nA. tension\nB. perform better\nC. releases heat\nD. winning or losing\nE. sweat\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When someone is physically competing what does their body do?\nA. tension\nB. perform better\nC. releases heat\nD. winning or losing\nE. sweat\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When someone is physically competing what does their body do?\nA. tension\nB. perform better\nC. releases heat\nD. winning or losing\nE. sweat\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When someone is physically competing what does their body do?\nA. tension\nB. perform better\nC. releases heat\nD. winning or losing\nE. sweat\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When someone is physically competing what does their body do?\nA. tension\nB. perform better\nC. releases heat\nD. winning or losing\nE. sweat\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.631518840789795", "False"]], [["-3.631518840789795", "False"]], [["-4.381518840789795", "False"]], [["-5.381518840789795", "False"]], [["-4.131518840789795", "False"]]], "filtered_resps": [["-3.631518840789795", "False"], ["-3.631518840789795", "False"], ["-4.381518840789795", "False"], ["-5.381518840789795", "False"], ["-4.131518840789795", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "06a21768b49e48585c624b7c95f0df0c5a9c3ebb6841f17c4ecd85e56c6eb768", "prompt_hash": "db2c3a161c52142ab80147004d1efb8d8cd8703a825d0221316c957174f95272", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 271, "doc": {"id": "114d310d1198abffaf8b88dab5a55aa7", "question": "How would you express information to a deaf person?", "question_concept": "express information", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["summarize main points", "close mouth", "write down", "may disagree", "talk"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: How would you express information to a deaf person?\nA. summarize main points\nB. close mouth\nC. write down\nD. may disagree\nE. talk\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How would you express information to a deaf person?\nA. summarize main points\nB. close mouth\nC. write down\nD. may disagree\nE. talk\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How would you express information to a deaf person?\nA. summarize main points\nB. close mouth\nC. write down\nD. may disagree\nE. talk\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How would you express information to a deaf person?\nA. summarize main points\nB. close mouth\nC. write down\nD. may disagree\nE. talk\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How would you express information to a deaf person?\nA. summarize main points\nB. close mouth\nC. write down\nD. may disagree\nE. talk\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.586721658706665", "True"]], [["-7.336721420288086", "False"]], [["-1.586721658706665", "True"]], [["-7.836721420288086", "False"]], [["-7.336721420288086", "False"]]], "filtered_resps": [["-1.586721658706665", "True"], ["-7.336721420288086", "False"], ["-1.586721658706665", "True"], ["-7.836721420288086", "False"], ["-7.336721420288086", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ada6d984ab61aa5c29fb2aa57e51a7a2fffbf271500f22a9d1ed1748e18866e2", "prompt_hash": "4af6d2a6abb45fe868d1cd6325210c301d3f5bfaf0accb0548c54620bcd37c56", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 272, "doc": {"id": "0f79faf5337706f2e0e39c15bbd2e99a", "question": "Printing on a printer can get expensive because it does what?", "question_concept": "printing on printer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["explode", "use paper", "store information", "queue", "noise"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Printing on a printer can get expensive because it does what?\nA. explode\nB. use paper\nC. store information\nD. queue\nE. noise\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Printing on a printer can get expensive because it does what?\nA. explode\nB. use paper\nC. store information\nD. queue\nE. noise\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Printing on a printer can get expensive because it does what?\nA. explode\nB. use paper\nC. store information\nD. queue\nE. noise\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Printing on a printer can get expensive because it does what?\nA. explode\nB. use paper\nC. store information\nD. queue\nE. noise\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Printing on a printer can get expensive because it does what?\nA. explode\nB. use paper\nC. store information\nD. queue\nE. noise\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.281685829162598", "False"]], [["-1.0316858291625977", "True"]], [["-5.531685829162598", "False"]], [["-7.281685829162598", "False"]], [["-8.781685829162598", "False"]]], "filtered_resps": [["-6.281685829162598", "False"], ["-1.0316858291625977", "True"], ["-5.531685829162598", "False"], ["-7.281685829162598", "False"], ["-8.781685829162598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ea61ff3f54d9a779fffe589850f83969041d5ca212a8cf8cd74a98c56535ad1e", "prompt_hash": "782baf3de228cb714610332e6f9262ba5052c2f0ef7ff5d1f338c823aedc49be", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 273, "doc": {"id": "b62d7d1b5eec31be0b65146a9fc069e0", "question": "What will god never do according to religion?", "question_concept": "god", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["anything", "judge people", "work miracles", "judge men", "everywhere"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What will god never do according to religion?\nA. anything\nB. judge people\nC. work miracles\nD. judge men\nE. everywhere\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will god never do according to religion?\nA. anything\nB. judge people\nC. work miracles\nD. judge men\nE. everywhere\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will god never do according to religion?\nA. anything\nB. judge people\nC. work miracles\nD. judge men\nE. everywhere\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will god never do according to religion?\nA. anything\nB. judge people\nC. work miracles\nD. judge men\nE. everywhere\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will god never do according to religion?\nA. anything\nB. judge people\nC. work miracles\nD. judge men\nE. everywhere\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.87208890914917", "False"]], [["-2.87208890914917", "False"]], [["-5.62208890914917", "False"]], [["-2.87208890914917", "False"]], [["-7.37208890914917", "False"]]], "filtered_resps": [["-2.87208890914917", "False"], ["-2.87208890914917", "False"], ["-5.62208890914917", "False"], ["-2.87208890914917", "False"], ["-7.37208890914917", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c131881cb6856bd13fbb1a3c2332e260187d478dc863b85ea1ac2b75887b68c3", "prompt_hash": "b85995aa0d0e17d0df68c9047fde5efd19d83bb1271b39691dba97b29d6ba5ca", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 274, "doc": {"id": "1342c6aec9f5179d6ea6fa5fefbe5188", "question": "One of the potential hazards of attending school is what?", "question_concept": "attending school", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cooties", "get smart", "boredom", "colds and flu", "taking tests"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: One of the potential hazards of attending school is what?\nA. cooties\nB. get smart\nC. boredom\nD. colds and flu\nE. taking tests\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: One of the potential hazards of attending school is what?\nA. cooties\nB. get smart\nC. boredom\nD. colds and flu\nE. taking tests\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: One of the potential hazards of attending school is what?\nA. cooties\nB. get smart\nC. boredom\nD. colds and flu\nE. taking tests\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: One of the potential hazards of attending school is what?\nA. cooties\nB. get smart\nC. boredom\nD. colds and flu\nE. taking tests\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: One of the potential hazards of attending school is what?\nA. cooties\nB. get smart\nC. boredom\nD. colds and flu\nE. taking tests\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.11134147644043", "False"]], [["-5.86134147644043", "False"]], [["-5.11134147644043", "False"]], [["-0.8613412380218506", "True"]], [["-8.86134147644043", "False"]]], "filtered_resps": [["-4.11134147644043", "False"], ["-5.86134147644043", "False"], ["-5.11134147644043", "False"], ["-0.8613412380218506", "True"], ["-8.86134147644043", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "aa41465e538e00254fa16c6576c7366d178416ec09cfa269bf78d74a65d705ea", "prompt_hash": "d27362e955a26df7765b2ca02efc53a80dee2b0fcf4d3dbdea36bef481afc85e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 275, "doc": {"id": "c74ae684ba6c76e2a913493483678c9d", "question": "What has a surface with many sides?", "question_concept": "surface", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tetrahedron", "object", "geometry problem", "lake", "triangle"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What has a surface with many sides?\nA. tetrahedron\nB. object\nC. geometry problem\nD. lake\nE. triangle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What has a surface with many sides?\nA. tetrahedron\nB. object\nC. geometry problem\nD. lake\nE. triangle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What has a surface with many sides?\nA. tetrahedron\nB. object\nC. geometry problem\nD. lake\nE. triangle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What has a surface with many sides?\nA. tetrahedron\nB. object\nC. geometry problem\nD. lake\nE. triangle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What has a surface with many sides?\nA. tetrahedron\nB. object\nC. geometry problem\nD. lake\nE. triangle\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.470267117023468", "True"]], [["-5.970267295837402", "False"]], [["-7.470267295837402", "False"]], [["-7.220267295837402", "False"]], [["-4.720267295837402", "False"]]], "filtered_resps": [["-0.470267117023468", "True"], ["-5.970267295837402", "False"], ["-7.470267295837402", "False"], ["-7.220267295837402", "False"], ["-4.720267295837402", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "13a74468d93202a2e343dbb48d40626f6f0caf0fb34b804d99e82b6e7a85a40b", "prompt_hash": "054988cdccd96b62f09a579c80f5d62cff06a9e075b485335c5bd7d07883832f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 276, "doc": {"id": "411e50225637b76187cc36b24fe3127c", "question": "What could bring a container from one place to another?", "question_concept": "container", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["food", "refrigerator", "cargo ship", "port", "fuel"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What could bring a container from one place to another?\nA. food\nB. refrigerator\nC. cargo ship\nD. port\nE. fuel\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could bring a container from one place to another?\nA. food\nB. refrigerator\nC. cargo ship\nD. port\nE. fuel\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could bring a container from one place to another?\nA. food\nB. refrigerator\nC. cargo ship\nD. port\nE. fuel\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could bring a container from one place to another?\nA. food\nB. refrigerator\nC. cargo ship\nD. port\nE. fuel\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could bring a container from one place to another?\nA. food\nB. refrigerator\nC. cargo ship\nD. port\nE. fuel\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8226182460784912", "False"]], [["-4.82261848449707", "False"]], [["-0.8226182460784912", "True"]], [["-6.07261848449707", "False"]], [["-5.82261848449707", "False"]]], "filtered_resps": [["-1.8226182460784912", "False"], ["-4.82261848449707", "False"], ["-0.8226182460784912", "True"], ["-6.07261848449707", "False"], ["-5.82261848449707", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "78d2e036c2e0443ef9e5fdba9453a336f224de9971ba2a1b58c714f6e2a239bf", "prompt_hash": "2f25e0327a5b774b9a6151b49da7df9dd63af23b826d8f20830f3c32d85fb95e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 277, "doc": {"id": "2a0e82bbf1471290c93c8f2a11af197f", "question": "The old style pop ups literally let you see the story when you did what?", "question_concept": "see story", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["giggle", "visualize", "open book", "reading", "go to movies"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The old style pop ups literally let you see the story when you did what?\nA. giggle\nB. visualize\nC. open book\nD. reading\nE. go to movies\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The old style pop ups literally let you see the story when you did what?\nA. giggle\nB. visualize\nC. open book\nD. reading\nE. go to movies\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The old style pop ups literally let you see the story when you did what?\nA. giggle\nB. visualize\nC. open book\nD. reading\nE. go to movies\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The old style pop ups literally let you see the story when you did what?\nA. giggle\nB. visualize\nC. open book\nD. reading\nE. go to movies\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The old style pop ups literally let you see the story when you did what?\nA. giggle\nB. visualize\nC. open book\nD. reading\nE. go to movies\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.753459930419922", "False"]], [["-2.753459930419922", "False"]], [["-2.253459930419922", "False"]], [["-2.253459930419922", "False"]], [["-7.003459930419922", "False"]]], "filtered_resps": [["-3.753459930419922", "False"], ["-2.753459930419922", "False"], ["-2.253459930419922", "False"], ["-2.253459930419922", "False"], ["-7.003459930419922", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1d21a7f880efc58d72ca26934771b19d8969de156963358df95dc72b6e2a952b", "prompt_hash": "2241e806fd27abc66dd1d4eccf656ed02a109af468e2b216f03e3c5baf382cc7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 278, "doc": {"id": "eaadd7a4b18cb48c00f85c3975750fe7", "question": "What is it called when you are talking to someone?", "question_concept": "talking to", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["communication", "quiet", "boredom", "persuaded", "learn"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is it called when you are talking to someone?\nA. communication\nB. quiet\nC. boredom\nD. persuaded\nE. learn\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is it called when you are talking to someone?\nA. communication\nB. quiet\nC. boredom\nD. persuaded\nE. learn\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is it called when you are talking to someone?\nA. communication\nB. quiet\nC. boredom\nD. persuaded\nE. learn\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is it called when you are talking to someone?\nA. communication\nB. quiet\nC. boredom\nD. persuaded\nE. learn\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is it called when you are talking to someone?\nA. communication\nB. quiet\nC. boredom\nD. persuaded\nE. learn\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.686223566532135", "True"]], [["-7.93622350692749", "False"]], [["-8.936223983764648", "False"]], [["-9.436223983764648", "False"]], [["-10.436223983764648", "False"]]], "filtered_resps": [["-0.686223566532135", "True"], ["-7.93622350692749", "False"], ["-8.936223983764648", "False"], ["-9.436223983764648", "False"], ["-10.436223983764648", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "310cf12cbc28277702d4d24e75a0c9cbe83e2a6835ac2e80c7a70cf6a0b2698b", "prompt_hash": "a7753fbd5d3d7a93e892ff704f6536a742ef6e0266dca99cd7c1ccc8a5d8191b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 279, "doc": {"id": "403c9b067ef7363efffa822bb08c5426", "question": "The family finished dinner, the child's chore was to load the dirty dishes where?", "question_concept": "dirty dishes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["restaurant kitchen", "dishwasher", "son's room", "cabinet", "party"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The family finished dinner, the child's chore was to load the dirty dishes where?\nA. restaurant kitchen\nB. dishwasher\nC. son's room\nD. cabinet\nE. party\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The family finished dinner, the child's chore was to load the dirty dishes where?\nA. restaurant kitchen\nB. dishwasher\nC. son's room\nD. cabinet\nE. party\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The family finished dinner, the child's chore was to load the dirty dishes where?\nA. restaurant kitchen\nB. dishwasher\nC. son's room\nD. cabinet\nE. party\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The family finished dinner, the child's chore was to load the dirty dishes where?\nA. restaurant kitchen\nB. dishwasher\nC. son's room\nD. cabinet\nE. party\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The family finished dinner, the child's chore was to load the dirty dishes where?\nA. restaurant kitchen\nB. dishwasher\nC. son's room\nD. cabinet\nE. party\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.556570053100586", "False"]], [["-1.0565701723098755", "True"]], [["-8.556570053100586", "False"]], [["-7.556570053100586", "False"]], [["-10.556570053100586", "False"]]], "filtered_resps": [["-5.556570053100586", "False"], ["-1.0565701723098755", "True"], ["-8.556570053100586", "False"], ["-7.556570053100586", "False"], ["-10.556570053100586", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "efc3bb1374dbeb0bed8b5347fe34e22b2267ed7626431fcfdb255846ac3a7771", "prompt_hash": "ba18ce279c7deb230fcdbcfbdfb1be3141ed3436ed81baee35d3469b4fd7ff3f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 280, "doc": {"id": "adf228312401c9ff421a4da1b46bb70a", "question": "Where could you find a bureau as well as many politicians?", "question_concept": "bureau", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["each city", "office building", "a zoo", "french government", "washington dc"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where could you find a bureau as well as many politicians?\nA. each city\nB. office building\nC. a zoo\nD. french government\nE. washington dc\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could you find a bureau as well as many politicians?\nA. each city\nB. office building\nC. a zoo\nD. french government\nE. washington dc\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could you find a bureau as well as many politicians?\nA. each city\nB. office building\nC. a zoo\nD. french government\nE. washington dc\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could you find a bureau as well as many politicians?\nA. each city\nB. office building\nC. a zoo\nD. french government\nE. washington dc\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could you find a bureau as well as many politicians?\nA. each city\nB. office building\nC. a zoo\nD. french government\nE. washington dc\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2109017372131348", "False"]], [["-1.7109016180038452", "False"]], [["-7.210901737213135", "False"]], [["-5.960901737213135", "False"]], [["-1.4609016180038452", "True"]]], "filtered_resps": [["-3.2109017372131348", "False"], ["-1.7109016180038452", "False"], ["-7.210901737213135", "False"], ["-5.960901737213135", "False"], ["-1.4609016180038452", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "971abb1004e786449ead777e5bf1b8bcdf5d3adaf954128db157b58432e03609", "prompt_hash": "b20167ae9dedfc9d2fa1c38c444a75de3be48c5bbefcb6092b78361bba50a5ae", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 281, "doc": {"id": "57c85e4c7ea2501ef9d8f304b524e2e4", "question": "Dad wanted to hide the check in his office, where did he put it?", "question_concept": "check", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cash register", "desk drawer", "fish tank", "bank", "pay envelope"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Dad wanted to hide the check in his office, where did he put it?\nA. cash register\nB. desk drawer\nC. fish tank\nD. bank\nE. pay envelope\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Dad wanted to hide the check in his office, where did he put it?\nA. cash register\nB. desk drawer\nC. fish tank\nD. bank\nE. pay envelope\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Dad wanted to hide the check in his office, where did he put it?\nA. cash register\nB. desk drawer\nC. fish tank\nD. bank\nE. pay envelope\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Dad wanted to hide the check in his office, where did he put it?\nA. cash register\nB. desk drawer\nC. fish tank\nD. bank\nE. pay envelope\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Dad wanted to hide the check in his office, where did he put it?\nA. cash register\nB. desk drawer\nC. fish tank\nD. bank\nE. pay envelope\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.203634262084961", "False"]], [["-0.9536340236663818", "True"]], [["-5.203634262084961", "False"]], [["-3.203634023666382", "False"]], [["-8.453634262084961", "False"]]], "filtered_resps": [["-5.203634262084961", "False"], ["-0.9536340236663818", "True"], ["-5.203634262084961", "False"], ["-3.203634023666382", "False"], ["-8.453634262084961", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "58c5a713eff1e539f84f1c73e50ada34a8cbdcc0ce5e54e18e9a9f2be33e6b00", "prompt_hash": "d9f7e293b5738ff46c46637afc654693be384de0492a4b5267f0d59f44f7c565", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 282, "doc": {"id": "c22f30eee57f7191ee07e9a916460f68", "question": "For some reason she was devoid of regular emotions, buying products was the only way she could feel what?", "question_concept": "buying products", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pleasure", "owning", "debt", "spending money", "smart"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: For some reason she was devoid of regular emotions, buying products was the only way she could feel what?\nA. pleasure\nB. owning\nC. debt\nD. spending money\nE. smart\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: For some reason she was devoid of regular emotions, buying products was the only way she could feel what?\nA. pleasure\nB. owning\nC. debt\nD. spending money\nE. smart\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: For some reason she was devoid of regular emotions, buying products was the only way she could feel what?\nA. pleasure\nB. owning\nC. debt\nD. spending money\nE. smart\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: For some reason she was devoid of regular emotions, buying products was the only way she could feel what?\nA. pleasure\nB. owning\nC. debt\nD. spending money\nE. smart\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: For some reason she was devoid of regular emotions, buying products was the only way she could feel what?\nA. pleasure\nB. owning\nC. debt\nD. spending money\nE. smart\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8047922253608704", "True"]], [["-3.8047921657562256", "False"]], [["-5.554792404174805", "False"]], [["-6.304792404174805", "False"]], [["-6.054792404174805", "False"]]], "filtered_resps": [["-0.8047922253608704", "True"], ["-3.8047921657562256", "False"], ["-5.554792404174805", "False"], ["-6.304792404174805", "False"], ["-6.054792404174805", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ee7ce464dec24bad206adf80af8bb3ce8bb2a127278f72b844d5ab550a0b53b2", "prompt_hash": "0553b0ca97027f3d68c114e1c40e05285ba0b61349f3ec07e3c7a6bc41814341", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 283, "doc": {"id": "026cb9c07a583ec933f2c4c67ae73836", "question": "Where are horses judged on appearance?", "question_concept": "horses", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["race track", "fair", "raised by humans", "in a field", "countryside"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where are horses judged on appearance?\nA. race track\nB. fair\nC. raised by humans\nD. in a field\nE. countryside\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are horses judged on appearance?\nA. race track\nB. fair\nC. raised by humans\nD. in a field\nE. countryside\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are horses judged on appearance?\nA. race track\nB. fair\nC. raised by humans\nD. in a field\nE. countryside\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are horses judged on appearance?\nA. race track\nB. fair\nC. raised by humans\nD. in a field\nE. countryside\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are horses judged on appearance?\nA. race track\nB. fair\nC. raised by humans\nD. in a field\nE. countryside\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.31729793548584", "False"]], [["-1.567298173904419", "True"]], [["-6.31729793548584", "False"]], [["-7.56729793548584", "False"]], [["-7.56729793548584", "False"]]], "filtered_resps": [["-4.31729793548584", "False"], ["-1.567298173904419", "True"], ["-6.31729793548584", "False"], ["-7.56729793548584", "False"], ["-7.56729793548584", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "15ec376a9ff63784fec4f108c0378365191a45b89d8e7aa3fe159c94acc2e25f", "prompt_hash": "2f6d391401b54da55b7a85c936a0d9f9955ae527caab18b162e26554677ddf3e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 284, "doc": {"id": "c57ed32566a2db1ec3d6e4fd595b9d05", "question": "Why do people read non fiction?", "question_concept": "read", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["having fun", "it's more relatable", "learn new things", "becoming absorbed", "falling asleep"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Why do people read non fiction?\nA. having fun\nB. it's more relatable\nC. learn new things\nD. becoming absorbed\nE. falling asleep\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why do people read non fiction?\nA. having fun\nB. it's more relatable\nC. learn new things\nD. becoming absorbed\nE. falling asleep\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why do people read non fiction?\nA. having fun\nB. it's more relatable\nC. learn new things\nD. becoming absorbed\nE. falling asleep\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why do people read non fiction?\nA. having fun\nB. it's more relatable\nC. learn new things\nD. becoming absorbed\nE. falling asleep\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why do people read non fiction?\nA. having fun\nB. it's more relatable\nC. learn new things\nD. becoming absorbed\nE. falling asleep\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.920915603637695", "False"]], [["-5.920915603637695", "False"]], [["-1.1709156036376953", "True"]], [["-6.420915603637695", "False"]], [["-8.420915603637695", "False"]]], "filtered_resps": [["-5.920915603637695", "False"], ["-5.920915603637695", "False"], ["-1.1709156036376953", "True"], ["-6.420915603637695", "False"], ["-8.420915603637695", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1e38dd18284d09f3aeb6ee747b1d272a756bc859dedbafcd73c4842bbe4243df", "prompt_hash": "8be094855267d2ca7a83ef18e08d087ebb87583dcc1c2c1a32a57dc517407a22", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 285, "doc": {"id": "93b52e7ea1acf10db891e9355e234123", "question": "While knitting you can do what using a radio?", "question_concept": "knitting", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["listen to music", "watch television", "making blankets", "eat", "watching tv"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: While knitting you can do what using a radio?\nA. listen to music\nB. watch television\nC. making blankets\nD. eat\nE. watching tv\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: While knitting you can do what using a radio?\nA. listen to music\nB. watch television\nC. making blankets\nD. eat\nE. watching tv\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: While knitting you can do what using a radio?\nA. listen to music\nB. watch television\nC. making blankets\nD. eat\nE. watching tv\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: While knitting you can do what using a radio?\nA. listen to music\nB. watch television\nC. making blankets\nD. eat\nE. watching tv\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: While knitting you can do what using a radio?\nA. listen to music\nB. watch television\nC. making blankets\nD. eat\nE. watching tv\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8173778653144836", "True"]], [["-4.317378044128418", "False"]], [["-4.567378044128418", "False"]], [["-8.317378044128418", "False"]], [["-6.317378044128418", "False"]]], "filtered_resps": [["-0.8173778653144836", "True"], ["-4.317378044128418", "False"], ["-4.567378044128418", "False"], ["-8.317378044128418", "False"], ["-6.317378044128418", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "28307bf80918501e0684dd80745924961c29044238f5b29e374bc6160c1da2b6", "prompt_hash": "1eed2dbb88e9e7a11bd550395cd22cde681b6c059a0398ea9354885732c830c0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 286, "doc": {"id": "dbdad44029098d4b1d202d6d857d6092", "question": "Where are you likely to set papers while working on them?", "question_concept": "papers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["table", "meeting", "drawer", "toilet", "garage"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where are you likely to set papers while working on them?\nA. table\nB. meeting\nC. drawer\nD. toilet\nE. garage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are you likely to set papers while working on them?\nA. table\nB. meeting\nC. drawer\nD. toilet\nE. garage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are you likely to set papers while working on them?\nA. table\nB. meeting\nC. drawer\nD. toilet\nE. garage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are you likely to set papers while working on them?\nA. table\nB. meeting\nC. drawer\nD. toilet\nE. garage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are you likely to set papers while working on them?\nA. table\nB. meeting\nC. drawer\nD. toilet\nE. garage\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.48327624797821045", "True"]], [["-6.9832763671875", "False"]], [["-6.2332763671875", "False"]], [["-7.7332763671875", "False"]], [["-8.7332763671875", "False"]]], "filtered_resps": [["-0.48327624797821045", "True"], ["-6.9832763671875", "False"], ["-6.2332763671875", "False"], ["-7.7332763671875", "False"], ["-8.7332763671875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "377a2b54e4bd7336ff1bbc766c851ede9da0cabd47752caad2df5187e8f7c0c9", "prompt_hash": "0bd40284d6664def65b7443eaad5755709c4f578045d7e01271d10909f9f79bc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 287, "doc": {"id": "69d0f70c173dda17934836d618ca7093", "question": "John had a massive debt to 50 million dollars.  Compared to that, Leo's 2000 dollar debt seemed what?", "question_concept": "massive", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dwarf", "inconsequential", "insubstantial", "lame", "tiny"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: John had a massive debt to 50 million dollars.  Compared to that, Leo's 2000 dollar debt seemed what?\nA. dwarf\nB. inconsequential\nC. insubstantial\nD. lame\nE. tiny\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John had a massive debt to 50 million dollars.  Compared to that, Leo's 2000 dollar debt seemed what?\nA. dwarf\nB. inconsequential\nC. insubstantial\nD. lame\nE. tiny\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John had a massive debt to 50 million dollars.  Compared to that, Leo's 2000 dollar debt seemed what?\nA. dwarf\nB. inconsequential\nC. insubstantial\nD. lame\nE. tiny\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John had a massive debt to 50 million dollars.  Compared to that, Leo's 2000 dollar debt seemed what?\nA. dwarf\nB. inconsequential\nC. insubstantial\nD. lame\nE. tiny\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John had a massive debt to 50 million dollars.  Compared to that, Leo's 2000 dollar debt seemed what?\nA. dwarf\nB. inconsequential\nC. insubstantial\nD. lame\nE. tiny\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9606833457946777", "False"]], [["-2.2106833457946777", "False"]], [["-5.710683345794678", "False"]], [["-7.210683345794678", "False"]], [["-2.2106833457946777", "False"]]], "filtered_resps": [["-3.9606833457946777", "False"], ["-2.2106833457946777", "False"], ["-5.710683345794678", "False"], ["-7.210683345794678", "False"], ["-2.2106833457946777", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a9e10b84d411371332b3c6b550d4850d41668937c1873adfb59e3ee047c51e12", "prompt_hash": "026dcd62811c10d6290770e37c19bc0871cef3fed533a2a99273306688539bb0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 288, "doc": {"id": "e5697a25935c5249d2108f55e245f3e4", "question": "The man flew his airplane over the city and saw pollution visibly in the sky, what was polluted?", "question_concept": "pollution", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["forest", "street", "air", "caused by humans", "car show"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The man flew his airplane over the city and saw pollution visibly in the sky, what was polluted?\nA. forest\nB. street\nC. air\nD. caused by humans\nE. car show\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man flew his airplane over the city and saw pollution visibly in the sky, what was polluted?\nA. forest\nB. street\nC. air\nD. caused by humans\nE. car show\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man flew his airplane over the city and saw pollution visibly in the sky, what was polluted?\nA. forest\nB. street\nC. air\nD. caused by humans\nE. car show\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man flew his airplane over the city and saw pollution visibly in the sky, what was polluted?\nA. forest\nB. street\nC. air\nD. caused by humans\nE. car show\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man flew his airplane over the city and saw pollution visibly in the sky, what was polluted?\nA. forest\nB. street\nC. air\nD. caused by humans\nE. car show\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.288695335388184", "False"]], [["-6.538695335388184", "False"]], [["-1.0386953353881836", "True"]], [["-6.538695335388184", "False"]], [["-9.538695335388184", "False"]]], "filtered_resps": [["-6.288695335388184", "False"], ["-6.538695335388184", "False"], ["-1.0386953353881836", "True"], ["-6.538695335388184", "False"], ["-9.538695335388184", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "15aa0a0682baaedcf3b53fed183ad1eb6dba7f8f8e42b12d4ed56f967a230d2b", "prompt_hash": "51b342106723d48053f44036b2a18c9b818149a127a019591f12622bafd4d67a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 289, "doc": {"id": "99af85081085e6228c6d78c95be01968", "question": "What is a very unlikely side effect of becoming inebriated?", "question_concept": "becoming inebriated", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fights", "drunkenness", "staggering", "puke", "paralysis"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is a very unlikely side effect of becoming inebriated?\nA. fights\nB. drunkenness\nC. staggering\nD. puke\nE. paralysis\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a very unlikely side effect of becoming inebriated?\nA. fights\nB. drunkenness\nC. staggering\nD. puke\nE. paralysis\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a very unlikely side effect of becoming inebriated?\nA. fights\nB. drunkenness\nC. staggering\nD. puke\nE. paralysis\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a very unlikely side effect of becoming inebriated?\nA. fights\nB. drunkenness\nC. staggering\nD. puke\nE. paralysis\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a very unlikely side effect of becoming inebriated?\nA. fights\nB. drunkenness\nC. staggering\nD. puke\nE. paralysis\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9520106315612793", "False"]], [["-4.702010631561279", "False"]], [["-7.452010631561279", "False"]], [["-6.452010631561279", "False"]], [["-0.9520107507705688", "True"]]], "filtered_resps": [["-2.9520106315612793", "False"], ["-4.702010631561279", "False"], ["-7.452010631561279", "False"], ["-6.452010631561279", "False"], ["-0.9520107507705688", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4bcbb78d6f4a12598167223b4a9d949a9a3f50bd65221c08182591edd173fb4f", "prompt_hash": "1c3cec6f14128d4182150f1bdf9fb70259901d7e691dcc82c7c3ebee44b4b224", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 290, "doc": {"id": "235094c966bcbdc94701b41b969f9c75", "question": "when communicating with my boss what should i do", "question_concept": "communicating", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["misunderstandings", "transfer of information", "learning", "confusion", "silence"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: when communicating with my boss what should i do\nA. misunderstandings\nB. transfer of information\nC. learning\nD. confusion\nE. silence\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: when communicating with my boss what should i do\nA. misunderstandings\nB. transfer of information\nC. learning\nD. confusion\nE. silence\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: when communicating with my boss what should i do\nA. misunderstandings\nB. transfer of information\nC. learning\nD. confusion\nE. silence\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: when communicating with my boss what should i do\nA. misunderstandings\nB. transfer of information\nC. learning\nD. confusion\nE. silence\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: when communicating with my boss what should i do\nA. misunderstandings\nB. transfer of information\nC. learning\nD. confusion\nE. silence\nAnswer:", "arg_1": " E"}}, "resps": [[["-7.066206455230713", "False"]], [["-2.316206455230713", "False"]], [["-7.566206455230713", "False"]], [["-9.066205978393555", "False"]], [["-7.816206455230713", "False"]]], "filtered_resps": [["-7.066206455230713", "False"], ["-2.316206455230713", "False"], ["-7.566206455230713", "False"], ["-9.066205978393555", "False"], ["-7.816206455230713", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8a32327a02f07e994d4d35756a8cf60229ec3bb3a54f6718fb4f6a499bc1e4f5", "prompt_hash": "751c0633496d2bd7048e86928ce9530e9ff7a82516145dc3252461a8a842a0f9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 291, "doc": {"id": "99789083502af9bf111876a00fae44ac", "question": "If not in a stream but in a market where will you find fish?", "question_concept": "fish", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stream", "aquarium", "refrigerator", "boat ride", "market"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If not in a stream but in a market where will you find fish?\nA. stream\nB. aquarium\nC. refrigerator\nD. boat ride\nE. market\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If not in a stream but in a market where will you find fish?\nA. stream\nB. aquarium\nC. refrigerator\nD. boat ride\nE. market\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If not in a stream but in a market where will you find fish?\nA. stream\nB. aquarium\nC. refrigerator\nD. boat ride\nE. market\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If not in a stream but in a market where will you find fish?\nA. stream\nB. aquarium\nC. refrigerator\nD. boat ride\nE. market\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If not in a stream but in a market where will you find fish?\nA. stream\nB. aquarium\nC. refrigerator\nD. boat ride\nE. market\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8835692405700684", "False"]], [["-2.6335692405700684", "False"]], [["-4.633569240570068", "False"]], [["-8.63356876373291", "False"]], [["-0.8835691213607788", "True"]]], "filtered_resps": [["-3.8835692405700684", "False"], ["-2.6335692405700684", "False"], ["-4.633569240570068", "False"], ["-8.63356876373291", "False"], ["-0.8835691213607788", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0de9cf642deecbed8fdc0b0262715e848d75859f7584496b31f375ef8f240b11", "prompt_hash": "8abf79a23e14e19f4588e69f7d0e11da157a7455f624c5dfb25ce61f76adda2d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 292, "doc": {"id": "1d44fb5f4b7f1e23ff6c1c083db81ba1", "question": "What are people likely to want to do with their friends?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["own land", "own home", "talk to each other", "believe in god", "spend time"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What are people likely to want to do with their friends?\nA. own land\nB. own home\nC. talk to each other\nD. believe in god\nE. spend time\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are people likely to want to do with their friends?\nA. own land\nB. own home\nC. talk to each other\nD. believe in god\nE. spend time\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are people likely to want to do with their friends?\nA. own land\nB. own home\nC. talk to each other\nD. believe in god\nE. spend time\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are people likely to want to do with their friends?\nA. own land\nB. own home\nC. talk to each other\nD. believe in god\nE. spend time\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are people likely to want to do with their friends?\nA. own land\nB. own home\nC. talk to each other\nD. believe in god\nE. spend time\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.93195915222168", "False"]], [["-6.18195915222168", "False"]], [["-1.4319591522216797", "True"]], [["-7.43195915222168", "False"]], [["-1.6819591522216797", "False"]]], "filtered_resps": [["-5.93195915222168", "False"], ["-6.18195915222168", "False"], ["-1.4319591522216797", "True"], ["-7.43195915222168", "False"], ["-1.6819591522216797", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "625a9a0b3e722f3b2d9c4fc33371d4f85c211b2426fee109d2a2e7f6ea96c460", "prompt_hash": "a4e6a0c8cc561ddce0e0c3cf1b9ab8478372b2e7726e4d567d5e9d8233c5b0cc", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 293, "doc": {"id": "194b66240f6fab75749c1e30ed09ea09", "question": "During a shark filled tornado where should you not be?", "question_concept": "shark", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["marine museum", "pool hall", "noodle house", "bad movie", "outside"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: During a shark filled tornado where should you not be?\nA. marine museum\nB. pool hall\nC. noodle house\nD. bad movie\nE. outside\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: During a shark filled tornado where should you not be?\nA. marine museum\nB. pool hall\nC. noodle house\nD. bad movie\nE. outside\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: During a shark filled tornado where should you not be?\nA. marine museum\nB. pool hall\nC. noodle house\nD. bad movie\nE. outside\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: During a shark filled tornado where should you not be?\nA. marine museum\nB. pool hall\nC. noodle house\nD. bad movie\nE. outside\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: During a shark filled tornado where should you not be?\nA. marine museum\nB. pool hall\nC. noodle house\nD. bad movie\nE. outside\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.308227062225342", "False"]], [["-3.558227062225342", "False"]], [["-5.558227062225342", "False"]], [["-4.058227062225342", "False"]], [["-1.3082270622253418", "True"]]], "filtered_resps": [["-3.308227062225342", "False"], ["-3.558227062225342", "False"], ["-5.558227062225342", "False"], ["-4.058227062225342", "False"], ["-1.3082270622253418", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d50b5722a33e087b5927a1d66c67bc7a905346edf8ec0ca8b1d2b54efc89d411", "prompt_hash": "643f37d474ea879034c88ec3a1d6dc12155278f2f58effc2d1fa3db6fd6b83cc", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 294, "doc": {"id": "83dad4fe630fddbdcd5b18ef890c66f2", "question": "What is the likely result of buying products in excess?", "question_concept": "buying products", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["running out of money", "spending money", "poverty", "comparison shopping", "overstocking"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is the likely result of buying products in excess?\nA. running out of money\nB. spending money\nC. poverty\nD. comparison shopping\nE. overstocking\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the likely result of buying products in excess?\nA. running out of money\nB. spending money\nC. poverty\nD. comparison shopping\nE. overstocking\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the likely result of buying products in excess?\nA. running out of money\nB. spending money\nC. poverty\nD. comparison shopping\nE. overstocking\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the likely result of buying products in excess?\nA. running out of money\nB. spending money\nC. poverty\nD. comparison shopping\nE. overstocking\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the likely result of buying products in excess?\nA. running out of money\nB. spending money\nC. poverty\nD. comparison shopping\nE. overstocking\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8568243980407715", "False"]], [["-1.606824278831482", "True"]], [["-6.1068243980407715", "False"]], [["-7.6068243980407715", "False"]], [["-1.856824278831482", "False"]]], "filtered_resps": [["-3.8568243980407715", "False"], ["-1.606824278831482", "True"], ["-6.1068243980407715", "False"], ["-7.6068243980407715", "False"], ["-1.856824278831482", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "633a88d36a8ca95650eccea72a3c0e8a60ee796249fee42b6d864c8a181e277c", "prompt_hash": "81e8d33bb994d763b13d9ceb05cf963c302742fc11f41105b2f03cf2b946bc2c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 295, "doc": {"id": "3ebc5ddd2e97fe37fcb52aa2a9e2e1a7", "question": "What is a person trying to accomplish when taking analgesics?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["acceptance", "avoid pain", "acknowledgment", "passing grade", "intellectual challenge"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is a person trying to accomplish when taking analgesics?\nA. acceptance\nB. avoid pain\nC. acknowledgment\nD. passing grade\nE. intellectual challenge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a person trying to accomplish when taking analgesics?\nA. acceptance\nB. avoid pain\nC. acknowledgment\nD. passing grade\nE. intellectual challenge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a person trying to accomplish when taking analgesics?\nA. acceptance\nB. avoid pain\nC. acknowledgment\nD. passing grade\nE. intellectual challenge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a person trying to accomplish when taking analgesics?\nA. acceptance\nB. avoid pain\nC. acknowledgment\nD. passing grade\nE. intellectual challenge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a person trying to accomplish when taking analgesics?\nA. acceptance\nB. avoid pain\nC. acknowledgment\nD. passing grade\nE. intellectual challenge\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8118717670440674", "False"]], [["-1.3118717670440674", "True"]], [["-8.561871528625488", "False"]], [["-8.311871528625488", "False"]], [["-10.311871528625488", "False"]]], "filtered_resps": [["-3.8118717670440674", "False"], ["-1.3118717670440674", "True"], ["-8.561871528625488", "False"], ["-8.311871528625488", "False"], ["-10.311871528625488", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "137253be005e2d76193dac5694b9af49929223f769d9f7a7266da26bab7fdefe", "prompt_hash": "a5ca86d636ba843fe18c0f4051c22b689a0eae14c7b7894ed7b44d3b26a71895", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 296, "doc": {"id": "9ed019338a48216de9eadf64faaf1ce0", "question": "Where would you put a glass after drinking from it?", "question_concept": "glass", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ocean", "water cooler", "cabinet", "dishwasher", "dining room"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put a glass after drinking from it?\nA. ocean\nB. water cooler\nC. cabinet\nD. dishwasher\nE. dining room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put a glass after drinking from it?\nA. ocean\nB. water cooler\nC. cabinet\nD. dishwasher\nE. dining room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put a glass after drinking from it?\nA. ocean\nB. water cooler\nC. cabinet\nD. dishwasher\nE. dining room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put a glass after drinking from it?\nA. ocean\nB. water cooler\nC. cabinet\nD. dishwasher\nE. dining room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put a glass after drinking from it?\nA. ocean\nB. water cooler\nC. cabinet\nD. dishwasher\nE. dining room\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8172788619995117", "False"]], [["-4.317278861999512", "False"]], [["-1.3172787427902222", "True"]], [["-3.0672788619995117", "False"]], [["-4.567278861999512", "False"]]], "filtered_resps": [["-3.8172788619995117", "False"], ["-4.317278861999512", "False"], ["-1.3172787427902222", "True"], ["-3.0672788619995117", "False"], ["-4.567278861999512", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "871dcee9a9142b1a379d470c268e8bf5be536067767d525fcad2a71409ef31fb", "prompt_hash": "5224821cd2f25cf3e8cf66611d881591dbfcc02d0a84e31bf78aa5881ccf5c50", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 297, "doc": {"id": "d1d2585e0ba1160948b7c5822a99b7a1", "question": "Where would you buy food?", "question_concept": "food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["freezer", "store", "home", "hatred", "kitchen"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you buy food?\nA. freezer\nB. store\nC. home\nD. hatred\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you buy food?\nA. freezer\nB. store\nC. home\nD. hatred\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you buy food?\nA. freezer\nB. store\nC. home\nD. hatred\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you buy food?\nA. freezer\nB. store\nC. home\nD. hatred\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you buy food?\nA. freezer\nB. store\nC. home\nD. hatred\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.606687068939209", "False"]], [["-1.106687068939209", "False"]], [["-7.356687068939209", "False"]], [["-9.106687545776367", "False"]], [["-8.106687545776367", "False"]]], "filtered_resps": [["-3.606687068939209", "False"], ["-1.106687068939209", "False"], ["-7.356687068939209", "False"], ["-9.106687545776367", "False"], ["-8.106687545776367", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9ae0681555b99c2fd184054d5010f529d50c8ffb992ec535f9abb5cc88413312", "prompt_hash": "b038c7c870944a63bb53474c634a4bb87e94ee15f423df9c04eeb3818f1731eb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 298, "doc": {"id": "e34a0d1331c6bd4574ffe308e3fbd389", "question": "When a person admits his mistakes, what are they doing?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["accident", "learn to swim", "thank god", "feel relieved", "act responsibly"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: When a person admits his mistakes, what are they doing?\nA. accident\nB. learn to swim\nC. thank god\nD. feel relieved\nE. act responsibly\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When a person admits his mistakes, what are they doing?\nA. accident\nB. learn to swim\nC. thank god\nD. feel relieved\nE. act responsibly\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When a person admits his mistakes, what are they doing?\nA. accident\nB. learn to swim\nC. thank god\nD. feel relieved\nE. act responsibly\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When a person admits his mistakes, what are they doing?\nA. accident\nB. learn to swim\nC. thank god\nD. feel relieved\nE. act responsibly\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When a person admits his mistakes, what are they doing?\nA. accident\nB. learn to swim\nC. thank god\nD. feel relieved\nE. act responsibly\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.736034393310547", "False"]], [["-6.736034393310547", "False"]], [["-6.986034393310547", "False"]], [["-6.736034393310547", "False"]], [["-1.7360343933105469", "True"]]], "filtered_resps": [["-3.736034393310547", "False"], ["-6.736034393310547", "False"], ["-6.986034393310547", "False"], ["-6.736034393310547", "False"], ["-1.7360343933105469", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9e44daf93990d3f5a631ff4005da1d92b93b0bfc51ea2a3066882b87a19b7e15", "prompt_hash": "b0a21de3aa2ae73fca134b1fd8cde013faf3428605541d030630d8cfc1e47441", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 299, "doc": {"id": "4858669d0193e5d9384dc37d4bb5c00c", "question": "Where do play a game for money?", "question_concept": "game", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["casino", "football ground", "ballpark", "family room", "toy store"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where do play a game for money?\nA. casino\nB. football ground\nC. ballpark\nD. family room\nE. toy store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do play a game for money?\nA. casino\nB. football ground\nC. ballpark\nD. family room\nE. toy store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do play a game for money?\nA. casino\nB. football ground\nC. ballpark\nD. family room\nE. toy store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do play a game for money?\nA. casino\nB. football ground\nC. ballpark\nD. family room\nE. toy store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do play a game for money?\nA. casino\nB. football ground\nC. ballpark\nD. family room\nE. toy store\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7743217945098877", "True"]], [["-7.524321556091309", "False"]], [["-5.774321556091309", "False"]], [["-9.274321556091309", "False"]], [["-10.524321556091309", "False"]]], "filtered_resps": [["-0.7743217945098877", "True"], ["-7.524321556091309", "False"], ["-5.774321556091309", "False"], ["-9.274321556091309", "False"], ["-10.524321556091309", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cab789ea8e520334d4c32c153a3ff3ad9571cff7f7f95c4204608259d1c28fe6", "prompt_hash": "17a50bafcd8e908fcb8daff5d9a57754155d786d50ec68103edaf179f7c12ea5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 300, "doc": {"id": "8fd82cdc253835814153fe7222e9967c", "question": "When you travel you should what in case of unexpected costs?", "question_concept": "travel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["go somewhere", "energy", "spend frivilously", "fly in airplane", "have money"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: When you travel you should what in case of unexpected costs?\nA. go somewhere\nB. energy\nC. spend frivilously\nD. fly in airplane\nE. have money\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you travel you should what in case of unexpected costs?\nA. go somewhere\nB. energy\nC. spend frivilously\nD. fly in airplane\nE. have money\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you travel you should what in case of unexpected costs?\nA. go somewhere\nB. energy\nC. spend frivilously\nD. fly in airplane\nE. have money\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you travel you should what in case of unexpected costs?\nA. go somewhere\nB. energy\nC. spend frivilously\nD. fly in airplane\nE. have money\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you travel you should what in case of unexpected costs?\nA. go somewhere\nB. energy\nC. spend frivilously\nD. fly in airplane\nE. have money\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.608278751373291", "False"]], [["-5.358278751373291", "False"]], [["-7.858278751373291", "False"]], [["-8.108278274536133", "False"]], [["-1.108278751373291", "True"]]], "filtered_resps": [["-4.608278751373291", "False"], ["-5.358278751373291", "False"], ["-7.858278751373291", "False"], ["-8.108278274536133", "False"], ["-1.108278751373291", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0d57c327db174035d325512bcfcd88e2376a70382c20f3e8790b68093e862932", "prompt_hash": "27850e6ad83e2af242ed1ef8be58e0a840be12387ae46f28866a46a48ba7b4d5", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 301, "doc": {"id": "66458bf8599c3ef1e7b50fa527531882", "question": "Donald is a prominent figure for the federal government, so in what city does he likely spend a lot of time?", "question_concept": "government", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["everything", "capitol building", "tourist sites", "canada", "washington d.c"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Donald is a prominent figure for the federal government, so in what city does he likely spend a lot of time?\nA. everything\nB. capitol building\nC. tourist sites\nD. canada\nE. washington d.c\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Donald is a prominent figure for the federal government, so in what city does he likely spend a lot of time?\nA. everything\nB. capitol building\nC. tourist sites\nD. canada\nE. washington d.c\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Donald is a prominent figure for the federal government, so in what city does he likely spend a lot of time?\nA. everything\nB. capitol building\nC. tourist sites\nD. canada\nE. washington d.c\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Donald is a prominent figure for the federal government, so in what city does he likely spend a lot of time?\nA. everything\nB. capitol building\nC. tourist sites\nD. canada\nE. washington d.c\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Donald is a prominent figure for the federal government, so in what city does he likely spend a lot of time?\nA. everything\nB. capitol building\nC. tourist sites\nD. canada\nE. washington d.c\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.942709445953369", "False"]], [["-5.442709445953369", "False"]], [["-9.442708969116211", "False"]], [["-7.942709445953369", "False"]], [["-2.192709445953369", "False"]]], "filtered_resps": [["-5.942709445953369", "False"], ["-5.442709445953369", "False"], ["-9.442708969116211", "False"], ["-7.942709445953369", "False"], ["-2.192709445953369", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d0bf3c78b6c2a2c23465846320d71574bbaf301d2bcbf5960f6e7e4bc357bc04", "prompt_hash": "a0bb421f42ab0419c34ce2d3478bbcba9fe8f20bbc5052c0fb901fa2bdc5baa3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 302, "doc": {"id": "879239b8a788f3c9e3dfdd0862f3d7c5", "question": "There was more than one bum asking for change or a ticket, it was the cheapest way to travel so it was no surprise sight at the what?", "question_concept": "bum", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["train station", "beach", "bus depot", "bridge", "stumblebum"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: There was more than one bum asking for change or a ticket, it was the cheapest way to travel so it was no surprise sight at the what?\nA. train station\nB. beach\nC. bus depot\nD. bridge\nE. stumblebum\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: There was more than one bum asking for change or a ticket, it was the cheapest way to travel so it was no surprise sight at the what?\nA. train station\nB. beach\nC. bus depot\nD. bridge\nE. stumblebum\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: There was more than one bum asking for change or a ticket, it was the cheapest way to travel so it was no surprise sight at the what?\nA. train station\nB. beach\nC. bus depot\nD. bridge\nE. stumblebum\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: There was more than one bum asking for change or a ticket, it was the cheapest way to travel so it was no surprise sight at the what?\nA. train station\nB. beach\nC. bus depot\nD. bridge\nE. stumblebum\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: There was more than one bum asking for change or a ticket, it was the cheapest way to travel so it was no surprise sight at the what?\nA. train station\nB. beach\nC. bus depot\nD. bridge\nE. stumblebum\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0283719301223755", "True"]], [["-5.028371810913086", "False"]], [["-4.528371810913086", "False"]], [["-7.278371810913086", "False"]], [["-5.278371810913086", "False"]]], "filtered_resps": [["-1.0283719301223755", "True"], ["-5.028371810913086", "False"], ["-4.528371810913086", "False"], ["-7.278371810913086", "False"], ["-5.278371810913086", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7b1cd1944331a2a842d542849bb8dbfd1899fe1278b4cde198a574248c23b675", "prompt_hash": "3fafcc27b38fc35a5ecc519b4dbbb45d59ec7ac39608ba2b269d8729cb418ebc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 303, "doc": {"id": "8a69e6df5e8ad6c9e6828aa66c59d046", "question": "John and Joe like planning games but Joe  was hit by a ball and fell down. What might have happened to Joe.", "question_concept": "playing game", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["anger", "good natured ribbing.", "enjoying", "injury", "enjoyment"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: John and Joe like planning games but Joe  was hit by a ball and fell down. What might have happened to Joe.\nA. anger\nB. good natured ribbing.\nC. enjoying\nD. injury\nE. enjoyment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John and Joe like planning games but Joe  was hit by a ball and fell down. What might have happened to Joe.\nA. anger\nB. good natured ribbing.\nC. enjoying\nD. injury\nE. enjoyment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John and Joe like planning games but Joe  was hit by a ball and fell down. What might have happened to Joe.\nA. anger\nB. good natured ribbing.\nC. enjoying\nD. injury\nE. enjoyment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John and Joe like planning games but Joe  was hit by a ball and fell down. What might have happened to Joe.\nA. anger\nB. good natured ribbing.\nC. enjoying\nD. injury\nE. enjoyment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John and Joe like planning games but Joe  was hit by a ball and fell down. What might have happened to Joe.\nA. anger\nB. good natured ribbing.\nC. enjoying\nD. injury\nE. enjoyment\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.501888275146484", "False"]], [["-5.251888275146484", "False"]], [["-7.251888275146484", "False"]], [["-2.0018885135650635", "False"]], [["-11.251888275146484", "False"]]], "filtered_resps": [["-6.501888275146484", "False"], ["-5.251888275146484", "False"], ["-7.251888275146484", "False"], ["-2.0018885135650635", "False"], ["-11.251888275146484", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f7e436d3552f3c24f3c66a540f205fbf7f68f2fe4279a2328e754e280e7c621d", "prompt_hash": "94e9b48230bbdbcc219a74ecb670b5c631bad68dfcd0beace5ccf0f7aa354ed7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 304, "doc": {"id": "8d275acea05fd16295c659c504576a9b", "question": "Where can you buy jeans at one of may indoor merchants?", "question_concept": "jeans", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["gap", "shopping mall", "bedroom", "laundromat", "bathroom"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you buy jeans at one of may indoor merchants?\nA. gap\nB. shopping mall\nC. bedroom\nD. laundromat\nE. bathroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you buy jeans at one of may indoor merchants?\nA. gap\nB. shopping mall\nC. bedroom\nD. laundromat\nE. bathroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you buy jeans at one of may indoor merchants?\nA. gap\nB. shopping mall\nC. bedroom\nD. laundromat\nE. bathroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you buy jeans at one of may indoor merchants?\nA. gap\nB. shopping mall\nC. bedroom\nD. laundromat\nE. bathroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you buy jeans at one of may indoor merchants?\nA. gap\nB. shopping mall\nC. bedroom\nD. laundromat\nE. bathroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1446115970611572", "False"]], [["-1.3946115970611572", "True"]], [["-8.144611358642578", "False"]], [["-10.144611358642578", "False"]], [["-11.144611358642578", "False"]]], "filtered_resps": [["-2.1446115970611572", "False"], ["-1.3946115970611572", "True"], ["-8.144611358642578", "False"], ["-10.144611358642578", "False"], ["-11.144611358642578", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "48b805040eee93a9f3ac2e9db8ca8318fb39a36bb0ebf986144418b78a96cf79", "prompt_hash": "b94270223693631aaa109ad590c6a852985156e86040e6627604022dc8fd8de0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 305, "doc": {"id": "91629c6f9e4af3e6acf385eb23fd8068", "question": "What do you write letter in in America?", "question_concept": "letter", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["syllable", "post office", "envelope", "english alphabet", "word"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What do you write letter in in America?\nA. syllable\nB. post office\nC. envelope\nD. english alphabet\nE. word\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you write letter in in America?\nA. syllable\nB. post office\nC. envelope\nD. english alphabet\nE. word\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you write letter in in America?\nA. syllable\nB. post office\nC. envelope\nD. english alphabet\nE. word\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you write letter in in America?\nA. syllable\nB. post office\nC. envelope\nD. english alphabet\nE. word\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you write letter in in America?\nA. syllable\nB. post office\nC. envelope\nD. english alphabet\nE. word\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.582592248916626", "False"]], [["-3.082592248916626", "False"]], [["-1.582592248916626", "True"]], [["-5.332592010498047", "False"]], [["-3.832592248916626", "False"]]], "filtered_resps": [["-3.582592248916626", "False"], ["-3.082592248916626", "False"], ["-1.582592248916626", "True"], ["-5.332592010498047", "False"], ["-3.832592248916626", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "265d0f50d3c071c9dbb6887dc77899cd9ba0444786109e6d5bfea763354ee93b", "prompt_hash": "01ebfe278f4a70b81fea1931751c8813eb4555cb26d2e03a81c46aa003a0fa70", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 306, "doc": {"id": "59eb56f366407ac7db72996be265883b", "question": "Joe owned back taxes as well as what other type of taxes?", "question_concept": "back", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["anterior", "front", "main", "front", "current"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Joe owned back taxes as well as what other type of taxes?\nA. anterior\nB. front\nC. main\nD. front\nE. current\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe owned back taxes as well as what other type of taxes?\nA. anterior\nB. front\nC. main\nD. front\nE. current\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe owned back taxes as well as what other type of taxes?\nA. anterior\nB. front\nC. main\nD. front\nE. current\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe owned back taxes as well as what other type of taxes?\nA. anterior\nB. front\nC. main\nD. front\nE. current\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe owned back taxes as well as what other type of taxes?\nA. anterior\nB. front\nC. main\nD. front\nE. current\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.348787784576416", "False"]], [["-5.098787784576416", "False"]], [["-6.598787784576416", "False"]], [["-6.848787784576416", "False"]], [["-1.3487879037857056", "True"]]], "filtered_resps": [["-5.348787784576416", "False"], ["-5.098787784576416", "False"], ["-6.598787784576416", "False"], ["-6.848787784576416", "False"], ["-1.3487879037857056", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fcd89beb9551817a34754f5833170ca4c2082f6aa2ec4b02c6a0cdcef41f6e29", "prompt_hash": "65269f62cc620562ecd3abf440756d039cc79df9e7d0a2ae7a944de0c5f9ea9f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 307, "doc": {"id": "4ab069f2e979d51f2c5929f590d09982", "question": "Where is a broadcast studio likely to be heard?", "question_concept": "broadcast studio", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["microphone", "arena", "radio station", "trees", "town"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a broadcast studio likely to be heard?\nA. microphone\nB. arena\nC. radio station\nD. trees\nE. town\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a broadcast studio likely to be heard?\nA. microphone\nB. arena\nC. radio station\nD. trees\nE. town\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a broadcast studio likely to be heard?\nA. microphone\nB. arena\nC. radio station\nD. trees\nE. town\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a broadcast studio likely to be heard?\nA. microphone\nB. arena\nC. radio station\nD. trees\nE. town\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a broadcast studio likely to be heard?\nA. microphone\nB. arena\nC. radio station\nD. trees\nE. town\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8191520571708679", "True"]], [["-3.3191521167755127", "False"]], [["-1.8191521167755127", "False"]], [["-7.819151878356934", "False"]], [["-8.069151878356934", "False"]]], "filtered_resps": [["-0.8191520571708679", "True"], ["-3.3191521167755127", "False"], ["-1.8191521167755127", "False"], ["-7.819151878356934", "False"], ["-8.069151878356934", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0e0f06c8aea625c0aca61e6b0c10a43ab3dcde3f2130f53000dda7ef945c2e6d", "prompt_hash": "7ac5a3f5087749bba04d60c0f554e90c4dff6608eabd6b3f6525502ac048f6f6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 308, "doc": {"id": "d6bb990e8c409d2b3af37a2da198e01f", "question": "Kramer wrote a self-referential book.  What might that book be about?", "question_concept": "book", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["counter", "coffee table", "school room", "backpack", "bedside table"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Kramer wrote a self-referential book.  What might that book be about?\nA. counter\nB. coffee table\nC. school room\nD. backpack\nE. bedside table\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Kramer wrote a self-referential book.  What might that book be about?\nA. counter\nB. coffee table\nC. school room\nD. backpack\nE. bedside table\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Kramer wrote a self-referential book.  What might that book be about?\nA. counter\nB. coffee table\nC. school room\nD. backpack\nE. bedside table\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Kramer wrote a self-referential book.  What might that book be about?\nA. counter\nB. coffee table\nC. school room\nD. backpack\nE. bedside table\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Kramer wrote a self-referential book.  What might that book be about?\nA. counter\nB. coffee table\nC. school room\nD. backpack\nE. bedside table\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6209685802459717", "False"]], [["-4.370968818664551", "False"]], [["-4.620968818664551", "False"]], [["-6.620968818664551", "False"]], [["-4.870968818664551", "False"]]], "filtered_resps": [["-1.6209685802459717", "False"], ["-4.370968818664551", "False"], ["-4.620968818664551", "False"], ["-6.620968818664551", "False"], ["-4.870968818664551", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e7c7d6d07447e7714d14255b4897cece58da720134a9cc86d4125ca419fe4029", "prompt_hash": "01c655d7306a14870ac1d719ca419ad6a8292a04c44bc4dc4cd27172342fdf4b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 309, "doc": {"id": "c5ad166ab5c5f5f067aa02b20f482523", "question": "Of all the sports, Billy enjoys football, but what does his concerned mother think of the sport?", "question_concept": "sports", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["very entertaining", "fun", "slow", "competitive", "violent"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Of all the sports, Billy enjoys football, but what does his concerned mother think of the sport?\nA. very entertaining\nB. fun\nC. slow\nD. competitive\nE. violent\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Of all the sports, Billy enjoys football, but what does his concerned mother think of the sport?\nA. very entertaining\nB. fun\nC. slow\nD. competitive\nE. violent\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Of all the sports, Billy enjoys football, but what does his concerned mother think of the sport?\nA. very entertaining\nB. fun\nC. slow\nD. competitive\nE. violent\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Of all the sports, Billy enjoys football, but what does his concerned mother think of the sport?\nA. very entertaining\nB. fun\nC. slow\nD. competitive\nE. violent\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Of all the sports, Billy enjoys football, but what does his concerned mother think of the sport?\nA. very entertaining\nB. fun\nC. slow\nD. competitive\nE. violent\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.950791358947754", "False"]], [["-4.450791358947754", "False"]], [["-6.450791358947754", "False"]], [["-6.200791358947754", "False"]], [["-2.200791120529175", "False"]]], "filtered_resps": [["-5.950791358947754", "False"], ["-4.450791358947754", "False"], ["-6.450791358947754", "False"], ["-6.200791358947754", "False"], ["-2.200791120529175", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b5dc98282d5661f319cb0114a1550ef5257c8f1f11300bdb0e2cea1a18f99739", "prompt_hash": "7e4be7bb295360bc87a300c744f83f4ecb8808d23a3c4f89dcfe112ee9d97a9c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 310, "doc": {"id": "ceafca2445b1b974d085a8cce38e8e44", "question": "What city will likely have many parking structures?", "question_concept": "parking structure", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["chicago", "big city", "large city building", "environment", "college campus"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What city will likely have many parking structures?\nA. chicago\nB. big city\nC. large city building\nD. environment\nE. college campus\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What city will likely have many parking structures?\nA. chicago\nB. big city\nC. large city building\nD. environment\nE. college campus\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What city will likely have many parking structures?\nA. chicago\nB. big city\nC. large city building\nD. environment\nE. college campus\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What city will likely have many parking structures?\nA. chicago\nB. big city\nC. large city building\nD. environment\nE. college campus\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What city will likely have many parking structures?\nA. chicago\nB. big city\nC. large city building\nD. environment\nE. college campus\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7448160648345947", "True"]], [["-4.244815826416016", "False"]], [["-3.7448160648345947", "False"]], [["-8.744815826416016", "False"]], [["-7.994815826416016", "False"]]], "filtered_resps": [["-0.7448160648345947", "True"], ["-4.244815826416016", "False"], ["-3.7448160648345947", "False"], ["-8.744815826416016", "False"], ["-7.994815826416016", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a8aaa0cc6ccd296707c6fd96af672c5d308bd88635d867b8c944a8ca85979c51", "prompt_hash": "24c7b79fbc545d28ca54dc91e21cf1f74f06ea1819a93e361b7b17102365d60b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 311, "doc": {"id": "2ef2ae21a2d3a9ecbd5c45ff378d10e3", "question": "Sally was afraid of danger and always double checked what?", "question_concept": "danger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fight enemy", "secure", "being safe", "safety", "vicinity"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Sally was afraid of danger and always double checked what?\nA. fight enemy\nB. secure\nC. being safe\nD. safety\nE. vicinity\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sally was afraid of danger and always double checked what?\nA. fight enemy\nB. secure\nC. being safe\nD. safety\nE. vicinity\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sally was afraid of danger and always double checked what?\nA. fight enemy\nB. secure\nC. being safe\nD. safety\nE. vicinity\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sally was afraid of danger and always double checked what?\nA. fight enemy\nB. secure\nC. being safe\nD. safety\nE. vicinity\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sally was afraid of danger and always double checked what?\nA. fight enemy\nB. secure\nC. being safe\nD. safety\nE. vicinity\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.561249732971191", "False"]], [["-1.5612497329711914", "True"]], [["-2.5612497329711914", "False"]], [["-2.3112497329711914", "False"]], [["-8.311249732971191", "False"]]], "filtered_resps": [["-5.561249732971191", "False"], ["-1.5612497329711914", "True"], ["-2.5612497329711914", "False"], ["-2.3112497329711914", "False"], ["-8.311249732971191", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0fde6d814c7230dce0bd188de1fd43530f919e7877daf325e54a41b5f38aeae2", "prompt_hash": "fff072715290203785e8617385eff23831df6ec5c7e7ba49ca29ff6ec7b24052", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 312, "doc": {"id": "793672da43fbc609e8c5760630c7e239", "question": "What is the habitat of the fox?", "question_concept": "fox", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hen house", "burrow", "california", "england", "mountains"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is the habitat of the fox?\nA. hen house\nB. burrow\nC. california\nD. england\nE. mountains\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the habitat of the fox?\nA. hen house\nB. burrow\nC. california\nD. england\nE. mountains\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the habitat of the fox?\nA. hen house\nB. burrow\nC. california\nD. england\nE. mountains\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the habitat of the fox?\nA. hen house\nB. burrow\nC. california\nD. england\nE. mountains\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the habitat of the fox?\nA. hen house\nB. burrow\nC. california\nD. england\nE. mountains\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.882043838500977", "False"]], [["-1.632043719291687", "False"]], [["-8.632043838500977", "False"]], [["-7.882043838500977", "False"]], [["-7.632043838500977", "False"]]], "filtered_resps": [["-4.882043838500977", "False"], ["-1.632043719291687", "False"], ["-8.632043838500977", "False"], ["-7.882043838500977", "False"], ["-7.632043838500977", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5cb2f2f248dd142e0935f75c997365937760a7fca56c5186408fc135fa05e809", "prompt_hash": "a04ff5463b89d1ed3b0edb99b25f9b719173e4026de0fe79da3b95dd175cf36d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 313, "doc": {"id": "558cb0bc25387ce38d71f64ef6f1fa57", "question": "People are very much like the animals, but one thing has secured or dominance over the planet.  We're better at doing what?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat eggs", "make tools", "eat dosa", "talk to each other", "smoke pot"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: People are very much like the animals, but one thing has secured or dominance over the planet.  We're better at doing what?\nA. eat eggs\nB. make tools\nC. eat dosa\nD. talk to each other\nE. smoke pot\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: People are very much like the animals, but one thing has secured or dominance over the planet.  We're better at doing what?\nA. eat eggs\nB. make tools\nC. eat dosa\nD. talk to each other\nE. smoke pot\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: People are very much like the animals, but one thing has secured or dominance over the planet.  We're better at doing what?\nA. eat eggs\nB. make tools\nC. eat dosa\nD. talk to each other\nE. smoke pot\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: People are very much like the animals, but one thing has secured or dominance over the planet.  We're better at doing what?\nA. eat eggs\nB. make tools\nC. eat dosa\nD. talk to each other\nE. smoke pot\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: People are very much like the animals, but one thing has secured or dominance over the planet.  We're better at doing what?\nA. eat eggs\nB. make tools\nC. eat dosa\nD. talk to each other\nE. smoke pot\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.547007083892822", "False"]], [["-1.2970072031021118", "True"]], [["-8.29700756072998", "False"]], [["-4.797007083892822", "False"]], [["-8.79700756072998", "False"]]], "filtered_resps": [["-6.547007083892822", "False"], ["-1.2970072031021118", "True"], ["-8.29700756072998", "False"], ["-4.797007083892822", "False"], ["-8.79700756072998", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b9fe759fcf67e3981c04238f168c08fc46138fe1b5d3fa45c2a103dd912cf7fb", "prompt_hash": "10d00c4f0683d662e418b9dce7f89dd06578681a547c3ac10f4f65d6c191899e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 314, "doc": {"id": "2c9f4a98ce774cd734b6e384d95051a7", "question": "They children loved having a back yard, and the parents loved that it was a safe what?", "question_concept": "back yard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["suburb", "neighborhood", "back of house", "roundabout", "property"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: They children loved having a back yard, and the parents loved that it was a safe what?\nA. suburb\nB. neighborhood\nC. back of house\nD. roundabout\nE. property\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They children loved having a back yard, and the parents loved that it was a safe what?\nA. suburb\nB. neighborhood\nC. back of house\nD. roundabout\nE. property\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They children loved having a back yard, and the parents loved that it was a safe what?\nA. suburb\nB. neighborhood\nC. back of house\nD. roundabout\nE. property\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They children loved having a back yard, and the parents loved that it was a safe what?\nA. suburb\nB. neighborhood\nC. back of house\nD. roundabout\nE. property\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They children loved having a back yard, and the parents loved that it was a safe what?\nA. suburb\nB. neighborhood\nC. back of house\nD. roundabout\nE. property\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.211188793182373", "False"]], [["-3.211188793182373", "False"]], [["-5.961188793182373", "False"]], [["-7.211188793182373", "False"]], [["-1.2111889123916626", "True"]]], "filtered_resps": [["-3.211188793182373", "False"], ["-3.211188793182373", "False"], ["-5.961188793182373", "False"], ["-7.211188793182373", "False"], ["-1.2111889123916626", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e15ce817905167e33bfc4448413ce311a8f8e3aee3f1c9e9a4193003a3dc625b", "prompt_hash": "c38a0d4ab7d3121d17afd7c247e9c878ce2c6eba410210c28cc267609cb94ff6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 315, "doc": {"id": "33c84708785f88c19737ef5b0e31a64b", "question": "While people just throw coins down them now, what originally had a pail to be lowered for it's intended use?", "question_concept": "pail", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["garage", "utility room", "slide", "wishing well", "garden"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: While people just throw coins down them now, what originally had a pail to be lowered for it's intended use?\nA. garage\nB. utility room\nC. slide\nD. wishing well\nE. garden\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: While people just throw coins down them now, what originally had a pail to be lowered for it's intended use?\nA. garage\nB. utility room\nC. slide\nD. wishing well\nE. garden\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: While people just throw coins down them now, what originally had a pail to be lowered for it's intended use?\nA. garage\nB. utility room\nC. slide\nD. wishing well\nE. garden\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: While people just throw coins down them now, what originally had a pail to be lowered for it's intended use?\nA. garage\nB. utility room\nC. slide\nD. wishing well\nE. garden\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: While people just throw coins down them now, what originally had a pail to be lowered for it's intended use?\nA. garage\nB. utility room\nC. slide\nD. wishing well\nE. garden\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.2332892417907715", "False"]], [["-7.2332892417907715", "False"]], [["-4.7332892417907715", "False"]], [["-2.4832892417907715", "False"]], [["-10.23328971862793", "False"]]], "filtered_resps": [["-4.2332892417907715", "False"], ["-7.2332892417907715", "False"], ["-4.7332892417907715", "False"], ["-2.4832892417907715", "False"], ["-10.23328971862793", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b2a5c944513c8d2008c618873ea63951adeca630cc7d1367906c0dcc1a2e5e74", "prompt_hash": "bf2810679f159390f7bf33ec0cb2a3ffc49d9d85cdb27604b8fae6e15a3d96f2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 316, "doc": {"id": "d867f76d000bdb59b9b4cb982bd7f0a0", "question": "Joe was thrown from his boat into the water.  The water was cold because it was the middle of winter and he cried out to his crew for help.  They couldn't hear him over the sound of the what?", "question_concept": "water", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["surface of earth", "teardrops", "snowflake", "typhoon", "motor"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Joe was thrown from his boat into the water.  The water was cold because it was the middle of winter and he cried out to his crew for help.  They couldn't hear him over the sound of the what?\nA. surface of earth\nB. teardrops\nC. snowflake\nD. typhoon\nE. motor\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe was thrown from his boat into the water.  The water was cold because it was the middle of winter and he cried out to his crew for help.  They couldn't hear him over the sound of the what?\nA. surface of earth\nB. teardrops\nC. snowflake\nD. typhoon\nE. motor\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe was thrown from his boat into the water.  The water was cold because it was the middle of winter and he cried out to his crew for help.  They couldn't hear him over the sound of the what?\nA. surface of earth\nB. teardrops\nC. snowflake\nD. typhoon\nE. motor\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe was thrown from his boat into the water.  The water was cold because it was the middle of winter and he cried out to his crew for help.  They couldn't hear him over the sound of the what?\nA. surface of earth\nB. teardrops\nC. snowflake\nD. typhoon\nE. motor\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe was thrown from his boat into the water.  The water was cold because it was the middle of winter and he cried out to his crew for help.  They couldn't hear him over the sound of the what?\nA. surface of earth\nB. teardrops\nC. snowflake\nD. typhoon\nE. motor\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.547604560852051", "False"]], [["-3.79760479927063", "False"]], [["-6.797604560852051", "False"]], [["-4.297604560852051", "False"]], [["-1.0476047992706299", "True"]]], "filtered_resps": [["-4.547604560852051", "False"], ["-3.79760479927063", "False"], ["-6.797604560852051", "False"], ["-4.297604560852051", "False"], ["-1.0476047992706299", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a63e899157fc9011fd4d4e3c3adb5d984735e176ac965b1f6b92204d489f4704", "prompt_hash": "030a3ef952bfb2b119784fda159c925aee7a08f70e462d9d0607e4594380d586", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 317, "doc": {"id": "8c607d2e2e897d74048fcc794137b683", "question": "When a human is earning money, where are they often found?", "question_concept": "human", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["deep thought", "park", "friend's house", "place of work", "school"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When a human is earning money, where are they often found?\nA. deep thought\nB. park\nC. friend's house\nD. place of work\nE. school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When a human is earning money, where are they often found?\nA. deep thought\nB. park\nC. friend's house\nD. place of work\nE. school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When a human is earning money, where are they often found?\nA. deep thought\nB. park\nC. friend's house\nD. place of work\nE. school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When a human is earning money, where are they often found?\nA. deep thought\nB. park\nC. friend's house\nD. place of work\nE. school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When a human is earning money, where are they often found?\nA. deep thought\nB. park\nC. friend's house\nD. place of work\nE. school\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.600440502166748", "False"]], [["-8.60044002532959", "False"]], [["-9.85044002532959", "False"]], [["-2.100440502166748", "False"]], [["-8.85044002532959", "False"]]], "filtered_resps": [["-4.600440502166748", "False"], ["-8.60044002532959", "False"], ["-9.85044002532959", "False"], ["-2.100440502166748", "False"], ["-8.85044002532959", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9c0e9e9d7e2c066a66ece48065e74d6188e7bc63843c5be3117404a4fe3027a8", "prompt_hash": "706cfdcac298b1a1663df2f26679292fc4cbbe83447302e263591fc20ca75213", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 318, "doc": {"id": "5215e26c99b2a9b376fb1c70096a388a", "question": "They passed a apple tree on their way to the racetrack, the were going to watch the biggest motorsport spectacle in the world where?", "question_concept": "apple tree", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["maryland", "indiana", "on tv", "park", "new jersey"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: They passed a apple tree on their way to the racetrack, the were going to watch the biggest motorsport spectacle in the world where?\nA. maryland\nB. indiana\nC. on tv\nD. park\nE. new jersey\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They passed a apple tree on their way to the racetrack, the were going to watch the biggest motorsport spectacle in the world where?\nA. maryland\nB. indiana\nC. on tv\nD. park\nE. new jersey\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They passed a apple tree on their way to the racetrack, the were going to watch the biggest motorsport spectacle in the world where?\nA. maryland\nB. indiana\nC. on tv\nD. park\nE. new jersey\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They passed a apple tree on their way to the racetrack, the were going to watch the biggest motorsport spectacle in the world where?\nA. maryland\nB. indiana\nC. on tv\nD. park\nE. new jersey\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They passed a apple tree on their way to the racetrack, the were going to watch the biggest motorsport spectacle in the world where?\nA. maryland\nB. indiana\nC. on tv\nD. park\nE. new jersey\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.059811115264893", "False"]], [["-4.309811115264893", "False"]], [["-4.809811115264893", "False"]], [["-7.809811115264893", "False"]], [["-1.8098111152648926", "False"]]], "filtered_resps": [["-5.059811115264893", "False"], ["-4.309811115264893", "False"], ["-4.809811115264893", "False"], ["-7.809811115264893", "False"], ["-1.8098111152648926", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "17d6d25be24021f3420e30439ef4cb1af1de2ac81256c1d6f8af85ad667ad679", "prompt_hash": "91b74e68ae4c7d55e1b22a7a3beacb2795a9e7f1989201be775acce661208636", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 319, "doc": {"id": "668dc6bce771b10cbf6336f3ec76520a", "question": "Why do people play chess on the weekends?", "question_concept": "play chess", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["satisfaction", "have fun", "thrilling", "made", "smart"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Why do people play chess on the weekends?\nA. satisfaction\nB. have fun\nC. thrilling\nD. made\nE. smart\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why do people play chess on the weekends?\nA. satisfaction\nB. have fun\nC. thrilling\nD. made\nE. smart\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why do people play chess on the weekends?\nA. satisfaction\nB. have fun\nC. thrilling\nD. made\nE. smart\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why do people play chess on the weekends?\nA. satisfaction\nB. have fun\nC. thrilling\nD. made\nE. smart\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why do people play chess on the weekends?\nA. satisfaction\nB. have fun\nC. thrilling\nD. made\nE. smart\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.110707759857178", "False"]], [["-1.6107078790664673", "False"]], [["-7.360707759857178", "False"]], [["-8.110708236694336", "False"]], [["-7.610707759857178", "False"]]], "filtered_resps": [["-6.110707759857178", "False"], ["-1.6107078790664673", "False"], ["-7.360707759857178", "False"], ["-8.110708236694336", "False"], ["-7.610707759857178", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a152784c149755077fa19a88d48fff2dd6f72d81658370dc4cab35aadee95192", "prompt_hash": "489d78bad0dcb9387685e332bec73a7bad69d67e10a9242708b2d880fd67cd68", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 320, "doc": {"id": "a339fe08f1f50463ee180b797e99ebcc", "question": "What do you need energy to do in gym class?", "question_concept": "energy", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["work", "tacos", "mass", "play sports", "wrestle"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What do you need energy to do in gym class?\nA. work\nB. tacos\nC. mass\nD. play sports\nE. wrestle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you need energy to do in gym class?\nA. work\nB. tacos\nC. mass\nD. play sports\nE. wrestle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you need energy to do in gym class?\nA. work\nB. tacos\nC. mass\nD. play sports\nE. wrestle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you need energy to do in gym class?\nA. work\nB. tacos\nC. mass\nD. play sports\nE. wrestle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you need energy to do in gym class?\nA. work\nB. tacos\nC. mass\nD. play sports\nE. wrestle\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4480799436569214", "True"]], [["-6.198080062866211", "False"]], [["-6.698080062866211", "False"]], [["-2.198080062866211", "False"]], [["-4.198080062866211", "False"]]], "filtered_resps": [["-1.4480799436569214", "True"], ["-6.198080062866211", "False"], ["-6.698080062866211", "False"], ["-2.198080062866211", "False"], ["-4.198080062866211", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2618bfadeae13e3191912e19aa62beb470dc95847c9c5bc4a18e24630ddadbec", "prompt_hash": "3dcd0dbe63601287c0cdb3a14a940b9f6adcb8c52f6aa780174e70a0a2f07f3f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 321, "doc": {"id": "526cd34f5b2afefbbb7830434785f298", "question": "Sarah dropped the marble because she wanted to do what?", "question_concept": "marble", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["game", "pouch", "home", "store", "jar"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Sarah dropped the marble because she wanted to do what?\nA. game\nB. pouch\nC. home\nD. store\nE. jar\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sarah dropped the marble because she wanted to do what?\nA. game\nB. pouch\nC. home\nD. store\nE. jar\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sarah dropped the marble because she wanted to do what?\nA. game\nB. pouch\nC. home\nD. store\nE. jar\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sarah dropped the marble because she wanted to do what?\nA. game\nB. pouch\nC. home\nD. store\nE. jar\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sarah dropped the marble because she wanted to do what?\nA. game\nB. pouch\nC. home\nD. store\nE. jar\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.0409586429595947", "True"]], [["-2.7909586429595947", "False"]], [["-4.290958404541016", "False"]], [["-3.7909586429595947", "False"]], [["-2.2909586429595947", "False"]]], "filtered_resps": [["-2.0409586429595947", "True"], ["-2.7909586429595947", "False"], ["-4.290958404541016", "False"], ["-3.7909586429595947", "False"], ["-2.2909586429595947", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "656c4adc82c9a94e9bbc93d071660711e4321c87b89784114a4c00c15c2b629d", "prompt_hash": "c780f31bab92cedac1442d6ffbb4ef25b6990752ec217711bf1d08dee3a9e58a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 322, "doc": {"id": "6c1c1c282cebe8917f607f0dbc1c102e", "question": "We are all human, and we all what?", "question_concept": "human", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["write", "eat cake", "smile", "think critically", "die"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: We are all human, and we all what?\nA. write\nB. eat cake\nC. smile\nD. think critically\nE. die\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: We are all human, and we all what?\nA. write\nB. eat cake\nC. smile\nD. think critically\nE. die\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: We are all human, and we all what?\nA. write\nB. eat cake\nC. smile\nD. think critically\nE. die\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: We are all human, and we all what?\nA. write\nB. eat cake\nC. smile\nD. think critically\nE. die\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: We are all human, and we all what?\nA. write\nB. eat cake\nC. smile\nD. think critically\nE. die\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.863067150115967", "False"]], [["-4.863067150115967", "False"]], [["-2.863067150115967", "False"]], [["-3.363067150115967", "False"]], [["-1.1130671501159668", "True"]]], "filtered_resps": [["-2.863067150115967", "False"], ["-4.863067150115967", "False"], ["-2.863067150115967", "False"], ["-3.363067150115967", "False"], ["-1.1130671501159668", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c137761659ab81970b9730a48652a4fabf060c0f4cd6de8e19cca6a908451b49", "prompt_hash": "79a3412b3ed97d910fb648b3762f69fa20e3fc759eda25e8602a1458eef11713", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 323, "doc": {"id": "b5baf77d3855935c87f01f5fb2216667", "question": "If a person were going to bed, what would be their goal?", "question_concept": "going to bed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lazy", "insomnia", "rest", "falling asleep", "dreaming of"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If a person were going to bed, what would be their goal?\nA. lazy\nB. insomnia\nC. rest\nD. falling asleep\nE. dreaming of\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a person were going to bed, what would be their goal?\nA. lazy\nB. insomnia\nC. rest\nD. falling asleep\nE. dreaming of\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a person were going to bed, what would be their goal?\nA. lazy\nB. insomnia\nC. rest\nD. falling asleep\nE. dreaming of\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a person were going to bed, what would be their goal?\nA. lazy\nB. insomnia\nC. rest\nD. falling asleep\nE. dreaming of\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a person were going to bed, what would be their goal?\nA. lazy\nB. insomnia\nC. rest\nD. falling asleep\nE. dreaming of\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.838946342468262", "False"]], [["-7.838946342468262", "False"]], [["-2.3389463424682617", "False"]], [["-1.5889464616775513", "True"]], [["-8.838946342468262", "False"]]], "filtered_resps": [["-4.838946342468262", "False"], ["-7.838946342468262", "False"], ["-2.3389463424682617", "False"], ["-1.5889464616775513", "True"], ["-8.838946342468262", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d03858dfbf946d18376d5338212113970c236353988ae900d6f421e2b965eeb0", "prompt_hash": "de6aff8b21a37b65a68b4fc6887961efd4adc2dbdaec7ce3b04d625b512721e2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 324, "doc": {"id": "83808e92381b2e5f4cdf55d1391645ae", "question": "What are candles good for eliminating?", "question_concept": "candle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shelf", "board", "church", "table", "dark"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What are candles good for eliminating?\nA. shelf\nB. board\nC. church\nD. table\nE. dark\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are candles good for eliminating?\nA. shelf\nB. board\nC. church\nD. table\nE. dark\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are candles good for eliminating?\nA. shelf\nB. board\nC. church\nD. table\nE. dark\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are candles good for eliminating?\nA. shelf\nB. board\nC. church\nD. table\nE. dark\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are candles good for eliminating?\nA. shelf\nB. board\nC. church\nD. table\nE. dark\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.986875057220459", "False"]], [["-5.986875057220459", "False"]], [["-4.736875057220459", "False"]], [["-5.986875057220459", "False"]], [["-0.986875057220459", "True"]]], "filtered_resps": [["-5.986875057220459", "False"], ["-5.986875057220459", "False"], ["-4.736875057220459", "False"], ["-5.986875057220459", "False"], ["-0.986875057220459", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a0c25f2dfbe49352233c89dbe436949f49880322bd920215dd97354d29c8713a", "prompt_hash": "75d04b5182d2b3bb54e2dd23b1224a9f784124de03b6c95b9f0d7613970d3449", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 325, "doc": {"id": "1a86310d7279097205a3403752c3b914", "question": "WHat leads to an early death?", "question_concept": "death", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["poisonous gas", "homicide", "cinder", "nuclear weapons", "cyanide"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: WHat leads to an early death?\nA. poisonous gas\nB. homicide\nC. cinder\nD. nuclear weapons\nE. cyanide\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: WHat leads to an early death?\nA. poisonous gas\nB. homicide\nC. cinder\nD. nuclear weapons\nE. cyanide\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: WHat leads to an early death?\nA. poisonous gas\nB. homicide\nC. cinder\nD. nuclear weapons\nE. cyanide\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: WHat leads to an early death?\nA. poisonous gas\nB. homicide\nC. cinder\nD. nuclear weapons\nE. cyanide\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: WHat leads to an early death?\nA. poisonous gas\nB. homicide\nC. cinder\nD. nuclear weapons\nE. cyanide\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.2492995262145996", "False"]], [["-4.9992995262146", "False"]], [["-4.2492995262146", "False"]], [["-5.7492995262146", "False"]], [["-2.2492995262145996", "False"]]], "filtered_resps": [["-2.2492995262145996", "False"], ["-4.9992995262146", "False"], ["-4.2492995262146", "False"], ["-5.7492995262146", "False"], ["-2.2492995262145996", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8a95923c86e72a542299c89cfa00838ba9a270a033db42d49ea51284ed08969e", "prompt_hash": "8efdc7311b885fbea807c123d86fa48ceea0da0338f535e62748e81d267ea69b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 326, "doc": {"id": "b4130d1790948134f3aeab9d3d79c181", "question": "What room would you find many bookcases and is used for contemplation?", "question_concept": "bookcase", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["study", "house", "homw", "kitchen", "den"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What room would you find many bookcases and is used for contemplation?\nA. study\nB. house\nC. homw\nD. kitchen\nE. den\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What room would you find many bookcases and is used for contemplation?\nA. study\nB. house\nC. homw\nD. kitchen\nE. den\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What room would you find many bookcases and is used for contemplation?\nA. study\nB. house\nC. homw\nD. kitchen\nE. den\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What room would you find many bookcases and is used for contemplation?\nA. study\nB. house\nC. homw\nD. kitchen\nE. den\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What room would you find many bookcases and is used for contemplation?\nA. study\nB. house\nC. homw\nD. kitchen\nE. den\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5268605947494507", "True"]], [["-8.276860237121582", "False"]], [["-7.77686071395874", "False"]], [["-8.526860237121582", "False"]], [["-8.276860237121582", "False"]]], "filtered_resps": [["-0.5268605947494507", "True"], ["-8.276860237121582", "False"], ["-7.77686071395874", "False"], ["-8.526860237121582", "False"], ["-8.276860237121582", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4035802fe46677e3a1985055797719ee525bc21af19329dbbfadfe7566079ece", "prompt_hash": "6eec4b468b22eb36fe5fd023af98439d8d1560c22ed50707e6096aa083fb76e2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 327, "doc": {"id": "a5097b7f56d20217679f28201801476f", "question": "Where do you head to travel to a star?", "question_concept": "star", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["night sky", "galaxy", "outer space", "hollywood", "eat cake"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you head to travel to a star?\nA. night sky\nB. galaxy\nC. outer space\nD. hollywood\nE. eat cake\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you head to travel to a star?\nA. night sky\nB. galaxy\nC. outer space\nD. hollywood\nE. eat cake\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you head to travel to a star?\nA. night sky\nB. galaxy\nC. outer space\nD. hollywood\nE. eat cake\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you head to travel to a star?\nA. night sky\nB. galaxy\nC. outer space\nD. hollywood\nE. eat cake\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you head to travel to a star?\nA. night sky\nB. galaxy\nC. outer space\nD. hollywood\nE. eat cake\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.858362197875977", "False"]], [["-5.358362197875977", "False"]], [["-0.858362078666687", "True"]], [["-7.108362197875977", "False"]], [["-5.858362197875977", "False"]]], "filtered_resps": [["-4.858362197875977", "False"], ["-5.358362197875977", "False"], ["-0.858362078666687", "True"], ["-7.108362197875977", "False"], ["-5.858362197875977", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4006c297fb5d99cd70bd4700975e0fca35be9cc21a8a561b90342f7f11065b60", "prompt_hash": "2285aee94f7f590b6ccad9c1292584ee375cc2dce1b99a4615958aaf4d8c229c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 328, "doc": {"id": "bcc5dd6292a64d8fa17cd07c360b335d", "question": "The player lifted his cornet and walked in rhythm, what was the player a member of?", "question_concept": "cornet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["museum", "high school band", "marching band", "orchestra", "band"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The player lifted his cornet and walked in rhythm, what was the player a member of?\nA. museum\nB. high school band\nC. marching band\nD. orchestra\nE. band\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The player lifted his cornet and walked in rhythm, what was the player a member of?\nA. museum\nB. high school band\nC. marching band\nD. orchestra\nE. band\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The player lifted his cornet and walked in rhythm, what was the player a member of?\nA. museum\nB. high school band\nC. marching band\nD. orchestra\nE. band\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The player lifted his cornet and walked in rhythm, what was the player a member of?\nA. museum\nB. high school band\nC. marching band\nD. orchestra\nE. band\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The player lifted his cornet and walked in rhythm, what was the player a member of?\nA. museum\nB. high school band\nC. marching band\nD. orchestra\nE. band\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.96384859085083", "False"]], [["-4.21384859085083", "False"]], [["-1.96384859085083", "False"]], [["-6.96384859085083", "False"]], [["-3.21384859085083", "False"]]], "filtered_resps": [["-3.96384859085083", "False"], ["-4.21384859085083", "False"], ["-1.96384859085083", "False"], ["-6.96384859085083", "False"], ["-3.21384859085083", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ae0550304a265f92c44b2d95a2eb4e8b1dd22f25221c64d515f34f7cd7b4bcd4", "prompt_hash": "21d9609a61fa47ebc99c9ec0b90f29cb379bccac0354d9d179262027feb3c015", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 329, "doc": {"id": "cfc7fccb8449a2a950c9d2a50991420e", "question": "What happens at soon as a living being is born?", "question_concept": "living", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["expiration", "growing older", "sometimes bad", "death", "start reproduction"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What happens at soon as a living being is born?\nA. expiration\nB. growing older\nC. sometimes bad\nD. death\nE. start reproduction\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens at soon as a living being is born?\nA. expiration\nB. growing older\nC. sometimes bad\nD. death\nE. start reproduction\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens at soon as a living being is born?\nA. expiration\nB. growing older\nC. sometimes bad\nD. death\nE. start reproduction\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens at soon as a living being is born?\nA. expiration\nB. growing older\nC. sometimes bad\nD. death\nE. start reproduction\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens at soon as a living being is born?\nA. expiration\nB. growing older\nC. sometimes bad\nD. death\nE. start reproduction\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.0911641120910645", "False"]], [["-2.0911641120910645", "False"]], [["-6.0911641120910645", "False"]], [["-7.0911641120910645", "False"]], [["-4.0911641120910645", "False"]]], "filtered_resps": [["-4.0911641120910645", "False"], ["-2.0911641120910645", "False"], ["-6.0911641120910645", "False"], ["-7.0911641120910645", "False"], ["-4.0911641120910645", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "31edbd421ace9efae09e74aa368fc83d55946ef732a2ec91f1ffef67a76f21bf", "prompt_hash": "86bfc8ab997cf945d8184ca77fcbe03bbcc235aa3c745d7b8cb3129af3c60126", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 330, "doc": {"id": "2e83c5989a018bec6d5f5ac7d3b72f49", "question": "When someone is talking and you missed something, what can you do to get them to repeat it?", "question_concept": "talking", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["walking", "ask question", "think", "write question in crayon", "sneeze"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: When someone is talking and you missed something, what can you do to get them to repeat it?\nA. walking\nB. ask question\nC. think\nD. write question in crayon\nE. sneeze\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When someone is talking and you missed something, what can you do to get them to repeat it?\nA. walking\nB. ask question\nC. think\nD. write question in crayon\nE. sneeze\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When someone is talking and you missed something, what can you do to get them to repeat it?\nA. walking\nB. ask question\nC. think\nD. write question in crayon\nE. sneeze\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When someone is talking and you missed something, what can you do to get them to repeat it?\nA. walking\nB. ask question\nC. think\nD. write question in crayon\nE. sneeze\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When someone is talking and you missed something, what can you do to get them to repeat it?\nA. walking\nB. ask question\nC. think\nD. write question in crayon\nE. sneeze\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.201272010803223", "False"]], [["-1.9512717723846436", "False"]], [["-7.701272010803223", "False"]], [["-8.201272010803223", "False"]], [["-8.701272010803223", "False"]]], "filtered_resps": [["-6.201272010803223", "False"], ["-1.9512717723846436", "False"], ["-7.701272010803223", "False"], ["-8.201272010803223", "False"], ["-8.701272010803223", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4840ae284ede6ed0918a0ccae3eae29091fba431c52aee66d83ee7248b05f72f", "prompt_hash": "a16a7369be205d87e80efb128af4dcb897f17c5edb0fe11d18a645b7f0be815e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 331, "doc": {"id": "34b2d6aecdb5af8efacf0b0aa7e3989f", "question": "Where does one store fabric in their own home?", "question_concept": "fabric", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sewing room", "clothing store", "tailor shop", "clothes store", "cotton mill"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where does one store fabric in their own home?\nA. sewing room\nB. clothing store\nC. tailor shop\nD. clothes store\nE. cotton mill\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where does one store fabric in their own home?\nA. sewing room\nB. clothing store\nC. tailor shop\nD. clothes store\nE. cotton mill\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where does one store fabric in their own home?\nA. sewing room\nB. clothing store\nC. tailor shop\nD. clothes store\nE. cotton mill\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where does one store fabric in their own home?\nA. sewing room\nB. clothing store\nC. tailor shop\nD. clothes store\nE. cotton mill\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where does one store fabric in their own home?\nA. sewing room\nB. clothing store\nC. tailor shop\nD. clothes store\nE. cotton mill\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4498237073421478", "True"]], [["-6.44982385635376", "False"]], [["-7.69982385635376", "False"]], [["-7.94982385635376", "False"]], [["-7.69982385635376", "False"]]], "filtered_resps": [["-0.4498237073421478", "True"], ["-6.44982385635376", "False"], ["-7.69982385635376", "False"], ["-7.94982385635376", "False"], ["-7.69982385635376", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3ea706fbd1aa2adc7aba32b998f10f69be61dee38028ae5dce9ab5a80edf6dd3", "prompt_hash": "de0c57f9d0bf5ca4bb5fec837d50f7c042d7509c4a4f1299d314269a84027766", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 332, "doc": {"id": "2ec7f8fe7948f9997e73f9bff7ba6e05", "question": "What do most companies not want to have relative to demand?", "question_concept": "want", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["oversupply", "plentitude", "stockpile", "superabundance", "busy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do most companies not want to have relative to demand?\nA. oversupply\nB. plentitude\nC. stockpile\nD. superabundance\nE. busy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do most companies not want to have relative to demand?\nA. oversupply\nB. plentitude\nC. stockpile\nD. superabundance\nE. busy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do most companies not want to have relative to demand?\nA. oversupply\nB. plentitude\nC. stockpile\nD. superabundance\nE. busy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do most companies not want to have relative to demand?\nA. oversupply\nB. plentitude\nC. stockpile\nD. superabundance\nE. busy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do most companies not want to have relative to demand?\nA. oversupply\nB. plentitude\nC. stockpile\nD. superabundance\nE. busy\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0080020427703857", "True"]], [["-6.008002281188965", "False"]], [["-7.008002281188965", "False"]], [["-5.758002281188965", "False"]], [["-8.758002281188965", "False"]]], "filtered_resps": [["-1.0080020427703857", "True"], ["-6.008002281188965", "False"], ["-7.008002281188965", "False"], ["-5.758002281188965", "False"], ["-8.758002281188965", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "708d869fb4eb102ebf59fdab55b9af2ea55f9ac2ee8f88fb6308d308972a676f", "prompt_hash": "d5f263afa4458b676660050683568f44d9ff3a6f4d4fedc16d1f5fe0866a03cb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 333, "doc": {"id": "651785ed4f7b0bd2e7ca9f70a42acea5", "question": "What is happening while he's playing basketball for such a long time?", "question_concept": "playing basketball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sweating", "pain", "having fun", "medium", "knee injury"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is happening while he's playing basketball for such a long time?\nA. sweating\nB. pain\nC. having fun\nD. medium\nE. knee injury\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is happening while he's playing basketball for such a long time?\nA. sweating\nB. pain\nC. having fun\nD. medium\nE. knee injury\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is happening while he's playing basketball for such a long time?\nA. sweating\nB. pain\nC. having fun\nD. medium\nE. knee injury\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is happening while he's playing basketball for such a long time?\nA. sweating\nB. pain\nC. having fun\nD. medium\nE. knee injury\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is happening while he's playing basketball for such a long time?\nA. sweating\nB. pain\nC. having fun\nD. medium\nE. knee injury\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7666492462158203", "True"]], [["-5.51664924621582", "False"]], [["-2.2666492462158203", "False"]], [["-6.26664924621582", "False"]], [["-7.26664924621582", "False"]]], "filtered_resps": [["-1.7666492462158203", "True"], ["-5.51664924621582", "False"], ["-2.2666492462158203", "False"], ["-6.26664924621582", "False"], ["-7.26664924621582", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1cca37dcd2f87ec8b97f953077783631a63a062f74cad833e8ad1d7b50f32597", "prompt_hash": "e19ac761ed5a554d71125233139976efbd0f3eaa67d31ae125da0341c3fc5d02", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 334, "doc": {"id": "ee46995407eb6357bb5410d49d378629", "question": "A traveler laments the fact that mass transit is limited in his city when his groceries get soaked by the rain as he waits where?", "question_concept": "traveller", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bus stop", "library", "motel", "airport", "subway"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A traveler laments the fact that mass transit is limited in his city when his groceries get soaked by the rain as he waits where?\nA. bus stop\nB. library\nC. motel\nD. airport\nE. subway\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A traveler laments the fact that mass transit is limited in his city when his groceries get soaked by the rain as he waits where?\nA. bus stop\nB. library\nC. motel\nD. airport\nE. subway\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A traveler laments the fact that mass transit is limited in his city when his groceries get soaked by the rain as he waits where?\nA. bus stop\nB. library\nC. motel\nD. airport\nE. subway\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A traveler laments the fact that mass transit is limited in his city when his groceries get soaked by the rain as he waits where?\nA. bus stop\nB. library\nC. motel\nD. airport\nE. subway\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A traveler laments the fact that mass transit is limited in his city when his groceries get soaked by the rain as he waits where?\nA. bus stop\nB. library\nC. motel\nD. airport\nE. subway\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.42025360465049744", "True"]], [["-5.670253753662109", "False"]], [["-5.670253753662109", "False"]], [["-6.920253753662109", "False"]], [["-4.170253753662109", "False"]]], "filtered_resps": [["-0.42025360465049744", "True"], ["-5.670253753662109", "False"], ["-5.670253753662109", "False"], ["-6.920253753662109", "False"], ["-4.170253753662109", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b4f8f76932de4a1419a204f0e53b9e7a5b56caeb7b1c4bf6eab9418e8105c2ab", "prompt_hash": "865be76b241f20ff046df7d25f6628f2bd2cadb993aba574d244bcb49b62e7d6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 335, "doc": {"id": "303aedda3a5ab8d853cbe4edc4b914c6", "question": "The person was in physical distress, where should he go?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["synagogue", "for help", "hospital", "bus stop", "building"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The person was in physical distress, where should he go?\nA. synagogue\nB. for help\nC. hospital\nD. bus stop\nE. building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The person was in physical distress, where should he go?\nA. synagogue\nB. for help\nC. hospital\nD. bus stop\nE. building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The person was in physical distress, where should he go?\nA. synagogue\nB. for help\nC. hospital\nD. bus stop\nE. building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The person was in physical distress, where should he go?\nA. synagogue\nB. for help\nC. hospital\nD. bus stop\nE. building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The person was in physical distress, where should he go?\nA. synagogue\nB. for help\nC. hospital\nD. bus stop\nE. building\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.690279483795166", "False"]], [["-2.690279483795166", "False"]], [["-2.440279483795166", "False"]], [["-9.440279006958008", "False"]], [["-10.940279006958008", "False"]]], "filtered_resps": [["-4.690279483795166", "False"], ["-2.690279483795166", "False"], ["-2.440279483795166", "False"], ["-9.440279006958008", "False"], ["-10.940279006958008", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "94969fc7aaa7569ed25c3d576f2a4ba5249734aad43e216b0908edd85812ed4f", "prompt_hash": "66b26a929ffe9d74b3eb8e852daf6ab5d88edb0e01639488da6ead1c2fb2e1d2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 336, "doc": {"id": "720b98fbc365736597147c984f6bd301", "question": "The cancer patient was expecting to die, so he made out his what?", "question_concept": "die", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["not to live", "write will", "never want", "seek help", "go to hell"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The cancer patient was expecting to die, so he made out his what?\nA. not to live\nB. write will\nC. never want\nD. seek help\nE. go to hell\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The cancer patient was expecting to die, so he made out his what?\nA. not to live\nB. write will\nC. never want\nD. seek help\nE. go to hell\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The cancer patient was expecting to die, so he made out his what?\nA. not to live\nB. write will\nC. never want\nD. seek help\nE. go to hell\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The cancer patient was expecting to die, so he made out his what?\nA. not to live\nB. write will\nC. never want\nD. seek help\nE. go to hell\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The cancer patient was expecting to die, so he made out his what?\nA. not to live\nB. write will\nC. never want\nD. seek help\nE. go to hell\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8108177185058594", "False"]], [["-1.3108175992965698", "True"]], [["-6.560817718505859", "False"]], [["-9.06081771850586", "False"]], [["-9.06081771850586", "False"]]], "filtered_resps": [["-2.8108177185058594", "False"], ["-1.3108175992965698", "True"], ["-6.560817718505859", "False"], ["-9.06081771850586", "False"], ["-9.06081771850586", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "252029633bf24e2a21891c258b1b708662d61160ed6c184c7f4d2b8c0122d8d0", "prompt_hash": "06585eee93d3fcede3106feba6fcc535ad0a0ca47ee09d1a7b1eda3c8b0d4b66", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 337, "doc": {"id": "c611875b43b67b91030b889b267bbcb3", "question": "There was a toll road that meandered from Maine to New Hampshire, where was it?", "question_concept": "toll road", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["massachusetts", "new england", "my house", "new jersey", "connecticut"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: There was a toll road that meandered from Maine to New Hampshire, where was it?\nA. massachusetts\nB. new england\nC. my house\nD. new jersey\nE. connecticut\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: There was a toll road that meandered from Maine to New Hampshire, where was it?\nA. massachusetts\nB. new england\nC. my house\nD. new jersey\nE. connecticut\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: There was a toll road that meandered from Maine to New Hampshire, where was it?\nA. massachusetts\nB. new england\nC. my house\nD. new jersey\nE. connecticut\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: There was a toll road that meandered from Maine to New Hampshire, where was it?\nA. massachusetts\nB. new england\nC. my house\nD. new jersey\nE. connecticut\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: There was a toll road that meandered from Maine to New Hampshire, where was it?\nA. massachusetts\nB. new england\nC. my house\nD. new jersey\nE. connecticut\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.8565673828125", "False"]], [["-1.6065672636032104", "False"]], [["-7.3565673828125", "False"]], [["-7.3565673828125", "False"]], [["-7.3565673828125", "False"]]], "filtered_resps": [["-4.8565673828125", "False"], ["-1.6065672636032104", "False"], ["-7.3565673828125", "False"], ["-7.3565673828125", "False"], ["-7.3565673828125", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "de3a7f8c82c50b9ab15c1abfba69da1e001a6d4d99ee71b26b50b89c3ce194ec", "prompt_hash": "edbf7f6a6c666b9f1b98590365b9249425d6c390e176700e6eddb314fd5464aa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 338, "doc": {"id": "0547da29ffab9b441bae8870cd0f9dab", "question": "If you partied all night you could find yourself already what, even when just beginning work?", "question_concept": "beginning work", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["getting tired", "working", "procrastination", "jumping", "sitting down"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If you partied all night you could find yourself already what, even when just beginning work?\nA. getting tired\nB. working\nC. procrastination\nD. jumping\nE. sitting down\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you partied all night you could find yourself already what, even when just beginning work?\nA. getting tired\nB. working\nC. procrastination\nD. jumping\nE. sitting down\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you partied all night you could find yourself already what, even when just beginning work?\nA. getting tired\nB. working\nC. procrastination\nD. jumping\nE. sitting down\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you partied all night you could find yourself already what, even when just beginning work?\nA. getting tired\nB. working\nC. procrastination\nD. jumping\nE. sitting down\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you partied all night you could find yourself already what, even when just beginning work?\nA. getting tired\nB. working\nC. procrastination\nD. jumping\nE. sitting down\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7025349140167236", "True"]], [["-6.2025346755981445", "False"]], [["-3.9525349140167236", "False"]], [["-7.7025346755981445", "False"]], [["-9.702534675598145", "False"]]], "filtered_resps": [["-0.7025349140167236", "True"], ["-6.2025346755981445", "False"], ["-3.9525349140167236", "False"], ["-7.7025346755981445", "False"], ["-9.702534675598145", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2c328dc3737a08c52b36fba51e9be3d0ad8da88579a282e8615d0bb0dfcef7c8", "prompt_hash": "3248347e9d7bdc934c2e9463a8cb689d945d18d7d59d8e1ad45c6436667631c3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 339, "doc": {"id": "21e312c7fd1a52341ce35b66457eab36", "question": "The cat carefully navigated the area, they do everything they can to avoid what?", "question_concept": "cat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get wet", "eat vegetables", "falling", "wool sweater", "sharp claws"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The cat carefully navigated the area, they do everything they can to avoid what?\nA. get wet\nB. eat vegetables\nC. falling\nD. wool sweater\nE. sharp claws\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The cat carefully navigated the area, they do everything they can to avoid what?\nA. get wet\nB. eat vegetables\nC. falling\nD. wool sweater\nE. sharp claws\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The cat carefully navigated the area, they do everything they can to avoid what?\nA. get wet\nB. eat vegetables\nC. falling\nD. wool sweater\nE. sharp claws\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The cat carefully navigated the area, they do everything they can to avoid what?\nA. get wet\nB. eat vegetables\nC. falling\nD. wool sweater\nE. sharp claws\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The cat carefully navigated the area, they do everything they can to avoid what?\nA. get wet\nB. eat vegetables\nC. falling\nD. wool sweater\nE. sharp claws\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2907453775405884", "True"]], [["-7.040745258331299", "False"]], [["-2.040745258331299", "False"]], [["-6.290745258331299", "False"]], [["-7.290745258331299", "False"]]], "filtered_resps": [["-1.2907453775405884", "True"], ["-7.040745258331299", "False"], ["-2.040745258331299", "False"], ["-6.290745258331299", "False"], ["-7.290745258331299", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f23b955ec71a534fcd2948ed6f68aec228b9bc27c4305381aea27f886440379e", "prompt_hash": "b71984fc26cb52128d7466db7f0e09c682d662e87e9ac509af2227c6a282453a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 340, "doc": {"id": "82e26bc22af89c38d54aa2d00dcb8a2b", "question": "What is someone usually doing if someone else is talking to him or her?", "question_concept": "talking to", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["listening", "language", "looking at eyes", "planning the perfect murder", "voice"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is someone usually doing if someone else is talking to him or her?\nA. listening\nB. language\nC. looking at eyes\nD. planning the perfect murder\nE. voice\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is someone usually doing if someone else is talking to him or her?\nA. listening\nB. language\nC. looking at eyes\nD. planning the perfect murder\nE. voice\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is someone usually doing if someone else is talking to him or her?\nA. listening\nB. language\nC. looking at eyes\nD. planning the perfect murder\nE. voice\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is someone usually doing if someone else is talking to him or her?\nA. listening\nB. language\nC. looking at eyes\nD. planning the perfect murder\nE. voice\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is someone usually doing if someone else is talking to him or her?\nA. listening\nB. language\nC. looking at eyes\nD. planning the perfect murder\nE. voice\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7966080904006958", "True"]], [["-7.046607971191406", "False"]], [["-7.546607971191406", "False"]], [["-8.296607971191406", "False"]], [["-9.296607971191406", "False"]]], "filtered_resps": [["-0.7966080904006958", "True"], ["-7.046607971191406", "False"], ["-7.546607971191406", "False"], ["-8.296607971191406", "False"], ["-9.296607971191406", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ec723d79a742eff40981dbf5aeeb3fbabfab820282f23394b7264fd79dad6d25", "prompt_hash": "45dfef0cd63eb74afe29ecf6914ab57fc06f164dc99aec1f1f6cde8fdc2b9085", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 341, "doc": {"id": "f75357e48c3026cfa4da3dba9f91bb21", "question": "What does the sky do before a rain?", "question_concept": "sky", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["appear beautiful", "appear blue", "shows a rainbow", "rain water", "cloud over"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What does the sky do before a rain?\nA. appear beautiful\nB. appear blue\nC. shows a rainbow\nD. rain water\nE. cloud over\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does the sky do before a rain?\nA. appear beautiful\nB. appear blue\nC. shows a rainbow\nD. rain water\nE. cloud over\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does the sky do before a rain?\nA. appear beautiful\nB. appear blue\nC. shows a rainbow\nD. rain water\nE. cloud over\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does the sky do before a rain?\nA. appear beautiful\nB. appear blue\nC. shows a rainbow\nD. rain water\nE. cloud over\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does the sky do before a rain?\nA. appear beautiful\nB. appear blue\nC. shows a rainbow\nD. rain water\nE. cloud over\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.353076934814453", "False"]], [["-4.853076934814453", "False"]], [["-5.853076934814453", "False"]], [["-6.603076934814453", "False"]], [["-1.6030768156051636", "False"]]], "filtered_resps": [["-4.353076934814453", "False"], ["-4.853076934814453", "False"], ["-5.853076934814453", "False"], ["-6.603076934814453", "False"], ["-1.6030768156051636", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5cfe5b922fdd5ec8c9ec90dc4db6370474f9f00e691f859e9eebd93a58669b8d", "prompt_hash": "4dd061eec8c00b6be052571b51eaabc91f8072726dcb0bf248cd6987b90a25b4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 342, "doc": {"id": "64931f9097155672bfe3e16f03b2c195", "question": "Pens, computers, text books and paper clips can all be found where?", "question_concept": "paper clips", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desktop", "university", "drawer", "table", "work"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Pens, computers, text books and paper clips can all be found where?\nA. desktop\nB. university\nC. drawer\nD. table\nE. work\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Pens, computers, text books and paper clips can all be found where?\nA. desktop\nB. university\nC. drawer\nD. table\nE. work\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Pens, computers, text books and paper clips can all be found where?\nA. desktop\nB. university\nC. drawer\nD. table\nE. work\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Pens, computers, text books and paper clips can all be found where?\nA. desktop\nB. university\nC. drawer\nD. table\nE. work\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Pens, computers, text books and paper clips can all be found where?\nA. desktop\nB. university\nC. drawer\nD. table\nE. work\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.912139415740967", "False"]], [["-2.412139415740967", "False"]], [["-2.412139415740967", "False"]], [["-4.662139415740967", "False"]], [["-6.912139415740967", "False"]]], "filtered_resps": [["-3.912139415740967", "False"], ["-2.412139415740967", "False"], ["-2.412139415740967", "False"], ["-4.662139415740967", "False"], ["-6.912139415740967", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "03cf009fe5121d41aac419d77de955160f356ea7bceb98be1ffc1cfd4128c589", "prompt_hash": "92a79f373b11c005f575efb52878efc975db877031d5b986f64802701eacd617", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 343, "doc": {"id": "5de3248caa2e5ed83dd0ec45a15eae18", "question": "What geographic area is a lizard likely to be?", "question_concept": "lizard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ball stopped", "west texas", "arid regions", "garden", "warm place"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What geographic area is a lizard likely to be?\nA. ball stopped\nB. west texas\nC. arid regions\nD. garden\nE. warm place\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What geographic area is a lizard likely to be?\nA. ball stopped\nB. west texas\nC. arid regions\nD. garden\nE. warm place\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What geographic area is a lizard likely to be?\nA. ball stopped\nB. west texas\nC. arid regions\nD. garden\nE. warm place\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What geographic area is a lizard likely to be?\nA. ball stopped\nB. west texas\nC. arid regions\nD. garden\nE. warm place\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What geographic area is a lizard likely to be?\nA. ball stopped\nB. west texas\nC. arid regions\nD. garden\nE. warm place\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0354342460632324", "False"]], [["-5.035434246063232", "False"]], [["-1.2854341268539429", "True"]], [["-6.285434246063232", "False"]], [["-6.035434246063232", "False"]]], "filtered_resps": [["-3.0354342460632324", "False"], ["-5.035434246063232", "False"], ["-1.2854341268539429", "True"], ["-6.285434246063232", "False"], ["-6.035434246063232", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1b6dc16a6044010b264651184143417f88457e2bc1208ddd8d0535c74ff51983", "prompt_hash": "93c246e421117124792eb4ab22e87b24e8fa500b542265c6401d35c77c0c1bda", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 344, "doc": {"id": "0611dfbf5114084723d75f59b4f67412", "question": "What do you use to carry your briefcase?", "question_concept": "briefcase", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["office building", "school", "courtroom", "airport", "hand"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do you use to carry your briefcase?\nA. office building\nB. school\nC. courtroom\nD. airport\nE. hand\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you use to carry your briefcase?\nA. office building\nB. school\nC. courtroom\nD. airport\nE. hand\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you use to carry your briefcase?\nA. office building\nB. school\nC. courtroom\nD. airport\nE. hand\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you use to carry your briefcase?\nA. office building\nB. school\nC. courtroom\nD. airport\nE. hand\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you use to carry your briefcase?\nA. office building\nB. school\nC. courtroom\nD. airport\nE. hand\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.258147716522217", "False"]], [["-5.508147716522217", "False"]], [["-7.008147716522217", "False"]], [["-7.508147716522217", "False"]], [["-0.5081478953361511", "True"]]], "filtered_resps": [["-4.258147716522217", "False"], ["-5.508147716522217", "False"], ["-7.008147716522217", "False"], ["-7.508147716522217", "False"], ["-0.5081478953361511", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8dd38f169b3373fc3554d80ef7e648cc787df98c5c4da2359360bc17add74fd6", "prompt_hash": "7cfd5568bc5ba367c75c3a642218b284da178f79ef071f72dac53cb46bdbdf59", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 345, "doc": {"id": "5b8d76889510384b38b72945e8d28f53", "question": "He picked up his pace to a run, he wanted to do what?", "question_concept": "run", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["learn to walk", "frightened", "get away from", "exercise", "go faster"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: He picked up his pace to a run, he wanted to do what?\nA. learn to walk\nB. frightened\nC. get away from\nD. exercise\nE. go faster\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He picked up his pace to a run, he wanted to do what?\nA. learn to walk\nB. frightened\nC. get away from\nD. exercise\nE. go faster\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He picked up his pace to a run, he wanted to do what?\nA. learn to walk\nB. frightened\nC. get away from\nD. exercise\nE. go faster\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He picked up his pace to a run, he wanted to do what?\nA. learn to walk\nB. frightened\nC. get away from\nD. exercise\nE. go faster\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He picked up his pace to a run, he wanted to do what?\nA. learn to walk\nB. frightened\nC. get away from\nD. exercise\nE. go faster\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.394781112670898", "False"]], [["-5.144781112670898", "False"]], [["-3.3947811126708984", "False"]], [["-1.8947809934616089", "True"]], [["-2.8947811126708984", "False"]]], "filtered_resps": [["-4.394781112670898", "False"], ["-5.144781112670898", "False"], ["-3.3947811126708984", "False"], ["-1.8947809934616089", "True"], ["-2.8947811126708984", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "09155ae87b0440b31b6bbee0f1b90c7ca4289054c7eb6afcccbbf68ade9e15bd", "prompt_hash": "64c454a2311c2d083b013fc07e5fc8d96bb368916a67aaeafd1e33b03a61a034", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 346, "doc": {"id": "d81f5c49bc060dc799681bf4cacac73a", "question": "What would a person do if they do not have any friends?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["talk to people", "try again", "fall asleep", "stand alone", "thank god"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What would a person do if they do not have any friends?\nA. talk to people\nB. try again\nC. fall asleep\nD. stand alone\nE. thank god\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would a person do if they do not have any friends?\nA. talk to people\nB. try again\nC. fall asleep\nD. stand alone\nE. thank god\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would a person do if they do not have any friends?\nA. talk to people\nB. try again\nC. fall asleep\nD. stand alone\nE. thank god\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would a person do if they do not have any friends?\nA. talk to people\nB. try again\nC. fall asleep\nD. stand alone\nE. thank god\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would a person do if they do not have any friends?\nA. talk to people\nB. try again\nC. fall asleep\nD. stand alone\nE. thank god\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7550001740455627", "True"]], [["-4.505000114440918", "False"]], [["-5.505000114440918", "False"]], [["-3.005000114440918", "False"]], [["-5.505000114440918", "False"]]], "filtered_resps": [["-0.7550001740455627", "True"], ["-4.505000114440918", "False"], ["-5.505000114440918", "False"], ["-3.005000114440918", "False"], ["-5.505000114440918", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4403285af428328cde2e7a85348a2cf9e512079e91cbfb34b081aa75a2b8c83b", "prompt_hash": "c4f7b6213b94d30b9364c686655398a18eee7156e42a7fe9fd48f51b15991a64", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 347, "doc": {"id": "aaf4fa38433c84b3bd0a86551259ce62", "question": "As a result of dying, what happens to organic material?", "question_concept": "dying", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["change of color", "stop breathing", "wake up", "death and decay", "getting cold"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: As a result of dying, what happens to organic material?\nA. change of color\nB. stop breathing\nC. wake up\nD. death and decay\nE. getting cold\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: As a result of dying, what happens to organic material?\nA. change of color\nB. stop breathing\nC. wake up\nD. death and decay\nE. getting cold\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: As a result of dying, what happens to organic material?\nA. change of color\nB. stop breathing\nC. wake up\nD. death and decay\nE. getting cold\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: As a result of dying, what happens to organic material?\nA. change of color\nB. stop breathing\nC. wake up\nD. death and decay\nE. getting cold\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: As a result of dying, what happens to organic material?\nA. change of color\nB. stop breathing\nC. wake up\nD. death and decay\nE. getting cold\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.8999762535095215", "False"]], [["-6.6499762535095215", "False"]], [["-8.149975776672363", "False"]], [["-1.399976134300232", "True"]], [["-8.149975776672363", "False"]]], "filtered_resps": [["-5.8999762535095215", "False"], ["-6.6499762535095215", "False"], ["-8.149975776672363", "False"], ["-1.399976134300232", "True"], ["-8.149975776672363", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1bc3ad49a3e265f68c6dbf252b30b7aca769a39347979580cb5c95af0504250a", "prompt_hash": "4c8f058359878280b59437d8ec3b39f70aac9a88f9beea98f859fa204c6375db", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 348, "doc": {"id": "33ea932a876ac0361c9eefeff1d24e92", "question": "What does everyone have in relation to other people?", "question_concept": "everyone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feelings", "food", "unique personality", "different standards", "values"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What does everyone have in relation to other people?\nA. feelings\nB. food\nC. unique personality\nD. different standards\nE. values\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does everyone have in relation to other people?\nA. feelings\nB. food\nC. unique personality\nD. different standards\nE. values\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does everyone have in relation to other people?\nA. feelings\nB. food\nC. unique personality\nD. different standards\nE. values\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does everyone have in relation to other people?\nA. feelings\nB. food\nC. unique personality\nD. different standards\nE. values\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does everyone have in relation to other people?\nA. feelings\nB. food\nC. unique personality\nD. different standards\nE. values\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.733383059501648", "True"]], [["-4.9833831787109375", "False"]], [["-2.9833831787109375", "False"]], [["-5.7333831787109375", "False"]], [["-3.4833831787109375", "False"]]], "filtered_resps": [["-1.733383059501648", "True"], ["-4.9833831787109375", "False"], ["-2.9833831787109375", "False"], ["-5.7333831787109375", "False"], ["-3.4833831787109375", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f1ace32105d171f4386d02e61f3dcb25c585d5b87295124208dbe9ed84dd0c44", "prompt_hash": "90b301ea0cf8ce4f8913e5c1a9f35d53b1c560df45ba0a4bdbd0143bc3cb0f36", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 349, "doc": {"id": "aead08289ca9abfcd169f935ea228ee5", "question": "What do you ask a child to do when you first meet her?", "question_concept": "child", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ask questions", "count to ten", "costume", "state name", "dress herself"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What do you ask a child to do when you first meet her?\nA. ask questions\nB. count to ten\nC. costume\nD. state name\nE. dress herself\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you ask a child to do when you first meet her?\nA. ask questions\nB. count to ten\nC. costume\nD. state name\nE. dress herself\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you ask a child to do when you first meet her?\nA. ask questions\nB. count to ten\nC. costume\nD. state name\nE. dress herself\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you ask a child to do when you first meet her?\nA. ask questions\nB. count to ten\nC. costume\nD. state name\nE. dress herself\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you ask a child to do when you first meet her?\nA. ask questions\nB. count to ten\nC. costume\nD. state name\nE. dress herself\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3249154090881348", "False"]], [["-5.074915409088135", "False"]], [["-6.324915409088135", "False"]], [["-1.5749154090881348", "False"]], [["-7.074915409088135", "False"]]], "filtered_resps": [["-3.3249154090881348", "False"], ["-5.074915409088135", "False"], ["-6.324915409088135", "False"], ["-1.5749154090881348", "False"], ["-7.074915409088135", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "20b586780177681ede5cb7e38a06e2bde1db832c82dcbde89a3adcdff818aab8", "prompt_hash": "6bdbfbea60c85cb5e72cbdaa4dc55453a9a8089f689bf8825beb0161afe75212", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 350, "doc": {"id": "adbddc80b10bf25f09c6c2bee4e3c59b", "question": "Where can you buy a clock, clothing and wrenches?", "question_concept": "clock", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["counter", "train station", "school room", "desk", "department store"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you buy a clock, clothing and wrenches?\nA. counter\nB. train station\nC. school room\nD. desk\nE. department store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you buy a clock, clothing and wrenches?\nA. counter\nB. train station\nC. school room\nD. desk\nE. department store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you buy a clock, clothing and wrenches?\nA. counter\nB. train station\nC. school room\nD. desk\nE. department store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you buy a clock, clothing and wrenches?\nA. counter\nB. train station\nC. school room\nD. desk\nE. department store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you buy a clock, clothing and wrenches?\nA. counter\nB. train station\nC. school room\nD. desk\nE. department store\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.663381814956665", "False"]], [["-5.413381576538086", "False"]], [["-7.413381576538086", "False"]], [["-8.663381576538086", "False"]], [["-1.163381814956665", "True"]]], "filtered_resps": [["-2.663381814956665", "False"], ["-5.413381576538086", "False"], ["-7.413381576538086", "False"], ["-8.663381576538086", "False"], ["-1.163381814956665", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "342ed5d407add43f8e1a12a80a2ee14bb73f4ea05af240ee592654ba543bd908", "prompt_hash": "68c2047ddcb1a9f169e7d81f9d62a47761d5c61656b6c0db0ddff4a46ee9f635", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 351, "doc": {"id": "1caf93d6a22dc8190e19c14bbe1fafda", "question": "What do you do when you're in a new place and want to see new things?", "question_concept": "see new", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["interesting", "look around", "take pictures", "change of surroundings", "new experience"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What do you do when you're in a new place and want to see new things?\nA. interesting\nB. look around\nC. take pictures\nD. change of surroundings\nE. new experience\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you do when you're in a new place and want to see new things?\nA. interesting\nB. look around\nC. take pictures\nD. change of surroundings\nE. new experience\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you do when you're in a new place and want to see new things?\nA. interesting\nB. look around\nC. take pictures\nD. change of surroundings\nE. new experience\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you do when you're in a new place and want to see new things?\nA. interesting\nB. look around\nC. take pictures\nD. change of surroundings\nE. new experience\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you do when you're in a new place and want to see new things?\nA. interesting\nB. look around\nC. take pictures\nD. change of surroundings\nE. new experience\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.609112739562988", "False"]], [["-1.3591125011444092", "True"]], [["-4.859112739562988", "False"]], [["-5.859112739562988", "False"]], [["-5.109112739562988", "False"]]], "filtered_resps": [["-4.609112739562988", "False"], ["-1.3591125011444092", "True"], ["-4.859112739562988", "False"], ["-5.859112739562988", "False"], ["-5.109112739562988", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6ece469a9a9f02ed7bcacc4887b0ff912ed8f706797143a184064243f3f713fa", "prompt_hash": "50cd2520c007153bdfc08777865c20590186b7e6da9d713ec02c56e143ff74b7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 352, "doc": {"id": "0bf4d64ad0eee7224acb3a4eb85accb2", "question": "What happens when to ice when it is in the sun?", "question_concept": "ice", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["carved", "melted", "ice cream", "antarctica", "sculptured"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What happens when to ice when it is in the sun?\nA. carved\nB. melted\nC. ice cream\nD. antarctica\nE. sculptured\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens when to ice when it is in the sun?\nA. carved\nB. melted\nC. ice cream\nD. antarctica\nE. sculptured\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens when to ice when it is in the sun?\nA. carved\nB. melted\nC. ice cream\nD. antarctica\nE. sculptured\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens when to ice when it is in the sun?\nA. carved\nB. melted\nC. ice cream\nD. antarctica\nE. sculptured\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens when to ice when it is in the sun?\nA. carved\nB. melted\nC. ice cream\nD. antarctica\nE. sculptured\nAnswer:", "arg_1": " E"}}, "resps": [[["-7.850664138793945", "False"]], [["-1.3506640195846558", "True"]], [["-8.850664138793945", "False"]], [["-9.850664138793945", "False"]], [["-10.100664138793945", "False"]]], "filtered_resps": [["-7.850664138793945", "False"], ["-1.3506640195846558", "True"], ["-8.850664138793945", "False"], ["-9.850664138793945", "False"], ["-10.100664138793945", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b8eeb2b06abe202b261bf3e75643376110d1e2ba7fbe3ec2ff9a044bb1ecb2e2", "prompt_hash": "1679c5e3aab455db711c40471772e1e11eac7e8530e402d74f651bf560d85b34", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 353, "doc": {"id": "b93532cae23e505628dd88568da3337e", "question": "Where can you store your dishes in your dwelling?", "question_concept": "dishes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["drawer", "shelf", "pantry", "apartment", "cabinet"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you store your dishes in your dwelling?\nA. drawer\nB. shelf\nC. pantry\nD. apartment\nE. cabinet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you store your dishes in your dwelling?\nA. drawer\nB. shelf\nC. pantry\nD. apartment\nE. cabinet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you store your dishes in your dwelling?\nA. drawer\nB. shelf\nC. pantry\nD. apartment\nE. cabinet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you store your dishes in your dwelling?\nA. drawer\nB. shelf\nC. pantry\nD. apartment\nE. cabinet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you store your dishes in your dwelling?\nA. drawer\nB. shelf\nC. pantry\nD. apartment\nE. cabinet\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.725165605545044", "False"]], [["-2.225165605545044", "False"]], [["-3.975165605545044", "False"]], [["-3.975165605545044", "False"]], [["-1.225165605545044", "True"]]], "filtered_resps": [["-3.725165605545044", "False"], ["-2.225165605545044", "False"], ["-3.975165605545044", "False"], ["-3.975165605545044", "False"], ["-1.225165605545044", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1bb2132809a2052404b2dcbecc121087d4ed5d1ccdf0223d1e8d921ac0fedb0d", "prompt_hash": "4b4389f2476b36b17a29ffd09da3479e57488c40cd4ae3b2a9f3e9fec1663c84", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 354, "doc": {"id": "2d3c9d3dff1a7a8253180cb3de1ceeea", "question": "The man laid on the soft moss and looked up at the trees, where was the man?", "question_concept": "moss", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["niagra falls", "forest", "waterfall", "ground", "tree"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The man laid on the soft moss and looked up at the trees, where was the man?\nA. niagra falls\nB. forest\nC. waterfall\nD. ground\nE. tree\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man laid on the soft moss and looked up at the trees, where was the man?\nA. niagra falls\nB. forest\nC. waterfall\nD. ground\nE. tree\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man laid on the soft moss and looked up at the trees, where was the man?\nA. niagra falls\nB. forest\nC. waterfall\nD. ground\nE. tree\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man laid on the soft moss and looked up at the trees, where was the man?\nA. niagra falls\nB. forest\nC. waterfall\nD. ground\nE. tree\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man laid on the soft moss and looked up at the trees, where was the man?\nA. niagra falls\nB. forest\nC. waterfall\nD. ground\nE. tree\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.429847240447998", "False"]], [["-1.4298473596572876", "False"]], [["-9.429847717285156", "False"]], [["-6.679847240447998", "False"]], [["-9.179847717285156", "False"]]], "filtered_resps": [["-4.429847240447998", "False"], ["-1.4298473596572876", "False"], ["-9.429847717285156", "False"], ["-6.679847240447998", "False"], ["-9.179847717285156", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "514f06a79551e8360a40e54c2fb76293029434a1d00a2f527432c1c20f60a099", "prompt_hash": "cbaebe298fa3bdfdf50a85728388b6bdf361615d3139555d87f0945773bbe296", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 355, "doc": {"id": "70701f5d1d62e58d5c74e2e303bb4065", "question": "What is someone doing if he or she is sitting quietly and his or her eyes are moving?", "question_concept": "sitting quietly", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["reading", "meditate", "fall asleep", "bunk", "think"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is someone doing if he or she is sitting quietly and his or her eyes are moving?\nA. reading\nB. meditate\nC. fall asleep\nD. bunk\nE. think\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is someone doing if he or she is sitting quietly and his or her eyes are moving?\nA. reading\nB. meditate\nC. fall asleep\nD. bunk\nE. think\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is someone doing if he or she is sitting quietly and his or her eyes are moving?\nA. reading\nB. meditate\nC. fall asleep\nD. bunk\nE. think\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is someone doing if he or she is sitting quietly and his or her eyes are moving?\nA. reading\nB. meditate\nC. fall asleep\nD. bunk\nE. think\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is someone doing if he or she is sitting quietly and his or her eyes are moving?\nA. reading\nB. meditate\nC. fall asleep\nD. bunk\nE. think\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.4831151962280273", "False"]], [["-1.9831151962280273", "False"]], [["-5.233115196228027", "False"]], [["-5.733115196228027", "False"]], [["-1.9831151962280273", "False"]]], "filtered_resps": [["-3.4831151962280273", "False"], ["-1.9831151962280273", "False"], ["-5.233115196228027", "False"], ["-5.733115196228027", "False"], ["-1.9831151962280273", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "91cc0fdc9e820f656e3ecb65cac1c5bc15dffc290b86c337191c8912dc4a7da7", "prompt_hash": "c3d41f1f50500fec971d6d22a415357fd2dd511fa32a5eeace853eaa9f119858", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 356, "doc": {"id": "eacd87f297193033669a93160ae3776f", "question": "Where can I find a stapler in many places?", "question_concept": "stapler", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desk drawer", "office building", "manual", "office supply store", "desktop"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where can I find a stapler in many places?\nA. desk drawer\nB. office building\nC. manual\nD. office supply store\nE. desktop\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can I find a stapler in many places?\nA. desk drawer\nB. office building\nC. manual\nD. office supply store\nE. desktop\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can I find a stapler in many places?\nA. desk drawer\nB. office building\nC. manual\nD. office supply store\nE. desktop\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can I find a stapler in many places?\nA. desk drawer\nB. office building\nC. manual\nD. office supply store\nE. desktop\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can I find a stapler in many places?\nA. desk drawer\nB. office building\nC. manual\nD. office supply store\nE. desktop\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1440951824188232", "True"]], [["-3.8940951824188232", "False"]], [["-5.144095420837402", "False"]], [["-1.8940951824188232", "False"]], [["-7.144095420837402", "False"]]], "filtered_resps": [["-1.1440951824188232", "True"], ["-3.8940951824188232", "False"], ["-5.144095420837402", "False"], ["-1.8940951824188232", "False"], ["-7.144095420837402", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5e4e58e6c51ca1829c6c1781ddeb7b45500a89bd628470280a08d0ba0f68b452", "prompt_hash": "c9a27e66ac0d03abb31ad437d28db06fb8769d235127898389df8ab8293ef88a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 357, "doc": {"id": "8e1b0792e441a5d54ae47a4b24f48977", "question": "A man takes a seat at a museum outside of Barcelona, where is he likely?", "question_concept": "seat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["in cinema", "martorell", "falling down", "show", "airplane"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: A man takes a seat at a museum outside of Barcelona, where is he likely?\nA. in cinema\nB. martorell\nC. falling down\nD. show\nE. airplane\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A man takes a seat at a museum outside of Barcelona, where is he likely?\nA. in cinema\nB. martorell\nC. falling down\nD. show\nE. airplane\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A man takes a seat at a museum outside of Barcelona, where is he likely?\nA. in cinema\nB. martorell\nC. falling down\nD. show\nE. airplane\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A man takes a seat at a museum outside of Barcelona, where is he likely?\nA. in cinema\nB. martorell\nC. falling down\nD. show\nE. airplane\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A man takes a seat at a museum outside of Barcelona, where is he likely?\nA. in cinema\nB. martorell\nC. falling down\nD. show\nE. airplane\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.318751335144043", "False"]], [["-3.818751335144043", "False"]], [["-5.318751335144043", "False"]], [["-1.8187512159347534", "False"]], [["-5.068751335144043", "False"]]], "filtered_resps": [["-2.318751335144043", "False"], ["-3.818751335144043", "False"], ["-5.318751335144043", "False"], ["-1.8187512159347534", "False"], ["-5.068751335144043", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "937c227fd8f3680f1c29c59cd40ceea15a231e288b48cee893a13dcee07e706b", "prompt_hash": "718f8378815f6e20514731934e9ea16732db22fab2b1aa8727d5710daea62788", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 358, "doc": {"id": "b4cde6a56fb19afc84876ebf2fb9e71a", "question": "Where would you find a toy soldier that is being played with?", "question_concept": "toy soldier", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["toy box", "movies", "child's hand", "toybos", "child park"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find a toy soldier that is being played with?\nA. toy box\nB. movies\nC. child's hand\nD. toybos\nE. child park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find a toy soldier that is being played with?\nA. toy box\nB. movies\nC. child's hand\nD. toybos\nE. child park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find a toy soldier that is being played with?\nA. toy box\nB. movies\nC. child's hand\nD. toybos\nE. child park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find a toy soldier that is being played with?\nA. toy box\nB. movies\nC. child's hand\nD. toybos\nE. child park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find a toy soldier that is being played with?\nA. toy box\nB. movies\nC. child's hand\nD. toybos\nE. child park\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.025617837905884", "False"]], [["-7.525617599487305", "False"]], [["-1.2756178379058838", "True"]], [["-8.025617599487305", "False"]], [["-6.775617599487305", "False"]]], "filtered_resps": [["-2.025617837905884", "False"], ["-7.525617599487305", "False"], ["-1.2756178379058838", "True"], ["-8.025617599487305", "False"], ["-6.775617599487305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "41fb5075f0fedb021f51b6cd661d86948374964b32dc8d9bfaa28ee164cf710d", "prompt_hash": "b80b5bfd8a3aee9d99553112af5d428200455cb3c09799194d1ce50b64435970", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 359, "doc": {"id": "095c5bc5fbaf12b384e9f7df47fdec16", "question": "Where are you when you're about to use your plane ticket?", "question_concept": "plane ticket", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pocket", "terrorists hands", "airport", "sea ship", "briefcase"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where are you when you're about to use your plane ticket?\nA. pocket\nB. terrorists hands\nC. airport\nD. sea ship\nE. briefcase\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are you when you're about to use your plane ticket?\nA. pocket\nB. terrorists hands\nC. airport\nD. sea ship\nE. briefcase\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are you when you're about to use your plane ticket?\nA. pocket\nB. terrorists hands\nC. airport\nD. sea ship\nE. briefcase\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are you when you're about to use your plane ticket?\nA. pocket\nB. terrorists hands\nC. airport\nD. sea ship\nE. briefcase\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are you when you're about to use your plane ticket?\nA. pocket\nB. terrorists hands\nC. airport\nD. sea ship\nE. briefcase\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3821592330932617", "False"]], [["-6.882159233093262", "False"]], [["-0.8821593523025513", "True"]], [["-7.882159233093262", "False"]], [["-6.382159233093262", "False"]]], "filtered_resps": [["-3.3821592330932617", "False"], ["-6.882159233093262", "False"], ["-0.8821593523025513", "True"], ["-7.882159233093262", "False"], ["-6.382159233093262", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "33900c4e754fca251627c73a8119b03c3bf3b5f1f77959e497afab203fe57953", "prompt_hash": "4a40a9998c123b0afa85bf2bc52194dcb60789806ee1e2215c6aa559ce80c9aa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 360, "doc": {"id": "494c501dbbfd36c602aae9e5b8e0cfff", "question": "Flowers make a good center focal point, just one of many arrangements that look good on a what?", "question_concept": "flowers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["market", "table", "countryside", "anthology", "vase"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Flowers make a good center focal point, just one of many arrangements that look good on a what?\nA. market\nB. table\nC. countryside\nD. anthology\nE. vase\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Flowers make a good center focal point, just one of many arrangements that look good on a what?\nA. market\nB. table\nC. countryside\nD. anthology\nE. vase\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Flowers make a good center focal point, just one of many arrangements that look good on a what?\nA. market\nB. table\nC. countryside\nD. anthology\nE. vase\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Flowers make a good center focal point, just one of many arrangements that look good on a what?\nA. market\nB. table\nC. countryside\nD. anthology\nE. vase\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Flowers make a good center focal point, just one of many arrangements that look good on a what?\nA. market\nB. table\nC. countryside\nD. anthology\nE. vase\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7858848571777344", "False"]], [["-2.7858848571777344", "False"]], [["-7.035884857177734", "False"]], [["-9.285884857177734", "False"]], [["-1.5358847379684448", "False"]]], "filtered_resps": [["-3.7858848571777344", "False"], ["-2.7858848571777344", "False"], ["-7.035884857177734", "False"], ["-9.285884857177734", "False"], ["-1.5358847379684448", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "64880e19a9c2df8cf5c32d49fd24d05752472372df8402a6edd463d1b27e0d24", "prompt_hash": "faf2abea40bf6a29157d5aacacfd2bd6deb84ee8acf1e7f53406e6be2c41c0bd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 361, "doc": {"id": "5a7f6fd97b2c9ad05f773bc8b2ecf441", "question": "How can a human cross a river and not mess up their hair?", "question_concept": "river", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wisconsin", "waterfall", "hatred", "bridge", "valley"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: How can a human cross a river and not mess up their hair?\nA. wisconsin\nB. waterfall\nC. hatred\nD. bridge\nE. valley\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How can a human cross a river and not mess up their hair?\nA. wisconsin\nB. waterfall\nC. hatred\nD. bridge\nE. valley\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How can a human cross a river and not mess up their hair?\nA. wisconsin\nB. waterfall\nC. hatred\nD. bridge\nE. valley\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How can a human cross a river and not mess up their hair?\nA. wisconsin\nB. waterfall\nC. hatred\nD. bridge\nE. valley\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How can a human cross a river and not mess up their hair?\nA. wisconsin\nB. waterfall\nC. hatred\nD. bridge\nE. valley\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7976542711257935", "False"]], [["-6.047654151916504", "False"]], [["-6.297654151916504", "False"]], [["-1.5476542711257935", "False"]], [["-8.797654151916504", "False"]]], "filtered_resps": [["-1.7976542711257935", "False"], ["-6.047654151916504", "False"], ["-6.297654151916504", "False"], ["-1.5476542711257935", "False"], ["-8.797654151916504", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "11f1187210145710778dafc4c39afb9c8eb9012c8fcfa1a8832e199f580881e7", "prompt_hash": "d1db74815f515d5753b2805abdf79c978e56ada09599be3ef90e0d3c7915e2ef", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 362, "doc": {"id": "5279a2ea333ba8a5bf3a7637a7279da1", "question": "Batman bought beer.  There were no bottles available.  He had to settle for what?.", "question_concept": "beer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shelf", "soccer game", "keg", "can", "refrigerator"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Batman bought beer.  There were no bottles available.  He had to settle for what?.\nA. shelf\nB. soccer game\nC. keg\nD. can\nE. refrigerator\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Batman bought beer.  There were no bottles available.  He had to settle for what?.\nA. shelf\nB. soccer game\nC. keg\nD. can\nE. refrigerator\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Batman bought beer.  There were no bottles available.  He had to settle for what?.\nA. shelf\nB. soccer game\nC. keg\nD. can\nE. refrigerator\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Batman bought beer.  There were no bottles available.  He had to settle for what?.\nA. shelf\nB. soccer game\nC. keg\nD. can\nE. refrigerator\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Batman bought beer.  There were no bottles available.  He had to settle for what?.\nA. shelf\nB. soccer game\nC. keg\nD. can\nE. refrigerator\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.73978328704834", "False"]], [["-3.48978328704834", "False"]], [["-1.7397832870483398", "False"]], [["-2.73978328704834", "False"]], [["-7.23978328704834", "False"]]], "filtered_resps": [["-4.73978328704834", "False"], ["-3.48978328704834", "False"], ["-1.7397832870483398", "False"], ["-2.73978328704834", "False"], ["-7.23978328704834", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "83a4ed337fb17e54046036f1c29085b5be4382b8c729213cd01cc595f2c353ef", "prompt_hash": "05dded4aa3af488a41fb52835cf40bf81e396383bec9d7b1c94493a1024850bf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 363, "doc": {"id": "42c46e28baf0fc617a07419286178c0a", "question": "You can find a monkey in what West African region on the Gulf of Guinea", "question_concept": "monkey", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["south american country", "rain forest", "pay debts", "works", "nigeria"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: You can find a monkey in what West African region on the Gulf of Guinea\nA. south american country\nB. rain forest\nC. pay debts\nD. works\nE. nigeria\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: You can find a monkey in what West African region on the Gulf of Guinea\nA. south american country\nB. rain forest\nC. pay debts\nD. works\nE. nigeria\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: You can find a monkey in what West African region on the Gulf of Guinea\nA. south american country\nB. rain forest\nC. pay debts\nD. works\nE. nigeria\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: You can find a monkey in what West African region on the Gulf of Guinea\nA. south american country\nB. rain forest\nC. pay debts\nD. works\nE. nigeria\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: You can find a monkey in what West African region on the Gulf of Guinea\nA. south american country\nB. rain forest\nC. pay debts\nD. works\nE. nigeria\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.438532829284668", "False"]], [["-3.688532829284668", "False"]], [["-8.438532829284668", "False"]], [["-8.688532829284668", "False"]], [["-1.1885327100753784", "True"]]], "filtered_resps": [["-4.438532829284668", "False"], ["-3.688532829284668", "False"], ["-8.438532829284668", "False"], ["-8.688532829284668", "False"], ["-1.1885327100753784", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ddc8e0bedc85d2223896230b8905f56e8e33cf5f0c6dd89ecd6f185a685b2c3d", "prompt_hash": "506ca430a158f61f729c6d5715a609afddf61ab3212976921d1666b66b9ff9ab", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 364, "doc": {"id": "c76304b4962f94ab9f20f09cf4a1a7c1", "question": "Surprising an angry person could lead to what?", "question_concept": "surprising", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["humor", "fight", "jocose", "laughter", "accidents"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Surprising an angry person could lead to what?\nA. humor\nB. fight\nC. jocose\nD. laughter\nE. accidents\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Surprising an angry person could lead to what?\nA. humor\nB. fight\nC. jocose\nD. laughter\nE. accidents\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Surprising an angry person could lead to what?\nA. humor\nB. fight\nC. jocose\nD. laughter\nE. accidents\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Surprising an angry person could lead to what?\nA. humor\nB. fight\nC. jocose\nD. laughter\nE. accidents\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Surprising an angry person could lead to what?\nA. humor\nB. fight\nC. jocose\nD. laughter\nE. accidents\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.4818220138549805", "False"]], [["-2.9818220138549805", "False"]], [["-5.7318220138549805", "False"]], [["-6.9818220138549805", "False"]], [["-1.73182213306427", "False"]]], "filtered_resps": [["-4.4818220138549805", "False"], ["-2.9818220138549805", "False"], ["-5.7318220138549805", "False"], ["-6.9818220138549805", "False"], ["-1.73182213306427", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "61bcb8c35287cf8de44abe8b23d912882cef7c21360c311bf2d8219d1a8ef648", "prompt_hash": "1ed63030624f6f376b867dc48d9444c634d9e1a51f8cb275a5b7165144506840", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 365, "doc": {"id": "8b23cd355ffc8b6e7aa5459ffb21b4e0", "question": "Where is a dining area likely to be small?", "question_concept": "dining area", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cruise ship", "home", "mall", "restaurant", "dark cave"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a dining area likely to be small?\nA. cruise ship\nB. home\nC. mall\nD. restaurant\nE. dark cave\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a dining area likely to be small?\nA. cruise ship\nB. home\nC. mall\nD. restaurant\nE. dark cave\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a dining area likely to be small?\nA. cruise ship\nB. home\nC. mall\nD. restaurant\nE. dark cave\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a dining area likely to be small?\nA. cruise ship\nB. home\nC. mall\nD. restaurant\nE. dark cave\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a dining area likely to be small?\nA. cruise ship\nB. home\nC. mall\nD. restaurant\nE. dark cave\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.269912838935852", "True"]], [["-2.2699127197265625", "False"]], [["-6.0199127197265625", "False"]], [["-3.5199127197265625", "False"]], [["-2.0199127197265625", "False"]]], "filtered_resps": [["-1.269912838935852", "True"], ["-2.2699127197265625", "False"], ["-6.0199127197265625", "False"], ["-3.5199127197265625", "False"], ["-2.0199127197265625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c519fcae235fe1fb341611e810114009f505b9a348c359a2646dba1dcc28519b", "prompt_hash": "3d5839b59b247c0420a27dedc675487b9db8a1e8b6dd57c970514b814917fb3d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 366, "doc": {"id": "c35f7de9e9005fcf654cb0b23f17acd6", "question": "Killing people should not cause what emotion?", "question_concept": "killing people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["vengeance", "going to prison", "joy", "afraid", "terrible"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Killing people should not cause what emotion?\nA. vengeance\nB. going to prison\nC. joy\nD. afraid\nE. terrible\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Killing people should not cause what emotion?\nA. vengeance\nB. going to prison\nC. joy\nD. afraid\nE. terrible\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Killing people should not cause what emotion?\nA. vengeance\nB. going to prison\nC. joy\nD. afraid\nE. terrible\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Killing people should not cause what emotion?\nA. vengeance\nB. going to prison\nC. joy\nD. afraid\nE. terrible\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Killing people should not cause what emotion?\nA. vengeance\nB. going to prison\nC. joy\nD. afraid\nE. terrible\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.6813812255859375", "False"]], [["-6.4313812255859375", "False"]], [["-0.9313812851905823", "True"]], [["-7.6813812255859375", "False"]], [["-6.1813812255859375", "False"]]], "filtered_resps": [["-4.6813812255859375", "False"], ["-6.4313812255859375", "False"], ["-0.9313812851905823", "True"], ["-7.6813812255859375", "False"], ["-6.1813812255859375", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "50085f88b7530d0634e5fda550abcb8e7760b4e138e2e1f52a35d87007c1e915", "prompt_hash": "4043c55eb31d2bcaffbc7c4876c5861101b5df0c75c8872f9ea335c0e19d8001", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 367, "doc": {"id": "d910859b9d1acae40456dbeaa8334bc0", "question": "James slamed into someone playing football, and not for the first time.  He was concerned about the consequences of many what?", "question_concept": "playing football", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["exhilaration", "interactions", "head injuries", "death", "having fun"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: James slamed into someone playing football, and not for the first time.  He was concerned about the consequences of many what?\nA. exhilaration\nB. interactions\nC. head injuries\nD. death\nE. having fun\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James slamed into someone playing football, and not for the first time.  He was concerned about the consequences of many what?\nA. exhilaration\nB. interactions\nC. head injuries\nD. death\nE. having fun\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James slamed into someone playing football, and not for the first time.  He was concerned about the consequences of many what?\nA. exhilaration\nB. interactions\nC. head injuries\nD. death\nE. having fun\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James slamed into someone playing football, and not for the first time.  He was concerned about the consequences of many what?\nA. exhilaration\nB. interactions\nC. head injuries\nD. death\nE. having fun\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James slamed into someone playing football, and not for the first time.  He was concerned about the consequences of many what?\nA. exhilaration\nB. interactions\nC. head injuries\nD. death\nE. having fun\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.760690212249756", "False"]], [["-5.510690212249756", "False"]], [["-1.5106900930404663", "False"]], [["-5.760690212249756", "False"]], [["-10.260689735412598", "False"]]], "filtered_resps": [["-6.760690212249756", "False"], ["-5.510690212249756", "False"], ["-1.5106900930404663", "False"], ["-5.760690212249756", "False"], ["-10.260689735412598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "376d4c7e8c91013d162611affbbfcfddae94de73a5020a7714d3718f76acb4e3", "prompt_hash": "b48a56dd26e524b4ee1ad1e779e5f83620278ab1b0f9ea1d577717240b913ffb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 368, "doc": {"id": "6ca8439d062de4d43d7d471c508b78db", "question": "More people should lower the guard and just have fun, we don't got long just what?", "question_concept": "have fun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["enjoy living", "happy", "enjoyable", "get laid", "do enjoy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: More people should lower the guard and just have fun, we don't got long just what?\nA. enjoy living\nB. happy\nC. enjoyable\nD. get laid\nE. do enjoy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: More people should lower the guard and just have fun, we don't got long just what?\nA. enjoy living\nB. happy\nC. enjoyable\nD. get laid\nE. do enjoy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: More people should lower the guard and just have fun, we don't got long just what?\nA. enjoy living\nB. happy\nC. enjoyable\nD. get laid\nE. do enjoy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: More people should lower the guard and just have fun, we don't got long just what?\nA. enjoy living\nB. happy\nC. enjoyable\nD. get laid\nE. do enjoy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: More people should lower the guard and just have fun, we don't got long just what?\nA. enjoy living\nB. happy\nC. enjoyable\nD. get laid\nE. do enjoy\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4859569072723389", "True"]], [["-4.985957145690918", "False"]], [["-3.985956907272339", "False"]], [["-5.485957145690918", "False"]], [["-4.485957145690918", "False"]]], "filtered_resps": [["-1.4859569072723389", "True"], ["-4.985957145690918", "False"], ["-3.985956907272339", "False"], ["-5.485957145690918", "False"], ["-4.485957145690918", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "477268e963eb6bd7e5226deb494a3f7dbf402a00fe34690d2641afcd5418920b", "prompt_hash": "b7bd2ac9b393e65c35e37921cc146471037b51da4b97a58c9245722a5d3087e1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 369, "doc": {"id": "ddd8c62ec94b4f94eeefdd05b9208a71", "question": "Where can you get a lizard to keep in your home?", "question_concept": "lizard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desert country", "dessert", "pet shop", "tropical areas", "zoo"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you get a lizard to keep in your home?\nA. desert country\nB. dessert\nC. pet shop\nD. tropical areas\nE. zoo\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you get a lizard to keep in your home?\nA. desert country\nB. dessert\nC. pet shop\nD. tropical areas\nE. zoo\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you get a lizard to keep in your home?\nA. desert country\nB. dessert\nC. pet shop\nD. tropical areas\nE. zoo\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you get a lizard to keep in your home?\nA. desert country\nB. dessert\nC. pet shop\nD. tropical areas\nE. zoo\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you get a lizard to keep in your home?\nA. desert country\nB. dessert\nC. pet shop\nD. tropical areas\nE. zoo\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.270951986312866", "False"]], [["-6.520952224731445", "False"]], [["-1.0209519863128662", "True"]], [["-7.770952224731445", "False"]], [["-8.270952224731445", "False"]]], "filtered_resps": [["-3.270951986312866", "False"], ["-6.520952224731445", "False"], ["-1.0209519863128662", "True"], ["-7.770952224731445", "False"], ["-8.270952224731445", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ececc9e882f1bdc5fc1f2cdde906026886a7acd914c1decbafde6e3bd5d01835", "prompt_hash": "4b22b96cc43cbe7d7d55c7a73f46474d3be28ea534a575d0cf147894259d1d7e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 370, "doc": {"id": "72b638200414a526b598de0e01a044df", "question": "What would use a musical instrument?", "question_concept": "musical instrument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["guitar", "music room", "orchestra", "case", "movie"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What would use a musical instrument?\nA. guitar\nB. music room\nC. orchestra\nD. case\nE. movie\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would use a musical instrument?\nA. guitar\nB. music room\nC. orchestra\nD. case\nE. movie\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would use a musical instrument?\nA. guitar\nB. music room\nC. orchestra\nD. case\nE. movie\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would use a musical instrument?\nA. guitar\nB. music room\nC. orchestra\nD. case\nE. movie\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would use a musical instrument?\nA. guitar\nB. music room\nC. orchestra\nD. case\nE. movie\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.714037299156189", "False"]], [["-4.4640374183654785", "False"]], [["-0.964037299156189", "True"]], [["-6.9640374183654785", "False"]], [["-6.4640374183654785", "False"]]], "filtered_resps": [["-1.714037299156189", "False"], ["-4.4640374183654785", "False"], ["-0.964037299156189", "True"], ["-6.9640374183654785", "False"], ["-6.4640374183654785", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4e45572d20123c3a700368dab1573bf948304455fc51ca77bfb0ed5bd23ea0b8", "prompt_hash": "c001d0e8373821f4f0e9c7eca2edf735e3faad77a10bd0b62757d8283206e15c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 371, "doc": {"id": "c770870c88f35f9d110217049c5a7334", "question": "She was in an affair, what did that end upon discovery by her husband?", "question_concept": "affair", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["relationship", "marriage", "fidelity", "love", "divorce"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: She was in an affair, what did that end upon discovery by her husband?\nA. relationship\nB. marriage\nC. fidelity\nD. love\nE. divorce\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: She was in an affair, what did that end upon discovery by her husband?\nA. relationship\nB. marriage\nC. fidelity\nD. love\nE. divorce\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: She was in an affair, what did that end upon discovery by her husband?\nA. relationship\nB. marriage\nC. fidelity\nD. love\nE. divorce\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: She was in an affair, what did that end upon discovery by her husband?\nA. relationship\nB. marriage\nC. fidelity\nD. love\nE. divorce\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: She was in an affair, what did that end upon discovery by her husband?\nA. relationship\nB. marriage\nC. fidelity\nD. love\nE. divorce\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3891146183013916", "False"]], [["-1.8891146183013916", "False"]], [["-5.3891143798828125", "False"]], [["-6.6391143798828125", "False"]], [["-2.1391146183013916", "False"]]], "filtered_resps": [["-3.3891146183013916", "False"], ["-1.8891146183013916", "False"], ["-5.3891143798828125", "False"], ["-6.6391143798828125", "False"], ["-2.1391146183013916", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d0ea6491cb76492fd1c248c0a73d0bbcf81f0dc7e94279b2cb048ad9485d9efa", "prompt_hash": "f01359a95c1e6a79aae3ec355ed0f8addf10e635d178270ba7db76b364b5cf0d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 372, "doc": {"id": "1d8d9e3504c8c58a3b923ddc155c19b0", "question": "What is the most famous constellation out of earth?", "question_concept": "earth", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["one moon", "milky way", "god's creation", "stars", "universe"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is the most famous constellation out of earth?\nA. one moon\nB. milky way\nC. god's creation\nD. stars\nE. universe\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the most famous constellation out of earth?\nA. one moon\nB. milky way\nC. god's creation\nD. stars\nE. universe\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the most famous constellation out of earth?\nA. one moon\nB. milky way\nC. god's creation\nD. stars\nE. universe\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the most famous constellation out of earth?\nA. one moon\nB. milky way\nC. god's creation\nD. stars\nE. universe\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the most famous constellation out of earth?\nA. one moon\nB. milky way\nC. god's creation\nD. stars\nE. universe\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.156136989593506", "False"]], [["-2.156136989593506", "False"]], [["-5.156136989593506", "False"]], [["-4.406136989593506", "False"]], [["-7.656136989593506", "False"]]], "filtered_resps": [["-6.156136989593506", "False"], ["-2.156136989593506", "False"], ["-5.156136989593506", "False"], ["-4.406136989593506", "False"], ["-7.656136989593506", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fdb9c576f2a7e2ffa17b8e8c6b2a0c157b95c00f88529b81edc265121123011c", "prompt_hash": "9026924ffc8affda8d26a601065d34a86f48a560a856d07bdab851fdd3bdda50", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 373, "doc": {"id": "95acebea992a26c3a7c3bfb45845fa83", "question": "If a reception is held with hotel guests walking by, what is the likely venue?", "question_concept": "reception", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["room service", "church basement", "lobby", "large room", "country club"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If a reception is held with hotel guests walking by, what is the likely venue?\nA. room service\nB. church basement\nC. lobby\nD. large room\nE. country club\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a reception is held with hotel guests walking by, what is the likely venue?\nA. room service\nB. church basement\nC. lobby\nD. large room\nE. country club\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a reception is held with hotel guests walking by, what is the likely venue?\nA. room service\nB. church basement\nC. lobby\nD. large room\nE. country club\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a reception is held with hotel guests walking by, what is the likely venue?\nA. room service\nB. church basement\nC. lobby\nD. large room\nE. country club\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a reception is held with hotel guests walking by, what is the likely venue?\nA. room service\nB. church basement\nC. lobby\nD. large room\nE. country club\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6046693325042725", "False"]], [["-6.854669570922852", "False"]], [["-1.1046693325042725", "True"]], [["-5.854669570922852", "False"]], [["-5.854669570922852", "False"]]], "filtered_resps": [["-3.6046693325042725", "False"], ["-6.854669570922852", "False"], ["-1.1046693325042725", "True"], ["-5.854669570922852", "False"], ["-5.854669570922852", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "caa9833b8c72d01397de3632123ccf2f0cb2e158d0595b766651dc22d597388e", "prompt_hash": "7d2156ddfaa64b86baf1373c5b3d2e6c9734f5db95454f5d24b14a73c1071eb5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 374, "doc": {"id": "c2c2a387fd9a6a26cff636008de21f71", "question": "What is a place that is far away from your house and where you could consume beer?", "question_concept": "beer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["refrigerator", "friend's house", "keg", "neighbor's house", "kitchen"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is a place that is far away from your house and where you could consume beer?\nA. refrigerator\nB. friend's house\nC. keg\nD. neighbor's house\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a place that is far away from your house and where you could consume beer?\nA. refrigerator\nB. friend's house\nC. keg\nD. neighbor's house\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a place that is far away from your house and where you could consume beer?\nA. refrigerator\nB. friend's house\nC. keg\nD. neighbor's house\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a place that is far away from your house and where you could consume beer?\nA. refrigerator\nB. friend's house\nC. keg\nD. neighbor's house\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a place that is far away from your house and where you could consume beer?\nA. refrigerator\nB. friend's house\nC. keg\nD. neighbor's house\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.6897475719451904", "False"]], [["-1.1897475719451904", "True"]], [["-4.1897478103637695", "False"]], [["-2.9397475719451904", "False"]], [["-7.9397478103637695", "False"]]], "filtered_resps": [["-2.6897475719451904", "False"], ["-1.1897475719451904", "True"], ["-4.1897478103637695", "False"], ["-2.9397475719451904", "False"], ["-7.9397478103637695", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "98f09ff5dceddabdb51ee5ce83612f9cfadd49f1d1349c7f6bb67e9eba6bdb8c", "prompt_hash": "2e976948d2ba615ff82ce5fde8c286a653cecec091f9a4cd57ea43ec904667d5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 375, "doc": {"id": "57e96118fee6e2bbac5f59790fc833c0", "question": "If a court case is dismissed after hearing testimony, what would be a likely cause?", "question_concept": "hearing testimony", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["change of heart", "anguish", "anger", "boredom", "anxiety"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If a court case is dismissed after hearing testimony, what would be a likely cause?\nA. change of heart\nB. anguish\nC. anger\nD. boredom\nE. anxiety\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a court case is dismissed after hearing testimony, what would be a likely cause?\nA. change of heart\nB. anguish\nC. anger\nD. boredom\nE. anxiety\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a court case is dismissed after hearing testimony, what would be a likely cause?\nA. change of heart\nB. anguish\nC. anger\nD. boredom\nE. anxiety\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a court case is dismissed after hearing testimony, what would be a likely cause?\nA. change of heart\nB. anguish\nC. anger\nD. boredom\nE. anxiety\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a court case is dismissed after hearing testimony, what would be a likely cause?\nA. change of heart\nB. anguish\nC. anger\nD. boredom\nE. anxiety\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3576983213424683", "True"]], [["-3.607698440551758", "False"]], [["-4.357698440551758", "False"]], [["-2.607698440551758", "False"]], [["-3.857698440551758", "False"]]], "filtered_resps": [["-1.3576983213424683", "True"], ["-3.607698440551758", "False"], ["-4.357698440551758", "False"], ["-2.607698440551758", "False"], ["-3.857698440551758", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f66bb5ca668838292a8407670e1ae849207d4de978416f7a9271e34abc4f09c2", "prompt_hash": "c438292d0f52d748535dd3232762e7a1d0a807039d6fe9c42426a82230acde13", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 376, "doc": {"id": "b9b82aa4c236cd342ff95455b8516a42", "question": "Sitting down quickly after eating beans could lead to what?", "question_concept": "sitting down", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["flatulence", "happiness", "laziness", "fall asleep", "comfort"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Sitting down quickly after eating beans could lead to what?\nA. flatulence\nB. happiness\nC. laziness\nD. fall asleep\nE. comfort\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sitting down quickly after eating beans could lead to what?\nA. flatulence\nB. happiness\nC. laziness\nD. fall asleep\nE. comfort\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sitting down quickly after eating beans could lead to what?\nA. flatulence\nB. happiness\nC. laziness\nD. fall asleep\nE. comfort\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sitting down quickly after eating beans could lead to what?\nA. flatulence\nB. happiness\nC. laziness\nD. fall asleep\nE. comfort\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sitting down quickly after eating beans could lead to what?\nA. flatulence\nB. happiness\nC. laziness\nD. fall asleep\nE. comfort\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5528026223182678", "True"]], [["-4.302802562713623", "False"]], [["-5.552802562713623", "False"]], [["-6.802802562713623", "False"]], [["-5.302802562713623", "False"]]], "filtered_resps": [["-0.5528026223182678", "True"], ["-4.302802562713623", "False"], ["-5.552802562713623", "False"], ["-6.802802562713623", "False"], ["-5.302802562713623", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "567a407c2f0e470b917f7bf9b463e2b4f07713bd9774ff0b2c8db2dfccabcf20", "prompt_hash": "28860972553404bd04bf0e38af8c09e7d31cdd792282b3ad7146e6d02515da37", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 377, "doc": {"id": "41fac392c6a5827c1b6682d5d3798e59", "question": "John was my neighbor, it was easy to talk to him. He was never what?", "question_concept": "neighbour", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["away", "distant", "remote person", "bore", "foe"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John was my neighbor, it was easy to talk to him. He was never what?\nA. away\nB. distant\nC. remote person\nD. bore\nE. foe\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John was my neighbor, it was easy to talk to him. He was never what?\nA. away\nB. distant\nC. remote person\nD. bore\nE. foe\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John was my neighbor, it was easy to talk to him. He was never what?\nA. away\nB. distant\nC. remote person\nD. bore\nE. foe\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John was my neighbor, it was easy to talk to him. He was never what?\nA. away\nB. distant\nC. remote person\nD. bore\nE. foe\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John was my neighbor, it was easy to talk to him. He was never what?\nA. away\nB. distant\nC. remote person\nD. bore\nE. foe\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.393925666809082", "False"]], [["-0.893925666809082", "True"]], [["-3.893925666809082", "False"]], [["-3.643925666809082", "False"]], [["-6.393925666809082", "False"]]], "filtered_resps": [["-4.393925666809082", "False"], ["-0.893925666809082", "True"], ["-3.893925666809082", "False"], ["-3.643925666809082", "False"], ["-6.393925666809082", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "010e3993aaa199c2c7abb657b6a66452cc318ef4534ef301169c9ced4752ab8b", "prompt_hash": "78242c05d01f5ce159826d564cee5fec8e11713e81b9d3fbbe1e84ec5a053ac6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 378, "doc": {"id": "5c224410a40c9269b1e542cfcb430d35", "question": "Where do people want to have a lot of coffee?", "question_concept": "cup of coffee", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["table", "office", "desk", "kitchen", "ocean"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where do people want to have a lot of coffee?\nA. table\nB. office\nC. desk\nD. kitchen\nE. ocean\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do people want to have a lot of coffee?\nA. table\nB. office\nC. desk\nD. kitchen\nE. ocean\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do people want to have a lot of coffee?\nA. table\nB. office\nC. desk\nD. kitchen\nE. ocean\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do people want to have a lot of coffee?\nA. table\nB. office\nC. desk\nD. kitchen\nE. ocean\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do people want to have a lot of coffee?\nA. table\nB. office\nC. desk\nD. kitchen\nE. ocean\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.488570213317871", "False"]], [["-4.738570213317871", "False"]], [["-4.738570213317871", "False"]], [["-1.7385703325271606", "False"]], [["-8.238570213317871", "False"]]], "filtered_resps": [["-3.488570213317871", "False"], ["-4.738570213317871", "False"], ["-4.738570213317871", "False"], ["-1.7385703325271606", "False"], ["-8.238570213317871", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f993a4b39889ce33193df3a7dfa74b22c0dfffb92095a2ae17f0970bd9a2e728", "prompt_hash": "8e1805cbde25fabd80feda124624b268874ce044ed9f652e7b15d0a1761b5e1d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 379, "doc": {"id": "0b90c6710a65eb55fea4cc92895bf601", "question": "You stop and have food all around you, what are you?", "question_concept": "have food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stay alive", "wanted to survive", "nutrition", "grew", "full"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: You stop and have food all around you, what are you?\nA. stay alive\nB. wanted to survive\nC. nutrition\nD. grew\nE. full\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: You stop and have food all around you, what are you?\nA. stay alive\nB. wanted to survive\nC. nutrition\nD. grew\nE. full\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: You stop and have food all around you, what are you?\nA. stay alive\nB. wanted to survive\nC. nutrition\nD. grew\nE. full\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: You stop and have food all around you, what are you?\nA. stay alive\nB. wanted to survive\nC. nutrition\nD. grew\nE. full\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: You stop and have food all around you, what are you?\nA. stay alive\nB. wanted to survive\nC. nutrition\nD. grew\nE. full\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.655163049697876", "False"]], [["-5.155162811279297", "False"]], [["-2.155163049697876", "False"]], [["-6.155162811279297", "False"]], [["-0.9051629900932312", "True"]]], "filtered_resps": [["-3.655163049697876", "False"], ["-5.155162811279297", "False"], ["-2.155163049697876", "False"], ["-6.155162811279297", "False"], ["-0.9051629900932312", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fd27581756fbe0dff6523cbaa4b3ea7a081c5fc63e65fc473b3579c7282242f7", "prompt_hash": "f81663345cb40118fb3349fdbe2511559578bdd1c631c56781ca41b9990a0f9b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 380, "doc": {"id": "70af2b5df22ec96901350dfa3c9ee74f", "question": "James was meeting a friend.  They had planed a slow day. They didn't want to do much.  They just wanted what?", "question_concept": "meeting friend", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["relaxation", "panic", "alarm", "joy", "cheer"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James was meeting a friend.  They had planed a slow day. They didn't want to do much.  They just wanted what?\nA. relaxation\nB. panic\nC. alarm\nD. joy\nE. cheer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James was meeting a friend.  They had planed a slow day. They didn't want to do much.  They just wanted what?\nA. relaxation\nB. panic\nC. alarm\nD. joy\nE. cheer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James was meeting a friend.  They had planed a slow day. They didn't want to do much.  They just wanted what?\nA. relaxation\nB. panic\nC. alarm\nD. joy\nE. cheer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James was meeting a friend.  They had planed a slow day. They didn't want to do much.  They just wanted what?\nA. relaxation\nB. panic\nC. alarm\nD. joy\nE. cheer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James was meeting a friend.  They had planed a slow day. They didn't want to do much.  They just wanted what?\nA. relaxation\nB. panic\nC. alarm\nD. joy\nE. cheer\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1018714904785156", "True"]], [["-8.101871490478516", "False"]], [["-9.851871490478516", "False"]], [["-9.601871490478516", "False"]], [["-11.601871490478516", "False"]]], "filtered_resps": [["-1.1018714904785156", "True"], ["-8.101871490478516", "False"], ["-9.851871490478516", "False"], ["-9.601871490478516", "False"], ["-11.601871490478516", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dc4049e8fc243c373d90b762771d6bcb20417efd26973c4f3c5c918d2b20ef63", "prompt_hash": "0a670ea41be867ce70af165a8a47f3a41c2d8e7ca1493d4842eeeaef0076a9e6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 381, "doc": {"id": "f9243ef9f0037657c337d3c6a9832f05", "question": "The car's steering seem quite loose, but he still considered purchasing it because he needed something small and what?", "question_concept": "loose", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sturdy", "faithful", "bound", "compact", "packaged"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The car's steering seem quite loose, but he still considered purchasing it because he needed something small and what?\nA. sturdy\nB. faithful\nC. bound\nD. compact\nE. packaged\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The car's steering seem quite loose, but he still considered purchasing it because he needed something small and what?\nA. sturdy\nB. faithful\nC. bound\nD. compact\nE. packaged\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The car's steering seem quite loose, but he still considered purchasing it because he needed something small and what?\nA. sturdy\nB. faithful\nC. bound\nD. compact\nE. packaged\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The car's steering seem quite loose, but he still considered purchasing it because he needed something small and what?\nA. sturdy\nB. faithful\nC. bound\nD. compact\nE. packaged\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The car's steering seem quite loose, but he still considered purchasing it because he needed something small and what?\nA. sturdy\nB. faithful\nC. bound\nD. compact\nE. packaged\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.178503513336182", "False"]], [["-4.178503513336182", "False"]], [["-6.678503513336182", "False"]], [["-1.428503394126892", "True"]], [["-9.928503036499023", "False"]]], "filtered_resps": [["-4.178503513336182", "False"], ["-4.178503513336182", "False"], ["-6.678503513336182", "False"], ["-1.428503394126892", "True"], ["-9.928503036499023", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d020e6b0aae1a7819903a7c94648286c56a4b88ca69ed73524cf39b5814b5dc2", "prompt_hash": "cdcc83f1bf861c10918aa8053f44e5fb694ec3bb0d93f16e825b2f2c91fa2fa8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 382, "doc": {"id": "27f2074270ea8a5e8f5ec2a017ec4a50", "question": "Dan was a farmer with just one heifer.  But that was okay, he only kept her for milk, and he didn't think he'd find good farmland in a place as cold as where?", "question_concept": "heifer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["arizona", "farm yard", "michigan", "german field", "dairy farm"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Dan was a farmer with just one heifer.  But that was okay, he only kept her for milk, and he didn't think he'd find good farmland in a place as cold as where?\nA. arizona\nB. farm yard\nC. michigan\nD. german field\nE. dairy farm\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Dan was a farmer with just one heifer.  But that was okay, he only kept her for milk, and he didn't think he'd find good farmland in a place as cold as where?\nA. arizona\nB. farm yard\nC. michigan\nD. german field\nE. dairy farm\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Dan was a farmer with just one heifer.  But that was okay, he only kept her for milk, and he didn't think he'd find good farmland in a place as cold as where?\nA. arizona\nB. farm yard\nC. michigan\nD. german field\nE. dairy farm\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Dan was a farmer with just one heifer.  But that was okay, he only kept her for milk, and he didn't think he'd find good farmland in a place as cold as where?\nA. arizona\nB. farm yard\nC. michigan\nD. german field\nE. dairy farm\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Dan was a farmer with just one heifer.  But that was okay, he only kept her for milk, and he didn't think he'd find good farmland in a place as cold as where?\nA. arizona\nB. farm yard\nC. michigan\nD. german field\nE. dairy farm\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7703146934509277", "False"]], [["-5.770314693450928", "False"]], [["-1.5203146934509277", "False"]], [["-6.770314693450928", "False"]], [["-7.270314693450928", "False"]]], "filtered_resps": [["-3.7703146934509277", "False"], ["-5.770314693450928", "False"], ["-1.5203146934509277", "False"], ["-6.770314693450928", "False"], ["-7.270314693450928", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4db005a32875530000c4b3fedf4c8563565ae2fc834f060862742891958ec1c5", "prompt_hash": "f56a1cc424fe8dc33bee32ba555668a7866c97aa213148d73ea70eae482eea08", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 383, "doc": {"id": "63b3652d54c8c0e571f6bb50de318bf0", "question": "It's Friday night and Alice puts off going to bed because she plans on doing what Saturday?", "question_concept": "going to bed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hatred", "sleeping in", "rest", "making love", "insomnia"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: It's Friday night and Alice puts off going to bed because she plans on doing what Saturday?\nA. hatred\nB. sleeping in\nC. rest\nD. making love\nE. insomnia\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: It's Friday night and Alice puts off going to bed because she plans on doing what Saturday?\nA. hatred\nB. sleeping in\nC. rest\nD. making love\nE. insomnia\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: It's Friday night and Alice puts off going to bed because she plans on doing what Saturday?\nA. hatred\nB. sleeping in\nC. rest\nD. making love\nE. insomnia\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: It's Friday night and Alice puts off going to bed because she plans on doing what Saturday?\nA. hatred\nB. sleeping in\nC. rest\nD. making love\nE. insomnia\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: It's Friday night and Alice puts off going to bed because she plans on doing what Saturday?\nA. hatred\nB. sleeping in\nC. rest\nD. making love\nE. insomnia\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.23796272277832", "False"]], [["-1.4879624843597412", "False"]], [["-7.73796272277832", "False"]], [["-6.48796272277832", "False"]], [["-9.98796272277832", "False"]]], "filtered_resps": [["-6.23796272277832", "False"], ["-1.4879624843597412", "False"], ["-7.73796272277832", "False"], ["-6.48796272277832", "False"], ["-9.98796272277832", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "aff3a71f5a9aa3e1021fbea06bd17f7788304bee4b2d96337d37a56c710b0adb", "prompt_hash": "7739db82787e02b5b0492ebbc8f9510fe31e548ad93e35799caf1f2190ee88be", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 384, "doc": {"id": "0843c51212a3c2eee660fab5648c9e19", "question": "His phone was dead and they couldn't find the expressway, he opened up the glove compartment and handed his passenger the what to navigate?", "question_concept": "expressway", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eastern united states", "michigan", "map", "choppers", "american city"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: His phone was dead and they couldn't find the expressway, he opened up the glove compartment and handed his passenger the what to navigate?\nA. eastern united states\nB. michigan\nC. map\nD. choppers\nE. american city\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: His phone was dead and they couldn't find the expressway, he opened up the glove compartment and handed his passenger the what to navigate?\nA. eastern united states\nB. michigan\nC. map\nD. choppers\nE. american city\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: His phone was dead and they couldn't find the expressway, he opened up the glove compartment and handed his passenger the what to navigate?\nA. eastern united states\nB. michigan\nC. map\nD. choppers\nE. american city\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: His phone was dead and they couldn't find the expressway, he opened up the glove compartment and handed his passenger the what to navigate?\nA. eastern united states\nB. michigan\nC. map\nD. choppers\nE. american city\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: His phone was dead and they couldn't find the expressway, he opened up the glove compartment and handed his passenger the what to navigate?\nA. eastern united states\nB. michigan\nC. map\nD. choppers\nE. american city\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.091012001037598", "False"]], [["-6.841012001037598", "False"]], [["-1.3410120010375977", "False"]], [["-8.841012001037598", "False"]], [["-10.591012001037598", "False"]]], "filtered_resps": [["-4.091012001037598", "False"], ["-6.841012001037598", "False"], ["-1.3410120010375977", "False"], ["-8.841012001037598", "False"], ["-10.591012001037598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1f1e48856ff0513b842ecfc376967543045e7a460dc82fbb8a10c60f888fb181", "prompt_hash": "6dd08f966c468c04ea333cef099d2af84e14b8dbccf385303bfd30b2e39f1610", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 385, "doc": {"id": "1b3d286458a7e7f069222de0376d06da", "question": "What would someone use a personal key for?", "question_concept": "key", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["car stand", "at hotel", "own home", "front door", "bus depot"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What would someone use a personal key for?\nA. car stand\nB. at hotel\nC. own home\nD. front door\nE. bus depot\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would someone use a personal key for?\nA. car stand\nB. at hotel\nC. own home\nD. front door\nE. bus depot\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would someone use a personal key for?\nA. car stand\nB. at hotel\nC. own home\nD. front door\nE. bus depot\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would someone use a personal key for?\nA. car stand\nB. at hotel\nC. own home\nD. front door\nE. bus depot\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would someone use a personal key for?\nA. car stand\nB. at hotel\nC. own home\nD. front door\nE. bus depot\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3730149269104004", "True"]], [["-3.8730149269104004", "False"]], [["-2.3730149269104004", "False"]], [["-1.8730149269104004", "False"]], [["-7.3730149269104", "False"]]], "filtered_resps": [["-1.3730149269104004", "True"], ["-3.8730149269104004", "False"], ["-2.3730149269104004", "False"], ["-1.8730149269104004", "False"], ["-7.3730149269104", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "922360f7ac93fa921c8dc336b40dbc6ed0ba0d442a2cabce9cfc2508ab32d8ed", "prompt_hash": "d2cdb46feb2650a6e18e7b795f5a0d8033a69e276040768bb81d0217ff92cb3d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 386, "doc": {"id": "86e2aabfb9d401567f04d87a648ff776", "question": "The cat kept pestering it's owner, it was that time of the day and it was what?", "question_concept": "cat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["litter tray", "whiskers", "hungry", "feline", "thirsty"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The cat kept pestering it's owner, it was that time of the day and it was what?\nA. litter tray\nB. whiskers\nC. hungry\nD. feline\nE. thirsty\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The cat kept pestering it's owner, it was that time of the day and it was what?\nA. litter tray\nB. whiskers\nC. hungry\nD. feline\nE. thirsty\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The cat kept pestering it's owner, it was that time of the day and it was what?\nA. litter tray\nB. whiskers\nC. hungry\nD. feline\nE. thirsty\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The cat kept pestering it's owner, it was that time of the day and it was what?\nA. litter tray\nB. whiskers\nC. hungry\nD. feline\nE. thirsty\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The cat kept pestering it's owner, it was that time of the day and it was what?\nA. litter tray\nB. whiskers\nC. hungry\nD. feline\nE. thirsty\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.047006607055664", "False"]], [["-6.047006607055664", "False"]], [["-1.047006607055664", "True"]], [["-6.547006607055664", "False"]], [["-3.547006607055664", "False"]]], "filtered_resps": [["-3.047006607055664", "False"], ["-6.047006607055664", "False"], ["-1.047006607055664", "True"], ["-6.547006607055664", "False"], ["-3.547006607055664", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "327651df15eb08ead01cccb03a26589f1d1abbafd75045632e016b10fc731423", "prompt_hash": "456ff7b71637b43d6d15422592cb89744edf457b1c5f166b1768ff3a37054463", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 387, "doc": {"id": "092c24369367b3c7457198f3ce160fe3", "question": "Her voice lent her to the alto section, what group did she join?", "question_concept": "alto", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["symphony", "concerto", "choir", "theater troupe", "marching band"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Her voice lent her to the alto section, what group did she join?\nA. symphony\nB. concerto\nC. choir\nD. theater troupe\nE. marching band\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Her voice lent her to the alto section, what group did she join?\nA. symphony\nB. concerto\nC. choir\nD. theater troupe\nE. marching band\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Her voice lent her to the alto section, what group did she join?\nA. symphony\nB. concerto\nC. choir\nD. theater troupe\nE. marching band\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Her voice lent her to the alto section, what group did she join?\nA. symphony\nB. concerto\nC. choir\nD. theater troupe\nE. marching band\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Her voice lent her to the alto section, what group did she join?\nA. symphony\nB. concerto\nC. choir\nD. theater troupe\nE. marching band\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.313345909118652", "False"]], [["-9.063345909118652", "False"]], [["-1.813346028327942", "False"]], [["-8.563345909118652", "False"]], [["-10.563345909118652", "False"]]], "filtered_resps": [["-4.313345909118652", "False"], ["-9.063345909118652", "False"], ["-1.813346028327942", "False"], ["-8.563345909118652", "False"], ["-10.563345909118652", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c7ec61deeddba77f9516309c6bd19552c3a262185785f6816507da48f477927f", "prompt_hash": "bf835c3357af910bab5fe12f5f346ef3c7553530f5a2a652b261b6e78fb49165", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 388, "doc": {"id": "cab9eea2a91b1bd5c0a01b11f594f154", "question": "Where are you likely to find a Japanese restaurant not run by people from Japan?", "question_concept": "japanese restaurant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["california", "downtown", "large town", "tokio", "china town"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where are you likely to find a Japanese restaurant not run by people from Japan?\nA. california\nB. downtown\nC. large town\nD. tokio\nE. china town\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are you likely to find a Japanese restaurant not run by people from Japan?\nA. california\nB. downtown\nC. large town\nD. tokio\nE. china town\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are you likely to find a Japanese restaurant not run by people from Japan?\nA. california\nB. downtown\nC. large town\nD. tokio\nE. china town\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are you likely to find a Japanese restaurant not run by people from Japan?\nA. california\nB. downtown\nC. large town\nD. tokio\nE. china town\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are you likely to find a Japanese restaurant not run by people from Japan?\nA. california\nB. downtown\nC. large town\nD. tokio\nE. china town\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5425424575805664", "False"]], [["-3.7925424575805664", "False"]], [["-5.042542457580566", "False"]], [["-6.542542457580566", "False"]], [["-1.792542576789856", "False"]]], "filtered_resps": [["-2.5425424575805664", "False"], ["-3.7925424575805664", "False"], ["-5.042542457580566", "False"], ["-6.542542457580566", "False"], ["-1.792542576789856", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "89aa5fe07a7bc7bb8abfe0dee3bc6a4c62941753cf4c3a537d4ba25e468a3f3c", "prompt_hash": "d306b166573dd5a788ed1eb56e7c021dc9402293e3c8757b170f2a56b5210a7b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 389, "doc": {"id": "6e77de03bee86d6c20780e14f00944d0", "question": "Animals who have hair and don't lay eggs are what?", "question_concept": "animals", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["reproduce asexually", "males", "mammals", "attack", "ocean"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Animals who have hair and don't lay eggs are what?\nA. reproduce asexually\nB. males\nC. mammals\nD. attack\nE. ocean\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Animals who have hair and don't lay eggs are what?\nA. reproduce asexually\nB. males\nC. mammals\nD. attack\nE. ocean\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Animals who have hair and don't lay eggs are what?\nA. reproduce asexually\nB. males\nC. mammals\nD. attack\nE. ocean\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Animals who have hair and don't lay eggs are what?\nA. reproduce asexually\nB. males\nC. mammals\nD. attack\nE. ocean\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Animals who have hair and don't lay eggs are what?\nA. reproduce asexually\nB. males\nC. mammals\nD. attack\nE. ocean\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.469691276550293", "False"]], [["-7.219691276550293", "False"]], [["-0.9696913957595825", "True"]], [["-8.969691276550293", "False"]], [["-8.719691276550293", "False"]]], "filtered_resps": [["-6.469691276550293", "False"], ["-7.219691276550293", "False"], ["-0.9696913957595825", "True"], ["-8.969691276550293", "False"], ["-8.719691276550293", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1971ae86107be05771c4e04996da31155588a170cc19eda51eee27398e15db18", "prompt_hash": "826989d0d5f4211e1c45c15e72c37adc59d49d412adbaaf25bca6a570c8b609e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 390, "doc": {"id": "7f25dbab26165b3c8800c2733ca759d6", "question": "John was an aristocratic fox hunter.  Where might he live?", "question_concept": "fox", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["england", "new hampshire", "street", "arkansas", "north dakota"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John was an aristocratic fox hunter.  Where might he live?\nA. england\nB. new hampshire\nC. street\nD. arkansas\nE. north dakota\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John was an aristocratic fox hunter.  Where might he live?\nA. england\nB. new hampshire\nC. street\nD. arkansas\nE. north dakota\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John was an aristocratic fox hunter.  Where might he live?\nA. england\nB. new hampshire\nC. street\nD. arkansas\nE. north dakota\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John was an aristocratic fox hunter.  Where might he live?\nA. england\nB. new hampshire\nC. street\nD. arkansas\nE. north dakota\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John was an aristocratic fox hunter.  Where might he live?\nA. england\nB. new hampshire\nC. street\nD. arkansas\nE. north dakota\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7628439664840698", "True"]], [["-6.262844085693359", "False"]], [["-7.262844085693359", "False"]], [["-8.01284408569336", "False"]], [["-10.26284408569336", "False"]]], "filtered_resps": [["-0.7628439664840698", "True"], ["-6.262844085693359", "False"], ["-7.262844085693359", "False"], ["-8.01284408569336", "False"], ["-10.26284408569336", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "860de6b879d3a58f53d8097d9ac61834ee0f1aa9f7a4ac84a6e2f7f5deb3f43b", "prompt_hash": "8e72c9b46f28721b3b842b1b7f1dfc2b33bc78f3398f001b7d78356853bb56af", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 391, "doc": {"id": "9024493a3edbaf555fda5b477e835bf5", "question": "Where is a grape likely to be being fed to someone else?", "question_concept": "grape", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["field", "bathroom", "michigan", "minnesota", "painting"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a grape likely to be being fed to someone else?\nA. field\nB. bathroom\nC. michigan\nD. minnesota\nE. painting\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a grape likely to be being fed to someone else?\nA. field\nB. bathroom\nC. michigan\nD. minnesota\nE. painting\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a grape likely to be being fed to someone else?\nA. field\nB. bathroom\nC. michigan\nD. minnesota\nE. painting\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a grape likely to be being fed to someone else?\nA. field\nB. bathroom\nC. michigan\nD. minnesota\nE. painting\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a grape likely to be being fed to someone else?\nA. field\nB. bathroom\nC. michigan\nD. minnesota\nE. painting\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7179408073425293", "True"]], [["-2.9679408073425293", "False"]], [["-5.217940807342529", "False"]], [["-7.467940807342529", "False"]], [["-6.467940807342529", "False"]]], "filtered_resps": [["-0.7179408073425293", "True"], ["-2.9679408073425293", "False"], ["-5.217940807342529", "False"], ["-7.467940807342529", "False"], ["-6.467940807342529", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f451ef6c75c5e8d4dbae2f2a0013d597d75502b615841f0400e3404ca2137ba3", "prompt_hash": "18fadf0d7677d9ba209e12551e390f7871f537a5a7826f82a499d75775be4b71", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 392, "doc": {"id": "fc59ab1a9e6d2b51126dd828d30e9167", "question": "Some food can be stored at room temperature until you open it, then you should keep it in what?", "question_concept": "food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shop", "bookcase", "shelf", "refrigerators", "kitchen"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Some food can be stored at room temperature until you open it, then you should keep it in what?\nA. shop\nB. bookcase\nC. shelf\nD. refrigerators\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Some food can be stored at room temperature until you open it, then you should keep it in what?\nA. shop\nB. bookcase\nC. shelf\nD. refrigerators\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Some food can be stored at room temperature until you open it, then you should keep it in what?\nA. shop\nB. bookcase\nC. shelf\nD. refrigerators\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Some food can be stored at room temperature until you open it, then you should keep it in what?\nA. shop\nB. bookcase\nC. shelf\nD. refrigerators\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Some food can be stored at room temperature until you open it, then you should keep it in what?\nA. shop\nB. bookcase\nC. shelf\nD. refrigerators\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.724013805389404", "False"]], [["-6.474013805389404", "False"]], [["-3.9740138053894043", "False"]], [["-1.4740138053894043", "True"]], [["-9.224014282226562", "False"]]], "filtered_resps": [["-5.724013805389404", "False"], ["-6.474013805389404", "False"], ["-3.9740138053894043", "False"], ["-1.4740138053894043", "True"], ["-9.224014282226562", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f7dde842de8c0210edd546a71eb546c3e4abdc918d3e742eee0e325732a09b7c", "prompt_hash": "beeed625df425345859b3af41ac2d674b1e22629f590e7498bc6abe7a1bf26a2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 393, "doc": {"id": "5a50ea4bb2d13dc4f620ebd45025d445", "question": "Sam couldn't get back to sleep because of a dream he had.  It was a what?", "question_concept": "dream", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["awake", "horror", "dreamworker", "reality", "nightmare"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Sam couldn't get back to sleep because of a dream he had.  It was a what?\nA. awake\nB. horror\nC. dreamworker\nD. reality\nE. nightmare\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sam couldn't get back to sleep because of a dream he had.  It was a what?\nA. awake\nB. horror\nC. dreamworker\nD. reality\nE. nightmare\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sam couldn't get back to sleep because of a dream he had.  It was a what?\nA. awake\nB. horror\nC. dreamworker\nD. reality\nE. nightmare\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sam couldn't get back to sleep because of a dream he had.  It was a what?\nA. awake\nB. horror\nC. dreamworker\nD. reality\nE. nightmare\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sam couldn't get back to sleep because of a dream he had.  It was a what?\nA. awake\nB. horror\nC. dreamworker\nD. reality\nE. nightmare\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.467691421508789", "False"]], [["-3.217691659927368", "False"]], [["-7.967691421508789", "False"]], [["-8.717691421508789", "False"]], [["-0.9676916599273682", "True"]]], "filtered_resps": [["-4.467691421508789", "False"], ["-3.217691659927368", "False"], ["-7.967691421508789", "False"], ["-8.717691421508789", "False"], ["-0.9676916599273682", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a2759fe043d44cf237a7b9dd6c830e28b9025c0749d7b8ed8f149facc32c1316", "prompt_hash": "4797df4ee21101f4432c839d11c02a827cb4a9b21f9d03ada11874a8244cb176", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 394, "doc": {"id": "8becd2ee4e86258566a9c2b0e6d9544e", "question": "If you're going to a party in a new town what are you hoping to make?", "question_concept": "going to party", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["getting drunk", "making new friends", "new contacts", "doing drugs", "set home"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you're going to a party in a new town what are you hoping to make?\nA. getting drunk\nB. making new friends\nC. new contacts\nD. doing drugs\nE. set home\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're going to a party in a new town what are you hoping to make?\nA. getting drunk\nB. making new friends\nC. new contacts\nD. doing drugs\nE. set home\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're going to a party in a new town what are you hoping to make?\nA. getting drunk\nB. making new friends\nC. new contacts\nD. doing drugs\nE. set home\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're going to a party in a new town what are you hoping to make?\nA. getting drunk\nB. making new friends\nC. new contacts\nD. doing drugs\nE. set home\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're going to a party in a new town what are you hoping to make?\nA. getting drunk\nB. making new friends\nC. new contacts\nD. doing drugs\nE. set home\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.772708415985107", "False"]], [["-0.7727084755897522", "True"]], [["-4.522708415985107", "False"]], [["-8.272708892822266", "False"]], [["-7.522708415985107", "False"]]], "filtered_resps": [["-5.772708415985107", "False"], ["-0.7727084755897522", "True"], ["-4.522708415985107", "False"], ["-8.272708892822266", "False"], ["-7.522708415985107", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b383de5219fe83a85f9d9232cf912100e53af38b81add32eea0b684c193ba6f0", "prompt_hash": "7baf6ff5d5f8abf5819fce9a1c8062315dc04779d8862c4c96d60b0d5e9d9da6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 395, "doc": {"id": "2a21820a135e1a49883525c055c74a0b", "question": "How is riding a bike getting it to move?", "question_concept": "riding bike", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["practice", "sense of balance", "driving", "good balance", "pedalling"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: How is riding a bike getting it to move?\nA. practice\nB. sense of balance\nC. driving\nD. good balance\nE. pedalling\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How is riding a bike getting it to move?\nA. practice\nB. sense of balance\nC. driving\nD. good balance\nE. pedalling\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How is riding a bike getting it to move?\nA. practice\nB. sense of balance\nC. driving\nD. good balance\nE. pedalling\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How is riding a bike getting it to move?\nA. practice\nB. sense of balance\nC. driving\nD. good balance\nE. pedalling\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How is riding a bike getting it to move?\nA. practice\nB. sense of balance\nC. driving\nD. good balance\nE. pedalling\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7078170776367188", "False"]], [["-3.2078170776367188", "False"]], [["-7.207817077636719", "False"]], [["-5.957817077636719", "False"]], [["-1.4578169584274292", "True"]]], "filtered_resps": [["-3.7078170776367188", "False"], ["-3.2078170776367188", "False"], ["-7.207817077636719", "False"], ["-5.957817077636719", "False"], ["-1.4578169584274292", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5ae2e8b6ce86747ece7df788b8cfdaaf478ad33ae4cc42b20d064c0bdc46315d", "prompt_hash": "6bbd6be05024c5b0f094f676b6399f3ad5c8e400f116e5ea4cb2467d657642e4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 396, "doc": {"id": "e5adfec0b5ba691ec752f9b5e0fb8084", "question": "Where does one usually keep literature?", "question_concept": "literature", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["books and magazines", "own home", "kitchen", "shelf", "meeting"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where does one usually keep literature?\nA. books and magazines\nB. own home\nC. kitchen\nD. shelf\nE. meeting\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where does one usually keep literature?\nA. books and magazines\nB. own home\nC. kitchen\nD. shelf\nE. meeting\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where does one usually keep literature?\nA. books and magazines\nB. own home\nC. kitchen\nD. shelf\nE. meeting\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where does one usually keep literature?\nA. books and magazines\nB. own home\nC. kitchen\nD. shelf\nE. meeting\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where does one usually keep literature?\nA. books and magazines\nB. own home\nC. kitchen\nD. shelf\nE. meeting\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.918652057647705", "False"]], [["-4.168652057647705", "False"]], [["-7.668652057647705", "False"]], [["-1.9186521768569946", "False"]], [["-9.668652534484863", "False"]]], "filtered_resps": [["-2.918652057647705", "False"], ["-4.168652057647705", "False"], ["-7.668652057647705", "False"], ["-1.9186521768569946", "False"], ["-9.668652534484863", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3c8d0e19b4aac5d17fb9e415917f79c683716bfaa413766ccc96e83413d8434f", "prompt_hash": "7509c3cb25bbd64f55fdf5e4b5264f1a3b6b7c5fdfb2377e4f9c5472ee0c27d6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 397, "doc": {"id": "406e15b76269d20b5448a91648094291", "question": "WHat type of keyboard is made up of one or more pipe divisions?", "question_concept": "keyboard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["killing", "typewriter", "office", "terminal", "organ"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: WHat type of keyboard is made up of one or more pipe divisions?\nA. killing\nB. typewriter\nC. office\nD. terminal\nE. organ\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: WHat type of keyboard is made up of one or more pipe divisions?\nA. killing\nB. typewriter\nC. office\nD. terminal\nE. organ\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: WHat type of keyboard is made up of one or more pipe divisions?\nA. killing\nB. typewriter\nC. office\nD. terminal\nE. organ\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: WHat type of keyboard is made up of one or more pipe divisions?\nA. killing\nB. typewriter\nC. office\nD. terminal\nE. organ\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: WHat type of keyboard is made up of one or more pipe divisions?\nA. killing\nB. typewriter\nC. office\nD. terminal\nE. organ\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.079624891281128", "True"]], [["-3.579624891281128", "False"]], [["-5.079625129699707", "False"]], [["-3.329624891281128", "False"]], [["-3.579624891281128", "False"]]], "filtered_resps": [["-2.079624891281128", "True"], ["-3.579624891281128", "False"], ["-5.079625129699707", "False"], ["-3.329624891281128", "False"], ["-3.579624891281128", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a5bb0c239de771c2d828df0eda0ce02374ab92ccdaeb5eda1bc0b4c352e338e2", "prompt_hash": "415c7870cac430e23ce6dbe6be6c60a3de0269c82740cfb00e9edd458231d965", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 398, "doc": {"id": "9c596382ea15768f95b5ef9ceec191dc", "question": "The bell rang, and the congregation began to what in to the church?", "question_concept": "bell", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["run away", "wind instrument", "funnel", "blunderbuss", "associated with telephones"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The bell rang, and the congregation began to what in to the church?\nA. run away\nB. wind instrument\nC. funnel\nD. blunderbuss\nE. associated with telephones\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The bell rang, and the congregation began to what in to the church?\nA. run away\nB. wind instrument\nC. funnel\nD. blunderbuss\nE. associated with telephones\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The bell rang, and the congregation began to what in to the church?\nA. run away\nB. wind instrument\nC. funnel\nD. blunderbuss\nE. associated with telephones\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The bell rang, and the congregation began to what in to the church?\nA. run away\nB. wind instrument\nC. funnel\nD. blunderbuss\nE. associated with telephones\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The bell rang, and the congregation began to what in to the church?\nA. run away\nB. wind instrument\nC. funnel\nD. blunderbuss\nE. associated with telephones\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5895278453826904", "False"]], [["-2.5895278453826904", "False"]], [["-3.3395278453826904", "False"]], [["-5.0895280838012695", "False"]], [["-3.0895278453826904", "False"]]], "filtered_resps": [["-3.5895278453826904", "False"], ["-2.5895278453826904", "False"], ["-3.3395278453826904", "False"], ["-5.0895280838012695", "False"], ["-3.0895278453826904", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "89700afdef1f08516057c778816dc20c7b78c51b3206513b938b8f8e6a2c7c5d", "prompt_hash": "803947cf38626329162c28c52f47059ee9dd128005a6e5fa86dfc2b56b252faf", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 399, "doc": {"id": "7a3d0c94438a5c8a09364aaebb848a2c", "question": "James needed smooth sandpaper, but instead he got what type?", "question_concept": "smooth", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rough", "non smooth", "uneven", "plastic", "bumpy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James needed smooth sandpaper, but instead he got what type?\nA. rough\nB. non smooth\nC. uneven\nD. plastic\nE. bumpy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James needed smooth sandpaper, but instead he got what type?\nA. rough\nB. non smooth\nC. uneven\nD. plastic\nE. bumpy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James needed smooth sandpaper, but instead he got what type?\nA. rough\nB. non smooth\nC. uneven\nD. plastic\nE. bumpy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James needed smooth sandpaper, but instead he got what type?\nA. rough\nB. non smooth\nC. uneven\nD. plastic\nE. bumpy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James needed smooth sandpaper, but instead he got what type?\nA. rough\nB. non smooth\nC. uneven\nD. plastic\nE. bumpy\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.281376600265503", "False"]], [["-2.031376600265503", "False"]], [["-4.531376838684082", "False"]], [["-5.531376838684082", "False"]], [["-6.531376838684082", "False"]]], "filtered_resps": [["-2.281376600265503", "False"], ["-2.031376600265503", "False"], ["-4.531376838684082", "False"], ["-5.531376838684082", "False"], ["-6.531376838684082", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "53585a94ccba5801aa11612189c8a0007c83c8d3f22eb41102421a16dd1de9e9", "prompt_hash": "90cdad2a29d252ed0283c33f10304fec72b06704dc16592c6336ac97b1c5fd45", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 400, "doc": {"id": "1ef68db97654f30cd3701b942fadc934", "question": "Where would you borrow furniture if you do not have any?", "question_concept": "furniture", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sewer", "neighbor's house", "apartment", "room", "floor"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you borrow furniture if you do not have any?\nA. sewer\nB. neighbor's house\nC. apartment\nD. room\nE. floor\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you borrow furniture if you do not have any?\nA. sewer\nB. neighbor's house\nC. apartment\nD. room\nE. floor\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you borrow furniture if you do not have any?\nA. sewer\nB. neighbor's house\nC. apartment\nD. room\nE. floor\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you borrow furniture if you do not have any?\nA. sewer\nB. neighbor's house\nC. apartment\nD. room\nE. floor\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you borrow furniture if you do not have any?\nA. sewer\nB. neighbor's house\nC. apartment\nD. room\nE. floor\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.12765645980835", "False"]], [["-0.8776564598083496", "True"]], [["-5.62765645980835", "False"]], [["-8.127656936645508", "False"]], [["-9.627656936645508", "False"]]], "filtered_resps": [["-5.12765645980835", "False"], ["-0.8776564598083496", "True"], ["-5.62765645980835", "False"], ["-8.127656936645508", "False"], ["-9.627656936645508", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "49d6e12c2a88977c4ea8ef0bf6df25782fb10241004938c666e8d6bfa6c47a7c", "prompt_hash": "776d257f25498d5d26d12aa24f02f31947a0bf0b189430c631f516678baa15ef", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 401, "doc": {"id": "abb090bbc572be1016bcd5f261f28e76", "question": "What must happen for an animal to and it's offspring to continue livng?", "question_concept": "living", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["death", "flying", "reproducing", "food consumed", "eventually die"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What must happen for an animal to and it's offspring to continue livng?\nA. death\nB. flying\nC. reproducing\nD. food consumed\nE. eventually die\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What must happen for an animal to and it's offspring to continue livng?\nA. death\nB. flying\nC. reproducing\nD. food consumed\nE. eventually die\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What must happen for an animal to and it's offspring to continue livng?\nA. death\nB. flying\nC. reproducing\nD. food consumed\nE. eventually die\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What must happen for an animal to and it's offspring to continue livng?\nA. death\nB. flying\nC. reproducing\nD. food consumed\nE. eventually die\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What must happen for an animal to and it's offspring to continue livng?\nA. death\nB. flying\nC. reproducing\nD. food consumed\nE. eventually die\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.712628364562988", "False"]], [["-7.962628364562988", "False"]], [["-0.9626283049583435", "True"]], [["-8.962628364562988", "False"]], [["-8.962628364562988", "False"]]], "filtered_resps": [["-5.712628364562988", "False"], ["-7.962628364562988", "False"], ["-0.9626283049583435", "True"], ["-8.962628364562988", "False"], ["-8.962628364562988", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1af9ec241e9d83e8567d9f262363fc95cb5b3f99743412644da5c59eb312217d", "prompt_hash": "d7e162bc6a35a0a2cad66e6709940701727d063aeab3662520222881ffbad0e2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 402, "doc": {"id": "91f2532a832a35cba1b08a3c767be6da", "question": "I want my wine stored in darkness, where should it go?", "question_concept": "darkness", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["movies", "bed", "moon", "vault", "cellar"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: I want my wine stored in darkness, where should it go?\nA. movies\nB. bed\nC. moon\nD. vault\nE. cellar\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I want my wine stored in darkness, where should it go?\nA. movies\nB. bed\nC. moon\nD. vault\nE. cellar\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I want my wine stored in darkness, where should it go?\nA. movies\nB. bed\nC. moon\nD. vault\nE. cellar\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I want my wine stored in darkness, where should it go?\nA. movies\nB. bed\nC. moon\nD. vault\nE. cellar\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I want my wine stored in darkness, where should it go?\nA. movies\nB. bed\nC. moon\nD. vault\nE. cellar\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8425636291503906", "False"]], [["-5.842563629150391", "False"]], [["-6.842563629150391", "False"]], [["-4.592563629150391", "False"]], [["-0.8425635099411011", "True"]]], "filtered_resps": [["-3.8425636291503906", "False"], ["-5.842563629150391", "False"], ["-6.842563629150391", "False"], ["-4.592563629150391", "False"], ["-0.8425635099411011", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "10e9ee2ab2b68dee5d1bf870706f6133370425c048e93992ab92693f5846e086", "prompt_hash": "7424aafa57c69ee149475d7c8fb8f9bf2b6d562b50c4c7c20373906d9528a327", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 403, "doc": {"id": "f8544c9679d27b747dfad3b8d7aac87a", "question": "If I want to open a steakhouse, what should I get first?", "question_concept": "steakhouse", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["michigan", "florida", "wine", "texas", "building"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If I want to open a steakhouse, what should I get first?\nA. michigan\nB. florida\nC. wine\nD. texas\nE. building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I want to open a steakhouse, what should I get first?\nA. michigan\nB. florida\nC. wine\nD. texas\nE. building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I want to open a steakhouse, what should I get first?\nA. michigan\nB. florida\nC. wine\nD. texas\nE. building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I want to open a steakhouse, what should I get first?\nA. michigan\nB. florida\nC. wine\nD. texas\nE. building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I want to open a steakhouse, what should I get first?\nA. michigan\nB. florida\nC. wine\nD. texas\nE. building\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.948665142059326", "False"]], [["-4.698665142059326", "False"]], [["-3.198665142059326", "False"]], [["-3.948665142059326", "False"]], [["-1.6986652612686157", "True"]]], "filtered_resps": [["-3.948665142059326", "False"], ["-4.698665142059326", "False"], ["-3.198665142059326", "False"], ["-3.948665142059326", "False"], ["-1.6986652612686157", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a2d54c227fd9090539fabc85126152c49caa680f067d5e965f01f2ec448f42ec", "prompt_hash": "bec0794901f48de7831e8de0ffc44df75643718ec734fa8f993275d5b817848d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 404, "doc": {"id": "a7f423c1636ba9e36d18e381928c5dcc", "question": "Sarah didn't like to play but she didn't want to be sedentary and bored, either, so she took up what?", "question_concept": "play", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["serious", "longplay", "musical", "eat cake", "doing nothing"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Sarah didn't like to play but she didn't want to be sedentary and bored, either, so she took up what?\nA. serious\nB. longplay\nC. musical\nD. eat cake\nE. doing nothing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sarah didn't like to play but she didn't want to be sedentary and bored, either, so she took up what?\nA. serious\nB. longplay\nC. musical\nD. eat cake\nE. doing nothing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sarah didn't like to play but she didn't want to be sedentary and bored, either, so she took up what?\nA. serious\nB. longplay\nC. musical\nD. eat cake\nE. doing nothing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sarah didn't like to play but she didn't want to be sedentary and bored, either, so she took up what?\nA. serious\nB. longplay\nC. musical\nD. eat cake\nE. doing nothing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sarah didn't like to play but she didn't want to be sedentary and bored, either, so she took up what?\nA. serious\nB. longplay\nC. musical\nD. eat cake\nE. doing nothing\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3008525371551514", "False"]], [["-3.3008525371551514", "False"]], [["-1.5508525371551514", "True"]], [["-4.3008527755737305", "False"]], [["-4.0508527755737305", "False"]]], "filtered_resps": [["-3.3008525371551514", "False"], ["-3.3008525371551514", "False"], ["-1.5508525371551514", "True"], ["-4.3008527755737305", "False"], ["-4.0508527755737305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "83168aa3aa1b5ca8f3be96cae4088936f7a7f1cbdbccc46ecc1cbdd8a7367608", "prompt_hash": "768dd35aa0af3664e08f2f5be0d2d1882ee976626fce08b86dd20be5f3b4fc64", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 405, "doc": {"id": "e1d354cbfcd620e5dacf83c17746c4b3", "question": "Joe found spiders while checking something outside.  What might that be?", "question_concept": "spiders", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cupboard", "closet", "storage bag", "mail box", "garage"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Joe found spiders while checking something outside.  What might that be?\nA. cupboard\nB. closet\nC. storage bag\nD. mail box\nE. garage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe found spiders while checking something outside.  What might that be?\nA. cupboard\nB. closet\nC. storage bag\nD. mail box\nE. garage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe found spiders while checking something outside.  What might that be?\nA. cupboard\nB. closet\nC. storage bag\nD. mail box\nE. garage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe found spiders while checking something outside.  What might that be?\nA. cupboard\nB. closet\nC. storage bag\nD. mail box\nE. garage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe found spiders while checking something outside.  What might that be?\nA. cupboard\nB. closet\nC. storage bag\nD. mail box\nE. garage\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.420085906982422", "False"]], [["-3.920085906982422", "False"]], [["-5.670085906982422", "False"]], [["-1.9200859069824219", "False"]], [["-2.170085906982422", "False"]]], "filtered_resps": [["-3.420085906982422", "False"], ["-3.920085906982422", "False"], ["-5.670085906982422", "False"], ["-1.9200859069824219", "False"], ["-2.170085906982422", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "34fc6439a887b52f33e6bc8be7911b0406528adababd46c3adcc47bbff3da441", "prompt_hash": "792c46442ba55f1394092ae0844d1c4aed4654ef91d1fe9516fa37f4c542470b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 406, "doc": {"id": "53e1e50d204f6ad5a0f69429eadae82e", "question": "What would you do if your date does not show up?", "question_concept": "date", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wait for", "bathe", "go for haircut", "plan revenge", "dress nice"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What would you do if your date does not show up?\nA. wait for\nB. bathe\nC. go for haircut\nD. plan revenge\nE. dress nice\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you do if your date does not show up?\nA. wait for\nB. bathe\nC. go for haircut\nD. plan revenge\nE. dress nice\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you do if your date does not show up?\nA. wait for\nB. bathe\nC. go for haircut\nD. plan revenge\nE. dress nice\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you do if your date does not show up?\nA. wait for\nB. bathe\nC. go for haircut\nD. plan revenge\nE. dress nice\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you do if your date does not show up?\nA. wait for\nB. bathe\nC. go for haircut\nD. plan revenge\nE. dress nice\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8948947191238403", "True"]], [["-6.144894599914551", "False"]], [["-6.394894599914551", "False"]], [["-5.144894599914551", "False"]], [["-5.144894599914551", "False"]]], "filtered_resps": [["-0.8948947191238403", "True"], ["-6.144894599914551", "False"], ["-6.394894599914551", "False"], ["-5.144894599914551", "False"], ["-5.144894599914551", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d145f39267d8fb008f05dbeecdb5d10e5029708beafd8f66da4c54f1b78c9a00", "prompt_hash": "0a715dcbfc1d2c9805b4c718b8c2c8b21e176ac71710b6a2c4e77f78d5e6d6fe", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 407, "doc": {"id": "48205cc84aab5e455b22e17c3cc7277d", "question": "What did the adult do before the job interview?", "question_concept": "adult", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["work", "dress himself", "marry", "dress herself", "drive train"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What did the adult do before the job interview?\nA. work\nB. dress himself\nC. marry\nD. dress herself\nE. drive train\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What did the adult do before the job interview?\nA. work\nB. dress himself\nC. marry\nD. dress herself\nE. drive train\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What did the adult do before the job interview?\nA. work\nB. dress himself\nC. marry\nD. dress herself\nE. drive train\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What did the adult do before the job interview?\nA. work\nB. dress himself\nC. marry\nD. dress herself\nE. drive train\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What did the adult do before the job interview?\nA. work\nB. dress himself\nC. marry\nD. dress herself\nE. drive train\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9060680866241455", "False"]], [["-1.6560680866241455", "True"]], [["-6.406067848205566", "False"]], [["-3.1560680866241455", "False"]], [["-7.406067848205566", "False"]]], "filtered_resps": [["-3.9060680866241455", "False"], ["-1.6560680866241455", "True"], ["-6.406067848205566", "False"], ["-3.1560680866241455", "False"], ["-7.406067848205566", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "574dcb625dc021b1108e61b100228c6b99d3490043d38e5160f14a7daef87c45", "prompt_hash": "322ed241dca025c5fbb34d046c95a5185e340b29efadfcd3feca47b4f3455791", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 408, "doc": {"id": "0f7419d25337e0a75503a015ae777905", "question": "Most items in retail stores are what even when they are on sale?", "question_concept": "sale", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["overpriced", "purchase", "expensive", "park", "buying"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Most items in retail stores are what even when they are on sale?\nA. overpriced\nB. purchase\nC. expensive\nD. park\nE. buying\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Most items in retail stores are what even when they are on sale?\nA. overpriced\nB. purchase\nC. expensive\nD. park\nE. buying\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Most items in retail stores are what even when they are on sale?\nA. overpriced\nB. purchase\nC. expensive\nD. park\nE. buying\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Most items in retail stores are what even when they are on sale?\nA. overpriced\nB. purchase\nC. expensive\nD. park\nE. buying\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Most items in retail stores are what even when they are on sale?\nA. overpriced\nB. purchase\nC. expensive\nD. park\nE. buying\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7538073062896729", "True"]], [["-3.003807306289673", "False"]], [["-1.7538073062896729", "True"]], [["-5.253807067871094", "False"]], [["-4.503807067871094", "False"]]], "filtered_resps": [["-1.7538073062896729", "True"], ["-3.003807306289673", "False"], ["-1.7538073062896729", "True"], ["-5.253807067871094", "False"], ["-4.503807067871094", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bb41ebad682c04507f0decaca38b0757bb8be76ef8bb99f8461670330fbf1578", "prompt_hash": "f168637db8cccb1ed76b8885bbde92a9225ffa790faf2aaa980363196802342e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 409, "doc": {"id": "5cac4da628f0a58db980649079bd5784", "question": "John farms anemone in what type of facility?", "question_concept": "anemone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["michigan", "swimming pool", "atlantic ocean", "nursery", "gulf of mexico"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: John farms anemone in what type of facility?\nA. michigan\nB. swimming pool\nC. atlantic ocean\nD. nursery\nE. gulf of mexico\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John farms anemone in what type of facility?\nA. michigan\nB. swimming pool\nC. atlantic ocean\nD. nursery\nE. gulf of mexico\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John farms anemone in what type of facility?\nA. michigan\nB. swimming pool\nC. atlantic ocean\nD. nursery\nE. gulf of mexico\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John farms anemone in what type of facility?\nA. michigan\nB. swimming pool\nC. atlantic ocean\nD. nursery\nE. gulf of mexico\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John farms anemone in what type of facility?\nA. michigan\nB. swimming pool\nC. atlantic ocean\nD. nursery\nE. gulf of mexico\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2646665573120117", "False"]], [["-4.264666557312012", "False"]], [["-4.764666557312012", "False"]], [["-1.7646665573120117", "False"]], [["-6.014666557312012", "False"]]], "filtered_resps": [["-3.2646665573120117", "False"], ["-4.264666557312012", "False"], ["-4.764666557312012", "False"], ["-1.7646665573120117", "False"], ["-6.014666557312012", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b05ae0f6bc5020811ff1f8434ebf25a0bbe74ae511dcd54b4423a4f0d581eee4", "prompt_hash": "44da614819f8e7fc013f4380e31609e09585ba702b64872521cf0521f6fe7509", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 410, "doc": {"id": "78d1218aeff70a70904767349e3c4c53", "question": "Brawn opened the curtains so that the sun could do what?", "question_concept": "sun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dry clothes", "warm house", "warm room", "shine brightly", "get dark"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Brawn opened the curtains so that the sun could do what?\nA. dry clothes\nB. warm house\nC. warm room\nD. shine brightly\nE. get dark\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Brawn opened the curtains so that the sun could do what?\nA. dry clothes\nB. warm house\nC. warm room\nD. shine brightly\nE. get dark\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Brawn opened the curtains so that the sun could do what?\nA. dry clothes\nB. warm house\nC. warm room\nD. shine brightly\nE. get dark\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Brawn opened the curtains so that the sun could do what?\nA. dry clothes\nB. warm house\nC. warm room\nD. shine brightly\nE. get dark\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Brawn opened the curtains so that the sun could do what?\nA. dry clothes\nB. warm house\nC. warm room\nD. shine brightly\nE. get dark\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.809531211853027", "False"]], [["-2.5595309734344482", "False"]], [["-1.3095309734344482", "True"]], [["-6.059531211853027", "False"]], [["-10.059531211853027", "False"]]], "filtered_resps": [["-4.809531211853027", "False"], ["-2.5595309734344482", "False"], ["-1.3095309734344482", "True"], ["-6.059531211853027", "False"], ["-10.059531211853027", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d93cf51f2e3e498a1f03a387c78d8e45d930353bb97877ae12f377f3be274158", "prompt_hash": "7c87427a13b25cb6ff10478a7ceb771d449e79b4c6257b071cbbdc054174f8a6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 411, "doc": {"id": "cce13a32fedb997c017d3fac87c34912", "question": "How might releasing energy that has built up feel?", "question_concept": "releasing energy", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["damage", "wonderful", "exhaustion", "orgasm", "lazy"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: How might releasing energy that has built up feel?\nA. damage\nB. wonderful\nC. exhaustion\nD. orgasm\nE. lazy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How might releasing energy that has built up feel?\nA. damage\nB. wonderful\nC. exhaustion\nD. orgasm\nE. lazy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How might releasing energy that has built up feel?\nA. damage\nB. wonderful\nC. exhaustion\nD. orgasm\nE. lazy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How might releasing energy that has built up feel?\nA. damage\nB. wonderful\nC. exhaustion\nD. orgasm\nE. lazy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How might releasing energy that has built up feel?\nA. damage\nB. wonderful\nC. exhaustion\nD. orgasm\nE. lazy\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8890023231506348", "False"]], [["-4.139002323150635", "False"]], [["-3.3890023231506348", "False"]], [["-1.6390023231506348", "True"]], [["-6.139002323150635", "False"]]], "filtered_resps": [["-3.8890023231506348", "False"], ["-4.139002323150635", "False"], ["-3.3890023231506348", "False"], ["-1.6390023231506348", "True"], ["-6.139002323150635", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "07c52ab667df0512e626412ef9e4556f0b646c6de8650865951024a01af91642", "prompt_hash": "a78c2a9abede6b0c5fdce6c106fa55aa1761dad316725a223f3014cc40967c12", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 412, "doc": {"id": "6714487b839f648e348ac972ed114af3", "question": "What would you do if you have curiosity but are blind and paralyzed?", "question_concept": "curiosity", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hear news", "analyse", "go somewhere", "examine thing", "see favorite show"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What would you do if you have curiosity but are blind and paralyzed?\nA. hear news\nB. analyse\nC. go somewhere\nD. examine thing\nE. see favorite show\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you do if you have curiosity but are blind and paralyzed?\nA. hear news\nB. analyse\nC. go somewhere\nD. examine thing\nE. see favorite show\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you do if you have curiosity but are blind and paralyzed?\nA. hear news\nB. analyse\nC. go somewhere\nD. examine thing\nE. see favorite show\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you do if you have curiosity but are blind and paralyzed?\nA. hear news\nB. analyse\nC. go somewhere\nD. examine thing\nE. see favorite show\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you do if you have curiosity but are blind and paralyzed?\nA. hear news\nB. analyse\nC. go somewhere\nD. examine thing\nE. see favorite show\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.763875126838684", "True"]], [["-4.0138750076293945", "False"]], [["-5.0138750076293945", "False"]], [["-5.0138750076293945", "False"]], [["-4.5138750076293945", "False"]]], "filtered_resps": [["-1.763875126838684", "True"], ["-4.0138750076293945", "False"], ["-5.0138750076293945", "False"], ["-5.0138750076293945", "False"], ["-4.5138750076293945", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "675ff96bfbdca422004f7eab48bc60a260038c4eace16a3580ed6af5243cd51e", "prompt_hash": "7577086b1ddf9f2a5df3805d323c9b2959abd0fa101116a477aee8b2fb68d840", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 413, "doc": {"id": "3e536d9253bfac45de83e8ee291ca143", "question": "Where might it be hard to get furniture to?", "question_concept": "furniture", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["apartment", "loft", "store", "rug", "stairs"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where might it be hard to get furniture to?\nA. apartment\nB. loft\nC. store\nD. rug\nE. stairs\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where might it be hard to get furniture to?\nA. apartment\nB. loft\nC. store\nD. rug\nE. stairs\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where might it be hard to get furniture to?\nA. apartment\nB. loft\nC. store\nD. rug\nE. stairs\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where might it be hard to get furniture to?\nA. apartment\nB. loft\nC. store\nD. rug\nE. stairs\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where might it be hard to get furniture to?\nA. apartment\nB. loft\nC. store\nD. rug\nE. stairs\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8066363334655762", "True"]], [["-2.806636333465576", "False"]], [["-2.806636333465576", "False"]], [["-3.306636333465576", "False"]], [["-2.056636333465576", "False"]]], "filtered_resps": [["-1.8066363334655762", "True"], ["-2.806636333465576", "False"], ["-2.806636333465576", "False"], ["-3.306636333465576", "False"], ["-2.056636333465576", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4444165cd57e7cd31131c6566bb4aa638607330a0a6cefcf289b9e80ede2c843", "prompt_hash": "5a5383e151915f0d4e397a0f9398dca84c3125f91e41d4a1d4a6cf7e37565f6a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 414, "doc": {"id": "9f830faa0f8e3d7fb3a658c15a5fbe63", "question": "A great teacher can be what when you are attending school?", "question_concept": "attending school", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["detention", "graduate", "follower", "inspiration", "boredom"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: A great teacher can be what when you are attending school?\nA. detention\nB. graduate\nC. follower\nD. inspiration\nE. boredom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A great teacher can be what when you are attending school?\nA. detention\nB. graduate\nC. follower\nD. inspiration\nE. boredom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A great teacher can be what when you are attending school?\nA. detention\nB. graduate\nC. follower\nD. inspiration\nE. boredom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A great teacher can be what when you are attending school?\nA. detention\nB. graduate\nC. follower\nD. inspiration\nE. boredom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A great teacher can be what when you are attending school?\nA. detention\nB. graduate\nC. follower\nD. inspiration\nE. boredom\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2435364723205566", "False"]], [["-5.493536472320557", "False"]], [["-6.493536472320557", "False"]], [["-0.9935365319252014", "True"]], [["-9.243536949157715", "False"]]], "filtered_resps": [["-1.2435364723205566", "False"], ["-5.493536472320557", "False"], ["-6.493536472320557", "False"], ["-0.9935365319252014", "True"], ["-9.243536949157715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fb48e53c8db0c8d6f8f501b434f1d7ea1917fa206ec385db7f7c2168ab0607e7", "prompt_hash": "8338c0dab2657168c7a5ad74080533c37a1e9e82aa34aee4b9e4e02cd358157f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 415, "doc": {"id": "bbcef409e0acb71b515acc144d5b402c_1", "question": "Where would you get jeans and other wearable items to take home with you?", "question_concept": "jeans", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shopping mall", "museum", "laundromat", "clothing store", "bedroom"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you get jeans and other wearable items to take home with you?\nA. shopping mall\nB. museum\nC. laundromat\nD. clothing store\nE. bedroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you get jeans and other wearable items to take home with you?\nA. shopping mall\nB. museum\nC. laundromat\nD. clothing store\nE. bedroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you get jeans and other wearable items to take home with you?\nA. shopping mall\nB. museum\nC. laundromat\nD. clothing store\nE. bedroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you get jeans and other wearable items to take home with you?\nA. shopping mall\nB. museum\nC. laundromat\nD. clothing store\nE. bedroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you get jeans and other wearable items to take home with you?\nA. shopping mall\nB. museum\nC. laundromat\nD. clothing store\nE. bedroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.407630681991577", "False"]], [["-5.907630920410156", "False"]], [["-7.157630920410156", "False"]], [["-1.1576306819915771", "True"]], [["-9.157630920410156", "False"]]], "filtered_resps": [["-2.407630681991577", "False"], ["-5.907630920410156", "False"], ["-7.157630920410156", "False"], ["-1.1576306819915771", "True"], ["-9.157630920410156", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65f78fcccf674a7af1e4fb80e93abff65ed87f7d7ab2af0ec86b1ef937e1103b", "prompt_hash": "30ac07fea2cb49565cc8f5882f4ed55fecfabbfa2eb869b63d8f718dff8d25b1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 416, "doc": {"id": "cbb0c9a69ca0922371a48177087ef407", "question": "In what substance do clouds float?", "question_concept": "clouds", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sky", "top of mountain", "air", "ground level", "outer space"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: In what substance do clouds float?\nA. sky\nB. top of mountain\nC. air\nD. ground level\nE. outer space\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: In what substance do clouds float?\nA. sky\nB. top of mountain\nC. air\nD. ground level\nE. outer space\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: In what substance do clouds float?\nA. sky\nB. top of mountain\nC. air\nD. ground level\nE. outer space\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: In what substance do clouds float?\nA. sky\nB. top of mountain\nC. air\nD. ground level\nE. outer space\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: In what substance do clouds float?\nA. sky\nB. top of mountain\nC. air\nD. ground level\nE. outer space\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.720949172973633", "False"]], [["-8.720949172973633", "False"]], [["-1.9709489345550537", "False"]], [["-9.470949172973633", "False"]], [["-10.720949172973633", "False"]]], "filtered_resps": [["-5.720949172973633", "False"], ["-8.720949172973633", "False"], ["-1.9709489345550537", "False"], ["-9.470949172973633", "False"], ["-10.720949172973633", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2304f441a410898027aa1bffd8cc626377616565bc21aaf3e0c6b282cd0aa2ce", "prompt_hash": "0cde32dc48ee0c6661b6881824c931cffd865d74a803be67cd0dd812216feaf7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 417, "doc": {"id": "b92f786638796fc028947ac0e9a44fef", "question": "Where is the large area location of the empire state building?", "question_concept": "empire state building", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["manhattan", "office", "the city", "fifth avenue", "new york city"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where is the large area location of the empire state building?\nA. manhattan\nB. office\nC. the city\nD. fifth avenue\nE. new york city\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is the large area location of the empire state building?\nA. manhattan\nB. office\nC. the city\nD. fifth avenue\nE. new york city\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is the large area location of the empire state building?\nA. manhattan\nB. office\nC. the city\nD. fifth avenue\nE. new york city\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is the large area location of the empire state building?\nA. manhattan\nB. office\nC. the city\nD. fifth avenue\nE. new york city\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is the large area location of the empire state building?\nA. manhattan\nB. office\nC. the city\nD. fifth avenue\nE. new york city\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7471256256103516", "False"]], [["-5.997125625610352", "False"]], [["-8.497125625610352", "False"]], [["-7.997125625610352", "False"]], [["-1.4971256256103516", "True"]]], "filtered_resps": [["-2.7471256256103516", "False"], ["-5.997125625610352", "False"], ["-8.497125625610352", "False"], ["-7.997125625610352", "False"], ["-1.4971256256103516", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "40e17d1342e57ca3c92651c30dc91044b64af624784d4c4467c41254672c54a4", "prompt_hash": "74466c672f974da191aef2b2f956df97c53527c9ee5166c3092c57c8be711a15", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 418, "doc": {"id": "5abeb4a2126597d4ef7b5a32e9e22abf", "question": "Where do most people make coffee?", "question_concept": "cup of coffee", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["coffee shop", "office", "table", "washing", "kitchen"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where do most people make coffee?\nA. coffee shop\nB. office\nC. table\nD. washing\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do most people make coffee?\nA. coffee shop\nB. office\nC. table\nD. washing\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do most people make coffee?\nA. coffee shop\nB. office\nC. table\nD. washing\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do most people make coffee?\nA. coffee shop\nB. office\nC. table\nD. washing\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do most people make coffee?\nA. coffee shop\nB. office\nC. table\nD. washing\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2104859352111816", "False"]], [["-4.710485935211182", "False"]], [["-5.710485935211182", "False"]], [["-7.710485935211182", "False"]], [["-1.4604860544204712", "True"]]], "filtered_resps": [["-3.2104859352111816", "False"], ["-4.710485935211182", "False"], ["-5.710485935211182", "False"], ["-7.710485935211182", "False"], ["-1.4604860544204712", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "23dbb01fda6140e5ce0bd6187bf07bd2f19a48dd798b1aa2306dcecb9dbcc790", "prompt_hash": "fe978cada3f18da7d0d920bef4b05d80fc59219013c92b47bcf717201da405f5", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 419, "doc": {"id": "8d4b0312f02be445e09a9462873d02bb", "question": "What kind of service is my body a part of when I'm no longer here?", "question_concept": "body", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bodycam", "home", "coffin", "funeral", "graveyard"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of service is my body a part of when I'm no longer here?\nA. bodycam\nB. home\nC. coffin\nD. funeral\nE. graveyard\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of service is my body a part of when I'm no longer here?\nA. bodycam\nB. home\nC. coffin\nD. funeral\nE. graveyard\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of service is my body a part of when I'm no longer here?\nA. bodycam\nB. home\nC. coffin\nD. funeral\nE. graveyard\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of service is my body a part of when I'm no longer here?\nA. bodycam\nB. home\nC. coffin\nD. funeral\nE. graveyard\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of service is my body a part of when I'm no longer here?\nA. bodycam\nB. home\nC. coffin\nD. funeral\nE. graveyard\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7632248401641846", "False"]], [["-6.7632246017456055", "False"]], [["-5.7632246017456055", "False"]], [["-1.7632248401641846", "False"]], [["-4.7632246017456055", "False"]]], "filtered_resps": [["-3.7632248401641846", "False"], ["-6.7632246017456055", "False"], ["-5.7632246017456055", "False"], ["-1.7632248401641846", "False"], ["-4.7632246017456055", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4424add79472448d544fc22f194bad81281fcbdceeffae0a799d93c266f1abf9", "prompt_hash": "0a2b80c0a5e70bb92e8d50bed80a94d46f34a84bfe810a9929e56d3e2e060622", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 420, "doc": {"id": "f7140f00ddd8d1c5d93b05ea32ad1fff", "question": "Many people wanted to leave their country estates for row houses, what did they need to move to?", "question_concept": "row house", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["living less expensively", "england", "prison", "city", "town"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Many people wanted to leave their country estates for row houses, what did they need to move to?\nA. living less expensively\nB. england\nC. prison\nD. city\nE. town\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Many people wanted to leave their country estates for row houses, what did they need to move to?\nA. living less expensively\nB. england\nC. prison\nD. city\nE. town\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Many people wanted to leave their country estates for row houses, what did they need to move to?\nA. living less expensively\nB. england\nC. prison\nD. city\nE. town\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Many people wanted to leave their country estates for row houses, what did they need to move to?\nA. living less expensively\nB. england\nC. prison\nD. city\nE. town\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Many people wanted to leave their country estates for row houses, what did they need to move to?\nA. living less expensively\nB. england\nC. prison\nD. city\nE. town\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7194676399230957", "False"]], [["-5.719467639923096", "False"]], [["-7.969467639923096", "False"]], [["-1.9694677591323853", "False"]], [["-4.469467639923096", "False"]]], "filtered_resps": [["-2.7194676399230957", "False"], ["-5.719467639923096", "False"], ["-7.969467639923096", "False"], ["-1.9694677591323853", "False"], ["-4.469467639923096", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "229347fbe40fb74232721f34c95f739f31bb7d561b6fffbdca4d3ca0bc7257c2", "prompt_hash": "41b24915f97f96df758e967fd4576f3d754ed552c69d69ab04b1350516f24f56", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 421, "doc": {"id": "8b3b598a647dfd2d63fcedce5f461040", "question": "Where can someone get a new saw?", "question_concept": "saw", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hardware store", "toolbox", "logging camp", "tool kit", "auger"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where can someone get a new saw?\nA. hardware store\nB. toolbox\nC. logging camp\nD. tool kit\nE. auger\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can someone get a new saw?\nA. hardware store\nB. toolbox\nC. logging camp\nD. tool kit\nE. auger\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can someone get a new saw?\nA. hardware store\nB. toolbox\nC. logging camp\nD. tool kit\nE. auger\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can someone get a new saw?\nA. hardware store\nB. toolbox\nC. logging camp\nD. tool kit\nE. auger\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can someone get a new saw?\nA. hardware store\nB. toolbox\nC. logging camp\nD. tool kit\nE. auger\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.204750418663025", "False"]], [["-7.4547505378723145", "False"]], [["-7.7047505378723145", "False"]], [["-9.454750061035156", "False"]], [["-9.704750061035156", "False"]]], "filtered_resps": [["-1.204750418663025", "False"], ["-7.4547505378723145", "False"], ["-7.7047505378723145", "False"], ["-9.454750061035156", "False"], ["-9.704750061035156", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "463dbcd576b01f6dc2f070ad78ffa5f06fbce3a901e2af8b3a84623632385d8d", "prompt_hash": "728affd7c64ebfc52d23cc30989ad138277280efef0f88d82aa012935229aa87", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 422, "doc": {"id": "7a900bc3a373806b6c56f0e19534005f", "question": "What would you do to a crime scene before asking a question?", "question_concept": "question", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["express information", "touch everything", "think", "give clue", "analyse"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What would you do to a crime scene before asking a question?\nA. express information\nB. touch everything\nC. think\nD. give clue\nE. analyse\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you do to a crime scene before asking a question?\nA. express information\nB. touch everything\nC. think\nD. give clue\nE. analyse\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you do to a crime scene before asking a question?\nA. express information\nB. touch everything\nC. think\nD. give clue\nE. analyse\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you do to a crime scene before asking a question?\nA. express information\nB. touch everything\nC. think\nD. give clue\nE. analyse\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you do to a crime scene before asking a question?\nA. express information\nB. touch everything\nC. think\nD. give clue\nE. analyse\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.444995403289795", "False"]], [["-5.944995403289795", "False"]], [["-4.194995403289795", "False"]], [["-6.694995403289795", "False"]], [["-0.9449955224990845", "True"]]], "filtered_resps": [["-3.444995403289795", "False"], ["-5.944995403289795", "False"], ["-4.194995403289795", "False"], ["-6.694995403289795", "False"], ["-0.9449955224990845", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "36379a938c7827fe7c11e4c2a38b53640ed447cc84630fe3fb439f95ae5e1402", "prompt_hash": "bd5099a83193c2701845814edbe82cff0d418b6c9fa61b1ff45d11fb2e4a3ae4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 423, "doc": {"id": "3d79c10ddf26a5ed7dc0bb168fb0b3ed", "question": "The man didn't do great in college, all his best memories were late night with his brothers at the what?", "question_concept": "college", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["big city", "fraternity house", "school", "building", "big town"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The man didn't do great in college, all his best memories were late night with his brothers at the what?\nA. big city\nB. fraternity house\nC. school\nD. building\nE. big town\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man didn't do great in college, all his best memories were late night with his brothers at the what?\nA. big city\nB. fraternity house\nC. school\nD. building\nE. big town\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man didn't do great in college, all his best memories were late night with his brothers at the what?\nA. big city\nB. fraternity house\nC. school\nD. building\nE. big town\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man didn't do great in college, all his best memories were late night with his brothers at the what?\nA. big city\nB. fraternity house\nC. school\nD. building\nE. big town\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man didn't do great in college, all his best memories were late night with his brothers at the what?\nA. big city\nB. fraternity house\nC. school\nD. building\nE. big town\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.584754467010498", "False"]], [["-1.334754467010498", "True"]], [["-7.834754467010498", "False"]], [["-8.584754943847656", "False"]], [["-9.084754943847656", "False"]]], "filtered_resps": [["-4.584754467010498", "False"], ["-1.334754467010498", "True"], ["-7.834754467010498", "False"], ["-8.584754943847656", "False"], ["-9.084754943847656", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "27b282681ebb1a4b8ddb5c89d86cc11908cd8339cb74ff072236c033dfb7d0b7", "prompt_hash": "bfeff4e28dbb8cc5ffbfb21d56f55e36dbcd94cb4d8c401eadc57441eed781b0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 424, "doc": {"id": "b7091d2bfcea421d787ce9e7982f104a", "question": "In a horror movie victims usually trip when the run in order to do what in regards to the killer?", "question_concept": "run", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["frightened", "run up stairs", "get away from", "go quickly", "go faster"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: In a horror movie victims usually trip when the run in order to do what in regards to the killer?\nA. frightened\nB. run up stairs\nC. get away from\nD. go quickly\nE. go faster\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: In a horror movie victims usually trip when the run in order to do what in regards to the killer?\nA. frightened\nB. run up stairs\nC. get away from\nD. go quickly\nE. go faster\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: In a horror movie victims usually trip when the run in order to do what in regards to the killer?\nA. frightened\nB. run up stairs\nC. get away from\nD. go quickly\nE. go faster\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: In a horror movie victims usually trip when the run in order to do what in regards to the killer?\nA. frightened\nB. run up stairs\nC. get away from\nD. go quickly\nE. go faster\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: In a horror movie victims usually trip when the run in order to do what in regards to the killer?\nA. frightened\nB. run up stairs\nC. get away from\nD. go quickly\nE. go faster\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.909842491149902", "False"]], [["-6.409842491149902", "False"]], [["-1.6598424911499023", "False"]], [["-7.409842491149902", "False"]], [["-8.159842491149902", "False"]]], "filtered_resps": [["-5.909842491149902", "False"], ["-6.409842491149902", "False"], ["-1.6598424911499023", "False"], ["-7.409842491149902", "False"], ["-8.159842491149902", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "366f1ec0fec4249c2764137ee37fce6e8e6a35d81180a228c127942369262e18", "prompt_hash": "e3f75bd913ba6560c59b07af5d0eaf23bb2f7fd924fe74d8bc98679ec18755de", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 425, "doc": {"id": "d060ab71d0efff3cab5960089a6bb3a2", "question": "The coach decided to make a lineup change, the team's effort was suffering from what?", "question_concept": "change", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stagnant", "stagnation", "tradition", "hunger", "paper money"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The coach decided to make a lineup change, the team's effort was suffering from what?\nA. stagnant\nB. stagnation\nC. tradition\nD. hunger\nE. paper money\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The coach decided to make a lineup change, the team's effort was suffering from what?\nA. stagnant\nB. stagnation\nC. tradition\nD. hunger\nE. paper money\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The coach decided to make a lineup change, the team's effort was suffering from what?\nA. stagnant\nB. stagnation\nC. tradition\nD. hunger\nE. paper money\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The coach decided to make a lineup change, the team's effort was suffering from what?\nA. stagnant\nB. stagnation\nC. tradition\nD. hunger\nE. paper money\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The coach decided to make a lineup change, the team's effort was suffering from what?\nA. stagnant\nB. stagnation\nC. tradition\nD. hunger\nE. paper money\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.452819585800171", "False"]], [["-0.9528195261955261", "True"]], [["-8.20281982421875", "False"]], [["-7.202819347381592", "False"]], [["-9.95281982421875", "False"]]], "filtered_resps": [["-3.452819585800171", "False"], ["-0.9528195261955261", "True"], ["-8.20281982421875", "False"], ["-7.202819347381592", "False"], ["-9.95281982421875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c01c0f5c94017c1afe6979822822a2fda22ea5030f0644d23459380012254d8f", "prompt_hash": "6ea0b4239ba3563bdffc545dee15fa33256442efc883152a62d3d23421b63967", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 426, "doc": {"id": "b399f6008d90dbd92bcce5abed4c1fd1", "question": "Where would you go if you want to buy some clothes?", "question_concept": "goods", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mall", "grocery store", "grocery store", "shop", "supermarket"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you go if you want to buy some clothes?\nA. mall\nB. grocery store\nC. grocery store\nD. shop\nE. supermarket\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you go if you want to buy some clothes?\nA. mall\nB. grocery store\nC. grocery store\nD. shop\nE. supermarket\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you go if you want to buy some clothes?\nA. mall\nB. grocery store\nC. grocery store\nD. shop\nE. supermarket\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you go if you want to buy some clothes?\nA. mall\nB. grocery store\nC. grocery store\nD. shop\nE. supermarket\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you go if you want to buy some clothes?\nA. mall\nB. grocery store\nC. grocery store\nD. shop\nE. supermarket\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7196338772773743", "True"]], [["-7.219634056091309", "False"]], [["-7.219634056091309", "False"]], [["-5.719634056091309", "False"]], [["-9.969634056091309", "False"]]], "filtered_resps": [["-0.7196338772773743", "True"], ["-7.219634056091309", "False"], ["-7.219634056091309", "False"], ["-5.719634056091309", "False"], ["-9.969634056091309", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "04634905108af1157475bd26ecb59b00d3142ad00fa4c2562b0c0c171fc38065", "prompt_hash": "9a98a0fb5fef424bd59be74cffb2e79725a3879698dba7da6c08a56268722047", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 427, "doc": {"id": "80c19c62338edae0e8a1f5c6fec0d29a", "question": "Where is food likely to stay dry?", "question_concept": "food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["etna", "cupboard", "oven", "stomach", "fridge"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where is food likely to stay dry?\nA. etna\nB. cupboard\nC. oven\nD. stomach\nE. fridge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is food likely to stay dry?\nA. etna\nB. cupboard\nC. oven\nD. stomach\nE. fridge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is food likely to stay dry?\nA. etna\nB. cupboard\nC. oven\nD. stomach\nE. fridge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is food likely to stay dry?\nA. etna\nB. cupboard\nC. oven\nD. stomach\nE. fridge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is food likely to stay dry?\nA. etna\nB. cupboard\nC. oven\nD. stomach\nE. fridge\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3037562370300293", "False"]], [["-1.0537561178207397", "True"]], [["-5.803756237030029", "False"]], [["-5.053756237030029", "False"]], [["-5.803756237030029", "False"]]], "filtered_resps": [["-3.3037562370300293", "False"], ["-1.0537561178207397", "True"], ["-5.803756237030029", "False"], ["-5.053756237030029", "False"], ["-5.803756237030029", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "23a56022944786610216b0edb6c97447e077b2e5e693af65527a6054482a20fb", "prompt_hash": "e34ca5497a6c324a75feb687ffb57b4288c08da55c7abf926fd69c9c4c525a28", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 428, "doc": {"id": "1a4e83b433620cb2d7d806882f8d57e4", "question": "What is it called when a person with mental illness is able to lead a relatively normal life?", "question_concept": "mental illness", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["managed", "effectively treated", "recur", "cause delusion", "illusion"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is it called when a person with mental illness is able to lead a relatively normal life?\nA. managed\nB. effectively treated\nC. recur\nD. cause delusion\nE. illusion\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is it called when a person with mental illness is able to lead a relatively normal life?\nA. managed\nB. effectively treated\nC. recur\nD. cause delusion\nE. illusion\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is it called when a person with mental illness is able to lead a relatively normal life?\nA. managed\nB. effectively treated\nC. recur\nD. cause delusion\nE. illusion\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is it called when a person with mental illness is able to lead a relatively normal life?\nA. managed\nB. effectively treated\nC. recur\nD. cause delusion\nE. illusion\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is it called when a person with mental illness is able to lead a relatively normal life?\nA. managed\nB. effectively treated\nC. recur\nD. cause delusion\nE. illusion\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.4626200199127197", "False"]], [["-1.7126200199127197", "True"]], [["-6.712619781494141", "False"]], [["-7.462619781494141", "False"]], [["-6.212619781494141", "False"]]], "filtered_resps": [["-2.4626200199127197", "False"], ["-1.7126200199127197", "True"], ["-6.712619781494141", "False"], ["-7.462619781494141", "False"], ["-6.212619781494141", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "72d6b369e48ff9adf390e057e30ac14d3966c7acd4638fc9e6f720d8c04c9e71", "prompt_hash": "a236cacd6af6c6b6c1f1eb212ac64b9099b1b0d5b4e93162e2f580bb60e34954", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 429, "doc": {"id": "b9e04a53c0ee7325b901de4d12d56884", "question": "Where do you keep musical instrument so it doesn't get scratched?", "question_concept": "musical instrument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bank", "orchestra", "case", "music room", "movie"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you keep musical instrument so it doesn't get scratched?\nA. bank\nB. orchestra\nC. case\nD. music room\nE. movie\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you keep musical instrument so it doesn't get scratched?\nA. bank\nB. orchestra\nC. case\nD. music room\nE. movie\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you keep musical instrument so it doesn't get scratched?\nA. bank\nB. orchestra\nC. case\nD. music room\nE. movie\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you keep musical instrument so it doesn't get scratched?\nA. bank\nB. orchestra\nC. case\nD. music room\nE. movie\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you keep musical instrument so it doesn't get scratched?\nA. bank\nB. orchestra\nC. case\nD. music room\nE. movie\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.174496650695801", "False"]], [["-7.924496650695801", "False"]], [["-1.4244964122772217", "False"]], [["-8.1744966506958", "False"]], [["-9.9244966506958", "False"]]], "filtered_resps": [["-4.174496650695801", "False"], ["-7.924496650695801", "False"], ["-1.4244964122772217", "False"], ["-8.1744966506958", "False"], ["-9.9244966506958", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f5fc90a1b8dbb6cb0efc4e1326e73e2268600c8d7cc75c9c8712dda728030407", "prompt_hash": "2a875e6e925ecad3aa21c702a2986e1969b2d1fe96c94dedba7768d0799a862c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 430, "doc": {"id": "7490aa460f66000555a8a94008179cbb", "question": "The woman is watching television and trying to forget her day, what is her goal?", "question_concept": "watching television", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["entertainment", "falling asleep", "getting fat", "crying", "relaxation"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The woman is watching television and trying to forget her day, what is her goal?\nA. entertainment\nB. falling asleep\nC. getting fat\nD. crying\nE. relaxation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The woman is watching television and trying to forget her day, what is her goal?\nA. entertainment\nB. falling asleep\nC. getting fat\nD. crying\nE. relaxation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The woman is watching television and trying to forget her day, what is her goal?\nA. entertainment\nB. falling asleep\nC. getting fat\nD. crying\nE. relaxation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The woman is watching television and trying to forget her day, what is her goal?\nA. entertainment\nB. falling asleep\nC. getting fat\nD. crying\nE. relaxation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The woman is watching television and trying to forget her day, what is her goal?\nA. entertainment\nB. falling asleep\nC. getting fat\nD. crying\nE. relaxation\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.9009978771209717", "False"]], [["-4.400998115539551", "False"]], [["-5.900998115539551", "False"]], [["-6.650998115539551", "False"]], [["-1.1509978771209717", "True"]]], "filtered_resps": [["-1.9009978771209717", "False"], ["-4.400998115539551", "False"], ["-5.900998115539551", "False"], ["-6.650998115539551", "False"], ["-1.1509978771209717", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a370fb3806b12d8ea05d9756ea704e9f8d5d5018dbd5b2dfbc21037394c46b15", "prompt_hash": "8acb79f7f4c5bfb1496a54ac46982f8b3f70e4845dd349952d19770ec9e13e75", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 431, "doc": {"id": "ad8ee2965a33ff4b0e3d2ac732676594", "question": "While John Candy and Dan Aykroyd didn't run into a gazelle, you'd have to go where to see one?", "question_concept": "gazelle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eastern hemisphere", "the city", "open plain", "television program", "great outdoors"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: While John Candy and Dan Aykroyd didn't run into a gazelle, you'd have to go where to see one?\nA. eastern hemisphere\nB. the city\nC. open plain\nD. television program\nE. great outdoors\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: While John Candy and Dan Aykroyd didn't run into a gazelle, you'd have to go where to see one?\nA. eastern hemisphere\nB. the city\nC. open plain\nD. television program\nE. great outdoors\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: While John Candy and Dan Aykroyd didn't run into a gazelle, you'd have to go where to see one?\nA. eastern hemisphere\nB. the city\nC. open plain\nD. television program\nE. great outdoors\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: While John Candy and Dan Aykroyd didn't run into a gazelle, you'd have to go where to see one?\nA. eastern hemisphere\nB. the city\nC. open plain\nD. television program\nE. great outdoors\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: While John Candy and Dan Aykroyd didn't run into a gazelle, you'd have to go where to see one?\nA. eastern hemisphere\nB. the city\nC. open plain\nD. television program\nE. great outdoors\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.325599193572998", "False"]], [["-4.575599193572998", "False"]], [["-1.5755990743637085", "True"]], [["-4.825599193572998", "False"]], [["-2.575599193572998", "False"]]], "filtered_resps": [["-2.325599193572998", "False"], ["-4.575599193572998", "False"], ["-1.5755990743637085", "True"], ["-4.825599193572998", "False"], ["-2.575599193572998", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0d4f794926e5b52b450e24b18163809c2820adcf23f1e92c614d2d4ff647912d", "prompt_hash": "e3952c763b1dc25a9766e2aa0fb8c72b307a3351e6832f6278bbb4266433877f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 432, "doc": {"id": "64d2310eff6b661baeb41b4ccc392e35", "question": "When we are running what are we doing?", "question_concept": "run", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stretches", "running from police", "learn to walk", "go quickly", "get out of bed"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When we are running what are we doing?\nA. stretches\nB. running from police\nC. learn to walk\nD. go quickly\nE. get out of bed\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When we are running what are we doing?\nA. stretches\nB. running from police\nC. learn to walk\nD. go quickly\nE. get out of bed\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When we are running what are we doing?\nA. stretches\nB. running from police\nC. learn to walk\nD. go quickly\nE. get out of bed\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When we are running what are we doing?\nA. stretches\nB. running from police\nC. learn to walk\nD. go quickly\nE. get out of bed\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When we are running what are we doing?\nA. stretches\nB. running from police\nC. learn to walk\nD. go quickly\nE. get out of bed\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.969749927520752", "False"]], [["-3.969749927520752", "False"]], [["-7.719749927520752", "False"]], [["-0.9697500467300415", "True"]], [["-8.21975040435791", "False"]]], "filtered_resps": [["-4.969749927520752", "False"], ["-3.969749927520752", "False"], ["-7.719749927520752", "False"], ["-0.9697500467300415", "True"], ["-8.21975040435791", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e9f931f1a079acd55e2de14a935b922615aece9b53ab7c4c65e97209e573ed29", "prompt_hash": "520e6922d5b7ea078c9c13b413452b038b6d3f7023d01752cf8b0f67025e8fce", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 433, "doc": {"id": "6b1f5ebd9d0dbc7e34a598456a6091a8", "question": "It's dangerous to let pet birds free so it's better to keep them what?", "question_concept": "free", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["slavery", "caught", "caged in", "topfree", "prisoner"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: It's dangerous to let pet birds free so it's better to keep them what?\nA. slavery\nB. caught\nC. caged in\nD. topfree\nE. prisoner\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: It's dangerous to let pet birds free so it's better to keep them what?\nA. slavery\nB. caught\nC. caged in\nD. topfree\nE. prisoner\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: It's dangerous to let pet birds free so it's better to keep them what?\nA. slavery\nB. caught\nC. caged in\nD. topfree\nE. prisoner\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: It's dangerous to let pet birds free so it's better to keep them what?\nA. slavery\nB. caught\nC. caged in\nD. topfree\nE. prisoner\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: It's dangerous to let pet birds free so it's better to keep them what?\nA. slavery\nB. caught\nC. caged in\nD. topfree\nE. prisoner\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.924317359924316", "False"]], [["-4.674317359924316", "False"]], [["-0.9243171811103821", "True"]], [["-5.424317359924316", "False"]], [["-5.924317359924316", "False"]]], "filtered_resps": [["-4.924317359924316", "False"], ["-4.674317359924316", "False"], ["-0.9243171811103821", "True"], ["-5.424317359924316", "False"], ["-5.924317359924316", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d88ff32c5bf9ab06074b99124778538985bd57e3719322b4cd293daa1ebac43c", "prompt_hash": "a03599c27fc4d6f88a19229817751fe49801c832590f162b605247954e12c1ff", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 434, "doc": {"id": "080ef6941410139d6869e78122bc741e", "question": "A beaver is know for building prowess, their supplies come from where?", "question_concept": "beaver", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["british columbia", "body of water", "wooded area", "pay debts", "zoo"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: A beaver is know for building prowess, their supplies come from where?\nA. british columbia\nB. body of water\nC. wooded area\nD. pay debts\nE. zoo\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A beaver is know for building prowess, their supplies come from where?\nA. british columbia\nB. body of water\nC. wooded area\nD. pay debts\nE. zoo\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A beaver is know for building prowess, their supplies come from where?\nA. british columbia\nB. body of water\nC. wooded area\nD. pay debts\nE. zoo\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A beaver is know for building prowess, their supplies come from where?\nA. british columbia\nB. body of water\nC. wooded area\nD. pay debts\nE. zoo\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A beaver is know for building prowess, their supplies come from where?\nA. british columbia\nB. body of water\nC. wooded area\nD. pay debts\nE. zoo\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.4876961708068848", "False"]], [["-1.7376962900161743", "False"]], [["-1.2376962900161743", "True"]], [["-7.737696170806885", "False"]], [["-8.737696647644043", "False"]]], "filtered_resps": [["-2.4876961708068848", "False"], ["-1.7376962900161743", "False"], ["-1.2376962900161743", "True"], ["-7.737696170806885", "False"], ["-8.737696647644043", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "87ee7a3d9456efebfc3d884540f72f28b60a12bd83bbf1b3f46a33333623127a", "prompt_hash": "17708e1a59c748afcf2db14499d00f7684d66dd7cc383ae44242195266248553", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 435, "doc": {"id": "6c70d98cfb8e97fda8caefcee761a229", "question": "Zane doesn't like answering questions.  He's not good at it because he suffers from what?", "question_concept": "answering questions", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["panic", "discussion", "attention", "confusion", "satisfaction"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Zane doesn't like answering questions.  He's not good at it because he suffers from what?\nA. panic\nB. discussion\nC. attention\nD. confusion\nE. satisfaction\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Zane doesn't like answering questions.  He's not good at it because he suffers from what?\nA. panic\nB. discussion\nC. attention\nD. confusion\nE. satisfaction\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Zane doesn't like answering questions.  He's not good at it because he suffers from what?\nA. panic\nB. discussion\nC. attention\nD. confusion\nE. satisfaction\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Zane doesn't like answering questions.  He's not good at it because he suffers from what?\nA. panic\nB. discussion\nC. attention\nD. confusion\nE. satisfaction\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Zane doesn't like answering questions.  He's not good at it because he suffers from what?\nA. panic\nB. discussion\nC. attention\nD. confusion\nE. satisfaction\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.283990740776062", "True"]], [["-4.783990859985352", "False"]], [["-5.533990859985352", "False"]], [["-3.2839908599853516", "False"]], [["-8.283990859985352", "False"]]], "filtered_resps": [["-1.283990740776062", "True"], ["-4.783990859985352", "False"], ["-5.533990859985352", "False"], ["-3.2839908599853516", "False"], ["-8.283990859985352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4f4ac14d81844e13324e76f0c26476c6dee9f6789dce4cfda94c4634fa8beced", "prompt_hash": "73769d3663c84628eafb8f6adf651d447c8c68a3f6752c994c67a167affd4400", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 436, "doc": {"id": "75ac594b4fdbfba006e61315d1b2c815", "question": "Going public about a common problem can gain what for a celebrity?", "question_concept": "going public", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wide acceptance", "a degree", "pain", "getting high", "press coverage"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Going public about a common problem can gain what for a celebrity?\nA. wide acceptance\nB. a degree\nC. pain\nD. getting high\nE. press coverage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Going public about a common problem can gain what for a celebrity?\nA. wide acceptance\nB. a degree\nC. pain\nD. getting high\nE. press coverage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Going public about a common problem can gain what for a celebrity?\nA. wide acceptance\nB. a degree\nC. pain\nD. getting high\nE. press coverage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Going public about a common problem can gain what for a celebrity?\nA. wide acceptance\nB. a degree\nC. pain\nD. getting high\nE. press coverage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Going public about a common problem can gain what for a celebrity?\nA. wide acceptance\nB. a degree\nC. pain\nD. getting high\nE. press coverage\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6212048530578613", "False"]], [["-5.621204853057861", "False"]], [["-6.371204853057861", "False"]], [["-6.371204853057861", "False"]], [["-1.6212049722671509", "False"]]], "filtered_resps": [["-3.6212048530578613", "False"], ["-5.621204853057861", "False"], ["-6.371204853057861", "False"], ["-6.371204853057861", "False"], ["-1.6212049722671509", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5e2e14d3465a544d035a386353c3ac86a46cb3f0ada10ed78f37a2c7e20743ca", "prompt_hash": "03c9e82f7735758e97792a2c4aef746514860960c829421b737a4bf14c62bc52", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 437, "doc": {"id": "5a8e7d2f97f76adb23fbd59a009d16f0", "question": "The electricity went out and everyone was shrouded in darkness.  They all remained in their seats, because it would have been dangerous to try to find there way out.  Where mihgt they have been?", "question_concept": "electricity", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["opera", "concert", "basement", "bedroom", "grand canyon"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The electricity went out and everyone was shrouded in darkness.  They all remained in their seats, because it would have been dangerous to try to find there way out.  Where mihgt they have been?\nA. opera\nB. concert\nC. basement\nD. bedroom\nE. grand canyon\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The electricity went out and everyone was shrouded in darkness.  They all remained in their seats, because it would have been dangerous to try to find there way out.  Where mihgt they have been?\nA. opera\nB. concert\nC. basement\nD. bedroom\nE. grand canyon\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The electricity went out and everyone was shrouded in darkness.  They all remained in their seats, because it would have been dangerous to try to find there way out.  Where mihgt they have been?\nA. opera\nB. concert\nC. basement\nD. bedroom\nE. grand canyon\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The electricity went out and everyone was shrouded in darkness.  They all remained in their seats, because it would have been dangerous to try to find there way out.  Where mihgt they have been?\nA. opera\nB. concert\nC. basement\nD. bedroom\nE. grand canyon\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The electricity went out and everyone was shrouded in darkness.  They all remained in their seats, because it would have been dangerous to try to find there way out.  Where mihgt they have been?\nA. opera\nB. concert\nC. basement\nD. bedroom\nE. grand canyon\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.163076162338257", "False"]], [["-4.413076400756836", "False"]], [["-1.9130761623382568", "False"]], [["-7.163076400756836", "False"]], [["-7.663076400756836", "False"]]], "filtered_resps": [["-2.163076162338257", "False"], ["-4.413076400756836", "False"], ["-1.9130761623382568", "False"], ["-7.163076400756836", "False"], ["-7.663076400756836", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f2f0242339fcb4732d12e794eba21902eb4c456311a67d207435c7c8a46d90a2", "prompt_hash": "792475100d3b35e90c1b63772723c22477ffece4332ea8b89a908c8b13e0e0d9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 438, "doc": {"id": "178cb8153123716aa94f286b615149d4", "question": "Where could you find hundreds of beauty salon?", "question_concept": "beauty salon", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["clerk", "mall", "strip mall", "city", "neighborhood"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where could you find hundreds of beauty salon?\nA. clerk\nB. mall\nC. strip mall\nD. city\nE. neighborhood\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could you find hundreds of beauty salon?\nA. clerk\nB. mall\nC. strip mall\nD. city\nE. neighborhood\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could you find hundreds of beauty salon?\nA. clerk\nB. mall\nC. strip mall\nD. city\nE. neighborhood\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could you find hundreds of beauty salon?\nA. clerk\nB. mall\nC. strip mall\nD. city\nE. neighborhood\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could you find hundreds of beauty salon?\nA. clerk\nB. mall\nC. strip mall\nD. city\nE. neighborhood\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.60465669631958", "False"]], [["-2.85465669631958", "False"]], [["-1.60465669631958", "False"]], [["-7.60465669631958", "False"]], [["-7.35465669631958", "False"]]], "filtered_resps": [["-3.60465669631958", "False"], ["-2.85465669631958", "False"], ["-1.60465669631958", "False"], ["-7.60465669631958", "False"], ["-7.35465669631958", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "73fd7b6fe894e2ae76d1395ad4a8efa1efa69c13fafe6cebf02fe6ca41dbc553", "prompt_hash": "45ef14411d87695880f59890228e8c08f061313fa0f51c6efa8ce63d0a1570ae", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 439, "doc": {"id": "cc917ca0e03c91a5141920f5a902a36c", "question": "If it is Chrismas time what came most recently before?", "question_concept": "christmas", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["halloween", "summer", "easter", "kwaanza", "give gift"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If it is Chrismas time what came most recently before?\nA. halloween\nB. summer\nC. easter\nD. kwaanza\nE. give gift\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If it is Chrismas time what came most recently before?\nA. halloween\nB. summer\nC. easter\nD. kwaanza\nE. give gift\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If it is Chrismas time what came most recently before?\nA. halloween\nB. summer\nC. easter\nD. kwaanza\nE. give gift\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If it is Chrismas time what came most recently before?\nA. halloween\nB. summer\nC. easter\nD. kwaanza\nE. give gift\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If it is Chrismas time what came most recently before?\nA. halloween\nB. summer\nC. easter\nD. kwaanza\nE. give gift\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8995795249938965", "False"]], [["-2.8995795249938965", "False"]], [["-2.6495795249938965", "False"]], [["-2.6495795249938965", "False"]], [["-5.3995795249938965", "False"]]], "filtered_resps": [["-2.8995795249938965", "False"], ["-2.8995795249938965", "False"], ["-2.6495795249938965", "False"], ["-2.6495795249938965", "False"], ["-5.3995795249938965", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c8fbef9c5564fcb6ee46712317615a78056436c085f67bb89ac2104644f6621e", "prompt_hash": "0d02f91ceb0211fbbac08e25baa664970039072bc238013fc260468270fc26c8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 440, "doc": {"id": "a7d51b753c2113d8b2dbd0ebb5375855_1", "question": "If someone found out their brother was having a daughter, they would have to add a niece limb to the what?", "question_concept": "niece", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["family picture book", "family reunion", "brother's house", "family tree", "baby shower"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If someone found out their brother was having a daughter, they would have to add a niece limb to the what?\nA. family picture book\nB. family reunion\nC. brother's house\nD. family tree\nE. baby shower\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If someone found out their brother was having a daughter, they would have to add a niece limb to the what?\nA. family picture book\nB. family reunion\nC. brother's house\nD. family tree\nE. baby shower\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If someone found out their brother was having a daughter, they would have to add a niece limb to the what?\nA. family picture book\nB. family reunion\nC. brother's house\nD. family tree\nE. baby shower\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If someone found out their brother was having a daughter, they would have to add a niece limb to the what?\nA. family picture book\nB. family reunion\nC. brother's house\nD. family tree\nE. baby shower\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If someone found out their brother was having a daughter, they would have to add a niece limb to the what?\nA. family picture book\nB. family reunion\nC. brother's house\nD. family tree\nE. baby shower\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.355994701385498", "False"]], [["-6.605994701385498", "False"]], [["-7.855994701385498", "False"]], [["-1.105994701385498", "True"]], [["-9.105995178222656", "False"]]], "filtered_resps": [["-3.355994701385498", "False"], ["-6.605994701385498", "False"], ["-7.855994701385498", "False"], ["-1.105994701385498", "True"], ["-9.105995178222656", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "25bc081a6de5315976f7346ec450c607cebcba8fa6ffb620461daf2bfb1644be", "prompt_hash": "84815d7bcbf33394144e5ed0817f9da8f8156a63955de6c96dc49c9ba9a78e3d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 441, "doc": {"id": "e71da9e95b321763c86e879a47bbd327", "question": "The criminal insisted he must do the crime to the bank teller, but she tried to convince him there were other ways in life and this was what?", "question_concept": "must", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["willing", "optional", "should not", "have to", "unnecessary"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The criminal insisted he must do the crime to the bank teller, but she tried to convince him there were other ways in life and this was what?\nA. willing\nB. optional\nC. should not\nD. have to\nE. unnecessary\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The criminal insisted he must do the crime to the bank teller, but she tried to convince him there were other ways in life and this was what?\nA. willing\nB. optional\nC. should not\nD. have to\nE. unnecessary\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The criminal insisted he must do the crime to the bank teller, but she tried to convince him there were other ways in life and this was what?\nA. willing\nB. optional\nC. should not\nD. have to\nE. unnecessary\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The criminal insisted he must do the crime to the bank teller, but she tried to convince him there were other ways in life and this was what?\nA. willing\nB. optional\nC. should not\nD. have to\nE. unnecessary\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The criminal insisted he must do the crime to the bank teller, but she tried to convince him there were other ways in life and this was what?\nA. willing\nB. optional\nC. should not\nD. have to\nE. unnecessary\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.288379669189453", "False"]], [["-4.038379669189453", "False"]], [["-2.788379669189453", "False"]], [["-2.538379669189453", "False"]], [["-4.038379669189453", "False"]]], "filtered_resps": [["-4.288379669189453", "False"], ["-4.038379669189453", "False"], ["-2.788379669189453", "False"], ["-2.538379669189453", "False"], ["-4.038379669189453", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "58e8f86902735799f667b40cc5c21b70d383141934a7c9325ddb027080978eb5", "prompt_hash": "cbb523da981eeaf818ac593fdf7cce76e1ad767e3f0ccd7ae2d874916a4d56a8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 442, "doc": {"id": "ec86900559a0faf2aef066e511a4cfa6", "question": "what do you fill with ink to write?", "question_concept": "ink", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["squid", "fountain pen", "pencil case", "newspaper", "printer"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: what do you fill with ink to write?\nA. squid\nB. fountain pen\nC. pencil case\nD. newspaper\nE. printer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: what do you fill with ink to write?\nA. squid\nB. fountain pen\nC. pencil case\nD. newspaper\nE. printer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: what do you fill with ink to write?\nA. squid\nB. fountain pen\nC. pencil case\nD. newspaper\nE. printer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: what do you fill with ink to write?\nA. squid\nB. fountain pen\nC. pencil case\nD. newspaper\nE. printer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: what do you fill with ink to write?\nA. squid\nB. fountain pen\nC. pencil case\nD. newspaper\nE. printer\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5522403717041016", "False"]], [["-0.8022403120994568", "True"]], [["-8.552240371704102", "False"]], [["-9.052240371704102", "False"]], [["-8.552240371704102", "False"]]], "filtered_resps": [["-2.5522403717041016", "False"], ["-0.8022403120994568", "True"], ["-8.552240371704102", "False"], ["-9.052240371704102", "False"], ["-8.552240371704102", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "20d832d06e2a23a2866f5252301e0b5ba9789f7a25aa9ec1ebaa65866f709c9f", "prompt_hash": "e3c385dd86393a4eba647c65322c2de16b6dd50b849034b1d4fdd95159a09a55", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 443, "doc": {"id": "d312741df1b14bcbe358f4f30aff3994", "question": "He walked into the room and had a great shock, his friends had what him?", "question_concept": "shock", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["expected", "wanting", "calm", "thundershock", "surprised"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: He walked into the room and had a great shock, his friends had what him?\nA. expected\nB. wanting\nC. calm\nD. thundershock\nE. surprised\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He walked into the room and had a great shock, his friends had what him?\nA. expected\nB. wanting\nC. calm\nD. thundershock\nE. surprised\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He walked into the room and had a great shock, his friends had what him?\nA. expected\nB. wanting\nC. calm\nD. thundershock\nE. surprised\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He walked into the room and had a great shock, his friends had what him?\nA. expected\nB. wanting\nC. calm\nD. thundershock\nE. surprised\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He walked into the room and had a great shock, his friends had what him?\nA. expected\nB. wanting\nC. calm\nD. thundershock\nE. surprised\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3885443210601807", "False"]], [["-5.638544082641602", "False"]], [["-7.388544082641602", "False"]], [["-5.388544082641602", "False"]], [["-1.1385443210601807", "True"]]], "filtered_resps": [["-2.3885443210601807", "False"], ["-5.638544082641602", "False"], ["-7.388544082641602", "False"], ["-5.388544082641602", "False"], ["-1.1385443210601807", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f9048187f70c989a756387c3deda1726e2a8db7b28305bd0408c46ee6f1d3c39", "prompt_hash": "b48e46b8f4731a09806017eb3325a5a266ab060f19a492c474323d95841fdffd", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 444, "doc": {"id": "0df3f58645b4bc306093845fb297a50e", "question": "He wasn't the hugging type, even when he meet friend he'd just do what?", "question_concept": "meet friend", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have sex", "smile", "hug each other", "conversation", "handshake"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: He wasn't the hugging type, even when he meet friend he'd just do what?\nA. have sex\nB. smile\nC. hug each other\nD. conversation\nE. handshake\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He wasn't the hugging type, even when he meet friend he'd just do what?\nA. have sex\nB. smile\nC. hug each other\nD. conversation\nE. handshake\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He wasn't the hugging type, even when he meet friend he'd just do what?\nA. have sex\nB. smile\nC. hug each other\nD. conversation\nE. handshake\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He wasn't the hugging type, even when he meet friend he'd just do what?\nA. have sex\nB. smile\nC. hug each other\nD. conversation\nE. handshake\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He wasn't the hugging type, even when he meet friend he'd just do what?\nA. have sex\nB. smile\nC. hug each other\nD. conversation\nE. handshake\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.805530309677124", "False"]], [["-2.055530309677124", "False"]], [["-5.305530548095703", "False"]], [["-4.805530548095703", "False"]], [["-1.055530309677124", "True"]]], "filtered_resps": [["-3.805530309677124", "False"], ["-2.055530309677124", "False"], ["-5.305530548095703", "False"], ["-4.805530548095703", "False"], ["-1.055530309677124", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c6d9e8de61671f8806581287c4e12bed95d42b8be2619567fe3ba9c8c6f65195", "prompt_hash": "b54f26f94a085f56fae08c7113e8cfeaba4e3030069f64e27aeb87de3d178c26", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 445, "doc": {"id": "27d9b4df2ca50112d282331df4923e96", "question": "If you were lost you might need a map, the best place to find one on the road is at any what?", "question_concept": "map", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["truck stop", "amusement park", "atlas", "mall", "gas station"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If you were lost you might need a map, the best place to find one on the road is at any what?\nA. truck stop\nB. amusement park\nC. atlas\nD. mall\nE. gas station\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you were lost you might need a map, the best place to find one on the road is at any what?\nA. truck stop\nB. amusement park\nC. atlas\nD. mall\nE. gas station\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you were lost you might need a map, the best place to find one on the road is at any what?\nA. truck stop\nB. amusement park\nC. atlas\nD. mall\nE. gas station\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you were lost you might need a map, the best place to find one on the road is at any what?\nA. truck stop\nB. amusement park\nC. atlas\nD. mall\nE. gas station\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you were lost you might need a map, the best place to find one on the road is at any what?\nA. truck stop\nB. amusement park\nC. atlas\nD. mall\nE. gas station\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.137725353240967", "False"]], [["-6.387725353240967", "False"]], [["-3.637725353240967", "False"]], [["-6.637725353240967", "False"]], [["-1.3877252340316772", "True"]]], "filtered_resps": [["-2.137725353240967", "False"], ["-6.387725353240967", "False"], ["-3.637725353240967", "False"], ["-6.637725353240967", "False"], ["-1.3877252340316772", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "731f8398d5ba220be2c61d8d409c52c4cc9cb6301e7133de4f28992fcff5dcf6", "prompt_hash": "a8ac06320c150525e777f9dd895a33976c31f3f41c2e9a0247195456dbe4b5c8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 446, "doc": {"id": "ab755203f41a2e241f0ee8a53c54f287", "question": "Where would you put a net if you wanted to use it?", "question_concept": "net", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sports", "fishing gear", "soccer game", "fishing boat", "badminton"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put a net if you wanted to use it?\nA. sports\nB. fishing gear\nC. soccer game\nD. fishing boat\nE. badminton\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put a net if you wanted to use it?\nA. sports\nB. fishing gear\nC. soccer game\nD. fishing boat\nE. badminton\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put a net if you wanted to use it?\nA. sports\nB. fishing gear\nC. soccer game\nD. fishing boat\nE. badminton\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put a net if you wanted to use it?\nA. sports\nB. fishing gear\nC. soccer game\nD. fishing boat\nE. badminton\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put a net if you wanted to use it?\nA. sports\nB. fishing gear\nC. soccer game\nD. fishing boat\nE. badminton\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.898226499557495", "False"]], [["-1.3982264995574951", "True"]], [["-4.398226737976074", "False"]], [["-2.398226499557495", "False"]], [["-8.148226737976074", "False"]]], "filtered_resps": [["-2.898226499557495", "False"], ["-1.3982264995574951", "True"], ["-4.398226737976074", "False"], ["-2.398226499557495", "False"], ["-8.148226737976074", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e870aecb308604e00cc891c963448ad7510b93afef6252a01c54d84230556f52", "prompt_hash": "5cbbf33133db8c2a95c7884235c7ec3cd31f3987655dfe21c599f525c04476d5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 447, "doc": {"id": "f13efb91090dd28fd2b3c1f4dde680fd", "question": "Sage loved communicating  He liked doing what with his peers?", "question_concept": "communicating", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["exchanging ideas", "confusion", "peer pressure", "response", "learning"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Sage loved communicating  He liked doing what with his peers?\nA. exchanging ideas\nB. confusion\nC. peer pressure\nD. response\nE. learning\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sage loved communicating  He liked doing what with his peers?\nA. exchanging ideas\nB. confusion\nC. peer pressure\nD. response\nE. learning\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sage loved communicating  He liked doing what with his peers?\nA. exchanging ideas\nB. confusion\nC. peer pressure\nD. response\nE. learning\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sage loved communicating  He liked doing what with his peers?\nA. exchanging ideas\nB. confusion\nC. peer pressure\nD. response\nE. learning\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sage loved communicating  He liked doing what with his peers?\nA. exchanging ideas\nB. confusion\nC. peer pressure\nD. response\nE. learning\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5381978750228882", "True"]], [["-7.038197994232178", "False"]], [["-7.788197994232178", "False"]], [["-7.038197994232178", "False"]], [["-5.038197994232178", "False"]]], "filtered_resps": [["-0.5381978750228882", "True"], ["-7.038197994232178", "False"], ["-7.788197994232178", "False"], ["-7.038197994232178", "False"], ["-5.038197994232178", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c3c469d1ee26ac221ab178b0e0f2cd363182bcd28892bfbbf096e62e3144a9da", "prompt_hash": "dcb638726a5eaced32e2bb3d060791e096b7a9cdd9da9b39977cadb1c6b61577", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 448, "doc": {"id": "e98031901c815e55040d9fe28c4d9387", "question": "Where would a cat snuggle up with their human?", "question_concept": "cat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["floor", "humane society", "bed", "comfortable chair", "window sill"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would a cat snuggle up with their human?\nA. floor\nB. humane society\nC. bed\nD. comfortable chair\nE. window sill\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would a cat snuggle up with their human?\nA. floor\nB. humane society\nC. bed\nD. comfortable chair\nE. window sill\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would a cat snuggle up with their human?\nA. floor\nB. humane society\nC. bed\nD. comfortable chair\nE. window sill\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would a cat snuggle up with their human?\nA. floor\nB. humane society\nC. bed\nD. comfortable chair\nE. window sill\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would a cat snuggle up with their human?\nA. floor\nB. humane society\nC. bed\nD. comfortable chair\nE. window sill\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.0561461448669434", "False"]], [["-6.556146144866943", "False"]], [["-0.8061460852622986", "True"]], [["-5.556146144866943", "False"]], [["-8.806145668029785", "False"]]], "filtered_resps": [["-2.0561461448669434", "False"], ["-6.556146144866943", "False"], ["-0.8061460852622986", "True"], ["-5.556146144866943", "False"], ["-8.806145668029785", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ab59d94d1f3bda597b734b6908ba2096e35ddf9f327613d8054f53373ace5ed4", "prompt_hash": "8ef7629072619843dffda6a4070e441a8dbe922feb1855d258bd7284964a1c5d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 449, "doc": {"id": "fb64149cf01c5b496d986f56852273e9", "question": "What is a place that has large cable hanging overhead?", "question_concept": "cable", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["radio shack", "electrical device", "shower", "substation", "television"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is a place that has large cable hanging overhead?\nA. radio shack\nB. electrical device\nC. shower\nD. substation\nE. television\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a place that has large cable hanging overhead?\nA. radio shack\nB. electrical device\nC. shower\nD. substation\nE. television\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a place that has large cable hanging overhead?\nA. radio shack\nB. electrical device\nC. shower\nD. substation\nE. television\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a place that has large cable hanging overhead?\nA. radio shack\nB. electrical device\nC. shower\nD. substation\nE. television\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a place that has large cable hanging overhead?\nA. radio shack\nB. electrical device\nC. shower\nD. substation\nE. television\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8964556455612183", "False"]], [["-6.646455764770508", "False"]], [["-7.646455764770508", "False"]], [["-1.6464556455612183", "False"]], [["-8.146455764770508", "False"]]], "filtered_resps": [["-1.8964556455612183", "False"], ["-6.646455764770508", "False"], ["-7.646455764770508", "False"], ["-1.6464556455612183", "False"], ["-8.146455764770508", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6e23fba908e639a25d878205142b0d36c1dbcfa0f52d96c9b04771e6da86385c", "prompt_hash": "27181c14cbd2c516e70c3a5ecc2959a2bc8e0d92776cc91fc80d7bfa6eb2d3b9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 450, "doc": {"id": "2ac72eaf30a633c410b1bd658bbef0ba", "question": "Where do cars usually travel at very high speeds?", "question_concept": "car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["freeway", "road", "race track", "alley", "parking lot"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where do cars usually travel at very high speeds?\nA. freeway\nB. road\nC. race track\nD. alley\nE. parking lot\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do cars usually travel at very high speeds?\nA. freeway\nB. road\nC. race track\nD. alley\nE. parking lot\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do cars usually travel at very high speeds?\nA. freeway\nB. road\nC. race track\nD. alley\nE. parking lot\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do cars usually travel at very high speeds?\nA. freeway\nB. road\nC. race track\nD. alley\nE. parking lot\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do cars usually travel at very high speeds?\nA. freeway\nB. road\nC. race track\nD. alley\nE. parking lot\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.862292289733887", "False"]], [["-8.112292289733887", "False"]], [["-0.8622921705245972", "True"]], [["-8.612292289733887", "False"]], [["-9.862292289733887", "False"]]], "filtered_resps": [["-4.862292289733887", "False"], ["-8.112292289733887", "False"], ["-0.8622921705245972", "True"], ["-8.612292289733887", "False"], ["-9.862292289733887", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6d7f4af4913f1074b0b5fd1d21f5a511a68c8c3ed54704253ce853d9d910b482", "prompt_hash": "b4be3036a799221938e7a1ba4d2bc42cf094742abc5361f4db19f3da51badc43", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 451, "doc": {"id": "22fc45d9e6d0baea4a5b0526504225b8", "question": "What might a person be watching if they see a man with a suitcase full of money?", "question_concept": "suitcase", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["baggage compartment", "movie", "subway", "airplane", "cargo hold"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What might a person be watching if they see a man with a suitcase full of money?\nA. baggage compartment\nB. movie\nC. subway\nD. airplane\nE. cargo hold\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might a person be watching if they see a man with a suitcase full of money?\nA. baggage compartment\nB. movie\nC. subway\nD. airplane\nE. cargo hold\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might a person be watching if they see a man with a suitcase full of money?\nA. baggage compartment\nB. movie\nC. subway\nD. airplane\nE. cargo hold\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might a person be watching if they see a man with a suitcase full of money?\nA. baggage compartment\nB. movie\nC. subway\nD. airplane\nE. cargo hold\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might a person be watching if they see a man with a suitcase full of money?\nA. baggage compartment\nB. movie\nC. subway\nD. airplane\nE. cargo hold\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.72223961353302", "True"]], [["-2.2222394943237305", "False"]], [["-4.4722394943237305", "False"]], [["-2.4722394943237305", "False"]], [["-4.9722394943237305", "False"]]], "filtered_resps": [["-1.72223961353302", "True"], ["-2.2222394943237305", "False"], ["-4.4722394943237305", "False"], ["-2.4722394943237305", "False"], ["-4.9722394943237305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c9e4f332b7034168c667435f2163f9f167404db460679b344d7beda929a47793", "prompt_hash": "85a61ef54119362b952eb944888e83728d638b1899332de58ef3c08d56dc2bc0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 452, "doc": {"id": "4ef3d70648ee3cea028bc5ed0fdfda28", "question": "Eating breakfast in bed while seeing a homeless person shivering outside your window may cause you to what?", "question_concept": "eating breakfast in bed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mess", "hungry", "feel guilty", "indigestion", "spills"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Eating breakfast in bed while seeing a homeless person shivering outside your window may cause you to what?\nA. mess\nB. hungry\nC. feel guilty\nD. indigestion\nE. spills\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Eating breakfast in bed while seeing a homeless person shivering outside your window may cause you to what?\nA. mess\nB. hungry\nC. feel guilty\nD. indigestion\nE. spills\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Eating breakfast in bed while seeing a homeless person shivering outside your window may cause you to what?\nA. mess\nB. hungry\nC. feel guilty\nD. indigestion\nE. spills\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Eating breakfast in bed while seeing a homeless person shivering outside your window may cause you to what?\nA. mess\nB. hungry\nC. feel guilty\nD. indigestion\nE. spills\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Eating breakfast in bed while seeing a homeless person shivering outside your window may cause you to what?\nA. mess\nB. hungry\nC. feel guilty\nD. indigestion\nE. spills\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.150888919830322", "False"]], [["-6.400888919830322", "False"]], [["-0.9008890390396118", "True"]], [["-8.40088939666748", "False"]], [["-2.6508889198303223", "False"]]], "filtered_resps": [["-5.150888919830322", "False"], ["-6.400888919830322", "False"], ["-0.9008890390396118", "True"], ["-8.40088939666748", "False"], ["-2.6508889198303223", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "503edb7e5d8e188be754e9b2d8a21a2bb39ab35aa4d5c848f544a7bab8267488", "prompt_hash": "e9503f914fba3ea534749b019f6c5e87747c524a4fb96941a58ee77f62461b4b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 453, "doc": {"id": "059155c50d1b04da7373e309868e67d2", "question": "If I put in my key and open a hinged door, where am I likely entering?", "question_concept": "hinged door", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kitchen", "safe", "own house", "building", "pantry"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If I put in my key and open a hinged door, where am I likely entering?\nA. kitchen\nB. safe\nC. own house\nD. building\nE. pantry\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I put in my key and open a hinged door, where am I likely entering?\nA. kitchen\nB. safe\nC. own house\nD. building\nE. pantry\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I put in my key and open a hinged door, where am I likely entering?\nA. kitchen\nB. safe\nC. own house\nD. building\nE. pantry\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I put in my key and open a hinged door, where am I likely entering?\nA. kitchen\nB. safe\nC. own house\nD. building\nE. pantry\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I put in my key and open a hinged door, where am I likely entering?\nA. kitchen\nB. safe\nC. own house\nD. building\nE. pantry\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.305731296539307", "False"]], [["-4.055731296539307", "False"]], [["-1.5557314157485962", "True"]], [["-2.8057312965393066", "False"]], [["-8.555731773376465", "False"]]], "filtered_resps": [["-4.305731296539307", "False"], ["-4.055731296539307", "False"], ["-1.5557314157485962", "True"], ["-2.8057312965393066", "False"], ["-8.555731773376465", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "396f09fb9b7dede47423242ca5f590b0d44ff39773c8eba80ac0ed18f33b9552", "prompt_hash": "010ce40c31b7de0b8d03531d550af8ae9883438a62ac140cc00afc858fe20edf", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 454, "doc": {"id": "33d023a6806390eb8195380331e17404_1", "question": "If somebody is working at a reception desk, they are located at the front entrance of the what?", "question_concept": "reception desk", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["motel", "hostel", "building", "lobby", "office park"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If somebody is working at a reception desk, they are located at the front entrance of the what?\nA. motel\nB. hostel\nC. building\nD. lobby\nE. office park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If somebody is working at a reception desk, they are located at the front entrance of the what?\nA. motel\nB. hostel\nC. building\nD. lobby\nE. office park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If somebody is working at a reception desk, they are located at the front entrance of the what?\nA. motel\nB. hostel\nC. building\nD. lobby\nE. office park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If somebody is working at a reception desk, they are located at the front entrance of the what?\nA. motel\nB. hostel\nC. building\nD. lobby\nE. office park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If somebody is working at a reception desk, they are located at the front entrance of the what?\nA. motel\nB. hostel\nC. building\nD. lobby\nE. office park\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.295931339263916", "False"]], [["-6.795931339263916", "False"]], [["-3.045931339263916", "False"]], [["-1.545931339263916", "True"]], [["-9.045930862426758", "False"]]], "filtered_resps": [["-3.295931339263916", "False"], ["-6.795931339263916", "False"], ["-3.045931339263916", "False"], ["-1.545931339263916", "True"], ["-9.045930862426758", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9e7612129581eed6331d38a3d226039a603a2a57b3b75afc03a12744923ac491", "prompt_hash": "067cdfc1b24e018ad42a610950b6a64c7603dcb3c16029bf36e16cffc07d085e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 455, "doc": {"id": "63f7ad481a63fc8c6dffe00519d4a167", "question": "If you're reading a newspaper from another country what are you doing?", "question_concept": "reading newspaper", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["learning about world", "education", "get angry", "concern", "eat cake"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If you're reading a newspaper from another country what are you doing?\nA. learning about world\nB. education\nC. get angry\nD. concern\nE. eat cake\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're reading a newspaper from another country what are you doing?\nA. learning about world\nB. education\nC. get angry\nD. concern\nE. eat cake\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're reading a newspaper from another country what are you doing?\nA. learning about world\nB. education\nC. get angry\nD. concern\nE. eat cake\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're reading a newspaper from another country what are you doing?\nA. learning about world\nB. education\nC. get angry\nD. concern\nE. eat cake\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're reading a newspaper from another country what are you doing?\nA. learning about world\nB. education\nC. get angry\nD. concern\nE. eat cake\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6326870918273926", "True"]], [["-4.882687091827393", "False"]], [["-6.882687091827393", "False"]], [["-7.382687091827393", "False"]], [["-7.382687091827393", "False"]]], "filtered_resps": [["-0.6326870918273926", "True"], ["-4.882687091827393", "False"], ["-6.882687091827393", "False"], ["-7.382687091827393", "False"], ["-7.382687091827393", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "83365eb1239cfd4c459604cbf0d92cadc82c9aa0ea63eae416bfd227dc7cbdeb", "prompt_hash": "f0f9e32b2006156dad29d226650a883d2f137c88f01113c07638237aa84451bc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 456, "doc": {"id": "a2daf73d33541af0846673afd8e49abe", "question": "They wanted to recognize his accomplishment, where should they put his name?", "question_concept": "name", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["certificate", "directory", "phone book", "lineup", "roster"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: They wanted to recognize his accomplishment, where should they put his name?\nA. certificate\nB. directory\nC. phone book\nD. lineup\nE. roster\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They wanted to recognize his accomplishment, where should they put his name?\nA. certificate\nB. directory\nC. phone book\nD. lineup\nE. roster\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They wanted to recognize his accomplishment, where should they put his name?\nA. certificate\nB. directory\nC. phone book\nD. lineup\nE. roster\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They wanted to recognize his accomplishment, where should they put his name?\nA. certificate\nB. directory\nC. phone book\nD. lineup\nE. roster\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They wanted to recognize his accomplishment, where should they put his name?\nA. certificate\nB. directory\nC. phone book\nD. lineup\nE. roster\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.694785475730896", "False"]], [["-6.9447855949401855", "False"]], [["-8.444785118103027", "False"]], [["-8.694785118103027", "False"]], [["-2.6947855949401855", "False"]]], "filtered_resps": [["-1.694785475730896", "False"], ["-6.9447855949401855", "False"], ["-8.444785118103027", "False"], ["-8.694785118103027", "False"], ["-2.6947855949401855", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "021602aa355b6f8afcb5be2a49a2f595ab66b120e20a61c5826ecea3c4d7f383", "prompt_hash": "4485aef9be8463febc79a24ff78de7c11c5c67e8b1534e8d49d0601dae304482", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 457, "doc": {"id": "7d70208061ae3185bcfc9e912ee9e141", "question": "What is it called when a person tends to leave things to the last minute?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["procrastinate", "complete collection", "headache", "good time management", "have to hold"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is it called when a person tends to leave things to the last minute?\nA. procrastinate\nB. complete collection\nC. headache\nD. good time management\nE. have to hold\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is it called when a person tends to leave things to the last minute?\nA. procrastinate\nB. complete collection\nC. headache\nD. good time management\nE. have to hold\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is it called when a person tends to leave things to the last minute?\nA. procrastinate\nB. complete collection\nC. headache\nD. good time management\nE. have to hold\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is it called when a person tends to leave things to the last minute?\nA. procrastinate\nB. complete collection\nC. headache\nD. good time management\nE. have to hold\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is it called when a person tends to leave things to the last minute?\nA. procrastinate\nB. complete collection\nC. headache\nD. good time management\nE. have to hold\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4882228970527649", "True"]], [["-8.2382230758667", "False"]], [["-9.9882230758667", "False"]], [["-9.9882230758667", "False"]], [["-10.9882230758667", "False"]]], "filtered_resps": [["-0.4882228970527649", "True"], ["-8.2382230758667", "False"], ["-9.9882230758667", "False"], ["-9.9882230758667", "False"], ["-10.9882230758667", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f5de8f6cb4d54e6399a14a4ef8ba38f5b8c6bbaa8cd5894bcef3577507eb60e5", "prompt_hash": "7a8b6b8146c443d463e019a93ca7ad8dbc3ad2691a1cdd01b2e7a650f2f4eeca", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 458, "doc": {"id": "9003c4748b08d5a734747e499599ff20", "question": "What will you do if you do not want to settle in one place?", "question_concept": "settle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["walk", "agitate", "wander", "remove", "disturb"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What will you do if you do not want to settle in one place?\nA. walk\nB. agitate\nC. wander\nD. remove\nE. disturb\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will you do if you do not want to settle in one place?\nA. walk\nB. agitate\nC. wander\nD. remove\nE. disturb\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will you do if you do not want to settle in one place?\nA. walk\nB. agitate\nC. wander\nD. remove\nE. disturb\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will you do if you do not want to settle in one place?\nA. walk\nB. agitate\nC. wander\nD. remove\nE. disturb\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will you do if you do not want to settle in one place?\nA. walk\nB. agitate\nC. wander\nD. remove\nE. disturb\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.545353889465332", "False"]], [["-7.795353889465332", "False"]], [["-1.545353889465332", "False"]], [["-7.045353889465332", "False"]], [["-7.795353889465332", "False"]]], "filtered_resps": [["-5.545353889465332", "False"], ["-7.795353889465332", "False"], ["-1.545353889465332", "False"], ["-7.045353889465332", "False"], ["-7.795353889465332", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "85250db2dfd20f7b3d481d711ee345b509a763babe7e8e98fc13139cd608ef05", "prompt_hash": "5d1fdcf3c9a134bffc386d65370010e64fe67ad29eb941a9f9de48ae44165afb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 459, "doc": {"id": "28aac6d39cdd270d2a6a28e1985484cb", "question": "Where would a person live that isn't in the metro area but still has good schools?", "question_concept": "bungalow", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["woods", "bed", "suburbs", "rural", "neighborhood"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would a person live that isn't in the metro area but still has good schools?\nA. woods\nB. bed\nC. suburbs\nD. rural\nE. neighborhood\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would a person live that isn't in the metro area but still has good schools?\nA. woods\nB. bed\nC. suburbs\nD. rural\nE. neighborhood\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would a person live that isn't in the metro area but still has good schools?\nA. woods\nB. bed\nC. suburbs\nD. rural\nE. neighborhood\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would a person live that isn't in the metro area but still has good schools?\nA. woods\nB. bed\nC. suburbs\nD. rural\nE. neighborhood\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would a person live that isn't in the metro area but still has good schools?\nA. woods\nB. bed\nC. suburbs\nD. rural\nE. neighborhood\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7182013988494873", "False"]], [["-8.718201637268066", "False"]], [["-1.2182013988494873", "True"]], [["-8.218201637268066", "False"]], [["-9.718201637268066", "False"]]], "filtered_resps": [["-2.7182013988494873", "False"], ["-8.718201637268066", "False"], ["-1.2182013988494873", "True"], ["-8.218201637268066", "False"], ["-9.718201637268066", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "391d8d35bcb41c4f9bbc2ea8e30155623bae7827e3f3d4e2d48cc467412fa478", "prompt_hash": "3c9b9eddd23536e5b6d6cbe09ecece86ad221d914bf98bf5f26f80f1e0f5a0d3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 460, "doc": {"id": "8bdbb8caefcc607a9ec7579aa0c87cba", "question": "Jane works for the government as a senator, where does she spend a lot of time?", "question_concept": "government", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["capitol building", "everything", "washington d.c", "russia", "canada"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Jane works for the government as a senator, where does she spend a lot of time?\nA. capitol building\nB. everything\nC. washington d.c\nD. russia\nE. canada\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Jane works for the government as a senator, where does she spend a lot of time?\nA. capitol building\nB. everything\nC. washington d.c\nD. russia\nE. canada\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Jane works for the government as a senator, where does she spend a lot of time?\nA. capitol building\nB. everything\nC. washington d.c\nD. russia\nE. canada\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Jane works for the government as a senator, where does she spend a lot of time?\nA. capitol building\nB. everything\nC. washington d.c\nD. russia\nE. canada\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Jane works for the government as a senator, where does she spend a lot of time?\nA. capitol building\nB. everything\nC. washington d.c\nD. russia\nE. canada\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2081365585327148", "True"]], [["-5.708136558532715", "False"]], [["-2.458136558532715", "False"]], [["-7.208136558532715", "False"]], [["-10.208136558532715", "False"]]], "filtered_resps": [["-1.2081365585327148", "True"], ["-5.708136558532715", "False"], ["-2.458136558532715", "False"], ["-7.208136558532715", "False"], ["-10.208136558532715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9a6a3e622e1cfbcaae3d4788a22453077c2eab6f5b304e81f2db834bee47ae16", "prompt_hash": "1e27afe714cfadca08bfad7a11463723e89e646c60e048628e92c0d84a2f2781", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 461, "doc": {"id": "95a85df48902d23eb3fda25a99fca1a0", "question": "What is it called when two people in love have children?", "question_concept": "love", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["take oath", "procreate", "matrimony", "please parents", "live life"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is it called when two people in love have children?\nA. take oath\nB. procreate\nC. matrimony\nD. please parents\nE. live life\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is it called when two people in love have children?\nA. take oath\nB. procreate\nC. matrimony\nD. please parents\nE. live life\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is it called when two people in love have children?\nA. take oath\nB. procreate\nC. matrimony\nD. please parents\nE. live life\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is it called when two people in love have children?\nA. take oath\nB. procreate\nC. matrimony\nD. please parents\nE. live life\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is it called when two people in love have children?\nA. take oath\nB. procreate\nC. matrimony\nD. please parents\nE. live life\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.5984649658203125", "False"]], [["-1.3484652042388916", "False"]], [["-5.3484649658203125", "False"]], [["-8.848464965820312", "False"]], [["-9.848464965820312", "False"]]], "filtered_resps": [["-6.5984649658203125", "False"], ["-1.3484652042388916", "False"], ["-5.3484649658203125", "False"], ["-8.848464965820312", "False"], ["-9.848464965820312", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e5c38fef5b065a5505a5520676be277032649256728d814cecc4c33683313f53", "prompt_hash": "56bc01494e2bd6d9c144ad99374be3deb59a6ef4812dd08e6e7facf25f84a274", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 462, "doc": {"id": "79c3378b7660d328902d7c0ad442a37f", "question": "What did the policemen do when they heard a cry from a distance?", "question_concept": "policemen", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["run away", "hurry along", "fine motorists", "direct traffic", "help"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What did the policemen do when they heard a cry from a distance?\nA. run away\nB. hurry along\nC. fine motorists\nD. direct traffic\nE. help\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What did the policemen do when they heard a cry from a distance?\nA. run away\nB. hurry along\nC. fine motorists\nD. direct traffic\nE. help\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What did the policemen do when they heard a cry from a distance?\nA. run away\nB. hurry along\nC. fine motorists\nD. direct traffic\nE. help\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What did the policemen do when they heard a cry from a distance?\nA. run away\nB. hurry along\nC. fine motorists\nD. direct traffic\nE. help\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What did the policemen do when they heard a cry from a distance?\nA. run away\nB. hurry along\nC. fine motorists\nD. direct traffic\nE. help\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.433775424957275", "False"]], [["-2.1837754249572754", "False"]], [["-6.933775424957275", "False"]], [["-6.183775424957275", "False"]], [["-1.6837754249572754", "True"]]], "filtered_resps": [["-5.433775424957275", "False"], ["-2.1837754249572754", "False"], ["-6.933775424957275", "False"], ["-6.183775424957275", "False"], ["-1.6837754249572754", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8f645fb14c4905d1af7a8a7bdd568f592025025d5d29ff5330d087cba00a2da1", "prompt_hash": "3f00b6f3cd3a15623ed02589093283a56a8aa49d0a92337613bf5169de1f0fc7", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 463, "doc": {"id": "8c12e5864463cfcd03f4d0ab67949d01", "question": "It takes ambition to complete a job, but the first step is to what?", "question_concept": "ambition", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["take care of proposals", "begin work", "in charge of project", "eat cake", "go to school"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: It takes ambition to complete a job, but the first step is to what?\nA. take care of proposals\nB. begin work\nC. in charge of project\nD. eat cake\nE. go to school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: It takes ambition to complete a job, but the first step is to what?\nA. take care of proposals\nB. begin work\nC. in charge of project\nD. eat cake\nE. go to school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: It takes ambition to complete a job, but the first step is to what?\nA. take care of proposals\nB. begin work\nC. in charge of project\nD. eat cake\nE. go to school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: It takes ambition to complete a job, but the first step is to what?\nA. take care of proposals\nB. begin work\nC. in charge of project\nD. eat cake\nE. go to school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: It takes ambition to complete a job, but the first step is to what?\nA. take care of proposals\nB. begin work\nC. in charge of project\nD. eat cake\nE. go to school\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.079000949859619", "False"]], [["-0.8290008306503296", "True"]], [["-8.079000473022461", "False"]], [["-8.579000473022461", "False"]], [["-7.579000949859619", "False"]]], "filtered_resps": [["-5.079000949859619", "False"], ["-0.8290008306503296", "True"], ["-8.079000473022461", "False"], ["-8.579000473022461", "False"], ["-7.579000949859619", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2baf8e41d443ece0bf68150d9393efc584d092ff57c36f3b408ec75265930eb2", "prompt_hash": "edf35c1ed4d20181b117d23fd3fad76496f62872e25240d234d11ea352254e74", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 464, "doc": {"id": "e145618c2062eb9ea8928fdb0d42185e", "question": "Where would I not want a fox?", "question_concept": "fox", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hen house", "english hunt", "mountains", "outside bedroom window", "england"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would I not want a fox?\nA. hen house\nB. english hunt\nC. mountains\nD. outside bedroom window\nE. england\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would I not want a fox?\nA. hen house\nB. english hunt\nC. mountains\nD. outside bedroom window\nE. england\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would I not want a fox?\nA. hen house\nB. english hunt\nC. mountains\nD. outside bedroom window\nE. england\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would I not want a fox?\nA. hen house\nB. english hunt\nC. mountains\nD. outside bedroom window\nE. england\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would I not want a fox?\nA. hen house\nB. english hunt\nC. mountains\nD. outside bedroom window\nE. england\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.034059762954712", "True"]], [["-5.034059524536133", "False"]], [["-5.784059524536133", "False"]], [["-2.284059762954712", "False"]], [["-5.784059524536133", "False"]]], "filtered_resps": [["-1.034059762954712", "True"], ["-5.034059524536133", "False"], ["-5.784059524536133", "False"], ["-2.284059762954712", "False"], ["-5.784059524536133", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "89eb45d354b7a14151a984c3832d0c9f6f2c445ba16139c1421b0febc76f55aa", "prompt_hash": "d3cac99ddc69c68228b06c8b5cdc81feb2858ed6b76f5496450f8c97e8aeedd5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 465, "doc": {"id": "35872be88df5f6c4a6600020266a5458", "question": "What type of building has the most top floor?", "question_concept": "top floor", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["go down", "apartment building", "tall building", "office building", "cabin"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What type of building has the most top floor?\nA. go down\nB. apartment building\nC. tall building\nD. office building\nE. cabin\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What type of building has the most top floor?\nA. go down\nB. apartment building\nC. tall building\nD. office building\nE. cabin\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What type of building has the most top floor?\nA. go down\nB. apartment building\nC. tall building\nD. office building\nE. cabin\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What type of building has the most top floor?\nA. go down\nB. apartment building\nC. tall building\nD. office building\nE. cabin\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What type of building has the most top floor?\nA. go down\nB. apartment building\nC. tall building\nD. office building\nE. cabin\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2478156089782715", "False"]], [["-3.9978156089782715", "False"]], [["-1.7478156089782715", "False"]], [["-5.7478156089782715", "False"]], [["-8.497815132141113", "False"]]], "filtered_resps": [["-3.2478156089782715", "False"], ["-3.9978156089782715", "False"], ["-1.7478156089782715", "False"], ["-5.7478156089782715", "False"], ["-8.497815132141113", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2bf5147ab241a3673b925e003381638cafe91f8986ab3677938652f96b1b8138", "prompt_hash": "85cb4724117cf35f94e573eaafa785fa2df51c705b8aa0b1d53b4d38cf3e87cc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 466, "doc": {"id": "055817d8d703d3c2802545e3fccdcde3", "question": "What do humans do to other humans after death?", "question_concept": "death", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["celebrate", "burial", "life", "rebirth", "decomposition"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What do humans do to other humans after death?\nA. celebrate\nB. burial\nC. life\nD. rebirth\nE. decomposition\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do humans do to other humans after death?\nA. celebrate\nB. burial\nC. life\nD. rebirth\nE. decomposition\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do humans do to other humans after death?\nA. celebrate\nB. burial\nC. life\nD. rebirth\nE. decomposition\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do humans do to other humans after death?\nA. celebrate\nB. burial\nC. life\nD. rebirth\nE. decomposition\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do humans do to other humans after death?\nA. celebrate\nB. burial\nC. life\nD. rebirth\nE. decomposition\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.231558799743652", "False"]], [["-1.7315585613250732", "True"]], [["-6.981558799743652", "False"]], [["-7.231558799743652", "False"]], [["-5.481558799743652", "False"]]], "filtered_resps": [["-5.231558799743652", "False"], ["-1.7315585613250732", "True"], ["-6.981558799743652", "False"], ["-7.231558799743652", "False"], ["-5.481558799743652", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2cb322dec9ae017ab16df3d040b84eb64318cf6ebbb6578fd3d3ae50e09bea99", "prompt_hash": "c93bcf5e76c6ecc54a34cd35f606ad0e6f17e264bf2ca19b95674a07f0966817", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 467, "doc": {"id": "5ef6cdb85468df482e3aa6fa339d6e41", "question": "Where can you find a restaurant's phone number?", "question_concept": "restaurant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["yellow pages", "town", "business sector", "town", "at hotel"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you find a restaurant's phone number?\nA. yellow pages\nB. town\nC. business sector\nD. town\nE. at hotel\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you find a restaurant's phone number?\nA. yellow pages\nB. town\nC. business sector\nD. town\nE. at hotel\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you find a restaurant's phone number?\nA. yellow pages\nB. town\nC. business sector\nD. town\nE. at hotel\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you find a restaurant's phone number?\nA. yellow pages\nB. town\nC. business sector\nD. town\nE. at hotel\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you find a restaurant's phone number?\nA. yellow pages\nB. town\nC. business sector\nD. town\nE. at hotel\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9003794193267822", "True"]], [["-5.900379180908203", "False"]], [["-5.150379180908203", "False"]], [["-6.400379180908203", "False"]], [["-5.900379180908203", "False"]]], "filtered_resps": [["-0.9003794193267822", "True"], ["-5.900379180908203", "False"], ["-5.150379180908203", "False"], ["-6.400379180908203", "False"], ["-5.900379180908203", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b6f0c40b4aa096346df538f67215c210db3bc0616a0ece64fe0920ea13b2760a", "prompt_hash": "2f0a296444ff8fdba87a6f312c08e609ac1a0f8366751562ab1b2aeebebd4ba2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 468, "doc": {"id": "1e939cc6fef999953d692b57caab254b", "question": "What would you put coins into to make it work?", "question_concept": "coins", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stove", "water fountain", "desk", "purse", "jar"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What would you put coins into to make it work?\nA. stove\nB. water fountain\nC. desk\nD. purse\nE. jar\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you put coins into to make it work?\nA. stove\nB. water fountain\nC. desk\nD. purse\nE. jar\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you put coins into to make it work?\nA. stove\nB. water fountain\nC. desk\nD. purse\nE. jar\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you put coins into to make it work?\nA. stove\nB. water fountain\nC. desk\nD. purse\nE. jar\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you put coins into to make it work?\nA. stove\nB. water fountain\nC. desk\nD. purse\nE. jar\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.584165334701538", "False"]], [["-2.834165334701538", "False"]], [["-6.584165573120117", "False"]], [["-3.834165334701538", "False"]], [["-0.8341653347015381", "True"]]], "filtered_resps": [["-3.584165334701538", "False"], ["-2.834165334701538", "False"], ["-6.584165573120117", "False"], ["-3.834165334701538", "False"], ["-0.8341653347015381", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a10b0dc111a47b9682376c9fe4cff046c453b523257f4a22d6100066e5fa36c4", "prompt_hash": "51ee5ab53ab5763bdab2f10322ff059c4ae575acae415ff9fd98bcf058f06cae", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 469, "doc": {"id": "3a3b5d4a517ef70d25eb558f1a622937", "question": "A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?", "question_concept": "bald eagle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["city", "canada", "minnesota", "thermal", "photograph"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nA. city\nB. canada\nC. minnesota\nD. thermal\nE. photograph\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nA. city\nB. canada\nC. minnesota\nD. thermal\nE. photograph\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nA. city\nB. canada\nC. minnesota\nD. thermal\nE. photograph\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nA. city\nB. canada\nC. minnesota\nD. thermal\nE. photograph\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nA. city\nB. canada\nC. minnesota\nD. thermal\nE. photograph\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.844895362854004", "False"]], [["-6.594895362854004", "False"]], [["-6.844895362854004", "False"]], [["-4.844895362854004", "False"]], [["-0.8448954820632935", "True"]]], "filtered_resps": [["-2.844895362854004", "False"], ["-6.594895362854004", "False"], ["-6.844895362854004", "False"], ["-4.844895362854004", "False"], ["-0.8448954820632935", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e67a3b1ec062b529598dac6cc30a66b714b5b0f3c8edb06ff767f0406b3d4d47", "prompt_hash": "aa0b6d8281df6c848856f6837f2ca83ae1527c42afd99df7caad179293da964f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 470, "doc": {"id": "a943522f7d407cef369d5d3f1bf48589", "question": "Where can you go to use a piano in your neighborhood if you don't have one?", "question_concept": "piano", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["music school", "music store", "neighbor's house", "lunch", "drawing room"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you go to use a piano in your neighborhood if you don't have one?\nA. music school\nB. music store\nC. neighbor's house\nD. lunch\nE. drawing room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you go to use a piano in your neighborhood if you don't have one?\nA. music school\nB. music store\nC. neighbor's house\nD. lunch\nE. drawing room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you go to use a piano in your neighborhood if you don't have one?\nA. music school\nB. music store\nC. neighbor's house\nD. lunch\nE. drawing room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you go to use a piano in your neighborhood if you don't have one?\nA. music school\nB. music store\nC. neighbor's house\nD. lunch\nE. drawing room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you go to use a piano in your neighborhood if you don't have one?\nA. music school\nB. music store\nC. neighbor's house\nD. lunch\nE. drawing room\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8467031717300415", "True"]], [["-4.596703052520752", "False"]], [["-2.096703052520752", "False"]], [["-6.596703052520752", "False"]], [["-7.846703052520752", "False"]]], "filtered_resps": [["-0.8467031717300415", "True"], ["-4.596703052520752", "False"], ["-2.096703052520752", "False"], ["-6.596703052520752", "False"], ["-7.846703052520752", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "10b1e4472c6c2d42a8d8d2937bca4991a2e42161608dc8d0720732714f399399", "prompt_hash": "0a015ed01b3755bbac63fb28b8eb4fa71374c0e70163ad5260936fc54f641162", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 471, "doc": {"id": "57a343d72031b668e5eb91868420e915", "question": "Where would you get a shower curtain if you do not have one?", "question_concept": "shower curtain", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["department store", "restaurant", "hotel", "dime store", "bathtub"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you get a shower curtain if you do not have one?\nA. department store\nB. restaurant\nC. hotel\nD. dime store\nE. bathtub\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you get a shower curtain if you do not have one?\nA. department store\nB. restaurant\nC. hotel\nD. dime store\nE. bathtub\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you get a shower curtain if you do not have one?\nA. department store\nB. restaurant\nC. hotel\nD. dime store\nE. bathtub\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you get a shower curtain if you do not have one?\nA. department store\nB. restaurant\nC. hotel\nD. dime store\nE. bathtub\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you get a shower curtain if you do not have one?\nA. department store\nB. restaurant\nC. hotel\nD. dime store\nE. bathtub\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9940958619117737", "True"]], [["-5.994095802307129", "False"]], [["-2.994095802307129", "False"]], [["-2.494095802307129", "False"]], [["-6.744095802307129", "False"]]], "filtered_resps": [["-0.9940958619117737", "True"], ["-5.994095802307129", "False"], ["-2.994095802307129", "False"], ["-2.494095802307129", "False"], ["-6.744095802307129", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "96fa12ae14ef30fee7c5e0411d210cececc982a325cf9ddf5b5ee8d67162e9ea", "prompt_hash": "52e5db5c9f26ca64ed1a64517615f0ccc25e0cc2863e90f582cf162f2767369b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 472, "doc": {"id": "c4b1a57e7880b9cb367f9c67abf5605f", "question": "Kissing is normally an activity reserved for your romantic what?", "question_concept": "kissing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["anus", "partner", "arousal", "trust", "cooperation"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Kissing is normally an activity reserved for your romantic what?\nA. anus\nB. partner\nC. arousal\nD. trust\nE. cooperation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Kissing is normally an activity reserved for your romantic what?\nA. anus\nB. partner\nC. arousal\nD. trust\nE. cooperation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Kissing is normally an activity reserved for your romantic what?\nA. anus\nB. partner\nC. arousal\nD. trust\nE. cooperation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Kissing is normally an activity reserved for your romantic what?\nA. anus\nB. partner\nC. arousal\nD. trust\nE. cooperation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Kissing is normally an activity reserved for your romantic what?\nA. anus\nB. partner\nC. arousal\nD. trust\nE. cooperation\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.961714267730713", "False"]], [["-0.9617142677307129", "True"]], [["-8.461713790893555", "False"]], [["-7.711714267730713", "False"]], [["-10.211713790893555", "False"]]], "filtered_resps": [["-6.961714267730713", "False"], ["-0.9617142677307129", "True"], ["-8.461713790893555", "False"], ["-7.711714267730713", "False"], ["-10.211713790893555", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "93f8e43950c76060c674817af1547a3e680f70bdff109b6cc8d0764e2bd1dd3c", "prompt_hash": "1cb76089b28600db3f111a1ab7b26a99a6b08185df317e6ef0270c9d8b12008a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 473, "doc": {"id": "e313d7967f72c2b880213daaaf4b7181", "question": "What does a child learn to do before school?", "question_concept": "child", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["count to ten", "state name", "dress herself", "clean room", "socialize"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What does a child learn to do before school?\nA. count to ten\nB. state name\nC. dress herself\nD. clean room\nE. socialize\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a child learn to do before school?\nA. count to ten\nB. state name\nC. dress herself\nD. clean room\nE. socialize\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a child learn to do before school?\nA. count to ten\nB. state name\nC. dress herself\nD. clean room\nE. socialize\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a child learn to do before school?\nA. count to ten\nB. state name\nC. dress herself\nD. clean room\nE. socialize\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a child learn to do before school?\nA. count to ten\nB. state name\nC. dress herself\nD. clean room\nE. socialize\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0329136848449707", "False"]], [["-2.5329136848449707", "False"]], [["-3.0329136848449707", "False"]], [["-5.532913684844971", "False"]], [["-3.0329136848449707", "False"]]], "filtered_resps": [["-3.0329136848449707", "False"], ["-2.5329136848449707", "False"], ["-3.0329136848449707", "False"], ["-5.532913684844971", "False"], ["-3.0329136848449707", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d533e09241f0f12aac2609805ca1037a2283da81cebc69f721ed00365a4354f5", "prompt_hash": "7425b7cef58c512cd6176d8743e9cc3f938a8115553bce5abd9001acd408bcef", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 474, "doc": {"id": "3c7992df7fda23bcdeacb1f1f6b73448", "question": "He was getting advice for the job interview, they told him when talking to the interviewer always make what?", "question_concept": "talking to", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get tired of", "small talk", "eye contact", "friendship", "social life"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: He was getting advice for the job interview, they told him when talking to the interviewer always make what?\nA. get tired of\nB. small talk\nC. eye contact\nD. friendship\nE. social life\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was getting advice for the job interview, they told him when talking to the interviewer always make what?\nA. get tired of\nB. small talk\nC. eye contact\nD. friendship\nE. social life\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was getting advice for the job interview, they told him when talking to the interviewer always make what?\nA. get tired of\nB. small talk\nC. eye contact\nD. friendship\nE. social life\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was getting advice for the job interview, they told him when talking to the interviewer always make what?\nA. get tired of\nB. small talk\nC. eye contact\nD. friendship\nE. social life\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was getting advice for the job interview, they told him when talking to the interviewer always make what?\nA. get tired of\nB. small talk\nC. eye contact\nD. friendship\nE. social life\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.354150772094727", "False"]], [["-3.6041507720947266", "False"]], [["-1.6041508913040161", "True"]], [["-6.604150772094727", "False"]], [["-8.104150772094727", "False"]]], "filtered_resps": [["-5.354150772094727", "False"], ["-3.6041507720947266", "False"], ["-1.6041508913040161", "True"], ["-6.604150772094727", "False"], ["-8.104150772094727", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9d2869fe84b43544b9612ad34d8c573b50861711a6f3f0ff0269736d833f1af6", "prompt_hash": "2bfc1810e825435e07318bab89f06011fe654facb01cbf7021d1e46161649906", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 475, "doc": {"id": "d6644eacdb543a60545d2eb1ac7e6dbd", "question": "According to what book did an apple tree lead to the downfall of man?", "question_concept": "apple tree", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bible", "spain", "harry potter", "new york", "woods"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: According to what book did an apple tree lead to the downfall of man?\nA. bible\nB. spain\nC. harry potter\nD. new york\nE. woods\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: According to what book did an apple tree lead to the downfall of man?\nA. bible\nB. spain\nC. harry potter\nD. new york\nE. woods\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: According to what book did an apple tree lead to the downfall of man?\nA. bible\nB. spain\nC. harry potter\nD. new york\nE. woods\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: According to what book did an apple tree lead to the downfall of man?\nA. bible\nB. spain\nC. harry potter\nD. new york\nE. woods\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: According to what book did an apple tree lead to the downfall of man?\nA. bible\nB. spain\nC. harry potter\nD. new york\nE. woods\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2331757545471191", "True"]], [["-5.983175754547119", "False"]], [["-6.483175754547119", "False"]], [["-7.233175754547119", "False"]], [["-6.983175754547119", "False"]]], "filtered_resps": [["-1.2331757545471191", "True"], ["-5.983175754547119", "False"], ["-6.483175754547119", "False"], ["-7.233175754547119", "False"], ["-6.983175754547119", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8ec591f2caee1291b8cacade172439a7068dca9e5929831e6fbab20e501437e4", "prompt_hash": "58eac8822bdd8404e9a3d68fc814cdb3e87d348bb9c969887ee00c682a20e24d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 476, "doc": {"id": "d1ad9b79f54205b6b9ac19a27f9c2be5", "question": "The neighborhood had a great sense of community, there was always a crowd at the landing of the what?", "question_concept": "landing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stairwell", "arena", "ocean", "airport", "apartment building"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The neighborhood had a great sense of community, there was always a crowd at the landing of the what?\nA. stairwell\nB. arena\nC. ocean\nD. airport\nE. apartment building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The neighborhood had a great sense of community, there was always a crowd at the landing of the what?\nA. stairwell\nB. arena\nC. ocean\nD. airport\nE. apartment building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The neighborhood had a great sense of community, there was always a crowd at the landing of the what?\nA. stairwell\nB. arena\nC. ocean\nD. airport\nE. apartment building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The neighborhood had a great sense of community, there was always a crowd at the landing of the what?\nA. stairwell\nB. arena\nC. ocean\nD. airport\nE. apartment building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The neighborhood had a great sense of community, there was always a crowd at the landing of the what?\nA. stairwell\nB. arena\nC. ocean\nD. airport\nE. apartment building\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.082625150680542", "True"]], [["-5.082625389099121", "False"]], [["-6.582625389099121", "False"]], [["-7.832625389099121", "False"]], [["-2.332625150680542", "False"]]], "filtered_resps": [["-1.082625150680542", "True"], ["-5.082625389099121", "False"], ["-6.582625389099121", "False"], ["-7.832625389099121", "False"], ["-2.332625150680542", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "001aec14c141d91eedf59c1bb7893c6f3b8f4dd0d7ade74cb5e787e20019b954", "prompt_hash": "1741fe4afacd8f24d46bfdc49aac91aeeee9617aa9fee9d5748c646cb6a21a61", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 477, "doc": {"id": "f116ee6620c0f171e5db54bc03a5f2e2", "question": "What might a kind person do?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cross street", "talk to themselves", "open doors", "throw away", "study greek"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What might a kind person do?\nA. cross street\nB. talk to themselves\nC. open doors\nD. throw away\nE. study greek\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might a kind person do?\nA. cross street\nB. talk to themselves\nC. open doors\nD. throw away\nE. study greek\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might a kind person do?\nA. cross street\nB. talk to themselves\nC. open doors\nD. throw away\nE. study greek\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might a kind person do?\nA. cross street\nB. talk to themselves\nC. open doors\nD. throw away\nE. study greek\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might a kind person do?\nA. cross street\nB. talk to themselves\nC. open doors\nD. throw away\nE. study greek\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.200119972229004", "True"]], [["-7.200119972229004", "False"]], [["-1.200119972229004", "True"]], [["-7.450119972229004", "False"]], [["-7.200119972229004", "False"]]], "filtered_resps": [["-1.200119972229004", "True"], ["-7.200119972229004", "False"], ["-1.200119972229004", "True"], ["-7.450119972229004", "False"], ["-7.200119972229004", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "03872b6dfa69d117cd80e7da5c1ae4486bb647394145de5380f816deda2d53d6", "prompt_hash": "e59e64b1981f10ac7c291027a0ed7d82a35b1c987d27fa5398dfa7eef61cb40d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 478, "doc": {"id": "ea82f9e938cbfce85fb498ce46264253", "question": "What will a person do at work?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cross street", "draw attention to themselves", "make money", "falling down", "come home"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What will a person do at work?\nA. cross street\nB. draw attention to themselves\nC. make money\nD. falling down\nE. come home\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will a person do at work?\nA. cross street\nB. draw attention to themselves\nC. make money\nD. falling down\nE. come home\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will a person do at work?\nA. cross street\nB. draw attention to themselves\nC. make money\nD. falling down\nE. come home\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will a person do at work?\nA. cross street\nB. draw attention to themselves\nC. make money\nD. falling down\nE. come home\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will a person do at work?\nA. cross street\nB. draw attention to themselves\nC. make money\nD. falling down\nE. come home\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9612998962402344", "False"]], [["-5.961299896240234", "False"]], [["-0.9612997770309448", "True"]], [["-7.711299896240234", "False"]], [["-6.711299896240234", "False"]]], "filtered_resps": [["-2.9612998962402344", "False"], ["-5.961299896240234", "False"], ["-0.9612997770309448", "True"], ["-7.711299896240234", "False"], ["-6.711299896240234", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "31c7ec91b47d4811dc266a598f9578cd4593f234a5e1652fdcef9f041389e861", "prompt_hash": "d8fc1e523c5f844b2e5b4fb04e0fb6957d2825c8897987bc1417084c7d71d77f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 479, "doc": {"id": "edbb57ac2f476679ae547f75ec2bef3e", "question": "John saw a fox running along the beach and was glad to be on the east coast.  Where might he have been?", "question_concept": "fox", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tennessee", "south carolina", "louisiana", "oklahoma", "mountains"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John saw a fox running along the beach and was glad to be on the east coast.  Where might he have been?\nA. tennessee\nB. south carolina\nC. louisiana\nD. oklahoma\nE. mountains\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John saw a fox running along the beach and was glad to be on the east coast.  Where might he have been?\nA. tennessee\nB. south carolina\nC. louisiana\nD. oklahoma\nE. mountains\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John saw a fox running along the beach and was glad to be on the east coast.  Where might he have been?\nA. tennessee\nB. south carolina\nC. louisiana\nD. oklahoma\nE. mountains\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John saw a fox running along the beach and was glad to be on the east coast.  Where might he have been?\nA. tennessee\nB. south carolina\nC. louisiana\nD. oklahoma\nE. mountains\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John saw a fox running along the beach and was glad to be on the east coast.  Where might he have been?\nA. tennessee\nB. south carolina\nC. louisiana\nD. oklahoma\nE. mountains\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.74139404296875", "False"]], [["-2.99139404296875", "False"]], [["-4.24139404296875", "False"]], [["-5.24139404296875", "False"]], [["-2.74139404296875", "False"]]], "filtered_resps": [["-3.74139404296875", "False"], ["-2.99139404296875", "False"], ["-4.24139404296875", "False"], ["-5.24139404296875", "False"], ["-2.74139404296875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fc63f8b327dfc5f587ecf337526933be8cf33268804d35e3bf2ca127c92ff8b2", "prompt_hash": "c150cf1a887aa6fed3e868b162f4419328ab5dfe0a7fbad33cb743889a416e30", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 480, "doc": {"id": "07a99d5f2ca7028febeb9f09604b36c8", "question": "Name a location where you would not want to find mice.", "question_concept": "mice", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["loft", "attic", "bell cat", "countryside", "laboratory"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Name a location where you would not want to find mice.\nA. loft\nB. attic\nC. bell cat\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Name a location where you would not want to find mice.\nA. loft\nB. attic\nC. bell cat\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Name a location where you would not want to find mice.\nA. loft\nB. attic\nC. bell cat\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Name a location where you would not want to find mice.\nA. loft\nB. attic\nC. bell cat\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Name a location where you would not want to find mice.\nA. loft\nB. attic\nC. bell cat\nD. countryside\nE. laboratory\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0579237937927246", "False"]], [["-2.0579237937927246", "True"]], [["-2.0579237937927246", "True"]], [["-5.807923793792725", "False"]], [["-2.0579237937927246", "True"]]], "filtered_resps": [["-3.0579237937927246", "False"], ["-2.0579237937927246", "True"], ["-2.0579237937927246", "True"], ["-5.807923793792725", "False"], ["-2.0579237937927246", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6368b3d28231fd9b758f30fd9bb010f909bc31a03c1b99e954bcc587f4b495d8", "prompt_hash": "a2cd4aa65e5cd57cde3881d46db78beeda89919c1240f1c7348bc38398b5f7fe", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 481, "doc": {"id": "b42ef8be1748c19fa5938de5396f8fad", "question": "The man started to learn jogging, what was he hoping to do?", "question_concept": "jogging", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["exhaustion", "getting in shape", "fitness", "injure himself", "fatigue"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The man started to learn jogging, what was he hoping to do?\nA. exhaustion\nB. getting in shape\nC. fitness\nD. injure himself\nE. fatigue\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man started to learn jogging, what was he hoping to do?\nA. exhaustion\nB. getting in shape\nC. fitness\nD. injure himself\nE. fatigue\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man started to learn jogging, what was he hoping to do?\nA. exhaustion\nB. getting in shape\nC. fitness\nD. injure himself\nE. fatigue\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man started to learn jogging, what was he hoping to do?\nA. exhaustion\nB. getting in shape\nC. fitness\nD. injure himself\nE. fatigue\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man started to learn jogging, what was he hoping to do?\nA. exhaustion\nB. getting in shape\nC. fitness\nD. injure himself\nE. fatigue\nAnswer:", "arg_1": " E"}}, "resps": [[["-7.053852558135986", "False"]], [["-1.8038525581359863", "False"]], [["-4.303852558135986", "False"]], [["-8.803852081298828", "False"]], [["-12.053852081298828", "False"]]], "filtered_resps": [["-7.053852558135986", "False"], ["-1.8038525581359863", "False"], ["-4.303852558135986", "False"], ["-8.803852081298828", "False"], ["-12.053852081298828", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4cd7fd0d17c2299a3b5dc866fdf287550dbaf07439e7708c2a124d083ccbbbfa", "prompt_hash": "b05610ef35fc468eaa28bbaf95003134c436e452d8cc57ce3a827f9df1c67874", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 482, "doc": {"id": "236691d38665d7bcdd0c9b9834252a51", "question": "Where do most people turn to get information on their phones?", "question_concept": "information", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["internet", "book", "online", "google", "manual"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where do most people turn to get information on their phones?\nA. internet\nB. book\nC. online\nD. google\nE. manual\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do most people turn to get information on their phones?\nA. internet\nB. book\nC. online\nD. google\nE. manual\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do most people turn to get information on their phones?\nA. internet\nB. book\nC. online\nD. google\nE. manual\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do most people turn to get information on their phones?\nA. internet\nB. book\nC. online\nD. google\nE. manual\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do most people turn to get information on their phones?\nA. internet\nB. book\nC. online\nD. google\nE. manual\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.004152774810791", "False"]], [["-6.504152774810791", "False"]], [["-5.254152774810791", "False"]], [["-3.504152774810791", "False"]], [["-9.004152297973633", "False"]]], "filtered_resps": [["-2.004152774810791", "False"], ["-6.504152774810791", "False"], ["-5.254152774810791", "False"], ["-3.504152774810791", "False"], ["-9.004152297973633", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cc776210c2332b3935106cbc667b3f9e674c50714f77722e6a741d80bcd96f1e", "prompt_hash": "b760babf7f6bfef2588cbad9ad9b2e47f9e7473d5ebc6a3662cea743aabf1e0d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 483, "doc": {"id": "8ef78abb86fc282ccb02bbc495f13030", "question": "What happens to a body after death?", "question_concept": "death", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rebirth", "human experience", "sadness", "decomposition", "obesity"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What happens to a body after death?\nA. rebirth\nB. human experience\nC. sadness\nD. decomposition\nE. obesity\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens to a body after death?\nA. rebirth\nB. human experience\nC. sadness\nD. decomposition\nE. obesity\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens to a body after death?\nA. rebirth\nB. human experience\nC. sadness\nD. decomposition\nE. obesity\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens to a body after death?\nA. rebirth\nB. human experience\nC. sadness\nD. decomposition\nE. obesity\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens to a body after death?\nA. rebirth\nB. human experience\nC. sadness\nD. decomposition\nE. obesity\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.800381183624268", "False"]], [["-7.050381183624268", "False"]], [["-7.800381183624268", "False"]], [["-1.5503813028335571", "False"]], [["-10.050381660461426", "False"]]], "filtered_resps": [["-4.800381183624268", "False"], ["-7.050381183624268", "False"], ["-7.800381183624268", "False"], ["-1.5503813028335571", "False"], ["-10.050381660461426", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5f8aadeca599a7c517060c864dd538d0cbe5c8fe79ec73f27d10c080f8b23e74", "prompt_hash": "891e26c23ac6023df1bfa7be5faf1d14d6326e459cfa7d7e4fb8c8457a921ea6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 484, "doc": {"id": "313d033c33ec475e04e628f87c5686bd", "question": "What type of non-vegetarian soup is one likely to find a potato?", "question_concept": "potato", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["beef stew", "own kitchen", "clam chowder", "kitchen cabinet", "pantry"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What type of non-vegetarian soup is one likely to find a potato?\nA. beef stew\nB. own kitchen\nC. clam chowder\nD. kitchen cabinet\nE. pantry\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What type of non-vegetarian soup is one likely to find a potato?\nA. beef stew\nB. own kitchen\nC. clam chowder\nD. kitchen cabinet\nE. pantry\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What type of non-vegetarian soup is one likely to find a potato?\nA. beef stew\nB. own kitchen\nC. clam chowder\nD. kitchen cabinet\nE. pantry\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What type of non-vegetarian soup is one likely to find a potato?\nA. beef stew\nB. own kitchen\nC. clam chowder\nD. kitchen cabinet\nE. pantry\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What type of non-vegetarian soup is one likely to find a potato?\nA. beef stew\nB. own kitchen\nC. clam chowder\nD. kitchen cabinet\nE. pantry\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.2136058807373047", "False"]], [["-6.213605880737305", "False"]], [["-1.4636057615280151", "True"]], [["-7.963605880737305", "False"]], [["-8.713605880737305", "False"]]], "filtered_resps": [["-2.2136058807373047", "False"], ["-6.213605880737305", "False"], ["-1.4636057615280151", "True"], ["-7.963605880737305", "False"], ["-8.713605880737305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0967da3f3796c205725bfd558b556edb1ac3b9f29139f619e8a66408c60b05c8", "prompt_hash": "92fc3bb480381cbd542828567c7ec31a67495019dec253e26649410fa5aeb57a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 485, "doc": {"id": "d581e0ad6a4c89465dc1a527bd2d3f77", "question": "Though she had a disability, what did her encouraging and positive coach see in her?", "question_concept": "disability", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["qualification", "strength", "pity", "competence", "potential"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Though she had a disability, what did her encouraging and positive coach see in her?\nA. qualification\nB. strength\nC. pity\nD. competence\nE. potential\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Though she had a disability, what did her encouraging and positive coach see in her?\nA. qualification\nB. strength\nC. pity\nD. competence\nE. potential\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Though she had a disability, what did her encouraging and positive coach see in her?\nA. qualification\nB. strength\nC. pity\nD. competence\nE. potential\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Though she had a disability, what did her encouraging and positive coach see in her?\nA. qualification\nB. strength\nC. pity\nD. competence\nE. potential\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Though she had a disability, what did her encouraging and positive coach see in her?\nA. qualification\nB. strength\nC. pity\nD. competence\nE. potential\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.819098711013794", "False"]], [["-3.569098711013794", "False"]], [["-7.319098472595215", "False"]], [["-4.069098472595215", "False"]], [["-1.819098711013794", "False"]]], "filtered_resps": [["-3.819098711013794", "False"], ["-3.569098711013794", "False"], ["-7.319098472595215", "False"], ["-4.069098472595215", "False"], ["-1.819098711013794", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "12d2b280319d45d2c1a8a73846fed997e64b9080f5b07476d9d16821f524ed03", "prompt_hash": "5fec81ebb67d9f8f4c92add7db657c29bbbfa6232af5d1644900e411fc945e09", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 486, "doc": {"id": "f232bfea2a7611999688a252e476c040", "question": "They had a theory of what they could do in t he big game, so over and over they would what?", "question_concept": "theory", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["park", "practice", "fact", "practical", "practise"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: They had a theory of what they could do in t he big game, so over and over they would what?\nA. park\nB. practice\nC. fact\nD. practical\nE. practise\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They had a theory of what they could do in t he big game, so over and over they would what?\nA. park\nB. practice\nC. fact\nD. practical\nE. practise\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They had a theory of what they could do in t he big game, so over and over they would what?\nA. park\nB. practice\nC. fact\nD. practical\nE. practise\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They had a theory of what they could do in t he big game, so over and over they would what?\nA. park\nB. practice\nC. fact\nD. practical\nE. practise\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They had a theory of what they could do in t he big game, so over and over they would what?\nA. park\nB. practice\nC. fact\nD. practical\nE. practise\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.350336074829102", "False"]], [["-1.6003360748291016", "True"]], [["-8.350336074829102", "False"]], [["-8.100336074829102", "False"]], [["-2.1003360748291016", "False"]]], "filtered_resps": [["-6.350336074829102", "False"], ["-1.6003360748291016", "True"], ["-8.350336074829102", "False"], ["-8.100336074829102", "False"], ["-2.1003360748291016", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5b685f60a2bcd895a24b110024d88fd5dfbe4125133e45d2e068710fa79b35b7", "prompt_hash": "f6f2359229e1ead1e959d9b17843e6dcf5bb6dcc2e29eba30516cba88448b99c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 487, "doc": {"id": "91756d8e475d8d59fa0a4e35f408e366", "question": "When you see something rise, you are where in relation to it?", "question_concept": "rise", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sun set", "near", "fall", "below", "lower"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When you see something rise, you are where in relation to it?\nA. sun set\nB. near\nC. fall\nD. below\nE. lower\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you see something rise, you are where in relation to it?\nA. sun set\nB. near\nC. fall\nD. below\nE. lower\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you see something rise, you are where in relation to it?\nA. sun set\nB. near\nC. fall\nD. below\nE. lower\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you see something rise, you are where in relation to it?\nA. sun set\nB. near\nC. fall\nD. below\nE. lower\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you see something rise, you are where in relation to it?\nA. sun set\nB. near\nC. fall\nD. below\nE. lower\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.389559745788574", "False"]], [["-1.6395596265792847", "False"]], [["-6.139559745788574", "False"]], [["-5.639559745788574", "False"]], [["-7.389559745788574", "False"]]], "filtered_resps": [["-5.389559745788574", "False"], ["-1.6395596265792847", "False"], ["-6.139559745788574", "False"], ["-5.639559745788574", "False"], ["-7.389559745788574", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2bc947aa38bcfea59a98848c4c28abc6b154c1a1824972e57545fffa874d3504", "prompt_hash": "8d0f6bdf687649292564f775e7c5b40030b2a048f903f7a1e8fd25074a586a67", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 488, "doc": {"id": "866ea9c668c0b42df19fa20865e31f77", "question": "They were getting ready for a really long hike, he put the food can in his what?", "question_concept": "food can", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cabinet", "house", "recycling center", "backpack", "make person sick"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: They were getting ready for a really long hike, he put the food can in his what?\nA. cabinet\nB. house\nC. recycling center\nD. backpack\nE. make person sick\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They were getting ready for a really long hike, he put the food can in his what?\nA. cabinet\nB. house\nC. recycling center\nD. backpack\nE. make person sick\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They were getting ready for a really long hike, he put the food can in his what?\nA. cabinet\nB. house\nC. recycling center\nD. backpack\nE. make person sick\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They were getting ready for a really long hike, he put the food can in his what?\nA. cabinet\nB. house\nC. recycling center\nD. backpack\nE. make person sick\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They were getting ready for a really long hike, he put the food can in his what?\nA. cabinet\nB. house\nC. recycling center\nD. backpack\nE. make person sick\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.287088871002197", "False"]], [["-7.537088871002197", "False"]], [["-10.037088394165039", "False"]], [["-0.7870886921882629", "True"]], [["-11.037088394165039", "False"]]], "filtered_resps": [["-5.287088871002197", "False"], ["-7.537088871002197", "False"], ["-10.037088394165039", "False"], ["-0.7870886921882629", "True"], ["-11.037088394165039", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a09cf9c80a07150ba7140621bde8f14992289b07c6e4304d42781b9642a3d04f", "prompt_hash": "5e158e59cadcd3cb6b448273d5776154740601b22127f603e7b3494f665c7276", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 489, "doc": {"id": "22015315e7ff79386877828b4fa27799", "question": "Where would you keep a rug near your front door?", "question_concept": "rug", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["persia", "desk", "table", "living room", "hall"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you keep a rug near your front door?\nA. persia\nB. desk\nC. table\nD. living room\nE. hall\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you keep a rug near your front door?\nA. persia\nB. desk\nC. table\nD. living room\nE. hall\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you keep a rug near your front door?\nA. persia\nB. desk\nC. table\nD. living room\nE. hall\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you keep a rug near your front door?\nA. persia\nB. desk\nC. table\nD. living room\nE. hall\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you keep a rug near your front door?\nA. persia\nB. desk\nC. table\nD. living room\nE. hall\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.4181835651397705", "False"]], [["-7.168183326721191", "False"]], [["-7.418183326721191", "False"]], [["-2.9181835651397705", "False"]], [["-1.1681835651397705", "True"]]], "filtered_resps": [["-2.4181835651397705", "False"], ["-7.168183326721191", "False"], ["-7.418183326721191", "False"], ["-2.9181835651397705", "False"], ["-1.1681835651397705", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a5ae6d52502bce92b9b7cbf977ab9cca450d16ea82ad337ad72db87574c9228d", "prompt_hash": "f9786e4c3be1cc742dcd42bd489732b875a08c17981cc52f3a993054b924a244", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 490, "doc": {"id": "484f6e4fb8e6431b010c299490b72e3c", "question": "When you slip from a ladder propped on anything what will you do?", "question_concept": "anything", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["obesity", "fall down", "matter to", "whatever", "surprise"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: When you slip from a ladder propped on anything what will you do?\nA. obesity\nB. fall down\nC. matter to\nD. whatever\nE. surprise\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you slip from a ladder propped on anything what will you do?\nA. obesity\nB. fall down\nC. matter to\nD. whatever\nE. surprise\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you slip from a ladder propped on anything what will you do?\nA. obesity\nB. fall down\nC. matter to\nD. whatever\nE. surprise\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you slip from a ladder propped on anything what will you do?\nA. obesity\nB. fall down\nC. matter to\nD. whatever\nE. surprise\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you slip from a ladder propped on anything what will you do?\nA. obesity\nB. fall down\nC. matter to\nD. whatever\nE. surprise\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.7974853515625", "False"]], [["-1.2974854707717896", "True"]], [["-7.7974853515625", "False"]], [["-7.5474853515625", "False"]], [["-8.2974853515625", "False"]]], "filtered_resps": [["-6.7974853515625", "False"], ["-1.2974854707717896", "True"], ["-7.7974853515625", "False"], ["-7.5474853515625", "False"], ["-8.2974853515625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "81c4d12915621ecdca0d44a84a783851e36666baa72338bb1ac977f60bd76860", "prompt_hash": "33122a263f40423acc77adc364e5ffcb66e096266b6860f831e23e49766accba", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 491, "doc": {"id": "7322d0dcf2e27c7032626a3639f5696b", "question": "What do you do when you need to get food?", "question_concept": "food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["table", "disneyland", "refrigerators", "pantry", "shop"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do you do when you need to get food?\nA. table\nB. disneyland\nC. refrigerators\nD. pantry\nE. shop\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you do when you need to get food?\nA. table\nB. disneyland\nC. refrigerators\nD. pantry\nE. shop\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you do when you need to get food?\nA. table\nB. disneyland\nC. refrigerators\nD. pantry\nE. shop\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you do when you need to get food?\nA. table\nB. disneyland\nC. refrigerators\nD. pantry\nE. shop\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you do when you need to get food?\nA. table\nB. disneyland\nC. refrigerators\nD. pantry\nE. shop\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.156039237976074", "False"]], [["-6.406039237976074", "False"]], [["-5.406039237976074", "False"]], [["-4.156039237976074", "False"]], [["-1.1560394763946533", "True"]]], "filtered_resps": [["-4.156039237976074", "False"], ["-6.406039237976074", "False"], ["-5.406039237976074", "False"], ["-4.156039237976074", "False"], ["-1.1560394763946533", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8def92c55096a882d9c6fbdeae97002eecdf99700cb47f809620e45283a92942", "prompt_hash": "ef210bc044aa2fd36fd9e5907683610988b8bcf256b080ef98ee6d8121985bf6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 492, "doc": {"id": "0519b0b0869681c2884f53dbfa43e538", "question": "Brad tried to arise from bed but he could not.  Instead, he just continued to do what?", "question_concept": "arise", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["go down", "fall down", "lie down", "lie to himself", "sit down"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Brad tried to arise from bed but he could not.  Instead, he just continued to do what?\nA. go down\nB. fall down\nC. lie down\nD. lie to himself\nE. sit down\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Brad tried to arise from bed but he could not.  Instead, he just continued to do what?\nA. go down\nB. fall down\nC. lie down\nD. lie to himself\nE. sit down\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Brad tried to arise from bed but he could not.  Instead, he just continued to do what?\nA. go down\nB. fall down\nC. lie down\nD. lie to himself\nE. sit down\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Brad tried to arise from bed but he could not.  Instead, he just continued to do what?\nA. go down\nB. fall down\nC. lie down\nD. lie to himself\nE. sit down\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Brad tried to arise from bed but he could not.  Instead, he just continued to do what?\nA. go down\nB. fall down\nC. lie down\nD. lie to himself\nE. sit down\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8951289653778076", "False"]], [["-3.1451289653778076", "False"]], [["-1.8951289653778076", "False"]], [["-6.645129203796387", "False"]], [["-7.895129203796387", "False"]]], "filtered_resps": [["-3.8951289653778076", "False"], ["-3.1451289653778076", "False"], ["-1.8951289653778076", "False"], ["-6.645129203796387", "False"], ["-7.895129203796387", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "263eab4ea8853d642a1850413aeabe8c5c796d65fea64c702a5d0b93646c2aff", "prompt_hash": "63432644728ce1879254688e793ceac70204b45d71993705005802e07641f299", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 493, "doc": {"id": "1ab04c0501b815b2a48f2581f04215a8", "question": "If a heifer is really high quality, you might take her where?", "question_concept": "heifer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["home", "dairy farm", "cattle show", "dairy barn", "corral"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If a heifer is really high quality, you might take her where?\nA. home\nB. dairy farm\nC. cattle show\nD. dairy barn\nE. corral\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a heifer is really high quality, you might take her where?\nA. home\nB. dairy farm\nC. cattle show\nD. dairy barn\nE. corral\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a heifer is really high quality, you might take her where?\nA. home\nB. dairy farm\nC. cattle show\nD. dairy barn\nE. corral\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a heifer is really high quality, you might take her where?\nA. home\nB. dairy farm\nC. cattle show\nD. dairy barn\nE. corral\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a heifer is really high quality, you might take her where?\nA. home\nB. dairy farm\nC. cattle show\nD. dairy barn\nE. corral\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6974551677703857", "False"]], [["-1.4474551677703857", "True"]], [["-4.697455406188965", "False"]], [["-3.4474551677703857", "False"]], [["-6.197455406188965", "False"]]], "filtered_resps": [["-1.6974551677703857", "False"], ["-1.4474551677703857", "True"], ["-4.697455406188965", "False"], ["-3.4474551677703857", "False"], ["-6.197455406188965", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c56fab5a6af9ad760350a8b8f3d8d9da4b7a707cfb6d3d7518cad662e460e01d", "prompt_hash": "0ab3b819fe30bed33a76eb47a0eaf0d0dee70d33ec48208135a567ca110dbe92", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 494, "doc": {"id": "7776b10c7bb96f3fe5e026678673634d", "question": "What do people want to acquire from opening business?", "question_concept": "opening business", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["home", "wealth", "bankruptcy", "lose money", "get rich"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What do people want to acquire from opening business?\nA. home\nB. wealth\nC. bankruptcy\nD. lose money\nE. get rich\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people want to acquire from opening business?\nA. home\nB. wealth\nC. bankruptcy\nD. lose money\nE. get rich\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people want to acquire from opening business?\nA. home\nB. wealth\nC. bankruptcy\nD. lose money\nE. get rich\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people want to acquire from opening business?\nA. home\nB. wealth\nC. bankruptcy\nD. lose money\nE. get rich\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people want to acquire from opening business?\nA. home\nB. wealth\nC. bankruptcy\nD. lose money\nE. get rich\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.159384250640869", "False"]], [["-2.409384250640869", "False"]], [["-8.159383773803711", "False"]], [["-7.659384250640869", "False"]], [["-2.159384250640869", "False"]]], "filtered_resps": [["-5.159384250640869", "False"], ["-2.409384250640869", "False"], ["-8.159383773803711", "False"], ["-7.659384250640869", "False"], ["-2.159384250640869", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e223143856be364eaf2ed060b00642a0623119fd16f55befbf8af25a0bd00106", "prompt_hash": "52c0625a6f8a3bbbfde2c6f9c89de9c3d28e0dd90105cb968df4c144bd343568", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 495, "doc": {"id": "f7c005244d406b9bde48dc8c22003af1", "question": "What has someone who had finished their undergraduate done?", "question_concept": "undergraduate", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["graduated", "masters", "postgraduate", "phd", "professor"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What has someone who had finished their undergraduate done?\nA. graduated\nB. masters\nC. postgraduate\nD. phd\nE. professor\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What has someone who had finished their undergraduate done?\nA. graduated\nB. masters\nC. postgraduate\nD. phd\nE. professor\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What has someone who had finished their undergraduate done?\nA. graduated\nB. masters\nC. postgraduate\nD. phd\nE. professor\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What has someone who had finished their undergraduate done?\nA. graduated\nB. masters\nC. postgraduate\nD. phd\nE. professor\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What has someone who had finished their undergraduate done?\nA. graduated\nB. masters\nC. postgraduate\nD. phd\nE. professor\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6073063611984253", "True"]], [["-7.357306480407715", "False"]], [["-8.607306480407715", "False"]], [["-9.607306480407715", "False"]], [["-10.607306480407715", "False"]]], "filtered_resps": [["-0.6073063611984253", "True"], ["-7.357306480407715", "False"], ["-8.607306480407715", "False"], ["-9.607306480407715", "False"], ["-10.607306480407715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9342de7a4a8cd42f5c0b8082aadb6d43afb5b638a22ea84afd6f424f04a3c2b3", "prompt_hash": "f2143d2ee29f8f33352a4e371de61c164e41ca3764d4de5585d91fcb8370e73c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 496, "doc": {"id": "88501d528c855e2b533b3fea2f86183d", "question": "Where are bus stops more common in what parts?", "question_concept": "bus stop", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ocean", "getting off of bus", "airport", "urban area", "towns"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where are bus stops more common in what parts?\nA. ocean\nB. getting off of bus\nC. airport\nD. urban area\nE. towns\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are bus stops more common in what parts?\nA. ocean\nB. getting off of bus\nC. airport\nD. urban area\nE. towns\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are bus stops more common in what parts?\nA. ocean\nB. getting off of bus\nC. airport\nD. urban area\nE. towns\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are bus stops more common in what parts?\nA. ocean\nB. getting off of bus\nC. airport\nD. urban area\nE. towns\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are bus stops more common in what parts?\nA. ocean\nB. getting off of bus\nC. airport\nD. urban area\nE. towns\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.866652965545654", "False"]], [["-3.3666529655456543", "False"]], [["-6.616652965545654", "False"]], [["-1.1166529655456543", "True"]], [["-9.366653442382812", "False"]]], "filtered_resps": [["-6.866652965545654", "False"], ["-3.3666529655456543", "False"], ["-6.616652965545654", "False"], ["-1.1166529655456543", "True"], ["-9.366653442382812", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5a1664c4408259a23cff0a3d9c383a08b73d1afcd4cb37b96c035b3182f75399", "prompt_hash": "f0c69b72d813d78a53370bb5e930e8bc7943ff38d92b94b65f73d0fe4bed717e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 497, "doc": {"id": "3d9c3253e24fb108cea9083e8a853cf2", "question": "Bill wanted to pick up a stranger, preferably a responsible one with kids.  Where might he look for one?", "question_concept": "stranger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bus station", "paradise", "train station", "park", "sea"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Bill wanted to pick up a stranger, preferably a responsible one with kids.  Where might he look for one?\nA. bus station\nB. paradise\nC. train station\nD. park\nE. sea\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Bill wanted to pick up a stranger, preferably a responsible one with kids.  Where might he look for one?\nA. bus station\nB. paradise\nC. train station\nD. park\nE. sea\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Bill wanted to pick up a stranger, preferably a responsible one with kids.  Where might he look for one?\nA. bus station\nB. paradise\nC. train station\nD. park\nE. sea\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Bill wanted to pick up a stranger, preferably a responsible one with kids.  Where might he look for one?\nA. bus station\nB. paradise\nC. train station\nD. park\nE. sea\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Bill wanted to pick up a stranger, preferably a responsible one with kids.  Where might he look for one?\nA. bus station\nB. paradise\nC. train station\nD. park\nE. sea\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6983306407928467", "False"]], [["-4.948330879211426", "False"]], [["-2.9483306407928467", "False"]], [["-4.448330879211426", "False"]], [["-11.198330879211426", "False"]]], "filtered_resps": [["-1.6983306407928467", "False"], ["-4.948330879211426", "False"], ["-2.9483306407928467", "False"], ["-4.448330879211426", "False"], ["-11.198330879211426", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "60c5f43efe31fc3615d90846d3159db692dd26817373f14632b90cfac50b1664", "prompt_hash": "3927ccd03d494937467c7b5d19986c82f716a1817691932f6341fbfab4296c7e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 498, "doc": {"id": "9808782b2e2e1bfbfa27c41e605bfffe", "question": "Where might a lemur frolic in the market?", "question_concept": "lemur", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desert", "hole", "india", "cage", "rain forest"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where might a lemur frolic in the market?\nA. desert\nB. hole\nC. india\nD. cage\nE. rain forest\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where might a lemur frolic in the market?\nA. desert\nB. hole\nC. india\nD. cage\nE. rain forest\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where might a lemur frolic in the market?\nA. desert\nB. hole\nC. india\nD. cage\nE. rain forest\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where might a lemur frolic in the market?\nA. desert\nB. hole\nC. india\nD. cage\nE. rain forest\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where might a lemur frolic in the market?\nA. desert\nB. hole\nC. india\nD. cage\nE. rain forest\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1338086128234863", "False"]], [["-5.383808612823486", "False"]], [["-7.383808612823486", "False"]], [["-4.133808612823486", "False"]], [["-0.8838086724281311", "True"]]], "filtered_resps": [["-3.1338086128234863", "False"], ["-5.383808612823486", "False"], ["-7.383808612823486", "False"], ["-4.133808612823486", "False"], ["-0.8838086724281311", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "35e9d8ff9b39fa04e24f7ee2b2dddab68180ed179717cee0e55c913f880261b1", "prompt_hash": "3b4b3fcdde7713c33b94f8286e501f2abc12fd4d91fa7a053b0644a1d98c976a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 499, "doc": {"id": "c432b860fcd7297751ff5254ec4a7956", "question": "What might I place under the furniture?", "question_concept": "furniture", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rug", "room", "toy", "friend's house", "building"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What might I place under the furniture?\nA. rug\nB. room\nC. toy\nD. friend's house\nE. building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might I place under the furniture?\nA. rug\nB. room\nC. toy\nD. friend's house\nE. building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might I place under the furniture?\nA. rug\nB. room\nC. toy\nD. friend's house\nE. building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might I place under the furniture?\nA. rug\nB. room\nC. toy\nD. friend's house\nE. building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might I place under the furniture?\nA. rug\nB. room\nC. toy\nD. friend's house\nE. building\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5746960043907166", "True"]], [["-5.824696063995361", "False"]], [["-5.824696063995361", "False"]], [["-8.324695587158203", "False"]], [["-8.574695587158203", "False"]]], "filtered_resps": [["-0.5746960043907166", "True"], ["-5.824696063995361", "False"], ["-5.824696063995361", "False"], ["-8.324695587158203", "False"], ["-8.574695587158203", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "217b7ba1259bcc77a148a41ce2fee76cc7c29d0b1ad3261f81f97241f333fa12", "prompt_hash": "d408f6c1f56ce03332e59b587c954b0099ac2ea45d2d82a5a6b14701b8256df5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 500, "doc": {"id": "732af155f677a51d05d0c9e080d598b6", "question": "Everybody began performing once their director stated what?", "question_concept": "performing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fear", "injury", "happiness", "action", "cut"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Everybody began performing once their director stated what?\nA. fear\nB. injury\nC. happiness\nD. action\nE. cut\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Everybody began performing once their director stated what?\nA. fear\nB. injury\nC. happiness\nD. action\nE. cut\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Everybody began performing once their director stated what?\nA. fear\nB. injury\nC. happiness\nD. action\nE. cut\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Everybody began performing once their director stated what?\nA. fear\nB. injury\nC. happiness\nD. action\nE. cut\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Everybody began performing once their director stated what?\nA. fear\nB. injury\nC. happiness\nD. action\nE. cut\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.00248908996582", "False"]], [["-4.75248908996582", "False"]], [["-5.25248908996582", "False"]], [["-2.7524890899658203", "False"]], [["-3.0024890899658203", "False"]]], "filtered_resps": [["-5.00248908996582", "False"], ["-4.75248908996582", "False"], ["-5.25248908996582", "False"], ["-2.7524890899658203", "False"], ["-3.0024890899658203", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "137004101bfe579303f26da1ec1b3100a952bb17408377773ab0dcbec85ed398", "prompt_hash": "19c12922fad217d012b9f2f38a78d9fbf8a27ec53e66c378bbcdb686cfb5af53", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 501, "doc": {"id": "48abc2c113623fd72f758502529f93a5", "question": "By learning about the world, many poor college students gain what?", "question_concept": "learning about world", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pleasure", "greater mobility", "desire to travel", "global warming", "increased security"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: By learning about the world, many poor college students gain what?\nA. pleasure\nB. greater mobility\nC. desire to travel\nD. global warming\nE. increased security\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: By learning about the world, many poor college students gain what?\nA. pleasure\nB. greater mobility\nC. desire to travel\nD. global warming\nE. increased security\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: By learning about the world, many poor college students gain what?\nA. pleasure\nB. greater mobility\nC. desire to travel\nD. global warming\nE. increased security\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: By learning about the world, many poor college students gain what?\nA. pleasure\nB. greater mobility\nC. desire to travel\nD. global warming\nE. increased security\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: By learning about the world, many poor college students gain what?\nA. pleasure\nB. greater mobility\nC. desire to travel\nD. global warming\nE. increased security\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.507659912109375", "False"]], [["-1.2576600313186646", "True"]], [["-7.757659912109375", "False"]], [["-7.007659912109375", "False"]], [["-4.757659912109375", "False"]]], "filtered_resps": [["-4.507659912109375", "False"], ["-1.2576600313186646", "True"], ["-7.757659912109375", "False"], ["-7.007659912109375", "False"], ["-4.757659912109375", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "aea2192dc128754e9301c394918aef089de6c3dfca9eaaf2a5810d98746ef835", "prompt_hash": "86441d058926a2653dfa62fafed30acde35317b539d6784372e16593983a1a19", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 502, "doc": {"id": "03f06f77aaf80b5f5e296ffbd11e9d82", "question": "Where are required to carry books all day?", "question_concept": "books", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["friend's house", "university", "large city", "storage", "table"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where are required to carry books all day?\nA. friend's house\nB. university\nC. large city\nD. storage\nE. table\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are required to carry books all day?\nA. friend's house\nB. university\nC. large city\nD. storage\nE. table\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are required to carry books all day?\nA. friend's house\nB. university\nC. large city\nD. storage\nE. table\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are required to carry books all day?\nA. friend's house\nB. university\nC. large city\nD. storage\nE. table\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are required to carry books all day?\nA. friend's house\nB. university\nC. large city\nD. storage\nE. table\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.224193572998047", "False"]], [["-0.974193811416626", "True"]], [["-9.224193572998047", "False"]], [["-9.224193572998047", "False"]], [["-10.474193572998047", "False"]]], "filtered_resps": [["-5.224193572998047", "False"], ["-0.974193811416626", "True"], ["-9.224193572998047", "False"], ["-9.224193572998047", "False"], ["-10.474193572998047", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cc89b3fdc839bc1f1e4fa1f7ad7226f323fe75e194c2d7031ec5d8b7eae192c4", "prompt_hash": "8c8180889529b27b87f2d62ade0eb73e26a6b2b028064ea8919e92bce9ce8002", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 503, "doc": {"id": "e7084c166ec67d0f983a26e055e845c6", "question": "where is seaweed from?", "question_concept": "seaweed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["beach", "sea", "ocean", "water", "sea plant"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: where is seaweed from?\nA. beach\nB. sea\nC. ocean\nD. water\nE. sea plant\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: where is seaweed from?\nA. beach\nB. sea\nC. ocean\nD. water\nE. sea plant\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: where is seaweed from?\nA. beach\nB. sea\nC. ocean\nD. water\nE. sea plant\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: where is seaweed from?\nA. beach\nB. sea\nC. ocean\nD. water\nE. sea plant\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: where is seaweed from?\nA. beach\nB. sea\nC. ocean\nD. water\nE. sea plant\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.605479717254639", "False"]], [["-3.1054797172546387", "False"]], [["-1.6054797172546387", "False"]], [["-7.355479717254639", "False"]], [["-5.105479717254639", "False"]]], "filtered_resps": [["-5.605479717254639", "False"], ["-3.1054797172546387", "False"], ["-1.6054797172546387", "False"], ["-7.355479717254639", "False"], ["-5.105479717254639", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "334344cab7b889178754a01463c9636220e4668be4d1a1e8abe3a42560395913", "prompt_hash": "db2797a87376a39ea1dddec6d74194de67e8d9ad189d40e2b6b853504d8d857d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 504, "doc": {"id": "c55c31b5a2aa996f3b75ad88c017a6b9", "question": "how can i store cooked steak?", "question_concept": "steak", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["oven", "freezer", "plate", "tupperware", "grill"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: how can i store cooked steak?\nA. oven\nB. freezer\nC. plate\nD. tupperware\nE. grill\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: how can i store cooked steak?\nA. oven\nB. freezer\nC. plate\nD. tupperware\nE. grill\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: how can i store cooked steak?\nA. oven\nB. freezer\nC. plate\nD. tupperware\nE. grill\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: how can i store cooked steak?\nA. oven\nB. freezer\nC. plate\nD. tupperware\nE. grill\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: how can i store cooked steak?\nA. oven\nB. freezer\nC. plate\nD. tupperware\nE. grill\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.052457332611084", "False"]], [["-1.5524574518203735", "True"]], [["-6.052457332611084", "False"]], [["-2.052457332611084", "False"]], [["-10.052457809448242", "False"]]], "filtered_resps": [["-6.052457332611084", "False"], ["-1.5524574518203735", "True"], ["-6.052457332611084", "False"], ["-2.052457332611084", "False"], ["-10.052457809448242", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bb6573d79e07d092f0d54496019a07ca6d29a826d272026cdb3413c95424efa7", "prompt_hash": "d547556aaa20857f70a76a0b636bd449e1598918163a1901a0325f37482c02f0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 505, "doc": {"id": "463521a93ae71e93bea8b97cdf7a6792", "question": "John wanted to clean all of the dust out of his place before settling down to watch his favorite shows.  What might he hardest do dust?", "question_concept": "dust", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["closet", "under the bed", "television", "attic", "most buildings"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: John wanted to clean all of the dust out of his place before settling down to watch his favorite shows.  What might he hardest do dust?\nA. closet\nB. under the bed\nC. television\nD. attic\nE. most buildings\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John wanted to clean all of the dust out of his place before settling down to watch his favorite shows.  What might he hardest do dust?\nA. closet\nB. under the bed\nC. television\nD. attic\nE. most buildings\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John wanted to clean all of the dust out of his place before settling down to watch his favorite shows.  What might he hardest do dust?\nA. closet\nB. under the bed\nC. television\nD. attic\nE. most buildings\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John wanted to clean all of the dust out of his place before settling down to watch his favorite shows.  What might he hardest do dust?\nA. closet\nB. under the bed\nC. television\nD. attic\nE. most buildings\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John wanted to clean all of the dust out of his place before settling down to watch his favorite shows.  What might he hardest do dust?\nA. closet\nB. under the bed\nC. television\nD. attic\nE. most buildings\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.345478057861328", "False"]], [["-2.345478057861328", "False"]], [["-4.595478057861328", "False"]], [["-2.095478057861328", "True"]], [["-2.845478057861328", "False"]]], "filtered_resps": [["-3.345478057861328", "False"], ["-2.345478057861328", "False"], ["-4.595478057861328", "False"], ["-2.095478057861328", "True"], ["-2.845478057861328", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65c6da0f9b65d773e29236cfdf3deb7c4b994a335c809ecbf811f24def33e4a5", "prompt_hash": "5c0791c1517bd744f1857035877f07c2913dfe0580c0f9e3577c3f8b4f9f8327", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 506, "doc": {"id": "c036ce033bc429ac1aba0a6ac8d057e1", "question": "Something had the nerve to break into the garbage last night, what did it?", "question_concept": "nerve", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eyes", "animal", "fingertips", "brainstem", "human body"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Something had the nerve to break into the garbage last night, what did it?\nA. eyes\nB. animal\nC. fingertips\nD. brainstem\nE. human body\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Something had the nerve to break into the garbage last night, what did it?\nA. eyes\nB. animal\nC. fingertips\nD. brainstem\nE. human body\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Something had the nerve to break into the garbage last night, what did it?\nA. eyes\nB. animal\nC. fingertips\nD. brainstem\nE. human body\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Something had the nerve to break into the garbage last night, what did it?\nA. eyes\nB. animal\nC. fingertips\nD. brainstem\nE. human body\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Something had the nerve to break into the garbage last night, what did it?\nA. eyes\nB. animal\nC. fingertips\nD. brainstem\nE. human body\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1614065170288086", "False"]], [["-1.161406397819519", "True"]], [["-7.661406517028809", "False"]], [["-8.161406517028809", "False"]], [["-7.161406517028809", "False"]]], "filtered_resps": [["-3.1614065170288086", "False"], ["-1.161406397819519", "True"], ["-7.661406517028809", "False"], ["-8.161406517028809", "False"], ["-7.161406517028809", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "870e0e7eac0b109fc84453e6983c79c5370293818a9b7590deef4ced5f1f7e92", "prompt_hash": "f19aa836de067df6ea6051007237fb016a030c12b36a6dd95caf970f8345a3c4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 507, "doc": {"id": "db7f2bfdabcf53d6778fd7af80b603d2", "question": "Where would you go to get some pamphlets if you want to own them?", "question_concept": "pamphlets", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bookstore", "drawer", "health department", "mail box", "library"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you go to get some pamphlets if you want to own them?\nA. bookstore\nB. drawer\nC. health department\nD. mail box\nE. library\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you go to get some pamphlets if you want to own them?\nA. bookstore\nB. drawer\nC. health department\nD. mail box\nE. library\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you go to get some pamphlets if you want to own them?\nA. bookstore\nB. drawer\nC. health department\nD. mail box\nE. library\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you go to get some pamphlets if you want to own them?\nA. bookstore\nB. drawer\nC. health department\nD. mail box\nE. library\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you go to get some pamphlets if you want to own them?\nA. bookstore\nB. drawer\nC. health department\nD. mail box\nE. library\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8743206262588501", "True"]], [["-5.8743205070495605", "False"]], [["-2.6243205070495605", "False"]], [["-7.3743205070495605", "False"]], [["-4.6243205070495605", "False"]]], "filtered_resps": [["-0.8743206262588501", "True"], ["-5.8743205070495605", "False"], ["-2.6243205070495605", "False"], ["-7.3743205070495605", "False"], ["-4.6243205070495605", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6dd6f429198295ff319c3c98b8e1158ee5fc70df99aaa8b9b1491671383a1b7f", "prompt_hash": "3cf586a5328a7be19e2af00d72a9c294f4a286b2b3f8856f913a042ebd73507b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 508, "doc": {"id": "8605fd2affc796d79073d0f3ef0761c9", "question": "The audience cheered when a goal was scored, what were they spectating?", "question_concept": "audience", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["school", "sporting event", "concert hall", "show", "television"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The audience cheered when a goal was scored, what were they spectating?\nA. school\nB. sporting event\nC. concert hall\nD. show\nE. television\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The audience cheered when a goal was scored, what were they spectating?\nA. school\nB. sporting event\nC. concert hall\nD. show\nE. television\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The audience cheered when a goal was scored, what were they spectating?\nA. school\nB. sporting event\nC. concert hall\nD. show\nE. television\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The audience cheered when a goal was scored, what were they spectating?\nA. school\nB. sporting event\nC. concert hall\nD. show\nE. television\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The audience cheered when a goal was scored, what were they spectating?\nA. school\nB. sporting event\nC. concert hall\nD. show\nE. television\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.70574688911438", "False"]], [["-0.9618686437606812", "True"]], [["-7.461868762969971", "False"]], [["-8.461868286132812", "False"]], [["-9.461868286132812", "False"]]], "filtered_resps": [["-3.70574688911438", "False"], ["-0.9618686437606812", "True"], ["-7.461868762969971", "False"], ["-8.461868286132812", "False"], ["-9.461868286132812", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c089e5d4472099ac00d0808b4d6567490caaec22cc956539d9e4d92b814b68cb", "prompt_hash": "449930e359f5df2b0b4dcaee4f3647fa91a503f0dfeea29dcae2bcffece67239", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 509, "doc": {"id": "ad37795fd9e3a65553683ff305b5113d", "question": "What western state has thousands of miles of shore?", "question_concept": "shore", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["picture of sea side", "seaside town", "beach", "california", "see side picture"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What western state has thousands of miles of shore?\nA. picture of sea side\nB. seaside town\nC. beach\nD. california\nE. see side picture\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What western state has thousands of miles of shore?\nA. picture of sea side\nB. seaside town\nC. beach\nD. california\nE. see side picture\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What western state has thousands of miles of shore?\nA. picture of sea side\nB. seaside town\nC. beach\nD. california\nE. see side picture\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What western state has thousands of miles of shore?\nA. picture of sea side\nB. seaside town\nC. beach\nD. california\nE. see side picture\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What western state has thousands of miles of shore?\nA. picture of sea side\nB. seaside town\nC. beach\nD. california\nE. see side picture\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.40639591217041", "False"]], [["-7.40639591217041", "False"]], [["-6.15639591217041", "False"]], [["-1.4063961505889893", "False"]], [["-9.90639591217041", "False"]]], "filtered_resps": [["-5.40639591217041", "False"], ["-7.40639591217041", "False"], ["-6.15639591217041", "False"], ["-1.4063961505889893", "False"], ["-9.90639591217041", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3f70ee7c4bc1588bc28ab533e8eed96ee36114f0c4b6c938799c4d0cc5ffc1c4", "prompt_hash": "cc535ed01639465759b922e4a4cc3c16ca4488b6b6d00adcfabc3c96e0dfa525", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 510, "doc": {"id": "bcd51af35d691f5c3b6b548096ab1559", "question": "Everybody seemed to be crying at the holy site, the tour guide explained that this was what?", "question_concept": "holy", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["profane", "halibut", "damaged", "common", "halibut"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Everybody seemed to be crying at the holy site, the tour guide explained that this was what?\nA. profane\nB. halibut\nC. damaged\nD. common\nE. halibut\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Everybody seemed to be crying at the holy site, the tour guide explained that this was what?\nA. profane\nB. halibut\nC. damaged\nD. common\nE. halibut\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Everybody seemed to be crying at the holy site, the tour guide explained that this was what?\nA. profane\nB. halibut\nC. damaged\nD. common\nE. halibut\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Everybody seemed to be crying at the holy site, the tour guide explained that this was what?\nA. profane\nB. halibut\nC. damaged\nD. common\nE. halibut\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Everybody seemed to be crying at the holy site, the tour guide explained that this was what?\nA. profane\nB. halibut\nC. damaged\nD. common\nE. halibut\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8346595764160156", "False"]], [["-4.084659576416016", "False"]], [["-6.334659576416016", "False"]], [["-2.3346595764160156", "False"]], [["-4.584659576416016", "False"]]], "filtered_resps": [["-2.8346595764160156", "False"], ["-4.084659576416016", "False"], ["-6.334659576416016", "False"], ["-2.3346595764160156", "False"], ["-4.584659576416016", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b97d61c5dd120fa851d493ca963011aa37e0fb72f363a3bfc574a60ddfb17593", "prompt_hash": "f2c0c467522c11d41e5f3c55267ae97810d17e2ffb57aa7657ddaf88a136478a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 511, "doc": {"id": "b5345f15d5b451562ab9e0851e7f394f", "question": "The smile gave away that the what was one of happiness?", "question_concept": "smile", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["manual", "rainbow", "cry", "frown", "make others happy too"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The smile gave away that the what was one of happiness?\nA. manual\nB. rainbow\nC. cry\nD. frown\nE. make others happy too\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The smile gave away that the what was one of happiness?\nA. manual\nB. rainbow\nC. cry\nD. frown\nE. make others happy too\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The smile gave away that the what was one of happiness?\nA. manual\nB. rainbow\nC. cry\nD. frown\nE. make others happy too\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The smile gave away that the what was one of happiness?\nA. manual\nB. rainbow\nC. cry\nD. frown\nE. make others happy too\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The smile gave away that the what was one of happiness?\nA. manual\nB. rainbow\nC. cry\nD. frown\nE. make others happy too\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.850745439529419", "False"]], [["-2.850745439529419", "False"]], [["-5.60074520111084", "False"]], [["-1.600745439529419", "True"]], [["-5.60074520111084", "False"]]], "filtered_resps": [["-1.850745439529419", "False"], ["-2.850745439529419", "False"], ["-5.60074520111084", "False"], ["-1.600745439529419", "True"], ["-5.60074520111084", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3ec977c97c475cdc06ee1d458884a0a6f0bd275192d2423cd427843ffedc7e76", "prompt_hash": "9132cd19a9658bee51d625d367fd819b7ca7ab871736ebc65aec42e02fc76311", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 512, "doc": {"id": "6a884d5d8febfdd86fcf68ff1a904d9b", "question": "Where is a public monument likely to be erected by a city?", "question_concept": "monument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["municipal park", "office", "state park", "cemetary", "public gardens"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a public monument likely to be erected by a city?\nA. municipal park\nB. office\nC. state park\nD. cemetary\nE. public gardens\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a public monument likely to be erected by a city?\nA. municipal park\nB. office\nC. state park\nD. cemetary\nE. public gardens\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a public monument likely to be erected by a city?\nA. municipal park\nB. office\nC. state park\nD. cemetary\nE. public gardens\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a public monument likely to be erected by a city?\nA. municipal park\nB. office\nC. state park\nD. cemetary\nE. public gardens\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a public monument likely to be erected by a city?\nA. municipal park\nB. office\nC. state park\nD. cemetary\nE. public gardens\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.87534099817276", "True"]], [["-6.375340938568115", "False"]], [["-5.375340938568115", "False"]], [["-3.6253409385681152", "False"]], [["-3.8753409385681152", "False"]]], "filtered_resps": [["-0.87534099817276", "True"], ["-6.375340938568115", "False"], ["-5.375340938568115", "False"], ["-3.6253409385681152", "False"], ["-3.8753409385681152", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "368038ecd9f89b07c308d0176a12951c5a1568cf99f22cffad4c3717f67f4dfb", "prompt_hash": "116e15be34820c3eaca12fb58af7c72499ba9be6402e248cde754325aee68f52", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 513, "doc": {"id": "a1303b5177df0a5b653c9abd7d5f5e08", "question": "Where would a person live if they wanted no neighbors?", "question_concept": "bungalow", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["housing estate", "neighborhood", "mars", "woods", "suburbs"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would a person live if they wanted no neighbors?\nA. housing estate\nB. neighborhood\nC. mars\nD. woods\nE. suburbs\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would a person live if they wanted no neighbors?\nA. housing estate\nB. neighborhood\nC. mars\nD. woods\nE. suburbs\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would a person live if they wanted no neighbors?\nA. housing estate\nB. neighborhood\nC. mars\nD. woods\nE. suburbs\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would a person live if they wanted no neighbors?\nA. housing estate\nB. neighborhood\nC. mars\nD. woods\nE. suburbs\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would a person live if they wanted no neighbors?\nA. housing estate\nB. neighborhood\nC. mars\nD. woods\nE. suburbs\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3981242179870605", "False"]], [["-6.6481242179870605", "False"]], [["-1.14812433719635", "True"]], [["-4.1481242179870605", "False"]], [["-8.648124694824219", "False"]]], "filtered_resps": [["-2.3981242179870605", "False"], ["-6.6481242179870605", "False"], ["-1.14812433719635", "True"], ["-4.1481242179870605", "False"], ["-8.648124694824219", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f3935083f5951bc6125c0a7182a278ab0eaffc2de1135111510022f1df42a765", "prompt_hash": "0e439d790e795b9e3f011388e4b79fa0a04237df4119bbde177a35faead31289", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 514, "doc": {"id": "315baf79f8dd3673f67a90de0758240e", "question": "Where is the control room that controls a PWR located?", "question_concept": "control room", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["building", "factory", "window", "prison", "nuclear power plant"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where is the control room that controls a PWR located?\nA. building\nB. factory\nC. window\nD. prison\nE. nuclear power plant\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is the control room that controls a PWR located?\nA. building\nB. factory\nC. window\nD. prison\nE. nuclear power plant\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is the control room that controls a PWR located?\nA. building\nB. factory\nC. window\nD. prison\nE. nuclear power plant\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is the control room that controls a PWR located?\nA. building\nB. factory\nC. window\nD. prison\nE. nuclear power plant\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is the control room that controls a PWR located?\nA. building\nB. factory\nC. window\nD. prison\nE. nuclear power plant\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9114413261413574", "False"]], [["-6.411441326141357", "False"]], [["-7.661441326141357", "False"]], [["-8.661441802978516", "False"]], [["-1.1614413261413574", "True"]]], "filtered_resps": [["-2.9114413261413574", "False"], ["-6.411441326141357", "False"], ["-7.661441326141357", "False"], ["-8.661441802978516", "False"], ["-1.1614413261413574", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fb5a1faab9e66b982936a913d3e5c8bc7c0d49af6de7474a258e4b9a23e463b2", "prompt_hash": "ab90f6936fd4e87df7753d42e9d18a629dd51102fe8a7fa29cddadd0a9eb2506", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 515, "doc": {"id": "01f01cc3ad152773ef42b30e926912bf", "question": "What happens to a dog before someone puts up posters of them?", "question_concept": "dogs", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get lost", "require water", "trained", "bark", "roll over"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What happens to a dog before someone puts up posters of them?\nA. get lost\nB. require water\nC. trained\nD. bark\nE. roll over\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens to a dog before someone puts up posters of them?\nA. get lost\nB. require water\nC. trained\nD. bark\nE. roll over\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens to a dog before someone puts up posters of them?\nA. get lost\nB. require water\nC. trained\nD. bark\nE. roll over\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens to a dog before someone puts up posters of them?\nA. get lost\nB. require water\nC. trained\nD. bark\nE. roll over\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens to a dog before someone puts up posters of them?\nA. get lost\nB. require water\nC. trained\nD. bark\nE. roll over\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7711513638496399", "True"]], [["-5.021151542663574", "False"]], [["-4.021151542663574", "False"]], [["-4.521151542663574", "False"]], [["-7.771151542663574", "False"]]], "filtered_resps": [["-0.7711513638496399", "True"], ["-5.021151542663574", "False"], ["-4.021151542663574", "False"], ["-4.521151542663574", "False"], ["-7.771151542663574", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "76491ea8c8e92443c2f0de3f1a169d31b9ee851dc5d6882659714b6a2147d51c", "prompt_hash": "61ccb91467a473d2271b48e79fb822206f29b0290c1ca195903f587cb3ac5930", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 516, "doc": {"id": "f192cfacbaa2f7e0e879f673c8e076a7", "question": "Where are the most famous BBQ steakhouses in america?", "question_concept": "steakhouse", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["texas", "building", "kansas city", "maine", "falling down"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where are the most famous BBQ steakhouses in america?\nA. texas\nB. building\nC. kansas city\nD. maine\nE. falling down\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are the most famous BBQ steakhouses in america?\nA. texas\nB. building\nC. kansas city\nD. maine\nE. falling down\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are the most famous BBQ steakhouses in america?\nA. texas\nB. building\nC. kansas city\nD. maine\nE. falling down\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are the most famous BBQ steakhouses in america?\nA. texas\nB. building\nC. kansas city\nD. maine\nE. falling down\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are the most famous BBQ steakhouses in america?\nA. texas\nB. building\nC. kansas city\nD. maine\nE. falling down\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8631796836853027", "False"]], [["-5.613179683685303", "False"]], [["-2.6131796836853027", "False"]], [["-9.113179206848145", "False"]], [["-10.613179206848145", "False"]]], "filtered_resps": [["-3.8631796836853027", "False"], ["-5.613179683685303", "False"], ["-2.6131796836853027", "False"], ["-9.113179206848145", "False"], ["-10.613179206848145", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "aa63af9c58c2390ba8f2307d279bd618b879aaba971987001045c3f5b7a6af79", "prompt_hash": "e1d5f99f94eab8d6a020c56d7c1f046487a751f3c59f0c269c8dea50f0877b12", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 517, "doc": {"id": "ab8d5e21a2cf34b60a04768b01f1f8e9", "question": "He kept plugging away in his cubicle, it seemed he was the only person not called into the what?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["building", "conference", "assessment", "demonstration", "garage"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: He kept plugging away in his cubicle, it seemed he was the only person not called into the what?\nA. building\nB. conference\nC. assessment\nD. demonstration\nE. garage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He kept plugging away in his cubicle, it seemed he was the only person not called into the what?\nA. building\nB. conference\nC. assessment\nD. demonstration\nE. garage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He kept plugging away in his cubicle, it seemed he was the only person not called into the what?\nA. building\nB. conference\nC. assessment\nD. demonstration\nE. garage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He kept plugging away in his cubicle, it seemed he was the only person not called into the what?\nA. building\nB. conference\nC. assessment\nD. demonstration\nE. garage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He kept plugging away in his cubicle, it seemed he was the only person not called into the what?\nA. building\nB. conference\nC. assessment\nD. demonstration\nE. garage\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8367910385131836", "False"]], [["-1.5867910385131836", "True"]], [["-4.836791038513184", "False"]], [["-6.336791038513184", "False"]], [["-8.086791038513184", "False"]]], "filtered_resps": [["-2.8367910385131836", "False"], ["-1.5867910385131836", "True"], ["-4.836791038513184", "False"], ["-6.336791038513184", "False"], ["-8.086791038513184", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0d1d6585199c7d265b3a0eef8a580d392c082aed3313cb3f3ef17c11c30ec167", "prompt_hash": "4f0cf2662edca458010d16599194deb4e05dc6e31acf745c80d46e37e578b903", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 518, "doc": {"id": "5d1df1daa886efb78db2103ddc1398eb", "question": "If you're attending school and are falling asleep you're likely experiencing what?", "question_concept": "attending school", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boredom", "malaria", "graduate", "inspiration", "detention"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If you're attending school and are falling asleep you're likely experiencing what?\nA. boredom\nB. malaria\nC. graduate\nD. inspiration\nE. detention\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're attending school and are falling asleep you're likely experiencing what?\nA. boredom\nB. malaria\nC. graduate\nD. inspiration\nE. detention\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're attending school and are falling asleep you're likely experiencing what?\nA. boredom\nB. malaria\nC. graduate\nD. inspiration\nE. detention\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're attending school and are falling asleep you're likely experiencing what?\nA. boredom\nB. malaria\nC. graduate\nD. inspiration\nE. detention\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're attending school and are falling asleep you're likely experiencing what?\nA. boredom\nB. malaria\nC. graduate\nD. inspiration\nE. detention\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7043792009353638", "True"]], [["-3.954379081726074", "False"]], [["-6.704379081726074", "False"]], [["-6.704379081726074", "False"]], [["-8.204379081726074", "False"]]], "filtered_resps": [["-0.7043792009353638", "True"], ["-3.954379081726074", "False"], ["-6.704379081726074", "False"], ["-6.704379081726074", "False"], ["-8.204379081726074", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "04954a2b3d90facd2cf28f1b42bc795151430b6bac3f116219ac1adf48caf86f", "prompt_hash": "a90fd2b2a2a5b6ebbc1b43f81e57e1d35d20d1bae3970c694f05c2be8c0812bd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 519, "doc": {"id": "2f8b35d352097cc9277599be49fab0b3", "question": "I want to buy a gong, where should I look for one?", "question_concept": "gong", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["orchestra", "church", "chinese temple", "chinatown", "music store"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: I want to buy a gong, where should I look for one?\nA. orchestra\nB. church\nC. chinese temple\nD. chinatown\nE. music store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I want to buy a gong, where should I look for one?\nA. orchestra\nB. church\nC. chinese temple\nD. chinatown\nE. music store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I want to buy a gong, where should I look for one?\nA. orchestra\nB. church\nC. chinese temple\nD. chinatown\nE. music store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I want to buy a gong, where should I look for one?\nA. orchestra\nB. church\nC. chinese temple\nD. chinatown\nE. music store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I want to buy a gong, where should I look for one?\nA. orchestra\nB. church\nC. chinese temple\nD. chinatown\nE. music store\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0914225578308105", "False"]], [["-4.5914225578308105", "False"]], [["-2.3414225578308105", "False"]], [["-2.3414225578308105", "False"]], [["-2.8414225578308105", "False"]]], "filtered_resps": [["-3.0914225578308105", "False"], ["-4.5914225578308105", "False"], ["-2.3414225578308105", "False"], ["-2.3414225578308105", "False"], ["-2.8414225578308105", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2137c46e660aafac99806cad582cc875ede9041ff29c98dd2832e6a711d9ee55", "prompt_hash": "199fb7ce0bd3ef60e5d6b45f8cf8f00f9d936bafa4e0c85e8d47b926ffd0d94f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 520, "doc": {"id": "18eb6a3b54ccf4989e268cfb9ea90f9c", "question": "What would friends do if they need each others' help?", "question_concept": "friends", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["call each other", "group together", "understand each other", "meet for lunch", "part company"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What would friends do if they need each others' help?\nA. call each other\nB. group together\nC. understand each other\nD. meet for lunch\nE. part company\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would friends do if they need each others' help?\nA. call each other\nB. group together\nC. understand each other\nD. meet for lunch\nE. part company\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would friends do if they need each others' help?\nA. call each other\nB. group together\nC. understand each other\nD. meet for lunch\nE. part company\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would friends do if they need each others' help?\nA. call each other\nB. group together\nC. understand each other\nD. meet for lunch\nE. part company\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would friends do if they need each others' help?\nA. call each other\nB. group together\nC. understand each other\nD. meet for lunch\nE. part company\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1024179458618164", "False"]], [["-2.1024179458618164", "True"]], [["-2.1024179458618164", "True"]], [["-5.602417945861816", "False"]], [["-9.102417945861816", "False"]]], "filtered_resps": [["-3.1024179458618164", "False"], ["-2.1024179458618164", "True"], ["-2.1024179458618164", "True"], ["-5.602417945861816", "False"], ["-9.102417945861816", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cc25444aed2497076f6788da56afcaaa30392034a0c8e87ebe58750650dce697", "prompt_hash": "b625b9f093be88428c83ddd48ca828b2676ba237faa8d68e3fbe9391f8fd4749", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 521, "doc": {"id": "3e12400bc5a2038a747edf2605787fe8", "question": "When people are playing a game, what is their motivation to play?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["believe in god", "dance", "desire to win", "destroy each other", "run amok"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: When people are playing a game, what is their motivation to play?\nA. believe in god\nB. dance\nC. desire to win\nD. destroy each other\nE. run amok\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When people are playing a game, what is their motivation to play?\nA. believe in god\nB. dance\nC. desire to win\nD. destroy each other\nE. run amok\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When people are playing a game, what is their motivation to play?\nA. believe in god\nB. dance\nC. desire to win\nD. destroy each other\nE. run amok\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When people are playing a game, what is their motivation to play?\nA. believe in god\nB. dance\nC. desire to win\nD. destroy each other\nE. run amok\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When people are playing a game, what is their motivation to play?\nA. believe in god\nB. dance\nC. desire to win\nD. destroy each other\nE. run amok\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.13625431060791", "False"]], [["-8.63625431060791", "False"]], [["-1.6362543106079102", "False"]], [["-7.13625431060791", "False"]], [["-8.63625431060791", "False"]]], "filtered_resps": [["-6.13625431060791", "False"], ["-8.63625431060791", "False"], ["-1.6362543106079102", "False"], ["-7.13625431060791", "False"], ["-8.63625431060791", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6a0454b76f17e1a50d7136bcccc8e5cb80d05000f4249b2a6c6a3c171b7c7ff9", "prompt_hash": "73fa1d662efe6378ae2f26a14779001913e33c3fe6e4a8cfb4ce8976af123e9b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 522, "doc": {"id": "72baf6ca5c4daa01c2cc7fda22183db8", "question": "Where could there be a battle that involves words?", "question_concept": "battle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["court room", "war", "video game", "iraq", "church"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where could there be a battle that involves words?\nA. court room\nB. war\nC. video game\nD. iraq\nE. church\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could there be a battle that involves words?\nA. court room\nB. war\nC. video game\nD. iraq\nE. church\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could there be a battle that involves words?\nA. court room\nB. war\nC. video game\nD. iraq\nE. church\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could there be a battle that involves words?\nA. court room\nB. war\nC. video game\nD. iraq\nE. church\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could there be a battle that involves words?\nA. court room\nB. war\nC. video game\nD. iraq\nE. church\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7674694061279297", "True"]], [["-4.26746940612793", "False"]], [["-4.76746940612793", "False"]], [["-6.76746940612793", "False"]], [["-8.01746940612793", "False"]]], "filtered_resps": [["-0.7674694061279297", "True"], ["-4.26746940612793", "False"], ["-4.76746940612793", "False"], ["-6.76746940612793", "False"], ["-8.01746940612793", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "df76fb17dd0cbc27795841dcb5f41b7c0f2722a75e022a4f73e2b01f60e77dfb", "prompt_hash": "ef8ada161bf91b39fb37dfef5d7706036f3cab219fa19d0d6f7a13715673dd52", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 523, "doc": {"id": "9bac07574c966cae34c85e9f25538cba", "question": "John didn't mind getting in line.  It was what game after that he hated.  The time, the sore feet. He did not like doing what?", "question_concept": "getting in line", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have to wait for", "standing in line", "eat cake", "less confusion", "being ordered"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John didn't mind getting in line.  It was what game after that he hated.  The time, the sore feet. He did not like doing what?\nA. have to wait for\nB. standing in line\nC. eat cake\nD. less confusion\nE. being ordered\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John didn't mind getting in line.  It was what game after that he hated.  The time, the sore feet. He did not like doing what?\nA. have to wait for\nB. standing in line\nC. eat cake\nD. less confusion\nE. being ordered\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John didn't mind getting in line.  It was what game after that he hated.  The time, the sore feet. He did not like doing what?\nA. have to wait for\nB. standing in line\nC. eat cake\nD. less confusion\nE. being ordered\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John didn't mind getting in line.  It was what game after that he hated.  The time, the sore feet. He did not like doing what?\nA. have to wait for\nB. standing in line\nC. eat cake\nD. less confusion\nE. being ordered\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John didn't mind getting in line.  It was what game after that he hated.  The time, the sore feet. He did not like doing what?\nA. have to wait for\nB. standing in line\nC. eat cake\nD. less confusion\nE. being ordered\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.086519241333008", "False"]], [["-1.3365193605422974", "False"]], [["-8.836519241333008", "False"]], [["-8.836519241333008", "False"]], [["-9.836519241333008", "False"]]], "filtered_resps": [["-5.086519241333008", "False"], ["-1.3365193605422974", "False"], ["-8.836519241333008", "False"], ["-8.836519241333008", "False"], ["-9.836519241333008", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1930f6fdd636bceb931eb1ed3c98fa7de4c1a4b2f8e715a61ea65a2f9f503601", "prompt_hash": "f5dd513cb808efafc0ae79e78efb9236e37a68fb14f7f9d5dc5f94ef20348558", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 524, "doc": {"id": "fe2a21ddb1bde76025a961126044a9a3", "question": "What is the process of going somewhere?", "question_concept": "going somewhere", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fire", "energy", "car", "transporting", "staying in place"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is the process of going somewhere?\nA. fire\nB. energy\nC. car\nD. transporting\nE. staying in place\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the process of going somewhere?\nA. fire\nB. energy\nC. car\nD. transporting\nE. staying in place\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the process of going somewhere?\nA. fire\nB. energy\nC. car\nD. transporting\nE. staying in place\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the process of going somewhere?\nA. fire\nB. energy\nC. car\nD. transporting\nE. staying in place\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the process of going somewhere?\nA. fire\nB. energy\nC. car\nD. transporting\nE. staying in place\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.752798080444336", "False"]], [["-7.252798080444336", "False"]], [["-7.502798080444336", "False"]], [["-1.252798318862915", "False"]], [["-10.502798080444336", "False"]]], "filtered_resps": [["-6.752798080444336", "False"], ["-7.252798080444336", "False"], ["-7.502798080444336", "False"], ["-1.252798318862915", "False"], ["-10.502798080444336", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f5ca6ec835149ee40edbbb6e793831e964c5d64a376b3fbc576a9305f599b096", "prompt_hash": "97636e6ebbb22c2e706b3a0c5de03d21d7b52ee794ca3a9c3e3c41da7b352d6a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 525, "doc": {"id": "d03e09b22927542d6b0d5ebe233e467c", "question": "The old man needed to have rest multiple times a day, he would do what on the couch and catnap?", "question_concept": "have rest", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lay in bed", "lay in bed", "go to bed", "relax", "lie down"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The old man needed to have rest multiple times a day, he would do what on the couch and catnap?\nA. lay in bed\nB. lay in bed\nC. go to bed\nD. relax\nE. lie down\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The old man needed to have rest multiple times a day, he would do what on the couch and catnap?\nA. lay in bed\nB. lay in bed\nC. go to bed\nD. relax\nE. lie down\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The old man needed to have rest multiple times a day, he would do what on the couch and catnap?\nA. lay in bed\nB. lay in bed\nC. go to bed\nD. relax\nE. lie down\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The old man needed to have rest multiple times a day, he would do what on the couch and catnap?\nA. lay in bed\nB. lay in bed\nC. go to bed\nD. relax\nE. lie down\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The old man needed to have rest multiple times a day, he would do what on the couch and catnap?\nA. lay in bed\nB. lay in bed\nC. go to bed\nD. relax\nE. lie down\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.639366626739502", "False"]], [["-3.889366626739502", "False"]], [["-6.139366626739502", "False"]], [["-4.139366626739502", "False"]], [["-1.139366626739502", "True"]]], "filtered_resps": [["-3.639366626739502", "False"], ["-3.889366626739502", "False"], ["-6.139366626739502", "False"], ["-4.139366626739502", "False"], ["-1.139366626739502", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3044c400f83449f0a3eaabf7bc520a29546e59dc216dc59a91e4c77cf0ccc115", "prompt_hash": "19dc48d2d7255aaa6f6ebcdec7e6377d93c7b15bc51e6a6a0fb6bc1af0d1d4e3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 526, "doc": {"id": "e63a210053cf7f961ca0b5a7e6eb355d", "question": "The end of the barrel of what primitive firearm is bell shaped?", "question_concept": "bell", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["barbell", "funnel", "blunderbuss", "wind instrument", "kettlebell"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The end of the barrel of what primitive firearm is bell shaped?\nA. barbell\nB. funnel\nC. blunderbuss\nD. wind instrument\nE. kettlebell\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The end of the barrel of what primitive firearm is bell shaped?\nA. barbell\nB. funnel\nC. blunderbuss\nD. wind instrument\nE. kettlebell\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The end of the barrel of what primitive firearm is bell shaped?\nA. barbell\nB. funnel\nC. blunderbuss\nD. wind instrument\nE. kettlebell\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The end of the barrel of what primitive firearm is bell shaped?\nA. barbell\nB. funnel\nC. blunderbuss\nD. wind instrument\nE. kettlebell\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The end of the barrel of what primitive firearm is bell shaped?\nA. barbell\nB. funnel\nC. blunderbuss\nD. wind instrument\nE. kettlebell\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8513078689575195", "False"]], [["-4.6013078689575195", "False"]], [["-1.351307988166809", "False"]], [["-8.60130786895752", "False"]], [["-10.60130786895752", "False"]]], "filtered_resps": [["-3.8513078689575195", "False"], ["-4.6013078689575195", "False"], ["-1.351307988166809", "False"], ["-8.60130786895752", "False"], ["-10.60130786895752", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "60e6d138bb0bc28b55cc262853039594f287686f4365272ddbd2c08a6782c5ec", "prompt_hash": "b35c8738e73547628a9b1eb1975b268fd792fa015928950bcb48eec7e7f1032d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 527, "doc": {"id": "a4b4242fab25e86a9d7ffedcaecdcdbe", "question": "Where is a good place to store pamphlets in your home or office?", "question_concept": "pamphlets", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["library", "health department", "mail box", "drawer", "bookstore"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a good place to store pamphlets in your home or office?\nA. library\nB. health department\nC. mail box\nD. drawer\nE. bookstore\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a good place to store pamphlets in your home or office?\nA. library\nB. health department\nC. mail box\nD. drawer\nE. bookstore\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a good place to store pamphlets in your home or office?\nA. library\nB. health department\nC. mail box\nD. drawer\nE. bookstore\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a good place to store pamphlets in your home or office?\nA. library\nB. health department\nC. mail box\nD. drawer\nE. bookstore\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a good place to store pamphlets in your home or office?\nA. library\nB. health department\nC. mail box\nD. drawer\nE. bookstore\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.322253704071045", "False"]], [["-5.822253704071045", "False"]], [["-5.572253704071045", "False"]], [["-1.072253704071045", "True"]], [["-8.572254180908203", "False"]]], "filtered_resps": [["-2.322253704071045", "False"], ["-5.822253704071045", "False"], ["-5.572253704071045", "False"], ["-1.072253704071045", "True"], ["-8.572254180908203", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c864785e4422fa0bc37ea60cca7426980461b97a9988e57c9574b74d35453cb3", "prompt_hash": "ad040697a637ce528c42fc988a31df4b43da3ff5b43de912468d43252a95d4cc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 528, "doc": {"id": "ec8797b12e3c6666ebe70b2a7680b66f", "question": "Many humans enjoy fishing and enjoy another relaxing activity at the same time, what activity is it?", "question_concept": "fishing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["getting tied up lure.", "looking for information", "get wet", "drink beer", "sit quietly"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Many humans enjoy fishing and enjoy another relaxing activity at the same time, what activity is it?\nA. getting tied up lure.\nB. looking for information\nC. get wet\nD. drink beer\nE. sit quietly\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Many humans enjoy fishing and enjoy another relaxing activity at the same time, what activity is it?\nA. getting tied up lure.\nB. looking for information\nC. get wet\nD. drink beer\nE. sit quietly\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Many humans enjoy fishing and enjoy another relaxing activity at the same time, what activity is it?\nA. getting tied up lure.\nB. looking for information\nC. get wet\nD. drink beer\nE. sit quietly\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Many humans enjoy fishing and enjoy another relaxing activity at the same time, what activity is it?\nA. getting tied up lure.\nB. looking for information\nC. get wet\nD. drink beer\nE. sit quietly\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Many humans enjoy fishing and enjoy another relaxing activity at the same time, what activity is it?\nA. getting tied up lure.\nB. looking for information\nC. get wet\nD. drink beer\nE. sit quietly\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.47033166885376", "False"]], [["-2.9703316688537598", "False"]], [["-5.47033166885376", "False"]], [["-2.4703316688537598", "False"]], [["-2.7203316688537598", "False"]]], "filtered_resps": [["-4.47033166885376", "False"], ["-2.9703316688537598", "False"], ["-5.47033166885376", "False"], ["-2.4703316688537598", "False"], ["-2.7203316688537598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ea3d66e6d2118c42b15f73b94576a2849fb6caa2edf32bdc925ef56dbd9c77ce", "prompt_hash": "c4524928e1ec805ee437c3aa4a7863038a4d57c536d38689b8dd7956c055f612", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 529, "doc": {"id": "4536489e5d8e02aadc3fcc7a55effe20", "question": "Where would you get some maps that you own?", "question_concept": "maps", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bookstore", "library", "electrical circuit", "cabinet", "important when traveling"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you get some maps that you own?\nA. bookstore\nB. library\nC. electrical circuit\nD. cabinet\nE. important when traveling\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you get some maps that you own?\nA. bookstore\nB. library\nC. electrical circuit\nD. cabinet\nE. important when traveling\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you get some maps that you own?\nA. bookstore\nB. library\nC. electrical circuit\nD. cabinet\nE. important when traveling\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you get some maps that you own?\nA. bookstore\nB. library\nC. electrical circuit\nD. cabinet\nE. important when traveling\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you get some maps that you own?\nA. bookstore\nB. library\nC. electrical circuit\nD. cabinet\nE. important when traveling\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8968214392662048", "True"]], [["-3.6468214988708496", "False"]], [["-6.14682149887085", "False"]], [["-3.6468214988708496", "False"]], [["-4.64682149887085", "False"]]], "filtered_resps": [["-0.8968214392662048", "True"], ["-3.6468214988708496", "False"], ["-6.14682149887085", "False"], ["-3.6468214988708496", "False"], ["-4.64682149887085", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6edff1cca7f8e1aee50ec2159f2ffd77a934a6ba0155561584138859bb8b9457", "prompt_hash": "f2a27c64e7040e1f31aa16c5ccb5baff04aed4c2a4f1b1aa2bb543d6c4cdd822", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 530, "doc": {"id": "0854478d174c9127064f0d4b58df7e62", "question": "Where is a good place to put a hamburger?", "question_concept": "hamburger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["resturant", "fast food restaurant", "mouth", "kitchen", "pizza"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a good place to put a hamburger?\nA. resturant\nB. fast food restaurant\nC. mouth\nD. kitchen\nE. pizza\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a good place to put a hamburger?\nA. resturant\nB. fast food restaurant\nC. mouth\nD. kitchen\nE. pizza\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a good place to put a hamburger?\nA. resturant\nB. fast food restaurant\nC. mouth\nD. kitchen\nE. pizza\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a good place to put a hamburger?\nA. resturant\nB. fast food restaurant\nC. mouth\nD. kitchen\nE. pizza\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a good place to put a hamburger?\nA. resturant\nB. fast food restaurant\nC. mouth\nD. kitchen\nE. pizza\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3035011291503906", "False"]], [["-3.5535011291503906", "False"]], [["-1.3035011291503906", "True"]], [["-8.30350112915039", "False"]], [["-8.80350112915039", "False"]]], "filtered_resps": [["-3.3035011291503906", "False"], ["-3.5535011291503906", "False"], ["-1.3035011291503906", "True"], ["-8.30350112915039", "False"], ["-8.80350112915039", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e6cdd15c6bdf98548bc6b3c5f6ceed6ff0558622c9419aff28eb41181933f676", "prompt_hash": "dadbb61f46340be7b33ccd3d2259e0d8c439f0fa8f52c89dc60f2b186b7506ea", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 531, "doc": {"id": "4b7d1b70060cd1f1a7321795f62a7325", "question": "Where is a handy place to store a steel pen in your office?", "question_concept": "steel pen", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["car shop", "desk drawer", "car.", "warehouse", "hand"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a handy place to store a steel pen in your office?\nA. car shop\nB. desk drawer\nC. car.\nD. warehouse\nE. hand\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a handy place to store a steel pen in your office?\nA. car shop\nB. desk drawer\nC. car.\nD. warehouse\nE. hand\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a handy place to store a steel pen in your office?\nA. car shop\nB. desk drawer\nC. car.\nD. warehouse\nE. hand\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a handy place to store a steel pen in your office?\nA. car shop\nB. desk drawer\nC. car.\nD. warehouse\nE. hand\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a handy place to store a steel pen in your office?\nA. car shop\nB. desk drawer\nC. car.\nD. warehouse\nE. hand\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.371577739715576", "False"]], [["-1.1215778589248657", "True"]], [["-8.621578216552734", "False"]], [["-9.371578216552734", "False"]], [["-8.871578216552734", "False"]]], "filtered_resps": [["-2.371577739715576", "False"], ["-1.1215778589248657", "True"], ["-8.621578216552734", "False"], ["-9.371578216552734", "False"], ["-8.871578216552734", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f22144e0f05e770e277373c333bf260e9a39b9a24ab96b592da960bda41fabee", "prompt_hash": "509548ffa45fed374e6aa1f7ea2da3d0dae8af4daf21cc723d4b0a938286df1d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 532, "doc": {"id": "0e6a005eec5e6746f3facf4d608bfd8b", "question": "A story about World War II would be set when?", "question_concept": "story", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["book or library", "book or magazine", "newspaper", "past", "future"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: A story about World War II would be set when?\nA. book or library\nB. book or magazine\nC. newspaper\nD. past\nE. future\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A story about World War II would be set when?\nA. book or library\nB. book or magazine\nC. newspaper\nD. past\nE. future\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A story about World War II would be set when?\nA. book or library\nB. book or magazine\nC. newspaper\nD. past\nE. future\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A story about World War II would be set when?\nA. book or library\nB. book or magazine\nC. newspaper\nD. past\nE. future\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A story about World War II would be set when?\nA. book or library\nB. book or magazine\nC. newspaper\nD. past\nE. future\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.68873929977417", "False"]], [["-6.18873929977417", "False"]], [["-4.93873929977417", "False"]], [["-0.9387392997741699", "True"]], [["-10.438739776611328", "False"]]], "filtered_resps": [["-3.68873929977417", "False"], ["-6.18873929977417", "False"], ["-4.93873929977417", "False"], ["-0.9387392997741699", "True"], ["-10.438739776611328", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "716c591993546a0eb12518c31b0ab97c3183eeb960f85e1441da1345e8b87bf0", "prompt_hash": "0be0696f04080a1170adc873d5c2a268d19c4e5fda3a20dce3ca83d866f827f3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 533, "doc": {"id": "2d2b69ad187b7c40273ab13caab7dc19", "question": "What type of geographic area will you find a marmot?", "question_concept": "marmot", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mountainous area", "wood pile", "jungle", "petting zoo", "animals"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What type of geographic area will you find a marmot?\nA. mountainous area\nB. wood pile\nC. jungle\nD. petting zoo\nE. animals\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What type of geographic area will you find a marmot?\nA. mountainous area\nB. wood pile\nC. jungle\nD. petting zoo\nE. animals\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What type of geographic area will you find a marmot?\nA. mountainous area\nB. wood pile\nC. jungle\nD. petting zoo\nE. animals\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What type of geographic area will you find a marmot?\nA. mountainous area\nB. wood pile\nC. jungle\nD. petting zoo\nE. animals\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What type of geographic area will you find a marmot?\nA. mountainous area\nB. wood pile\nC. jungle\nD. petting zoo\nE. animals\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4430356025695801", "True"]], [["-6.44303560256958", "False"]], [["-9.193035125732422", "False"]], [["-8.693035125732422", "False"]], [["-8.693035125732422", "False"]]], "filtered_resps": [["-0.4430356025695801", "True"], ["-6.44303560256958", "False"], ["-9.193035125732422", "False"], ["-8.693035125732422", "False"], ["-8.693035125732422", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2fad92bba7921ae4ee3f55ff4a7069eb60af2beb5ea9510ab9c7c603b5f3307e", "prompt_hash": "454c4bbe69cf5bba1834b7aeeb0e2840f37a337d155f4bc4b4b0517fec933fb0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 534, "doc": {"id": "fde1f9bfc33da302449c0b950d16c0ea", "question": "Most people make stupid assumptions that are based on their prejudices.  What might they do instead to achieve better outcomes?", "question_concept": "most people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["set table", "think", "read books", "play games", "lie"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Most people make stupid assumptions that are based on their prejudices.  What might they do instead to achieve better outcomes?\nA. set table\nB. think\nC. read books\nD. play games\nE. lie\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Most people make stupid assumptions that are based on their prejudices.  What might they do instead to achieve better outcomes?\nA. set table\nB. think\nC. read books\nD. play games\nE. lie\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Most people make stupid assumptions that are based on their prejudices.  What might they do instead to achieve better outcomes?\nA. set table\nB. think\nC. read books\nD. play games\nE. lie\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Most people make stupid assumptions that are based on their prejudices.  What might they do instead to achieve better outcomes?\nA. set table\nB. think\nC. read books\nD. play games\nE. lie\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Most people make stupid assumptions that are based on their prejudices.  What might they do instead to achieve better outcomes?\nA. set table\nB. think\nC. read books\nD. play games\nE. lie\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.613177299499512", "False"]], [["-2.363177537918091", "False"]], [["-4.113177299499512", "False"]], [["-8.363177299499512", "False"]], [["-7.113177299499512", "False"]]], "filtered_resps": [["-4.613177299499512", "False"], ["-2.363177537918091", "False"], ["-4.113177299499512", "False"], ["-8.363177299499512", "False"], ["-7.113177299499512", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "86428da12ca8203a5b31bc7f21e3185d71168812aa3a0ed6def2785169c8121d", "prompt_hash": "812c6dc24a7b209c5779fbc3236cf7b622b2dfc2c2d2824c6908ef3e40365910", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 535, "doc": {"id": "3c90a632f46aeab11fbb73aa59a33892", "question": "What is something children can do while traveling in a car?", "question_concept": "children", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["listen to music", "watch television", "play chess", "walk", "play basketball"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is something children can do while traveling in a car?\nA. listen to music\nB. watch television\nC. play chess\nD. walk\nE. play basketball\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is something children can do while traveling in a car?\nA. listen to music\nB. watch television\nC. play chess\nD. walk\nE. play basketball\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is something children can do while traveling in a car?\nA. listen to music\nB. watch television\nC. play chess\nD. walk\nE. play basketball\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is something children can do while traveling in a car?\nA. listen to music\nB. watch television\nC. play chess\nD. walk\nE. play basketball\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is something children can do while traveling in a car?\nA. listen to music\nB. watch television\nC. play chess\nD. walk\nE. play basketball\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4852330684661865", "True"]], [["-5.485233306884766", "False"]], [["-4.235233306884766", "False"]], [["-6.735233306884766", "False"]], [["-8.985233306884766", "False"]]], "filtered_resps": [["-0.4852330684661865", "True"], ["-5.485233306884766", "False"], ["-4.235233306884766", "False"], ["-6.735233306884766", "False"], ["-8.985233306884766", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5e890bbe1c316bb881f92b0e85a957fea43a3e817d93736a17f56388871d372c", "prompt_hash": "0421869a4e5d88c98372e03cd7bb1746bd1ad609141c7a59c5b62cae025f7921", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 536, "doc": {"id": "1f3ccb722600da7d862531416934949a", "question": "Where would you hear a trumpet along with other instruments made from the same material?", "question_concept": "trumpet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["music store", "bass", "brass band", "orchestra", "marching band"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you hear a trumpet along with other instruments made from the same material?\nA. music store\nB. bass\nC. brass band\nD. orchestra\nE. marching band\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you hear a trumpet along with other instruments made from the same material?\nA. music store\nB. bass\nC. brass band\nD. orchestra\nE. marching band\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you hear a trumpet along with other instruments made from the same material?\nA. music store\nB. bass\nC. brass band\nD. orchestra\nE. marching band\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you hear a trumpet along with other instruments made from the same material?\nA. music store\nB. bass\nC. brass band\nD. orchestra\nE. marching band\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you hear a trumpet along with other instruments made from the same material?\nA. music store\nB. bass\nC. brass band\nD. orchestra\nE. marching band\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.09072208404541", "False"]], [["-4.84072208404541", "False"]], [["-1.3407222032546997", "True"]], [["-6.59072208404541", "False"]], [["-3.34072208404541", "False"]]], "filtered_resps": [["-2.09072208404541", "False"], ["-4.84072208404541", "False"], ["-1.3407222032546997", "True"], ["-6.59072208404541", "False"], ["-3.34072208404541", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8c592eefb7cc6a68c531999b1f61876d348fb6b7b33b48375ca4b9950cea16a5", "prompt_hash": "cbe21ddad728446e28e12389c6896f96f60343ad3dde321b7b863389b5198cb5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 537, "doc": {"id": "46ba5d2b8cfc6708e5e2618568d8730e", "question": "The audience listened to the orchestra play, where were they watching the performance?", "question_concept": "audience", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["concert hall", "museum", "school", "hockey game", "sporting event"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The audience listened to the orchestra play, where were they watching the performance?\nA. concert hall\nB. museum\nC. school\nD. hockey game\nE. sporting event\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The audience listened to the orchestra play, where were they watching the performance?\nA. concert hall\nB. museum\nC. school\nD. hockey game\nE. sporting event\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The audience listened to the orchestra play, where were they watching the performance?\nA. concert hall\nB. museum\nC. school\nD. hockey game\nE. sporting event\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The audience listened to the orchestra play, where were they watching the performance?\nA. concert hall\nB. museum\nC. school\nD. hockey game\nE. sporting event\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The audience listened to the orchestra play, where were they watching the performance?\nA. concert hall\nB. museum\nC. school\nD. hockey game\nE. sporting event\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.697909414768219", "True"]], [["-5.947909355163574", "False"]], [["-5.447909355163574", "False"]], [["-7.947909355163574", "False"]], [["-9.197909355163574", "False"]]], "filtered_resps": [["-0.697909414768219", "True"], ["-5.947909355163574", "False"], ["-5.447909355163574", "False"], ["-7.947909355163574", "False"], ["-9.197909355163574", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "933ad9cc3a13e1ca0167a418e300019a9e61dfd0daa896da4790cb982b8527fb", "prompt_hash": "6a3fa8bb7e3b1ab0b3a6f34d880af630075e8f4d4ed8fe9f5ed4de5a824f187b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 538, "doc": {"id": "f8a2cbc7189b92a809ce9cd857030621", "question": "Stabbing to death of a person is what sort of way to die?", "question_concept": "stabbing to death", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pool of blood", "gruesome", "charming", "being arrested", "killing"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Stabbing to death of a person is what sort of way to die?\nA. pool of blood\nB. gruesome\nC. charming\nD. being arrested\nE. killing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Stabbing to death of a person is what sort of way to die?\nA. pool of blood\nB. gruesome\nC. charming\nD. being arrested\nE. killing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Stabbing to death of a person is what sort of way to die?\nA. pool of blood\nB. gruesome\nC. charming\nD. being arrested\nE. killing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Stabbing to death of a person is what sort of way to die?\nA. pool of blood\nB. gruesome\nC. charming\nD. being arrested\nE. killing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Stabbing to death of a person is what sort of way to die?\nA. pool of blood\nB. gruesome\nC. charming\nD. being arrested\nE. killing\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.935587167739868", "False"]], [["-0.6855871081352234", "True"]], [["-7.435586929321289", "False"]], [["-6.935586929321289", "False"]], [["-6.685586929321289", "False"]]], "filtered_resps": [["-2.935587167739868", "False"], ["-0.6855871081352234", "True"], ["-7.435586929321289", "False"], ["-6.935586929321289", "False"], ["-6.685586929321289", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c62a3153d67630ba815f6b511c58ad0a16e21e2b42d2621a6395f56b72f0f17e", "prompt_hash": "1b3a6c6b370d855828c217a128be9f7e57129b9f47db9862fbdf521914fb2b45", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 539, "doc": {"id": "225287e06c993feee34e0f06b25f6ba8", "question": "What are you getting from you boss at the end of the week?", "question_concept": "getting", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["asking for", "money", "food", "work", "energy"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What are you getting from you boss at the end of the week?\nA. asking for\nB. money\nC. food\nD. work\nE. energy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are you getting from you boss at the end of the week?\nA. asking for\nB. money\nC. food\nD. work\nE. energy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are you getting from you boss at the end of the week?\nA. asking for\nB. money\nC. food\nD. work\nE. energy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are you getting from you boss at the end of the week?\nA. asking for\nB. money\nC. food\nD. work\nE. energy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are you getting from you boss at the end of the week?\nA. asking for\nB. money\nC. food\nD. work\nE. energy\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.531728744506836", "True"]], [["-1.781728744506836", "False"]], [["-6.281728744506836", "False"]], [["-3.031728744506836", "False"]], [["-6.031728744506836", "False"]]], "filtered_resps": [["-1.531728744506836", "True"], ["-1.781728744506836", "False"], ["-6.281728744506836", "False"], ["-3.031728744506836", "False"], ["-6.031728744506836", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "71f409cea72f19ae0e9bc3f3a8b9d4caed645765e913c40f3caa6f30236857c6", "prompt_hash": "dbddd1774d289ec92cae1ebfb30c3648e6aae9b5db27f2325497f03c4f0f0c72", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 540, "doc": {"id": "e211b1a3f3401d164c8b0bfc10160caa", "question": "If you have a ticket and you are planning to eat hot dogs, where would you go?", "question_concept": "ticket", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lottery", "person's hand", "baseball stadium", "movie", "kitchen"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If you have a ticket and you are planning to eat hot dogs, where would you go?\nA. lottery\nB. person's hand\nC. baseball stadium\nD. movie\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you have a ticket and you are planning to eat hot dogs, where would you go?\nA. lottery\nB. person's hand\nC. baseball stadium\nD. movie\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you have a ticket and you are planning to eat hot dogs, where would you go?\nA. lottery\nB. person's hand\nC. baseball stadium\nD. movie\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you have a ticket and you are planning to eat hot dogs, where would you go?\nA. lottery\nB. person's hand\nC. baseball stadium\nD. movie\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you have a ticket and you are planning to eat hot dogs, where would you go?\nA. lottery\nB. person's hand\nC. baseball stadium\nD. movie\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.511051654815674", "False"]], [["-6.011051654815674", "False"]], [["-0.5110515356063843", "True"]], [["-7.761051654815674", "False"]], [["-8.011051177978516", "False"]]], "filtered_resps": [["-4.511051654815674", "False"], ["-6.011051654815674", "False"], ["-0.5110515356063843", "True"], ["-7.761051654815674", "False"], ["-8.011051177978516", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "724197eb1b2b1504d81b507d36bbef56ffffdad2b6e0100a5801d35dbaa7743c", "prompt_hash": "676c1ab53a12b64dd7eba19f85c691f087f8b245318509dfa2851a09ea0bc83f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 541, "doc": {"id": "fce1c5d069758aea57a787fc98dcf7a9", "question": "Where is a great place to buy fresh fruit?", "question_concept": "fruit", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["san francisco", "refrigerator", "big box retailer", "tree", "market"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a great place to buy fresh fruit?\nA. san francisco\nB. refrigerator\nC. big box retailer\nD. tree\nE. market\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a great place to buy fresh fruit?\nA. san francisco\nB. refrigerator\nC. big box retailer\nD. tree\nE. market\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a great place to buy fresh fruit?\nA. san francisco\nB. refrigerator\nC. big box retailer\nD. tree\nE. market\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a great place to buy fresh fruit?\nA. san francisco\nB. refrigerator\nC. big box retailer\nD. tree\nE. market\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a great place to buy fresh fruit?\nA. san francisco\nB. refrigerator\nC. big box retailer\nD. tree\nE. market\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3616242408752441", "False"]], [["-5.861624240875244", "False"]], [["-3.861624240875244", "False"]], [["-6.861624240875244", "False"]], [["-1.1116242408752441", "True"]]], "filtered_resps": [["-1.3616242408752441", "False"], ["-5.861624240875244", "False"], ["-3.861624240875244", "False"], ["-6.861624240875244", "False"], ["-1.1116242408752441", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c6bfd4da5ff88d7f6ded1fbabd5ce47faa890aaad36dcd4cd818a60d12a2d085", "prompt_hash": "3599309f74e44433250b33c7da787351d86431aa76b30cdb6b0fa97fd6ddfbdb", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 542, "doc": {"id": "c0d75f9fbf30aa3a612f16edb20d6b8d", "question": "The man took paperwork to other people to consult over it, where was he heading?", "question_concept": "paperwork", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desk", "meeting", "office", "table", "work"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The man took paperwork to other people to consult over it, where was he heading?\nA. desk\nB. meeting\nC. office\nD. table\nE. work\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man took paperwork to other people to consult over it, where was he heading?\nA. desk\nB. meeting\nC. office\nD. table\nE. work\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man took paperwork to other people to consult over it, where was he heading?\nA. desk\nB. meeting\nC. office\nD. table\nE. work\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man took paperwork to other people to consult over it, where was he heading?\nA. desk\nB. meeting\nC. office\nD. table\nE. work\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man took paperwork to other people to consult over it, where was he heading?\nA. desk\nB. meeting\nC. office\nD. table\nE. work\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.4132251739501953", "False"]], [["-1.9132252931594849", "False"]], [["-3.9132251739501953", "False"]], [["-6.663225173950195", "False"]], [["-4.913225173950195", "False"]]], "filtered_resps": [["-3.4132251739501953", "False"], ["-1.9132252931594849", "False"], ["-3.9132251739501953", "False"], ["-6.663225173950195", "False"], ["-4.913225173950195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4d4e6264c4af699ce96d08df0bfa94cc56672b1581dc50d9de68992e015e5f4c", "prompt_hash": "65848791ba51229b06468d83f5f838404f1a4ca8901e594ed85d0d5d09894c81", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 543, "doc": {"id": "d07f149d8d953dcc45dda432194c375e", "question": "Stark was just having fun, and he wasn't hurting anyone.  What might have he been doing?", "question_concept": "having fun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["painting his nails", "playing marbles", "constructing", "need for rest", "wild ride"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Stark was just having fun, and he wasn't hurting anyone.  What might have he been doing?\nA. painting his nails\nB. playing marbles\nC. constructing\nD. need for rest\nE. wild ride\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Stark was just having fun, and he wasn't hurting anyone.  What might have he been doing?\nA. painting his nails\nB. playing marbles\nC. constructing\nD. need for rest\nE. wild ride\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Stark was just having fun, and he wasn't hurting anyone.  What might have he been doing?\nA. painting his nails\nB. playing marbles\nC. constructing\nD. need for rest\nE. wild ride\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Stark was just having fun, and he wasn't hurting anyone.  What might have he been doing?\nA. painting his nails\nB. playing marbles\nC. constructing\nD. need for rest\nE. wild ride\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Stark was just having fun, and he wasn't hurting anyone.  What might have he been doing?\nA. painting his nails\nB. playing marbles\nC. constructing\nD. need for rest\nE. wild ride\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.738842964172363", "False"]], [["-1.2388427257537842", "True"]], [["-5.988842964172363", "False"]], [["-7.238842964172363", "False"]], [["-7.488842964172363", "False"]]], "filtered_resps": [["-4.738842964172363", "False"], ["-1.2388427257537842", "True"], ["-5.988842964172363", "False"], ["-7.238842964172363", "False"], ["-7.488842964172363", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7e37563bc9fe43f551df452dde00da4ec106f280fbac3c2037bbb8fa0f8c3f4f", "prompt_hash": "325bf9c97117df44d5fc45e4678edd8ef5b84bbb9cf8f8d7ecdfb5475d4088c4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 544, "doc": {"id": "080a9cf2d6447a9a4d98b0af311e10da", "question": "The church was giving assistance, what were they hoping to accomplish?", "question_concept": "giving assistance", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["exhilliration", "hardship", "risk taking", "helping others", "happiness"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The church was giving assistance, what were they hoping to accomplish?\nA. exhilliration\nB. hardship\nC. risk taking\nD. helping others\nE. happiness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The church was giving assistance, what were they hoping to accomplish?\nA. exhilliration\nB. hardship\nC. risk taking\nD. helping others\nE. happiness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The church was giving assistance, what were they hoping to accomplish?\nA. exhilliration\nB. hardship\nC. risk taking\nD. helping others\nE. happiness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The church was giving assistance, what were they hoping to accomplish?\nA. exhilliration\nB. hardship\nC. risk taking\nD. helping others\nE. happiness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The church was giving assistance, what were they hoping to accomplish?\nA. exhilliration\nB. hardship\nC. risk taking\nD. helping others\nE. happiness\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.6679368019104", "False"]], [["-8.167937278747559", "False"]], [["-9.167937278747559", "False"]], [["-1.41793692111969", "False"]], [["-9.667937278747559", "False"]]], "filtered_resps": [["-6.6679368019104", "False"], ["-8.167937278747559", "False"], ["-9.167937278747559", "False"], ["-1.41793692111969", "False"], ["-9.667937278747559", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cd1c9d30ffb06ff5886cbec9ce53d4340af4abf02db1737ecf1c3334088daa63", "prompt_hash": "8d5e18202dae9fae305fe94456efd9f1ac02f870f4ab571697c650a32d915fc0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 545, "doc": {"id": "111501a49dd41ceed9c2073eed5d2b72", "question": "I you believe in god, where will you go when you die?", "question_concept": "god", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["heaven", "church", "imagination", "synagogue", "monastery"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: I you believe in god, where will you go when you die?\nA. heaven\nB. church\nC. imagination\nD. synagogue\nE. monastery\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I you believe in god, where will you go when you die?\nA. heaven\nB. church\nC. imagination\nD. synagogue\nE. monastery\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I you believe in god, where will you go when you die?\nA. heaven\nB. church\nC. imagination\nD. synagogue\nE. monastery\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I you believe in god, where will you go when you die?\nA. heaven\nB. church\nC. imagination\nD. synagogue\nE. monastery\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I you believe in god, where will you go when you die?\nA. heaven\nB. church\nC. imagination\nD. synagogue\nE. monastery\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2569842338562012", "True"]], [["-5.756984233856201", "False"]], [["-6.506984233856201", "False"]], [["-7.006984233856201", "False"]], [["-9.00698471069336", "False"]]], "filtered_resps": [["-1.2569842338562012", "True"], ["-5.756984233856201", "False"], ["-6.506984233856201", "False"], ["-7.006984233856201", "False"], ["-9.00698471069336", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9dcf81f4f1bc51e7ca9904cb35f5c8df81767dbbda0ca62c5721335111431a36", "prompt_hash": "ddd5c86c84077512756c26b7ca2709fae812a55342ad8b92fcb1863d2f7d675e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 546, "doc": {"id": "7bb87c6d8eab57d4e983f60025b1f0dc", "question": "What can eating hamburger cause immediately after eating it?", "question_concept": "eating hamburger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tasty", "health problems", "eat cake", "indigestion", "gain weight"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What can eating hamburger cause immediately after eating it?\nA. tasty\nB. health problems\nC. eat cake\nD. indigestion\nE. gain weight\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can eating hamburger cause immediately after eating it?\nA. tasty\nB. health problems\nC. eat cake\nD. indigestion\nE. gain weight\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can eating hamburger cause immediately after eating it?\nA. tasty\nB. health problems\nC. eat cake\nD. indigestion\nE. gain weight\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can eating hamburger cause immediately after eating it?\nA. tasty\nB. health problems\nC. eat cake\nD. indigestion\nE. gain weight\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can eating hamburger cause immediately after eating it?\nA. tasty\nB. health problems\nC. eat cake\nD. indigestion\nE. gain weight\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.83787727355957", "False"]], [["-3.5878772735595703", "False"]], [["-5.83787727355957", "False"]], [["-1.5878773927688599", "True"]], [["-1.5878773927688599", "True"]]], "filtered_resps": [["-4.83787727355957", "False"], ["-3.5878772735595703", "False"], ["-5.83787727355957", "False"], ["-1.5878773927688599", "True"], ["-1.5878773927688599", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "69555e1eedc1dfdc208dd1f2be4c601be3c49b081a786590bd957f796b5131c8", "prompt_hash": "dcf5542bded52a764d360d3d807c11d7fca717adae5c11d15b5eef73dc0bef93", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 547, "doc": {"id": "5c2bc4335c8860342ec2d568ceb6ac6b", "question": "Where is a shelf likely to be hidden behind a door?", "question_concept": "shelf", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["refrigerator", "bookstore", "cupboard", "school building", "wardrobe"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a shelf likely to be hidden behind a door?\nA. refrigerator\nB. bookstore\nC. cupboard\nD. school building\nE. wardrobe\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a shelf likely to be hidden behind a door?\nA. refrigerator\nB. bookstore\nC. cupboard\nD. school building\nE. wardrobe\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a shelf likely to be hidden behind a door?\nA. refrigerator\nB. bookstore\nC. cupboard\nD. school building\nE. wardrobe\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a shelf likely to be hidden behind a door?\nA. refrigerator\nB. bookstore\nC. cupboard\nD. school building\nE. wardrobe\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a shelf likely to be hidden behind a door?\nA. refrigerator\nB. bookstore\nC. cupboard\nD. school building\nE. wardrobe\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.24422287940979", "False"]], [["-5.494222640991211", "False"]], [["-2.24422287940979", "False"]], [["-6.744222640991211", "False"]], [["-1.74422287940979", "True"]]], "filtered_resps": [["-2.24422287940979", "False"], ["-5.494222640991211", "False"], ["-2.24422287940979", "False"], ["-6.744222640991211", "False"], ["-1.74422287940979", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "78ce3cd577dcccc16e12e644034b8242024cc132bd49dccedf8fe69f8f6b33e4", "prompt_hash": "b4c052dbe6859d78c0aa5047b59463aef9f8239052b7d5e40b0277c61f055668", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 548, "doc": {"id": "083861fc5ebb9226fff70544f3f83d2b", "question": "The man got a pail to catch the draining motor oil, where was he likely doing this at home?", "question_concept": "pail", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["garage", "hardware store", "utility room", "wishing well", "laundry"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The man got a pail to catch the draining motor oil, where was he likely doing this at home?\nA. garage\nB. hardware store\nC. utility room\nD. wishing well\nE. laundry\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man got a pail to catch the draining motor oil, where was he likely doing this at home?\nA. garage\nB. hardware store\nC. utility room\nD. wishing well\nE. laundry\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man got a pail to catch the draining motor oil, where was he likely doing this at home?\nA. garage\nB. hardware store\nC. utility room\nD. wishing well\nE. laundry\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man got a pail to catch the draining motor oil, where was he likely doing this at home?\nA. garage\nB. hardware store\nC. utility room\nD. wishing well\nE. laundry\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man got a pail to catch the draining motor oil, where was he likely doing this at home?\nA. garage\nB. hardware store\nC. utility room\nD. wishing well\nE. laundry\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8223766088485718", "True"]], [["-7.322376728057861", "False"]], [["-6.572376728057861", "False"]], [["-7.572376728057861", "False"]], [["-9.322376251220703", "False"]]], "filtered_resps": [["-0.8223766088485718", "True"], ["-7.322376728057861", "False"], ["-6.572376728057861", "False"], ["-7.572376728057861", "False"], ["-9.322376251220703", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fe90dd9828dde36711e28a30a36c792f067bd271622d43daa68dd3e88dffd94e", "prompt_hash": "a803ec70f2041253974bda35122e311a431a003a1ff3873c907541fee79d042f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 549, "doc": {"id": "520b0eea9148e3cb4d45aa69a55491eb", "question": "What kind of cold storage could you find in your house?", "question_concept": "cold storage", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ice pack", "freezer", "laboratory", "warehouse", "refrigerator"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of cold storage could you find in your house?\nA. ice pack\nB. freezer\nC. laboratory\nD. warehouse\nE. refrigerator\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of cold storage could you find in your house?\nA. ice pack\nB. freezer\nC. laboratory\nD. warehouse\nE. refrigerator\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of cold storage could you find in your house?\nA. ice pack\nB. freezer\nC. laboratory\nD. warehouse\nE. refrigerator\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of cold storage could you find in your house?\nA. ice pack\nB. freezer\nC. laboratory\nD. warehouse\nE. refrigerator\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of cold storage could you find in your house?\nA. ice pack\nB. freezer\nC. laboratory\nD. warehouse\nE. refrigerator\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.527496814727783", "False"]], [["-1.5274966955184937", "True"]], [["-4.777496814727783", "False"]], [["-7.277496814727783", "False"]], [["-1.7774966955184937", "False"]]], "filtered_resps": [["-2.527496814727783", "False"], ["-1.5274966955184937", "True"], ["-4.777496814727783", "False"], ["-7.277496814727783", "False"], ["-1.7774966955184937", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7eff96c2e0257613400f59d79544755f46b52b6d982a3c83f40fcd02b6593f64", "prompt_hash": "c601c0486a3024000621b24aa2454eac547665da9f6fa1bfd91f104deb146d90", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 550, "doc": {"id": "ef6ede0af827ddd1dc7bbeb36a6fdd22", "question": "Where could you go to between 1000 and 10000 restaurant?", "question_concept": "restaurant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["big city", "town", "small town", "canada", "yellow pages"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where could you go to between 1000 and 10000 restaurant?\nA. big city\nB. town\nC. small town\nD. canada\nE. yellow pages\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could you go to between 1000 and 10000 restaurant?\nA. big city\nB. town\nC. small town\nD. canada\nE. yellow pages\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could you go to between 1000 and 10000 restaurant?\nA. big city\nB. town\nC. small town\nD. canada\nE. yellow pages\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could you go to between 1000 and 10000 restaurant?\nA. big city\nB. town\nC. small town\nD. canada\nE. yellow pages\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could you go to between 1000 and 10000 restaurant?\nA. big city\nB. town\nC. small town\nD. canada\nE. yellow pages\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2031961679458618", "True"]], [["-5.453196048736572", "False"]], [["-7.203196048736572", "False"]], [["-8.20319652557373", "False"]], [["-3.2031960487365723", "False"]]], "filtered_resps": [["-1.2031961679458618", "True"], ["-5.453196048736572", "False"], ["-7.203196048736572", "False"], ["-8.20319652557373", "False"], ["-3.2031960487365723", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "556fcbafa865039692ba1316bce217a8cd16255a7b64ff09151a3a2a724b5095", "prompt_hash": "972bfcd25ca6a8f4c1e987f22cdb2643c2c62cd30703d8e97181c516ddbad75e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 551, "doc": {"id": "d47986deb91d64b2b15d385da3d2f483", "question": "The pitcher stepped on the mound ready to throw, where was he located specifically?", "question_concept": "mound", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hell", "baseball stadium", "golf course", "africa", "baseball diamond"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The pitcher stepped on the mound ready to throw, where was he located specifically?\nA. hell\nB. baseball stadium\nC. golf course\nD. africa\nE. baseball diamond\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The pitcher stepped on the mound ready to throw, where was he located specifically?\nA. hell\nB. baseball stadium\nC. golf course\nD. africa\nE. baseball diamond\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The pitcher stepped on the mound ready to throw, where was he located specifically?\nA. hell\nB. baseball stadium\nC. golf course\nD. africa\nE. baseball diamond\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The pitcher stepped on the mound ready to throw, where was he located specifically?\nA. hell\nB. baseball stadium\nC. golf course\nD. africa\nE. baseball diamond\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The pitcher stepped on the mound ready to throw, where was he located specifically?\nA. hell\nB. baseball stadium\nC. golf course\nD. africa\nE. baseball diamond\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.309133291244507", "False"]], [["-1.5591332912445068", "True"]], [["-8.309133529663086", "False"]], [["-9.059133529663086", "False"]], [["-1.8091332912445068", "False"]]], "filtered_resps": [["-3.309133291244507", "False"], ["-1.5591332912445068", "True"], ["-8.309133529663086", "False"], ["-9.059133529663086", "False"], ["-1.8091332912445068", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4a357e274ba8587feb9a599eafc633c1fa870de63a2bd87d3686ba39002735e3", "prompt_hash": "80540234665b8bbdcba549e48fab9a98f21cb72172b93f7f623d37e855556d4f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 552, "doc": {"id": "c3b7f4196b12714940ac1b9417194df4", "question": "Where is a statute found on a platform?", "question_concept": "platform", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["below", "arena", "concert hall", "museum", "building"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a statute found on a platform?\nA. below\nB. arena\nC. concert hall\nD. museum\nE. building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a statute found on a platform?\nA. below\nB. arena\nC. concert hall\nD. museum\nE. building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a statute found on a platform?\nA. below\nB. arena\nC. concert hall\nD. museum\nE. building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a statute found on a platform?\nA. below\nB. arena\nC. concert hall\nD. museum\nE. building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a statute found on a platform?\nA. below\nB. arena\nC. concert hall\nD. museum\nE. building\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4683821201324463", "True"]], [["-5.218381881713867", "False"]], [["-7.468381881713867", "False"]], [["-6.218381881713867", "False"]], [["-1.9683821201324463", "False"]]], "filtered_resps": [["-1.4683821201324463", "True"], ["-5.218381881713867", "False"], ["-7.468381881713867", "False"], ["-6.218381881713867", "False"], ["-1.9683821201324463", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cf7be21d48483826301a1edc14baa6ca911fb011077434a0f830c0553c042ba7", "prompt_hash": "523bae74a0ed6304af0b3eec91200234a31aa7e34d8ff72121d2c5a88bb0af28", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 553, "doc": {"id": "5d03ad171fd661a28da5b6eb79967a6b", "question": "If it's not used for hair a round brush is an example of what?", "question_concept": "round brush", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hair brush", "ladies bathroom", "art supplies", "shower", "hair salon"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If it's not used for hair a round brush is an example of what?\nA. hair brush\nB. ladies bathroom\nC. art supplies\nD. shower\nE. hair salon\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If it's not used for hair a round brush is an example of what?\nA. hair brush\nB. ladies bathroom\nC. art supplies\nD. shower\nE. hair salon\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If it's not used for hair a round brush is an example of what?\nA. hair brush\nB. ladies bathroom\nC. art supplies\nD. shower\nE. hair salon\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If it's not used for hair a round brush is an example of what?\nA. hair brush\nB. ladies bathroom\nC. art supplies\nD. shower\nE. hair salon\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If it's not used for hair a round brush is an example of what?\nA. hair brush\nB. ladies bathroom\nC. art supplies\nD. shower\nE. hair salon\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4796527028083801", "True"]], [["-5.7296528816223145", "False"]], [["-4.9796528816223145", "False"]], [["-8.479652404785156", "False"]], [["-6.7296528816223145", "False"]]], "filtered_resps": [["-0.4796527028083801", "True"], ["-5.7296528816223145", "False"], ["-4.9796528816223145", "False"], ["-8.479652404785156", "False"], ["-6.7296528816223145", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c3048fdfa3ffaaa112bc77e6195b332ce18c37f4ea83590e8e3c532cee940fc2", "prompt_hash": "0fbea625952545ebd017acc0f4353dc362443143657e970faeddc1ceba257d73", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 554, "doc": {"id": "7c95d753943c58757fe6e1ccff8aea14", "question": "His parents thought he was suffering from boredom, but the teen loved to lay in bed and just do what?", "question_concept": "boredom", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["meet interesting people", "lift weights", "listen to music", "play chess", "entertain"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: His parents thought he was suffering from boredom, but the teen loved to lay in bed and just do what?\nA. meet interesting people\nB. lift weights\nC. listen to music\nD. play chess\nE. entertain\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: His parents thought he was suffering from boredom, but the teen loved to lay in bed and just do what?\nA. meet interesting people\nB. lift weights\nC. listen to music\nD. play chess\nE. entertain\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: His parents thought he was suffering from boredom, but the teen loved to lay in bed and just do what?\nA. meet interesting people\nB. lift weights\nC. listen to music\nD. play chess\nE. entertain\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: His parents thought he was suffering from boredom, but the teen loved to lay in bed and just do what?\nA. meet interesting people\nB. lift weights\nC. listen to music\nD. play chess\nE. entertain\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: His parents thought he was suffering from boredom, but the teen loved to lay in bed and just do what?\nA. meet interesting people\nB. lift weights\nC. listen to music\nD. play chess\nE. entertain\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.200538158416748", "False"]], [["-3.200538158416748", "False"]], [["-2.450538158416748", "False"]], [["-5.950538158416748", "False"]], [["-2.700538158416748", "False"]]], "filtered_resps": [["-4.200538158416748", "False"], ["-3.200538158416748", "False"], ["-2.450538158416748", "False"], ["-5.950538158416748", "False"], ["-2.700538158416748", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9a4df1c76346a6d824c4d8ff2801619ae2234c25ddd2dc2756ba2bdfd743585b", "prompt_hash": "3e1541747d7a94e1061eaa2f07599e70909a19eed28ec1c4b59ce82211548639", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 555, "doc": {"id": "88d8bfb9dc8e77ef642acbe1a129f3db", "question": "At the picnic she was stuck eating hamburger, she was worried because she forgot her chewables to prevent what?", "question_concept": "eating hamburger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat cake", "have fun", "food poisoning", "heartburn", "gain weight"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: At the picnic she was stuck eating hamburger, she was worried because she forgot her chewables to prevent what?\nA. eat cake\nB. have fun\nC. food poisoning\nD. heartburn\nE. gain weight\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: At the picnic she was stuck eating hamburger, she was worried because she forgot her chewables to prevent what?\nA. eat cake\nB. have fun\nC. food poisoning\nD. heartburn\nE. gain weight\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: At the picnic she was stuck eating hamburger, she was worried because she forgot her chewables to prevent what?\nA. eat cake\nB. have fun\nC. food poisoning\nD. heartburn\nE. gain weight\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: At the picnic she was stuck eating hamburger, she was worried because she forgot her chewables to prevent what?\nA. eat cake\nB. have fun\nC. food poisoning\nD. heartburn\nE. gain weight\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: At the picnic she was stuck eating hamburger, she was worried because she forgot her chewables to prevent what?\nA. eat cake\nB. have fun\nC. food poisoning\nD. heartburn\nE. gain weight\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.44896125793457", "False"]], [["-4.94896125793457", "False"]], [["-0.9489611983299255", "True"]], [["-4.19896125793457", "False"]], [["-7.19896125793457", "False"]]], "filtered_resps": [["-5.44896125793457", "False"], ["-4.94896125793457", "False"], ["-0.9489611983299255", "True"], ["-4.19896125793457", "False"], ["-7.19896125793457", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b1e1a00ff13d413285eab0997244b07affdcbf3e75ff35c27037f7f26a7d6951", "prompt_hash": "31d7d65e80d6aace5f8a168559b310f5be0c754b3ab0661a0ed4b1221bb21430", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 556, "doc": {"id": "b1a9b20793b46e46e1beedadbf852f84", "question": "The electrode wouldn't spark, it turned out that the what hadn't been connected?", "question_concept": "electrode", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["battery", "electronic equipment", "electrolytic cell", "charge", "tube"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The electrode wouldn't spark, it turned out that the what hadn't been connected?\nA. battery\nB. electronic equipment\nC. electrolytic cell\nD. charge\nE. tube\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The electrode wouldn't spark, it turned out that the what hadn't been connected?\nA. battery\nB. electronic equipment\nC. electrolytic cell\nD. charge\nE. tube\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The electrode wouldn't spark, it turned out that the what hadn't been connected?\nA. battery\nB. electronic equipment\nC. electrolytic cell\nD. charge\nE. tube\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The electrode wouldn't spark, it turned out that the what hadn't been connected?\nA. battery\nB. electronic equipment\nC. electrolytic cell\nD. charge\nE. tube\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The electrode wouldn't spark, it turned out that the what hadn't been connected?\nA. battery\nB. electronic equipment\nC. electrolytic cell\nD. charge\nE. tube\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.829169511795044", "True"]], [["-5.579169273376465", "False"]], [["-4.329169273376465", "False"]], [["-5.579169273376465", "False"]], [["-4.579169273376465", "False"]]], "filtered_resps": [["-0.829169511795044", "True"], ["-5.579169273376465", "False"], ["-4.329169273376465", "False"], ["-5.579169273376465", "False"], ["-4.579169273376465", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8d18dc98e67b6836ac99a872221420b384117d26d4ba9cbdffaffc04e387650a", "prompt_hash": "aad90aafca49118196b9e3c5503d90e5c8dbd3b3caafd5b5f207d971608b6f03", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 557, "doc": {"id": "81e016974d33fe383c848b6c819791cd", "question": "For what entity should the government work?", "question_concept": "government", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["country", "democracy", "canada", "civilization", "tax office"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: For what entity should the government work?\nA. country\nB. democracy\nC. canada\nD. civilization\nE. tax office\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: For what entity should the government work?\nA. country\nB. democracy\nC. canada\nD. civilization\nE. tax office\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: For what entity should the government work?\nA. country\nB. democracy\nC. canada\nD. civilization\nE. tax office\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: For what entity should the government work?\nA. country\nB. democracy\nC. canada\nD. civilization\nE. tax office\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: For what entity should the government work?\nA. country\nB. democracy\nC. canada\nD. civilization\nE. tax office\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9413740634918213", "False"]], [["-1.6913740634918213", "False"]], [["-7.941373825073242", "False"]], [["-5.941373825073242", "False"]], [["-8.691373825073242", "False"]]], "filtered_resps": [["-2.9413740634918213", "False"], ["-1.6913740634918213", "False"], ["-7.941373825073242", "False"], ["-5.941373825073242", "False"], ["-8.691373825073242", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e6cc42e879e4a4f815654e0d7362ee0fd6bdd58a0b3bd3dd71032eea18676618", "prompt_hash": "768bec855f8e75917d0e983ef5c1a8ba6d3df64003f4dc05a94e754b2935a50b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 558, "doc": {"id": "7cf54544d54818d53e7088c0749a3eca", "question": "What must a student in engineering do?", "question_concept": "student", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["answer question", "learn language", "do mathematics", "be able to count", "begin to study"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What must a student in engineering do?\nA. answer question\nB. learn language\nC. do mathematics\nD. be able to count\nE. begin to study\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What must a student in engineering do?\nA. answer question\nB. learn language\nC. do mathematics\nD. be able to count\nE. begin to study\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What must a student in engineering do?\nA. answer question\nB. learn language\nC. do mathematics\nD. be able to count\nE. begin to study\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What must a student in engineering do?\nA. answer question\nB. learn language\nC. do mathematics\nD. be able to count\nE. begin to study\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What must a student in engineering do?\nA. answer question\nB. learn language\nC. do mathematics\nD. be able to count\nE. begin to study\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.162858486175537", "False"]], [["-5.662858486175537", "False"]], [["-1.162858486175537", "True"]], [["-6.162858486175537", "False"]], [["-1.912858486175537", "False"]]], "filtered_resps": [["-3.162858486175537", "False"], ["-5.662858486175537", "False"], ["-1.162858486175537", "True"], ["-6.162858486175537", "False"], ["-1.912858486175537", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "55322dc91ac5bf1ac73abc034822d64fdc65611682475083d712c2f56f1557fc", "prompt_hash": "e66e693066aaa42b99af6c4b5c254d80ff4e1dd363f304179ddd8bcf39eceac2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 559, "doc": {"id": "6acd88b9b5dd15e23bbcc3fd679100a8", "question": "The teacher knew her students understood division, what was she hoping they would learn next?", "question_concept": "division", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["multiplication", "multiply", "putting together", "unity", "pay debts"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The teacher knew her students understood division, what was she hoping they would learn next?\nA. multiplication\nB. multiply\nC. putting together\nD. unity\nE. pay debts\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The teacher knew her students understood division, what was she hoping they would learn next?\nA. multiplication\nB. multiply\nC. putting together\nD. unity\nE. pay debts\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The teacher knew her students understood division, what was she hoping they would learn next?\nA. multiplication\nB. multiply\nC. putting together\nD. unity\nE. pay debts\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The teacher knew her students understood division, what was she hoping they would learn next?\nA. multiplication\nB. multiply\nC. putting together\nD. unity\nE. pay debts\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The teacher knew her students understood division, what was she hoping they would learn next?\nA. multiplication\nB. multiply\nC. putting together\nD. unity\nE. pay debts\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0087177753448486", "True"]], [["-4.0087175369262695", "False"]], [["-5.0087175369262695", "False"]], [["-5.7587175369262695", "False"]], [["-6.2587175369262695", "False"]]], "filtered_resps": [["-1.0087177753448486", "True"], ["-4.0087175369262695", "False"], ["-5.0087175369262695", "False"], ["-5.7587175369262695", "False"], ["-6.2587175369262695", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ca572bbbffc89c534f039e5de493c24fc686d4c372cdffef468d7252d834da72", "prompt_hash": "76b35c1265160c00f6fcc6a067b75c175bb81c1e28460e74068a608cbf2eac72", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 560, "doc": {"id": "c96a86957a9ab1d8ca0aeeb7f040d87a_1", "question": "There were times where kids wanted to know a definition, so there was a nice big dictionary in the what?", "question_concept": "dictionary", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pocket", "classroom", "table", "library", "shelf"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: There were times where kids wanted to know a definition, so there was a nice big dictionary in the what?\nA. pocket\nB. classroom\nC. table\nD. library\nE. shelf\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: There were times where kids wanted to know a definition, so there was a nice big dictionary in the what?\nA. pocket\nB. classroom\nC. table\nD. library\nE. shelf\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: There were times where kids wanted to know a definition, so there was a nice big dictionary in the what?\nA. pocket\nB. classroom\nC. table\nD. library\nE. shelf\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: There were times where kids wanted to know a definition, so there was a nice big dictionary in the what?\nA. pocket\nB. classroom\nC. table\nD. library\nE. shelf\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: There were times where kids wanted to know a definition, so there was a nice big dictionary in the what?\nA. pocket\nB. classroom\nC. table\nD. library\nE. shelf\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3014626502990723", "False"]], [["-3.8014626502990723", "False"]], [["-5.801462650299072", "False"]], [["-1.5514625310897827", "True"]], [["-4.270100116729736", "False"]]], "filtered_resps": [["-3.3014626502990723", "False"], ["-3.8014626502990723", "False"], ["-5.801462650299072", "False"], ["-1.5514625310897827", "True"], ["-4.270100116729736", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a697e9a5cdba291453d44bb0f667e8a79485e00bf4f19483e84eeb2903367278", "prompt_hash": "4c051135a183c9058173ed159b12cc37a780956fa3637c59a352fcb282311459", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 561, "doc": {"id": "6a1bf527af9ed0685ac5e2bf0bd76647", "question": "Riding a bike for a long time can cause what?", "question_concept": "riding bike", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["enjoyment", "fatigue", "falling down", "getting lost", "thirst"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Riding a bike for a long time can cause what?\nA. enjoyment\nB. fatigue\nC. falling down\nD. getting lost\nE. thirst\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Riding a bike for a long time can cause what?\nA. enjoyment\nB. fatigue\nC. falling down\nD. getting lost\nE. thirst\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Riding a bike for a long time can cause what?\nA. enjoyment\nB. fatigue\nC. falling down\nD. getting lost\nE. thirst\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Riding a bike for a long time can cause what?\nA. enjoyment\nB. fatigue\nC. falling down\nD. getting lost\nE. thirst\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Riding a bike for a long time can cause what?\nA. enjoyment\nB. fatigue\nC. falling down\nD. getting lost\nE. thirst\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.151828765869141", "False"]], [["-0.9018285274505615", "True"]], [["-7.401828765869141", "False"]], [["-8.15182876586914", "False"]], [["-8.90182876586914", "False"]]], "filtered_resps": [["-6.151828765869141", "False"], ["-0.9018285274505615", "True"], ["-7.401828765869141", "False"], ["-8.15182876586914", "False"], ["-8.90182876586914", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7139a61792e2ceed934fb5734f16c1b749052082aad68bf8e7618d16bdf789f0", "prompt_hash": "4d847d8a19866dc1519750aa03fc202a6947ae2ad3cede327dd10c8ab635d831", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 562, "doc": {"id": "094fe91b20b03c647325fa2ee94470b3", "question": "What could happen to a cat other than wanting food?", "question_concept": "cat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feline", "thirsty", "sharp claws", "pussycat", "hungry"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What could happen to a cat other than wanting food?\nA. feline\nB. thirsty\nC. sharp claws\nD. pussycat\nE. hungry\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could happen to a cat other than wanting food?\nA. feline\nB. thirsty\nC. sharp claws\nD. pussycat\nE. hungry\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could happen to a cat other than wanting food?\nA. feline\nB. thirsty\nC. sharp claws\nD. pussycat\nE. hungry\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could happen to a cat other than wanting food?\nA. feline\nB. thirsty\nC. sharp claws\nD. pussycat\nE. hungry\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could happen to a cat other than wanting food?\nA. feline\nB. thirsty\nC. sharp claws\nD. pussycat\nE. hungry\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.752209186553955", "False"]], [["-1.0022090673446655", "True"]], [["-1.7522090673446655", "False"]], [["-6.502209186553955", "False"]], [["-3.502209186553955", "False"]]], "filtered_resps": [["-2.752209186553955", "False"], ["-1.0022090673446655", "True"], ["-1.7522090673446655", "False"], ["-6.502209186553955", "False"], ["-3.502209186553955", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "289b36cb4578d1ea5f3e2e84ee8e6afdad08aa02b49ded4c79827fc5326fe1c4", "prompt_hash": "482ca8ca009e2c257aa8333d78f05e52961bed35a70cf071fb4a2b8400a9c04f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 563, "doc": {"id": "bee2a6eadfaf7a4fa0a214e341ddbe5b", "question": "If you turn off the music in a room with no other noise that room would be what?", "question_concept": "music", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["silent", "opera", "silence", "television", "elevator"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If you turn off the music in a room with no other noise that room would be what?\nA. silent\nB. opera\nC. silence\nD. television\nE. elevator\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you turn off the music in a room with no other noise that room would be what?\nA. silent\nB. opera\nC. silence\nD. television\nE. elevator\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you turn off the music in a room with no other noise that room would be what?\nA. silent\nB. opera\nC. silence\nD. television\nE. elevator\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you turn off the music in a room with no other noise that room would be what?\nA. silent\nB. opera\nC. silence\nD. television\nE. elevator\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you turn off the music in a room with no other noise that room would be what?\nA. silent\nB. opera\nC. silence\nD. television\nE. elevator\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8681731224060059", "False"]], [["-6.868173122406006", "False"]], [["-1.8681731224060059", "False"]], [["-9.618173599243164", "False"]], [["-10.118173599243164", "False"]]], "filtered_resps": [["-1.8681731224060059", "False"], ["-6.868173122406006", "False"], ["-1.8681731224060059", "False"], ["-9.618173599243164", "False"], ["-10.118173599243164", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0a2e19fc87862feac53492ef09552f7c950a441cdf5bc7b000401b84fd671f03", "prompt_hash": "c448fafa7b872f63266698d7473fef12a69f42353f017f2fd16a272007a94706", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 564, "doc": {"id": "2f97a77d155cb99092e8a7c055737b03_1", "question": "In what country are the most fast food restaurants?", "question_concept": "fast food restaurant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["new york", "blocks of flats", "center of town", "america", "big cities"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: In what country are the most fast food restaurants?\nA. new york\nB. blocks of flats\nC. center of town\nD. america\nE. big cities\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: In what country are the most fast food restaurants?\nA. new york\nB. blocks of flats\nC. center of town\nD. america\nE. big cities\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: In what country are the most fast food restaurants?\nA. new york\nB. blocks of flats\nC. center of town\nD. america\nE. big cities\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: In what country are the most fast food restaurants?\nA. new york\nB. blocks of flats\nC. center of town\nD. america\nE. big cities\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: In what country are the most fast food restaurants?\nA. new york\nB. blocks of flats\nC. center of town\nD. america\nE. big cities\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5932090282440186", "False"]], [["-7.093209266662598", "False"]], [["-7.093209266662598", "False"]], [["-2.3432090282440186", "False"]], [["-4.343209266662598", "False"]]], "filtered_resps": [["-3.5932090282440186", "False"], ["-7.093209266662598", "False"], ["-7.093209266662598", "False"], ["-2.3432090282440186", "False"], ["-4.343209266662598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "639cf4475b1b298e684fb1cbaf10baacb692bd6bae68b516291b9effe448fa59", "prompt_hash": "4e3534dcb2d80c46adc0d6f1a764daab57dd597bb7cc181def457e61099700c9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 565, "doc": {"id": "bc268cd19e2c95c78967fd6b9092fb90", "question": "I want to use string to keep something from moving, how should I do it?", "question_concept": "string", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tie around", "wind around", "weave", "stringbed", "ball up"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: I want to use string to keep something from moving, how should I do it?\nA. tie around\nB. wind around\nC. weave\nD. stringbed\nE. ball up\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I want to use string to keep something from moving, how should I do it?\nA. tie around\nB. wind around\nC. weave\nD. stringbed\nE. ball up\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I want to use string to keep something from moving, how should I do it?\nA. tie around\nB. wind around\nC. weave\nD. stringbed\nE. ball up\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I want to use string to keep something from moving, how should I do it?\nA. tie around\nB. wind around\nC. weave\nD. stringbed\nE. ball up\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I want to use string to keep something from moving, how should I do it?\nA. tie around\nB. wind around\nC. weave\nD. stringbed\nE. ball up\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.154740333557129", "False"]], [["-3.154740333557129", "False"]], [["-3.404740333557129", "False"]], [["-6.654740333557129", "False"]], [["-5.904740333557129", "False"]]], "filtered_resps": [["-4.154740333557129", "False"], ["-3.154740333557129", "False"], ["-3.404740333557129", "False"], ["-6.654740333557129", "False"], ["-5.904740333557129", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "359427f2e90fddf5af86e15bf57b49e44ffc3d0feb1a996d3cf6e74250f1d9f4", "prompt_hash": "8fa6b03d40fc94295c2eee843a72c454ce4fba4e977428ccdf211dee6fbca4c6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 566, "doc": {"id": "060cad0d3c007ceb151db9907bfcb214", "question": "Where would walk through a central passage to catch an elevator?", "question_concept": "central passage", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tomb", "arena", "access rooms", "public building", "house"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would walk through a central passage to catch an elevator?\nA. tomb\nB. arena\nC. access rooms\nD. public building\nE. house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would walk through a central passage to catch an elevator?\nA. tomb\nB. arena\nC. access rooms\nD. public building\nE. house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would walk through a central passage to catch an elevator?\nA. tomb\nB. arena\nC. access rooms\nD. public building\nE. house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would walk through a central passage to catch an elevator?\nA. tomb\nB. arena\nC. access rooms\nD. public building\nE. house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would walk through a central passage to catch an elevator?\nA. tomb\nB. arena\nC. access rooms\nD. public building\nE. house\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.946328639984131", "False"]], [["-6.446328639984131", "False"]], [["-2.196328639984131", "False"]], [["-1.9463287591934204", "True"]], [["-4.446328639984131", "False"]]], "filtered_resps": [["-2.946328639984131", "False"], ["-6.446328639984131", "False"], ["-2.196328639984131", "False"], ["-1.9463287591934204", "True"], ["-4.446328639984131", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "92412a483e5540cb217e84b5902d8f65433884f28270bd1f062f6702617d582d", "prompt_hash": "b9d16954851e70d9ecc1f9ec53c4e07d9bb97a8bd9cf4fc81e1cc24f85c2d4dc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 567, "doc": {"id": "29c2cc0ba85b4afb9c9d29801469a68f", "question": "A potato is kept in the cellar, where is likely to be stored?", "question_concept": "potato", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["farmer's market", "grocery bag", "pantry", "bushel basket", "fridge"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: A potato is kept in the cellar, where is likely to be stored?\nA. farmer's market\nB. grocery bag\nC. pantry\nD. bushel basket\nE. fridge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A potato is kept in the cellar, where is likely to be stored?\nA. farmer's market\nB. grocery bag\nC. pantry\nD. bushel basket\nE. fridge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A potato is kept in the cellar, where is likely to be stored?\nA. farmer's market\nB. grocery bag\nC. pantry\nD. bushel basket\nE. fridge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A potato is kept in the cellar, where is likely to be stored?\nA. farmer's market\nB. grocery bag\nC. pantry\nD. bushel basket\nE. fridge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A potato is kept in the cellar, where is likely to be stored?\nA. farmer's market\nB. grocery bag\nC. pantry\nD. bushel basket\nE. fridge\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4151251316070557", "True"]], [["-4.915124893188477", "False"]], [["-3.4151251316070557", "False"]], [["-2.9151251316070557", "False"]], [["-1.9151251316070557", "False"]]], "filtered_resps": [["-1.4151251316070557", "True"], ["-4.915124893188477", "False"], ["-3.4151251316070557", "False"], ["-2.9151251316070557", "False"], ["-1.9151251316070557", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c2a28a856f046c7bc8bbae43e61deeb29e51650013b640a452e4555fe5f47bb1", "prompt_hash": "5795241b771a093b481957e2f728b5c650ae582e11f34233ec285125a969a2d6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 568, "doc": {"id": "6cb895ce89995f6be422f7c4167c7638", "question": "What do people do when networking?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["build trust", "hurry home", "ignore people", "believe in god", "jump to conclusions"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do people do when networking?\nA. build trust\nB. hurry home\nC. ignore people\nD. believe in god\nE. jump to conclusions\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people do when networking?\nA. build trust\nB. hurry home\nC. ignore people\nD. believe in god\nE. jump to conclusions\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people do when networking?\nA. build trust\nB. hurry home\nC. ignore people\nD. believe in god\nE. jump to conclusions\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people do when networking?\nA. build trust\nB. hurry home\nC. ignore people\nD. believe in god\nE. jump to conclusions\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people do when networking?\nA. build trust\nB. hurry home\nC. ignore people\nD. believe in god\nE. jump to conclusions\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.387514352798462", "True"]], [["-6.387514114379883", "False"]], [["-9.137514114379883", "False"]], [["-8.637514114379883", "False"]], [["-9.637514114379883", "False"]]], "filtered_resps": [["-1.387514352798462", "True"], ["-6.387514114379883", "False"], ["-9.137514114379883", "False"], ["-8.637514114379883", "False"], ["-9.637514114379883", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f5e6fe8ca7c2cf6a06529f0b4572c44fe607f6acdd26b4c4b01b1ab3ab3fdaa2", "prompt_hash": "53513817b6537c14547bae2d41fb338d58b5dc342016850c37f0aeb5d70c0793", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 569, "doc": {"id": "839f3c37622c1ed5eebc9cd0b9d658e8", "question": "Where can you store you spare linens near your socks?", "question_concept": "linen", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hospital", "chest", "home", "dresser drawers", "cabinet"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you store you spare linens near your socks?\nA. hospital\nB. chest\nC. home\nD. dresser drawers\nE. cabinet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you store you spare linens near your socks?\nA. hospital\nB. chest\nC. home\nD. dresser drawers\nE. cabinet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you store you spare linens near your socks?\nA. hospital\nB. chest\nC. home\nD. dresser drawers\nE. cabinet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you store you spare linens near your socks?\nA. hospital\nB. chest\nC. home\nD. dresser drawers\nE. cabinet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you store you spare linens near your socks?\nA. hospital\nB. chest\nC. home\nD. dresser drawers\nE. cabinet\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.358108043670654", "False"]], [["-2.8581080436706543", "False"]], [["-2.6081080436706543", "False"]], [["-2.1081080436706543", "False"]], [["-5.358108043670654", "False"]]], "filtered_resps": [["-4.358108043670654", "False"], ["-2.8581080436706543", "False"], ["-2.6081080436706543", "False"], ["-2.1081080436706543", "False"], ["-5.358108043670654", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "22d421380c4168ce81af34837c6eb40f5f7e7c04b11f523c372f1c8edd103edb", "prompt_hash": "623602e4a8b343c0b329de08e4c1eae0f0a2f609879880a9fbf691008514abf4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 570, "doc": {"id": "3957ac6bab96fc9d4f173ada4692d16b", "question": "What do people do when they think too quickly?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["jump to conclusions", "hurry home", "build trust", "pay bills", "sing"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do people do when they think too quickly?\nA. jump to conclusions\nB. hurry home\nC. build trust\nD. pay bills\nE. sing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people do when they think too quickly?\nA. jump to conclusions\nB. hurry home\nC. build trust\nD. pay bills\nE. sing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people do when they think too quickly?\nA. jump to conclusions\nB. hurry home\nC. build trust\nD. pay bills\nE. sing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people do when they think too quickly?\nA. jump to conclusions\nB. hurry home\nC. build trust\nD. pay bills\nE. sing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people do when they think too quickly?\nA. jump to conclusions\nB. hurry home\nC. build trust\nD. pay bills\nE. sing\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7235490083694458", "True"]], [["-6.973548889160156", "False"]], [["-7.973548889160156", "False"]], [["-8.723548889160156", "False"]], [["-10.473548889160156", "False"]]], "filtered_resps": [["-0.7235490083694458", "True"], ["-6.973548889160156", "False"], ["-7.973548889160156", "False"], ["-8.723548889160156", "False"], ["-10.473548889160156", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4ee490474da7ae1eb171308227931c32448b1cfb8cb9c119d6f6c612a07b938b", "prompt_hash": "1d1efa57bb7e503bd566dcd43c350c64fb5a9fe9a24a34a0923c2a0200b737f2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 571, "doc": {"id": "a4f5e5412f0f8ac9190db1730db07a90", "question": "What is someone likely to want as a result of sex?", "question_concept": "sex", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sexploiter", "chicken", "reproductive cycle", "procreation", "human experience"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is someone likely to want as a result of sex?\nA. sexploiter\nB. chicken\nC. reproductive cycle\nD. procreation\nE. human experience\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is someone likely to want as a result of sex?\nA. sexploiter\nB. chicken\nC. reproductive cycle\nD. procreation\nE. human experience\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is someone likely to want as a result of sex?\nA. sexploiter\nB. chicken\nC. reproductive cycle\nD. procreation\nE. human experience\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is someone likely to want as a result of sex?\nA. sexploiter\nB. chicken\nC. reproductive cycle\nD. procreation\nE. human experience\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is someone likely to want as a result of sex?\nA. sexploiter\nB. chicken\nC. reproductive cycle\nD. procreation\nE. human experience\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.934115886688232", "False"]], [["-5.434115886688232", "False"]], [["-5.184115886688232", "False"]], [["-1.4341157674789429", "True"]], [["-5.184115886688232", "False"]]], "filtered_resps": [["-4.934115886688232", "False"], ["-5.434115886688232", "False"], ["-5.184115886688232", "False"], ["-1.4341157674789429", "True"], ["-5.184115886688232", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c8dfd4b4b2b68a0697004fed34ba6a9a7b96737aedd142d5e89239e4b37d544e", "prompt_hash": "46a2de901d988d6ab56e32951bbd8b06831e1606d2b01f0a79e55e0fe09ddf33", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 572, "doc": {"id": "cb5b39878be0e05a3ffe783801adbc3b", "question": "What might someone do after they finish creating art?", "question_concept": "creating art", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["frustration", "relax", "eat", "enlightenment", "communication"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What might someone do after they finish creating art?\nA. frustration\nB. relax\nC. eat\nD. enlightenment\nE. communication\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might someone do after they finish creating art?\nA. frustration\nB. relax\nC. eat\nD. enlightenment\nE. communication\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might someone do after they finish creating art?\nA. frustration\nB. relax\nC. eat\nD. enlightenment\nE. communication\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might someone do after they finish creating art?\nA. frustration\nB. relax\nC. eat\nD. enlightenment\nE. communication\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might someone do after they finish creating art?\nA. frustration\nB. relax\nC. eat\nD. enlightenment\nE. communication\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.5808587074279785", "False"]], [["-2.3308587074279785", "False"]], [["-5.5808587074279785", "False"]], [["-4.5808587074279785", "False"]], [["-3.3308587074279785", "False"]]], "filtered_resps": [["-4.5808587074279785", "False"], ["-2.3308587074279785", "False"], ["-5.5808587074279785", "False"], ["-4.5808587074279785", "False"], ["-3.3308587074279785", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "88f24ba89702915c36eb3bc353c8630b6ae8dd1f73dd8bb57e98b5a3e05a7753", "prompt_hash": "c391ac2e60bded6448c356975c2dd70c481428f692126cd5ccc0a0e9f6d7f52a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 573, "doc": {"id": "985a4f1a3f31f1ba6654f4fc48f504df", "question": "To get clean clothes you to what to them?", "question_concept": "clean clothes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get dirty", "writing", "use water", "launder", "soap"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: To get clean clothes you to what to them?\nA. get dirty\nB. writing\nC. use water\nD. launder\nE. soap\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: To get clean clothes you to what to them?\nA. get dirty\nB. writing\nC. use water\nD. launder\nE. soap\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: To get clean clothes you to what to them?\nA. get dirty\nB. writing\nC. use water\nD. launder\nE. soap\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: To get clean clothes you to what to them?\nA. get dirty\nB. writing\nC. use water\nD. launder\nE. soap\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: To get clean clothes you to what to them?\nA. get dirty\nB. writing\nC. use water\nD. launder\nE. soap\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.1064629554748535", "False"]], [["-6.6064629554748535", "False"]], [["-3.8564629554748535", "False"]], [["-1.356462836265564", "True"]], [["-8.356462478637695", "False"]]], "filtered_resps": [["-6.1064629554748535", "False"], ["-6.6064629554748535", "False"], ["-3.8564629554748535", "False"], ["-1.356462836265564", "True"], ["-8.356462478637695", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "78b4f88423c873bff3b5997ad15d1b65dcbfc72cee3eccb2744bd1629088ea22", "prompt_hash": "44750ffe86931d85e653ec2799b038a9c8c102b8e80dc5644264b093e9c5fa74", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 574, "doc": {"id": "5d687fe9c95436ce84230c996d34382d", "question": "The person tried to reduce his weight with a shrink ray, but he got it backwards and only did what?", "question_concept": "reduce", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["grow", "gain weight", "make larger", "augment", "get bigger"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The person tried to reduce his weight with a shrink ray, but he got it backwards and only did what?\nA. grow\nB. gain weight\nC. make larger\nD. augment\nE. get bigger\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The person tried to reduce his weight with a shrink ray, but he got it backwards and only did what?\nA. grow\nB. gain weight\nC. make larger\nD. augment\nE. get bigger\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The person tried to reduce his weight with a shrink ray, but he got it backwards and only did what?\nA. grow\nB. gain weight\nC. make larger\nD. augment\nE. get bigger\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The person tried to reduce his weight with a shrink ray, but he got it backwards and only did what?\nA. grow\nB. gain weight\nC. make larger\nD. augment\nE. get bigger\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The person tried to reduce his weight with a shrink ray, but he got it backwards and only did what?\nA. grow\nB. gain weight\nC. make larger\nD. augment\nE. get bigger\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.368131399154663", "False"]], [["-1.868131399154663", "True"]], [["-5.618131637573242", "False"]], [["-6.118131637573242", "False"]], [["-2.368131399154663", "False"]]], "filtered_resps": [["-3.368131399154663", "False"], ["-1.868131399154663", "True"], ["-5.618131637573242", "False"], ["-6.118131637573242", "False"], ["-2.368131399154663", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dd95d4b66a40e02c8c4643840d9f626c4f108a8df933822cb07756dab9ff4df9", "prompt_hash": "131161ceb1a6d4ce6d6ada6dd67c33e8368deb3948edac6ce8292812266c1681", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 575, "doc": {"id": "af11faa29097b71141fe192ad019d1dd", "question": "Christine couldn't be having a baby at her age, she thought to herself. What was Christine?", "question_concept": "baby", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["old person", "begin to talk", "adult", "old man", "girl"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Christine couldn't be having a baby at her age, she thought to herself. What was Christine?\nA. old person\nB. begin to talk\nC. adult\nD. old man\nE. girl\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Christine couldn't be having a baby at her age, she thought to herself. What was Christine?\nA. old person\nB. begin to talk\nC. adult\nD. old man\nE. girl\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Christine couldn't be having a baby at her age, she thought to herself. What was Christine?\nA. old person\nB. begin to talk\nC. adult\nD. old man\nE. girl\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Christine couldn't be having a baby at her age, she thought to herself. What was Christine?\nA. old person\nB. begin to talk\nC. adult\nD. old man\nE. girl\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Christine couldn't be having a baby at her age, she thought to herself. What was Christine?\nA. old person\nB. begin to talk\nC. adult\nD. old man\nE. girl\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4338531494140625", "True"]], [["-5.6838531494140625", "False"]], [["-1.6838531494140625", "False"]], [["-7.4338531494140625", "False"]], [["-5.6838531494140625", "False"]]], "filtered_resps": [["-1.4338531494140625", "True"], ["-5.6838531494140625", "False"], ["-1.6838531494140625", "False"], ["-7.4338531494140625", "False"], ["-5.6838531494140625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5d103142c5bb9fcb46c87ea50473f498457e3fbb797c426407aa70d639e12959", "prompt_hash": "432fe4684024ebfe9e8838b1f34f9aae02699022539590141d6ae3057416ea5e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 576, "doc": {"id": "07fd8b0aed06406fedb137d11b07a890", "question": "Joe plays a percussion instrument in something.  What might be play in?", "question_concept": "percussion instrument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["own home", "music store", "marching band", "orchestra", "party"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Joe plays a percussion instrument in something.  What might be play in?\nA. own home\nB. music store\nC. marching band\nD. orchestra\nE. party\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe plays a percussion instrument in something.  What might be play in?\nA. own home\nB. music store\nC. marching band\nD. orchestra\nE. party\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe plays a percussion instrument in something.  What might be play in?\nA. own home\nB. music store\nC. marching band\nD. orchestra\nE. party\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe plays a percussion instrument in something.  What might be play in?\nA. own home\nB. music store\nC. marching band\nD. orchestra\nE. party\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe plays a percussion instrument in something.  What might be play in?\nA. own home\nB. music store\nC. marching band\nD. orchestra\nE. party\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.1573805809021", "False"]], [["-3.9073805809020996", "False"]], [["-1.65738046169281", "True"]], [["-2.1573805809020996", "False"]], [["-6.1573805809021", "False"]]], "filtered_resps": [["-4.1573805809021", "False"], ["-3.9073805809020996", "False"], ["-1.65738046169281", "True"], ["-2.1573805809020996", "False"], ["-6.1573805809021", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a6aa09a3bebf99e93c380729b11b3108d9f1c2123b8bd1156ff4f69744979191", "prompt_hash": "8c7286ee64ca1a5980494e5227d43b67c688e5f4731865959cfb7f05dfdd320b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 577, "doc": {"id": "7044d82a456d0fa6f0210abb03cbf2c4", "question": "If I'm playing ball, I'm mostly trying to do what?", "question_concept": "playing ball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["losing", "competition", "having fun", "win", "injury"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If I'm playing ball, I'm mostly trying to do what?\nA. losing\nB. competition\nC. having fun\nD. win\nE. injury\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I'm playing ball, I'm mostly trying to do what?\nA. losing\nB. competition\nC. having fun\nD. win\nE. injury\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I'm playing ball, I'm mostly trying to do what?\nA. losing\nB. competition\nC. having fun\nD. win\nE. injury\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I'm playing ball, I'm mostly trying to do what?\nA. losing\nB. competition\nC. having fun\nD. win\nE. injury\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I'm playing ball, I'm mostly trying to do what?\nA. losing\nB. competition\nC. having fun\nD. win\nE. injury\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.546011924743652", "False"]], [["-3.5460116863250732", "False"]], [["-1.7960116863250732", "True"]], [["-4.296011924743652", "False"]], [["-6.546011924743652", "False"]]], "filtered_resps": [["-6.546011924743652", "False"], ["-3.5460116863250732", "False"], ["-1.7960116863250732", "True"], ["-4.296011924743652", "False"], ["-6.546011924743652", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6c71e45d64cd0e2db8ce3cd4c11bdeecad0bb37dc8d54ad3d724ccc1d3ecd51e", "prompt_hash": "52f363e97291a20ac3a716ae9208b0c90acdc4947ddabcd29fa06466794a0dd9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 578, "doc": {"id": "e53ba4c7d2a818bdb6001e6924bc8896", "question": "What do the terms need to be in order to compete against someone?", "question_concept": "compete against", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cheat", "fair", "in competition", "practice", "sabotage"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What do the terms need to be in order to compete against someone?\nA. cheat\nB. fair\nC. in competition\nD. practice\nE. sabotage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do the terms need to be in order to compete against someone?\nA. cheat\nB. fair\nC. in competition\nD. practice\nE. sabotage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do the terms need to be in order to compete against someone?\nA. cheat\nB. fair\nC. in competition\nD. practice\nE. sabotage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do the terms need to be in order to compete against someone?\nA. cheat\nB. fair\nC. in competition\nD. practice\nE. sabotage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do the terms need to be in order to compete against someone?\nA. cheat\nB. fair\nC. in competition\nD. practice\nE. sabotage\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.663201332092285", "False"]], [["-1.1632015705108643", "True"]], [["-5.663201332092285", "False"]], [["-7.163201332092285", "False"]], [["-9.663201332092285", "False"]]], "filtered_resps": [["-5.663201332092285", "False"], ["-1.1632015705108643", "True"], ["-5.663201332092285", "False"], ["-7.163201332092285", "False"], ["-9.663201332092285", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6b6776fc61cddcd6ed8b35aa79b3df8af04f506054fe78da18844a754f713b15", "prompt_hash": "18d85eead3e1a486125178e8bf59fa080f2e184c7a33b34bc7af09d893fa387e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 579, "doc": {"id": "ecbc1ab06ad1ed6c53e5293d7a90ebd3", "question": "If you wanted to show off silk, what item could it be on?", "question_concept": "silk", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["jean", "mulberry tree", "garments", "expensive clothing", "parachutes"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If you wanted to show off silk, what item could it be on?\nA. jean\nB. mulberry tree\nC. garments\nD. expensive clothing\nE. parachutes\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you wanted to show off silk, what item could it be on?\nA. jean\nB. mulberry tree\nC. garments\nD. expensive clothing\nE. parachutes\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you wanted to show off silk, what item could it be on?\nA. jean\nB. mulberry tree\nC. garments\nD. expensive clothing\nE. parachutes\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you wanted to show off silk, what item could it be on?\nA. jean\nB. mulberry tree\nC. garments\nD. expensive clothing\nE. parachutes\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you wanted to show off silk, what item could it be on?\nA. jean\nB. mulberry tree\nC. garments\nD. expensive clothing\nE. parachutes\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.419891595840454", "False"]], [["-5.919891357421875", "False"]], [["-1.419891595840454", "True"]], [["-1.669891595840454", "False"]], [["-4.169891357421875", "False"]]], "filtered_resps": [["-3.419891595840454", "False"], ["-5.919891357421875", "False"], ["-1.419891595840454", "True"], ["-1.669891595840454", "False"], ["-4.169891357421875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ed7fbf61d44579e660e04712bcefc24a8220d5e1aef8e5baf5c0284b78ad387e", "prompt_hash": "2158d31af6dea19b4bc72525e9efc4098bed4534ac5341ec9db24c3de6f09d1e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 580, "doc": {"id": "9a356ff463c042d04ba45bfd627bac20", "question": "Where is known to be a wealth of information?", "question_concept": "information", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["park", "internet", "meeting", "library", "book"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is known to be a wealth of information?\nA. park\nB. internet\nC. meeting\nD. library\nE. book\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is known to be a wealth of information?\nA. park\nB. internet\nC. meeting\nD. library\nE. book\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is known to be a wealth of information?\nA. park\nB. internet\nC. meeting\nD. library\nE. book\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is known to be a wealth of information?\nA. park\nB. internet\nC. meeting\nD. library\nE. book\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is known to be a wealth of information?\nA. park\nB. internet\nC. meeting\nD. library\nE. book\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.550242900848389", "False"]], [["-1.0502430200576782", "True"]], [["-7.800242900848389", "False"]], [["-3.3002429008483887", "False"]], [["-7.800242900848389", "False"]]], "filtered_resps": [["-4.550242900848389", "False"], ["-1.0502430200576782", "True"], ["-7.800242900848389", "False"], ["-3.3002429008483887", "False"], ["-7.800242900848389", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "111da9a8f8916488ecd526fd8d4ecba078b82fa6028b26bd2177ecf73cc2fd59", "prompt_hash": "8ff594b8d2b92e08167446a0c615961fc6041090cf649f50134eb3b98dede50e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 581, "doc": {"id": "0a5c069836784c3d574828d85a20a074", "question": "I saw the receptionist carelessly toss my resume into the drawer, where did I want it to end up?", "question_concept": "drawer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["file cabinet", "nightstand", "kitchen cabinet", "office desk", "the floor"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: I saw the receptionist carelessly toss my resume into the drawer, where did I want it to end up?\nA. file cabinet\nB. nightstand\nC. kitchen cabinet\nD. office desk\nE. the floor\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I saw the receptionist carelessly toss my resume into the drawer, where did I want it to end up?\nA. file cabinet\nB. nightstand\nC. kitchen cabinet\nD. office desk\nE. the floor\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I saw the receptionist carelessly toss my resume into the drawer, where did I want it to end up?\nA. file cabinet\nB. nightstand\nC. kitchen cabinet\nD. office desk\nE. the floor\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I saw the receptionist carelessly toss my resume into the drawer, where did I want it to end up?\nA. file cabinet\nB. nightstand\nC. kitchen cabinet\nD. office desk\nE. the floor\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I saw the receptionist carelessly toss my resume into the drawer, where did I want it to end up?\nA. file cabinet\nB. nightstand\nC. kitchen cabinet\nD. office desk\nE. the floor\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9477629661560059", "True"]], [["-4.197762966156006", "False"]], [["-3.947762966156006", "False"]], [["-2.947762966156006", "False"]], [["-2.697762966156006", "False"]]], "filtered_resps": [["-0.9477629661560059", "True"], ["-4.197762966156006", "False"], ["-3.947762966156006", "False"], ["-2.947762966156006", "False"], ["-2.697762966156006", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eeb943b9d9e30a7efd3923b77f96a6cda5976a9f10131ef7a759778f77f60a9b", "prompt_hash": "80b71d0964b3da17d2c148ee82bfc81dccdcf6218e584dfd2c3bfc1bf5858f7f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 582, "doc": {"id": "f996430ce208606452868fd2e739d409", "question": "What will happen if you inject water into yourself?", "question_concept": "water", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dilute", "thin blood", "take several forms", "wet clothes", "move mountains"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What will happen if you inject water into yourself?\nA. dilute\nB. thin blood\nC. take several forms\nD. wet clothes\nE. move mountains\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will happen if you inject water into yourself?\nA. dilute\nB. thin blood\nC. take several forms\nD. wet clothes\nE. move mountains\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will happen if you inject water into yourself?\nA. dilute\nB. thin blood\nC. take several forms\nD. wet clothes\nE. move mountains\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will happen if you inject water into yourself?\nA. dilute\nB. thin blood\nC. take several forms\nD. wet clothes\nE. move mountains\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will happen if you inject water into yourself?\nA. dilute\nB. thin blood\nC. take several forms\nD. wet clothes\nE. move mountains\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5823564529418945", "False"]], [["-2.0823564529418945", "False"]], [["-3.3323564529418945", "False"]], [["-4.8323564529418945", "False"]], [["-4.5823564529418945", "False"]]], "filtered_resps": [["-2.5823564529418945", "False"], ["-2.0823564529418945", "False"], ["-3.3323564529418945", "False"], ["-4.8323564529418945", "False"], ["-4.5823564529418945", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "848de15efe4a89f9d2c5452a84cb708c2ba898cbcfb174be0044e51b9b51f216", "prompt_hash": "ddb2ff9dc378b2e7ed2457d51e80da9f564225a1f1f5eba6968cf2d9be739cba", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 583, "doc": {"id": "26c854d933d2115e7636fdcde57eb463", "question": "Athletes soak in hot tubs to relieve what after playing baseball?", "question_concept": "playing baseball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fame", "errors", "pain", "strikes", "sore muscles"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Athletes soak in hot tubs to relieve what after playing baseball?\nA. fame\nB. errors\nC. pain\nD. strikes\nE. sore muscles\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Athletes soak in hot tubs to relieve what after playing baseball?\nA. fame\nB. errors\nC. pain\nD. strikes\nE. sore muscles\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Athletes soak in hot tubs to relieve what after playing baseball?\nA. fame\nB. errors\nC. pain\nD. strikes\nE. sore muscles\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Athletes soak in hot tubs to relieve what after playing baseball?\nA. fame\nB. errors\nC. pain\nD. strikes\nE. sore muscles\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Athletes soak in hot tubs to relieve what after playing baseball?\nA. fame\nB. errors\nC. pain\nD. strikes\nE. sore muscles\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.652581691741943", "False"]], [["-4.402581691741943", "False"]], [["-4.152581691741943", "False"]], [["-7.402581691741943", "False"]], [["-1.1525816917419434", "True"]]], "filtered_resps": [["-4.652581691741943", "False"], ["-4.402581691741943", "False"], ["-4.152581691741943", "False"], ["-7.402581691741943", "False"], ["-1.1525816917419434", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0e6ff34d2a547042de0d14d03f2f3e21f7edecb240c1f4bb6dcb89594347f447", "prompt_hash": "990472b1ab212feac57941c36b606f7925cf0acc0ec9f1f366efda9f9d88b3eb", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 584, "doc": {"id": "83c25b9a5db5f9b3fd1ff6c7453d23d0", "question": "What does a gambler do that causes him or her to be unhappy?", "question_concept": "gambler", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["play cards", "double winnings", "lose money", "play poker", "to win the prize"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What does a gambler do that causes him or her to be unhappy?\nA. play cards\nB. double winnings\nC. lose money\nD. play poker\nE. to win the prize\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a gambler do that causes him or her to be unhappy?\nA. play cards\nB. double winnings\nC. lose money\nD. play poker\nE. to win the prize\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a gambler do that causes him or her to be unhappy?\nA. play cards\nB. double winnings\nC. lose money\nD. play poker\nE. to win the prize\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a gambler do that causes him or her to be unhappy?\nA. play cards\nB. double winnings\nC. lose money\nD. play poker\nE. to win the prize\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a gambler do that causes him or her to be unhappy?\nA. play cards\nB. double winnings\nC. lose money\nD. play poker\nE. to win the prize\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.873148202896118", "False"]], [["-6.123147964477539", "False"]], [["-1.3731482028961182", "False"]], [["-7.873147964477539", "False"]], [["-8.873147964477539", "False"]]], "filtered_resps": [["-2.873148202896118", "False"], ["-6.123147964477539", "False"], ["-1.3731482028961182", "False"], ["-7.873147964477539", "False"], ["-8.873147964477539", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "016fee6ac8025b2cf6b00cceb544ca6ed78e329a5acec467fa133c7b52d11c34", "prompt_hash": "aadb943c1126e22be27a69e3c0a7ea196ea8898db538ec977286cd55c494ab67", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 585, "doc": {"id": "a0d02fc32878efdf0b0d420972943492", "question": "There's one obvious reason to eat vegetables, they're plain what you?", "question_concept": "eat vegetables", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lose weight", "good for", "bland", "chewing", "fibre"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: There's one obvious reason to eat vegetables, they're plain what you?\nA. lose weight\nB. good for\nC. bland\nD. chewing\nE. fibre\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: There's one obvious reason to eat vegetables, they're plain what you?\nA. lose weight\nB. good for\nC. bland\nD. chewing\nE. fibre\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: There's one obvious reason to eat vegetables, they're plain what you?\nA. lose weight\nB. good for\nC. bland\nD. chewing\nE. fibre\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: There's one obvious reason to eat vegetables, they're plain what you?\nA. lose weight\nB. good for\nC. bland\nD. chewing\nE. fibre\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: There's one obvious reason to eat vegetables, they're plain what you?\nA. lose weight\nB. good for\nC. bland\nD. chewing\nE. fibre\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.534010171890259", "False"]], [["-1.2840101718902588", "True"]], [["-5.78400993347168", "False"]], [["-7.03400993347168", "False"]], [["-1.7840101718902588", "False"]]], "filtered_resps": [["-3.534010171890259", "False"], ["-1.2840101718902588", "True"], ["-5.78400993347168", "False"], ["-7.03400993347168", "False"], ["-1.7840101718902588", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "05efe8f420fa2175d776b8e54ea66c198ef9c572cfeb2fecb809d121254760bd", "prompt_hash": "47370587c9fc59fcb48608f691a50a12dbe71d89bf35f771768d004610eb8fc1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 586, "doc": {"id": "73fbd2caac2c3786ca810adfe7030273", "question": "John was a bit think in the head, but he knew that he never saw the lady before.  They were what?", "question_concept": "thick", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pay debts", "slender", "unacquainted", "free flowing", "sparse"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: John was a bit think in the head, but he knew that he never saw the lady before.  They were what?\nA. pay debts\nB. slender\nC. unacquainted\nD. free flowing\nE. sparse\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John was a bit think in the head, but he knew that he never saw the lady before.  They were what?\nA. pay debts\nB. slender\nC. unacquainted\nD. free flowing\nE. sparse\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John was a bit think in the head, but he knew that he never saw the lady before.  They were what?\nA. pay debts\nB. slender\nC. unacquainted\nD. free flowing\nE. sparse\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John was a bit think in the head, but he knew that he never saw the lady before.  They were what?\nA. pay debts\nB. slender\nC. unacquainted\nD. free flowing\nE. sparse\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John was a bit think in the head, but he knew that he never saw the lady before.  They were what?\nA. pay debts\nB. slender\nC. unacquainted\nD. free flowing\nE. sparse\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.564666748046875", "False"]], [["-7.314666748046875", "False"]], [["-0.8146666288375854", "True"]], [["-8.564666748046875", "False"]], [["-9.814666748046875", "False"]]], "filtered_resps": [["-6.564666748046875", "False"], ["-7.314666748046875", "False"], ["-0.8146666288375854", "True"], ["-8.564666748046875", "False"], ["-9.814666748046875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d63c9a16d25d5640835cbfb8d85ff9e62d71e970b681df0021103d789724cb88", "prompt_hash": "f28355cf0e68f6d4b14d250c88499c17311f555f3875057f7b0b0dc47dc0c2d0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 587, "doc": {"id": "6c515b068b4d3aa88a5382224d9b866d", "question": "Where would you hear a violin along side many string and wind instruments?", "question_concept": "violin", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["school", "string quartet", "orchestra", "kitchen", "music room"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you hear a violin along side many string and wind instruments?\nA. school\nB. string quartet\nC. orchestra\nD. kitchen\nE. music room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you hear a violin along side many string and wind instruments?\nA. school\nB. string quartet\nC. orchestra\nD. kitchen\nE. music room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you hear a violin along side many string and wind instruments?\nA. school\nB. string quartet\nC. orchestra\nD. kitchen\nE. music room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you hear a violin along side many string and wind instruments?\nA. school\nB. string quartet\nC. orchestra\nD. kitchen\nE. music room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you hear a violin along side many string and wind instruments?\nA. school\nB. string quartet\nC. orchestra\nD. kitchen\nE. music room\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.939373254776001", "False"]], [["-5.189373016357422", "False"]], [["-1.189373254776001", "True"]], [["-7.939373016357422", "False"]], [["-6.939373016357422", "False"]]], "filtered_resps": [["-2.939373254776001", "False"], ["-5.189373016357422", "False"], ["-1.189373254776001", "True"], ["-7.939373016357422", "False"], ["-6.939373016357422", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "96d752bf3003b4366f5f7eeb0b200716a99431b745cd81b8c2ce0c54f686d796", "prompt_hash": "6f4f809df26732326fd67bad8c99b6ee40d1b12bd5870849ae1120c7e7c72941", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 588, "doc": {"id": "0af371b94fb414860b13eea6009ccc31", "question": "What is the sun ultimately responsible for?", "question_concept": "sun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["earth warming", "sun tan", "light", "life on earth", "heat"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is the sun ultimately responsible for?\nA. earth warming\nB. sun tan\nC. light\nD. life on earth\nE. heat\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the sun ultimately responsible for?\nA. earth warming\nB. sun tan\nC. light\nD. life on earth\nE. heat\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the sun ultimately responsible for?\nA. earth warming\nB. sun tan\nC. light\nD. life on earth\nE. heat\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the sun ultimately responsible for?\nA. earth warming\nB. sun tan\nC. light\nD. life on earth\nE. heat\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the sun ultimately responsible for?\nA. earth warming\nB. sun tan\nC. light\nD. life on earth\nE. heat\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.958415985107422", "False"]], [["-7.208415985107422", "False"]], [["-6.208415985107422", "False"]], [["-1.9584161043167114", "False"]], [["-5.958415985107422", "False"]]], "filtered_resps": [["-5.958415985107422", "False"], ["-7.208415985107422", "False"], ["-6.208415985107422", "False"], ["-1.9584161043167114", "False"], ["-5.958415985107422", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "01bce0ee400cd2c554427b4c5a6cf8b22ad394fb3a9f29b11e84a164a33e4897", "prompt_hash": "cf6b0762f22721eba61fa069c0fb2cd3aa2b34d97c3cea33c1c2d80e64548a7a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 589, "doc": {"id": "38e61d4be0da46b3cbbd76dc20bce677", "question": "Mandy lived in a train station.  She longed to see distant places. Where might she imagine going?", "question_concept": "train station", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["downtown area", "centre of town", "bedroom", "europe", "big city"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Mandy lived in a train station.  She longed to see distant places. Where might she imagine going?\nA. downtown area\nB. centre of town\nC. bedroom\nD. europe\nE. big city\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Mandy lived in a train station.  She longed to see distant places. Where might she imagine going?\nA. downtown area\nB. centre of town\nC. bedroom\nD. europe\nE. big city\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Mandy lived in a train station.  She longed to see distant places. Where might she imagine going?\nA. downtown area\nB. centre of town\nC. bedroom\nD. europe\nE. big city\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Mandy lived in a train station.  She longed to see distant places. Where might she imagine going?\nA. downtown area\nB. centre of town\nC. bedroom\nD. europe\nE. big city\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Mandy lived in a train station.  She longed to see distant places. Where might she imagine going?\nA. downtown area\nB. centre of town\nC. bedroom\nD. europe\nE. big city\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.937629699707031", "False"]], [["-6.687629699707031", "False"]], [["-8.187629699707031", "False"]], [["-1.4376299381256104", "False"]], [["-9.937629699707031", "False"]]], "filtered_resps": [["-4.937629699707031", "False"], ["-6.687629699707031", "False"], ["-8.187629699707031", "False"], ["-1.4376299381256104", "False"], ["-9.937629699707031", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5e6a8dd3a1ef896833de36f7751193434d51f4824c0e5611007c55f4f2954002", "prompt_hash": "892f59ac318fac80c1c63ced90791ffbf1a50877cc04a06a94fc491433e234a8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 590, "doc": {"id": "cebc07bd5080cc72862cb333b10d782d", "question": "Joe is a  squirrel, which is an animal. He probably lives in what sort of place.", "question_concept": "animal", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pet store", "outside", "woodland", "ocean", "cafe"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Joe is a  squirrel, which is an animal. He probably lives in what sort of place.\nA. pet store\nB. outside\nC. woodland\nD. ocean\nE. cafe\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe is a  squirrel, which is an animal. He probably lives in what sort of place.\nA. pet store\nB. outside\nC. woodland\nD. ocean\nE. cafe\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe is a  squirrel, which is an animal. He probably lives in what sort of place.\nA. pet store\nB. outside\nC. woodland\nD. ocean\nE. cafe\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe is a  squirrel, which is an animal. He probably lives in what sort of place.\nA. pet store\nB. outside\nC. woodland\nD. ocean\nE. cafe\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe is a  squirrel, which is an animal. He probably lives in what sort of place.\nA. pet store\nB. outside\nC. woodland\nD. ocean\nE. cafe\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.918676853179932", "False"]], [["-5.168676853179932", "False"]], [["-1.6686770915985107", "False"]], [["-8.66867733001709", "False"]], [["-10.16867733001709", "False"]]], "filtered_resps": [["-4.918676853179932", "False"], ["-5.168676853179932", "False"], ["-1.6686770915985107", "False"], ["-8.66867733001709", "False"], ["-10.16867733001709", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "568a09668487899a329f1fa0496409d5f00fe6890c707af98cd9f48311b0d6cd", "prompt_hash": "91114e82bb4a50146f8ef01254fc8fdf43f45f65a89c79fcc8e41142e220ef3f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 591, "doc": {"id": "de0386024f32cdf277a785a851b97544", "question": "Where could a personal ficus live?", "question_concept": "ficus", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cabin in the woods", "california", "front yard", "conservatory", "tropical forest"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where could a personal ficus live?\nA. cabin in the woods\nB. california\nC. front yard\nD. conservatory\nE. tropical forest\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could a personal ficus live?\nA. cabin in the woods\nB. california\nC. front yard\nD. conservatory\nE. tropical forest\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could a personal ficus live?\nA. cabin in the woods\nB. california\nC. front yard\nD. conservatory\nE. tropical forest\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could a personal ficus live?\nA. cabin in the woods\nB. california\nC. front yard\nD. conservatory\nE. tropical forest\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could a personal ficus live?\nA. cabin in the woods\nB. california\nC. front yard\nD. conservatory\nE. tropical forest\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5477583408355713", "True"]], [["-4.547758102416992", "False"]], [["-5.547758102416992", "False"]], [["-1.5477583408355713", "True"]], [["-2.7977583408355713", "False"]]], "filtered_resps": [["-1.5477583408355713", "True"], ["-4.547758102416992", "False"], ["-5.547758102416992", "False"], ["-1.5477583408355713", "True"], ["-2.7977583408355713", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "42cdcc00b37303b1574f27d25bcc6321df97436bffe5fdfd049136e15a7998ae", "prompt_hash": "13a59bbf8f479265180d3fef10ad8ba55910d72f74f355dee903b4e62f0bacc4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 592, "doc": {"id": "9b62cd7f89716f393239e6c6ff3e11d5", "question": "The shark actually counted as evidence, so where did the legal team bring it?", "question_concept": "shark", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["court room", "shallow waters", "poker game", "sea world", "pond arena"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The shark actually counted as evidence, so where did the legal team bring it?\nA. court room\nB. shallow waters\nC. poker game\nD. sea world\nE. pond arena\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The shark actually counted as evidence, so where did the legal team bring it?\nA. court room\nB. shallow waters\nC. poker game\nD. sea world\nE. pond arena\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The shark actually counted as evidence, so where did the legal team bring it?\nA. court room\nB. shallow waters\nC. poker game\nD. sea world\nE. pond arena\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The shark actually counted as evidence, so where did the legal team bring it?\nA. court room\nB. shallow waters\nC. poker game\nD. sea world\nE. pond arena\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The shark actually counted as evidence, so where did the legal team bring it?\nA. court room\nB. shallow waters\nC. poker game\nD. sea world\nE. pond arena\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7184538841247559", "True"]], [["-5.718453884124756", "False"]], [["-5.468453884124756", "False"]], [["-6.718453884124756", "False"]], [["-8.718454360961914", "False"]]], "filtered_resps": [["-0.7184538841247559", "True"], ["-5.718453884124756", "False"], ["-5.468453884124756", "False"], ["-6.718453884124756", "False"], ["-8.718454360961914", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "92bc225e0228699997c5810a447f02939c400463cf4d15545ccd4f33eb80ac19", "prompt_hash": "a90175d05989d5ac1566d27c0dfec522e38bbdae818eae6dad21b6b91420f9f3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 593, "doc": {"id": "8b25332de2894ab38784235838d38cec", "question": "If the president wanted to ban snakes, where would he issue such a decree?", "question_concept": "snake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["street", "tropical forest", "garden of eden", "new mexico", "white house"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If the president wanted to ban snakes, where would he issue such a decree?\nA. street\nB. tropical forest\nC. garden of eden\nD. new mexico\nE. white house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If the president wanted to ban snakes, where would he issue such a decree?\nA. street\nB. tropical forest\nC. garden of eden\nD. new mexico\nE. white house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If the president wanted to ban snakes, where would he issue such a decree?\nA. street\nB. tropical forest\nC. garden of eden\nD. new mexico\nE. white house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If the president wanted to ban snakes, where would he issue such a decree?\nA. street\nB. tropical forest\nC. garden of eden\nD. new mexico\nE. white house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If the president wanted to ban snakes, where would he issue such a decree?\nA. street\nB. tropical forest\nC. garden of eden\nD. new mexico\nE. white house\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.107450485229492", "False"]], [["-5.607450485229492", "False"]], [["-7.107450485229492", "False"]], [["-7.607450485229492", "False"]], [["-0.8574506640434265", "True"]]], "filtered_resps": [["-4.107450485229492", "False"], ["-5.607450485229492", "False"], ["-7.107450485229492", "False"], ["-7.607450485229492", "False"], ["-0.8574506640434265", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dbdcdbbff44de27ec771ede7df1ce768e2ba5345487f059a5c4ff77f207239df", "prompt_hash": "64dca5594fce8beb960649cde167747d57c42eec47f9e71f3dec3eb19bb5efbd", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 594, "doc": {"id": "dd4a811d18549f1ae1954cf938b28536", "question": "They were searching for rocks, so they missed the birds overhead as they stared at the what?", "question_concept": "rocks", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ground", "drawer", "surface of earth", "pizza", "waterfall"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: They were searching for rocks, so they missed the birds overhead as they stared at the what?\nA. ground\nB. drawer\nC. surface of earth\nD. pizza\nE. waterfall\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They were searching for rocks, so they missed the birds overhead as they stared at the what?\nA. ground\nB. drawer\nC. surface of earth\nD. pizza\nE. waterfall\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They were searching for rocks, so they missed the birds overhead as they stared at the what?\nA. ground\nB. drawer\nC. surface of earth\nD. pizza\nE. waterfall\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They were searching for rocks, so they missed the birds overhead as they stared at the what?\nA. ground\nB. drawer\nC. surface of earth\nD. pizza\nE. waterfall\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They were searching for rocks, so they missed the birds overhead as they stared at the what?\nA. ground\nB. drawer\nC. surface of earth\nD. pizza\nE. waterfall\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0973283052444458", "True"]], [["-5.847328186035156", "False"]], [["-2.5973281860351562", "False"]], [["-8.097328186035156", "False"]], [["-10.097328186035156", "False"]]], "filtered_resps": [["-1.0973283052444458", "True"], ["-5.847328186035156", "False"], ["-2.5973281860351562", "False"], ["-8.097328186035156", "False"], ["-10.097328186035156", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f6144715aacf6ca792cb8cc674d5fa1ece02980975a46c564760ef446d58f2e9", "prompt_hash": "c4bc0f33ac504c4215b0620729edaa66b1da15e04f330c289c4c8830f2fb0489", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 595, "doc": {"id": "e2ff952c17faf1c56a970502630d4c86", "question": "Her son scraped his knee, she fetched a bottle of peroxide from the what?", "question_concept": "bottle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["supermarket", "diaper bag", "liquor store", "hollow log", "medicine cabinet"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Her son scraped his knee, she fetched a bottle of peroxide from the what?\nA. supermarket\nB. diaper bag\nC. liquor store\nD. hollow log\nE. medicine cabinet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Her son scraped his knee, she fetched a bottle of peroxide from the what?\nA. supermarket\nB. diaper bag\nC. liquor store\nD. hollow log\nE. medicine cabinet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Her son scraped his knee, she fetched a bottle of peroxide from the what?\nA. supermarket\nB. diaper bag\nC. liquor store\nD. hollow log\nE. medicine cabinet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Her son scraped his knee, she fetched a bottle of peroxide from the what?\nA. supermarket\nB. diaper bag\nC. liquor store\nD. hollow log\nE. medicine cabinet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Her son scraped his knee, she fetched a bottle of peroxide from the what?\nA. supermarket\nB. diaper bag\nC. liquor store\nD. hollow log\nE. medicine cabinet\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.911642074584961", "False"]], [["-2.411642074584961", "False"]], [["-7.411642074584961", "False"]], [["-9.161642074584961", "False"]], [["-1.161642074584961", "True"]]], "filtered_resps": [["-3.911642074584961", "False"], ["-2.411642074584961", "False"], ["-7.411642074584961", "False"], ["-9.161642074584961", "False"], ["-1.161642074584961", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5f567257bfb5ce0db3adbce25e08220c821a65a3b47eb2d2bfed673774483b0e", "prompt_hash": "e2c24ce60e26a4cccebe83acf0a0ff9049e9a50cd70c3317b4900f4afb4554e0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 596, "doc": {"id": "3a6140e475cbbd3ee1da5ba9a6953597_1", "question": "Where would you expect to find a dictionary along side other writings you can borrow?", "question_concept": "dictionary", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["classroom", "shelf", "explain meaning of words", "table", "library"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you expect to find a dictionary along side other writings you can borrow?\nA. classroom\nB. shelf\nC. explain meaning of words\nD. table\nE. library\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you expect to find a dictionary along side other writings you can borrow?\nA. classroom\nB. shelf\nC. explain meaning of words\nD. table\nE. library\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you expect to find a dictionary along side other writings you can borrow?\nA. classroom\nB. shelf\nC. explain meaning of words\nD. table\nE. library\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you expect to find a dictionary along side other writings you can borrow?\nA. classroom\nB. shelf\nC. explain meaning of words\nD. table\nE. library\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you expect to find a dictionary along side other writings you can borrow?\nA. classroom\nB. shelf\nC. explain meaning of words\nD. table\nE. library\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.181413412094116", "False"]], [["-4.431413650512695", "False"]], [["-8.181413650512695", "False"]], [["-4.931413650512695", "False"]], [["-1.4314134120941162", "True"]]], "filtered_resps": [["-2.181413412094116", "False"], ["-4.431413650512695", "False"], ["-8.181413650512695", "False"], ["-4.931413650512695", "False"], ["-1.4314134120941162", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5f8006b9ae0f5b51f1336997b1c609f46ec3cfb4c6bb99c5f76d69bde638fe86", "prompt_hash": "6a047e53d806e1b181a6e0e2a5c77ed40ef589f654ef8bb4e6c1f938ad2840d7", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 597, "doc": {"id": "e75e0c11e2d5a7b634455a1b4b76856c", "question": "What would be necessary for getting in shape?", "question_concept": "getting in shape", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["good health", "exercise", "muscle tone", "sweat", "feel better"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What would be necessary for getting in shape?\nA. good health\nB. exercise\nC. muscle tone\nD. sweat\nE. feel better\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would be necessary for getting in shape?\nA. good health\nB. exercise\nC. muscle tone\nD. sweat\nE. feel better\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would be necessary for getting in shape?\nA. good health\nB. exercise\nC. muscle tone\nD. sweat\nE. feel better\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would be necessary for getting in shape?\nA. good health\nB. exercise\nC. muscle tone\nD. sweat\nE. feel better\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would be necessary for getting in shape?\nA. good health\nB. exercise\nC. muscle tone\nD. sweat\nE. feel better\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.446277618408203", "False"]], [["-1.1962774991989136", "True"]], [["-6.196277618408203", "False"]], [["-6.446277618408203", "False"]], [["-6.196277618408203", "False"]]], "filtered_resps": [["-4.446277618408203", "False"], ["-1.1962774991989136", "True"], ["-6.196277618408203", "False"], ["-6.446277618408203", "False"], ["-6.196277618408203", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "993d1f3eb4bcc30d39075c8b051a1f5bb11cefe6749dcc31a7a2ae37c9abe99a", "prompt_hash": "3227b1278b598af96643e0a1ee282a1245c7405ea7bda039d7e6e2e7c4d73512", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 598, "doc": {"id": "3b9ccdcb1c932c46a38e040d3e6c7f5b", "question": "A statue that shoots liquid is called a what?", "question_concept": "statue", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["water fountain", "large city", "museum", "pool", "central park"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A statue that shoots liquid is called a what?\nA. water fountain\nB. large city\nC. museum\nD. pool\nE. central park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A statue that shoots liquid is called a what?\nA. water fountain\nB. large city\nC. museum\nD. pool\nE. central park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A statue that shoots liquid is called a what?\nA. water fountain\nB. large city\nC. museum\nD. pool\nE. central park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A statue that shoots liquid is called a what?\nA. water fountain\nB. large city\nC. museum\nD. pool\nE. central park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A statue that shoots liquid is called a what?\nA. water fountain\nB. large city\nC. museum\nD. pool\nE. central park\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.3214441239833832", "True"]], [["-7.071444034576416", "False"]], [["-7.571444034576416", "False"]], [["-7.571444034576416", "False"]], [["-8.071444511413574", "False"]]], "filtered_resps": [["-0.3214441239833832", "True"], ["-7.071444034576416", "False"], ["-7.571444034576416", "False"], ["-7.571444034576416", "False"], ["-8.071444511413574", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c82aad23212f2f1cc7540e5594a51c325daaef4e11ff74200c975506eda3fcff", "prompt_hash": "68c7704fa471cf317581cc3c81d2607dfc46ea6de5a7ee78c46fbb382ff6ff31", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 599, "doc": {"id": "6a29b657b29e1506284d8328dffbbd21", "question": "If you have a child who gets in trouble for being hyperactive you may need to teach them how to what down?", "question_concept": "trouble", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["park", "calm", "being good", "good behavior", "safe"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you have a child who gets in trouble for being hyperactive you may need to teach them how to what down?\nA. park\nB. calm\nC. being good\nD. good behavior\nE. safe\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you have a child who gets in trouble for being hyperactive you may need to teach them how to what down?\nA. park\nB. calm\nC. being good\nD. good behavior\nE. safe\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you have a child who gets in trouble for being hyperactive you may need to teach them how to what down?\nA. park\nB. calm\nC. being good\nD. good behavior\nE. safe\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you have a child who gets in trouble for being hyperactive you may need to teach them how to what down?\nA. park\nB. calm\nC. being good\nD. good behavior\nE. safe\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you have a child who gets in trouble for being hyperactive you may need to teach them how to what down?\nA. park\nB. calm\nC. being good\nD. good behavior\nE. safe\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.653865337371826", "False"]], [["-1.1538652181625366", "True"]], [["-4.903865337371826", "False"]], [["-4.153865337371826", "False"]], [["-6.653865337371826", "False"]]], "filtered_resps": [["-4.653865337371826", "False"], ["-1.1538652181625366", "True"], ["-4.903865337371826", "False"], ["-4.153865337371826", "False"], ["-6.653865337371826", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "631233828d4b89146e0ba6c301d48a1f48dc6e3f2d260607957cde389d3e9399", "prompt_hash": "b1e2768cab47881f60be3c0268ef5386541eb07d76c7dcc679b8238ae257fa5f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 600, "doc": {"id": "96cb628fb7ed2f53245598f707ed2b80", "question": "John loved to paint houses.  How did he usually do it?", "question_concept": "paint", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["clothes get stained", "with brush", "wallpaper", "electrical circuit", "draw"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John loved to paint houses.  How did he usually do it?\nA. clothes get stained\nB. with brush\nC. wallpaper\nD. electrical circuit\nE. draw\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John loved to paint houses.  How did he usually do it?\nA. clothes get stained\nB. with brush\nC. wallpaper\nD. electrical circuit\nE. draw\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John loved to paint houses.  How did he usually do it?\nA. clothes get stained\nB. with brush\nC. wallpaper\nD. electrical circuit\nE. draw\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John loved to paint houses.  How did he usually do it?\nA. clothes get stained\nB. with brush\nC. wallpaper\nD. electrical circuit\nE. draw\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John loved to paint houses.  How did he usually do it?\nA. clothes get stained\nB. with brush\nC. wallpaper\nD. electrical circuit\nE. draw\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.852628707885742", "False"]], [["-1.1026289463043213", "False"]], [["-7.602628707885742", "False"]], [["-8.352628707885742", "False"]], [["-8.102628707885742", "False"]]], "filtered_resps": [["-4.852628707885742", "False"], ["-1.1026289463043213", "False"], ["-7.602628707885742", "False"], ["-8.352628707885742", "False"], ["-8.102628707885742", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d524c16bb974147a455e199f65fe71a7d2bf2751f17de5e7ec659f7f1b6f3c7b", "prompt_hash": "26e7e041b1059831065cd28cada1a1a6a880e3e8c0e4a9cc2605461475a20d82", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 601, "doc": {"id": "bd4e80fa6642a76c064d0bc924411fb0", "question": "When you wipe you feet on the door mat and walk through the door where do you enter?", "question_concept": "mat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["a chair", "school", "living room", "doorway", "bathroom"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: When you wipe you feet on the door mat and walk through the door where do you enter?\nA. a chair\nB. school\nC. living room\nD. doorway\nE. bathroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you wipe you feet on the door mat and walk through the door where do you enter?\nA. a chair\nB. school\nC. living room\nD. doorway\nE. bathroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you wipe you feet on the door mat and walk through the door where do you enter?\nA. a chair\nB. school\nC. living room\nD. doorway\nE. bathroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you wipe you feet on the door mat and walk through the door where do you enter?\nA. a chair\nB. school\nC. living room\nD. doorway\nE. bathroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you wipe you feet on the door mat and walk through the door where do you enter?\nA. a chair\nB. school\nC. living room\nD. doorway\nE. bathroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.120830059051514", "False"]], [["-5.370830059051514", "False"]], [["-3.8708300590515137", "False"]], [["-1.8708300590515137", "False"]], [["-7.120830059051514", "False"]]], "filtered_resps": [["-5.120830059051514", "False"], ["-5.370830059051514", "False"], ["-3.8708300590515137", "False"], ["-1.8708300590515137", "False"], ["-7.120830059051514", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bdc608faf9726eb847dc874799484bea5776e97e198edb13de19390ad96a852a", "prompt_hash": "a90d99f767dedd3057a74648146a6dddef6dd64d5804725f8df79160266cb650", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 602, "doc": {"id": "05490e6c191fbc3c2fe0033ed0bd8aa0", "question": "What can you use to store a book while traveling?", "question_concept": "book", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["library of congress", "pocket", "backpack", "suitcase", "synagogue"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What can you use to store a book while traveling?\nA. library of congress\nB. pocket\nC. backpack\nD. suitcase\nE. synagogue\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can you use to store a book while traveling?\nA. library of congress\nB. pocket\nC. backpack\nD. suitcase\nE. synagogue\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can you use to store a book while traveling?\nA. library of congress\nB. pocket\nC. backpack\nD. suitcase\nE. synagogue\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can you use to store a book while traveling?\nA. library of congress\nB. pocket\nC. backpack\nD. suitcase\nE. synagogue\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can you use to store a book while traveling?\nA. library of congress\nB. pocket\nC. backpack\nD. suitcase\nE. synagogue\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0412440299987793", "False"]], [["-2.7912440299987793", "False"]], [["-1.2912441492080688", "True"]], [["-5.541244029998779", "False"]], [["-7.541244029998779", "False"]]], "filtered_resps": [["-3.0412440299987793", "False"], ["-2.7912440299987793", "False"], ["-1.2912441492080688", "True"], ["-5.541244029998779", "False"], ["-7.541244029998779", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c76a2027f184939fdc3727e54166d7608ece119cb30a8b9966907358c8aef5b9", "prompt_hash": "80a97f12b9691b1311fcc2afcc5c08a7fd0c60aa3da13677f1299dd6b2339d98", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 603, "doc": {"id": "6abd34442438509b4a00c69d6fd24764", "question": "Where would you find gazelle under a G?", "question_concept": "gazelle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["open field", "ivory coast", "dictionary", "steppe", "encyclopedia"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find gazelle under a G?\nA. open field\nB. ivory coast\nC. dictionary\nD. steppe\nE. encyclopedia\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find gazelle under a G?\nA. open field\nB. ivory coast\nC. dictionary\nD. steppe\nE. encyclopedia\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find gazelle under a G?\nA. open field\nB. ivory coast\nC. dictionary\nD. steppe\nE. encyclopedia\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find gazelle under a G?\nA. open field\nB. ivory coast\nC. dictionary\nD. steppe\nE. encyclopedia\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find gazelle under a G?\nA. open field\nB. ivory coast\nC. dictionary\nD. steppe\nE. encyclopedia\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.6272389888763428", "False"]], [["-5.377239227294922", "False"]], [["-4.627239227294922", "False"]], [["-1.6272389888763428", "False"]], [["-7.377239227294922", "False"]]], "filtered_resps": [["-2.6272389888763428", "False"], ["-5.377239227294922", "False"], ["-4.627239227294922", "False"], ["-1.6272389888763428", "False"], ["-7.377239227294922", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9fabccb783de8de996102649235654841522c3b5128cebf9b521b4aae0c053f5", "prompt_hash": "a5ff2ccb2726c9488fbf970021d18df212e713af85b30ce3442ece01c66cdeb1", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 604, "doc": {"id": "e58eb0ec4197c29e961a7bdd4d67de4e", "question": "Competing can lead to great highs, and also great lows when suffering what?", "question_concept": "competing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["winning or losing", "aggression", "gain", "defeat", "sweat"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Competing can lead to great highs, and also great lows when suffering what?\nA. winning or losing\nB. aggression\nC. gain\nD. defeat\nE. sweat\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Competing can lead to great highs, and also great lows when suffering what?\nA. winning or losing\nB. aggression\nC. gain\nD. defeat\nE. sweat\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Competing can lead to great highs, and also great lows when suffering what?\nA. winning or losing\nB. aggression\nC. gain\nD. defeat\nE. sweat\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Competing can lead to great highs, and also great lows when suffering what?\nA. winning or losing\nB. aggression\nC. gain\nD. defeat\nE. sweat\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Competing can lead to great highs, and also great lows when suffering what?\nA. winning or losing\nB. aggression\nC. gain\nD. defeat\nE. sweat\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.6785459518432617", "False"]], [["-6.678545951843262", "False"]], [["-6.178545951843262", "False"]], [["-1.6785458326339722", "False"]], [["-7.178545951843262", "False"]]], "filtered_resps": [["-2.6785459518432617", "False"], ["-6.678545951843262", "False"], ["-6.178545951843262", "False"], ["-1.6785458326339722", "False"], ["-7.178545951843262", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f6fdb8a7b82339d3a77a67d2e5da41e35350e1de0c406e0d7824d6d4d4f24cae", "prompt_hash": "d7b0e5e56e76422fb37b0d5abd2084e38a24eeac540d7a7ee92fdca3a47b76a8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 605, "doc": {"id": "597d2a1c9df7962218d8b807df1f8212", "question": "What blocks sunshine?", "question_concept": "sunshine", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["summer", "park", "desktop", "sea", "moon"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What blocks sunshine?\nA. summer\nB. park\nC. desktop\nD. sea\nE. moon\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What blocks sunshine?\nA. summer\nB. park\nC. desktop\nD. sea\nE. moon\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What blocks sunshine?\nA. summer\nB. park\nC. desktop\nD. sea\nE. moon\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What blocks sunshine?\nA. summer\nB. park\nC. desktop\nD. sea\nE. moon\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What blocks sunshine?\nA. summer\nB. park\nC. desktop\nD. sea\nE. moon\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.203657865524292", "False"]], [["-3.453657865524292", "False"]], [["-4.703658103942871", "False"]], [["-4.453658103942871", "False"]], [["-1.703657865524292", "True"]]], "filtered_resps": [["-3.203657865524292", "False"], ["-3.453657865524292", "False"], ["-4.703658103942871", "False"], ["-4.453658103942871", "False"], ["-1.703657865524292", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "db42d0955484853d01a65f1f2b3e218a65411b5aaf0638be270de5f75e553ac1", "prompt_hash": "24660509c2b882955d4b31c6d78e86f18659620d081d1d6468a70e99e03b0202", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 606, "doc": {"id": "68f6ac445cc008d93f931b999b44b0ba", "question": "When you feel too much heat in your home you can turn on what?", "question_concept": "heat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["coolness", "fan", "get wet", "coldness", "air conditioning"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: When you feel too much heat in your home you can turn on what?\nA. coolness\nB. fan\nC. get wet\nD. coldness\nE. air conditioning\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you feel too much heat in your home you can turn on what?\nA. coolness\nB. fan\nC. get wet\nD. coldness\nE. air conditioning\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you feel too much heat in your home you can turn on what?\nA. coolness\nB. fan\nC. get wet\nD. coldness\nE. air conditioning\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you feel too much heat in your home you can turn on what?\nA. coolness\nB. fan\nC. get wet\nD. coldness\nE. air conditioning\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you feel too much heat in your home you can turn on what?\nA. coolness\nB. fan\nC. get wet\nD. coldness\nE. air conditioning\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.630397319793701", "False"]], [["-2.380397319793701", "False"]], [["-7.380397319793701", "False"]], [["-8.13039779663086", "False"]], [["-1.3803974390029907", "True"]]], "filtered_resps": [["-4.630397319793701", "False"], ["-2.380397319793701", "False"], ["-7.380397319793701", "False"], ["-8.13039779663086", "False"], ["-1.3803974390029907", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f44c2decaed57896a97c6129c04ae5e2deff5f8f71b1b5bfd04063e21017abbf", "prompt_hash": "e4124d3196a9ca5b3caf9cc1b80c54db57f444af400502f318cc2502844518a6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 607, "doc": {"id": "aa4c5d2d348796b8d7fa324f27f4c34f", "question": "Where would you store a pillow case that is not in use?", "question_concept": "pillow case", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kitchen cupboard", "bedding store", "england", "drawer", "bedroom"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you store a pillow case that is not in use?\nA. kitchen cupboard\nB. bedding store\nC. england\nD. drawer\nE. bedroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you store a pillow case that is not in use?\nA. kitchen cupboard\nB. bedding store\nC. england\nD. drawer\nE. bedroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you store a pillow case that is not in use?\nA. kitchen cupboard\nB. bedding store\nC. england\nD. drawer\nE. bedroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you store a pillow case that is not in use?\nA. kitchen cupboard\nB. bedding store\nC. england\nD. drawer\nE. bedroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you store a pillow case that is not in use?\nA. kitchen cupboard\nB. bedding store\nC. england\nD. drawer\nE. bedroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.683847188949585", "False"]], [["-5.933847427368164", "False"]], [["-6.933847427368164", "False"]], [["-1.183847188949585", "True"]], [["-2.933847188949585", "False"]]], "filtered_resps": [["-2.683847188949585", "False"], ["-5.933847427368164", "False"], ["-6.933847427368164", "False"], ["-1.183847188949585", "True"], ["-2.933847188949585", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b20574d609f008ddf4c74b5213ccaa6f3da80f087e9b87059dd3db7321103cd2", "prompt_hash": "a99b5e8d754b4c1b23e014d5d9f48fb5be9f895dc9d360f30d6367d34b453c0a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 608, "doc": {"id": "7400e9c4a2c8e600a0f7e2d162a07837", "question": "If the kitten was going to grow up to be a mouser like it's mother, where should it spend most of it's time?", "question_concept": "kitten", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shelter", "floor", "warm place", "farmhouse", "living room"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If the kitten was going to grow up to be a mouser like it's mother, where should it spend most of it's time?\nA. shelter\nB. floor\nC. warm place\nD. farmhouse\nE. living room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If the kitten was going to grow up to be a mouser like it's mother, where should it spend most of it's time?\nA. shelter\nB. floor\nC. warm place\nD. farmhouse\nE. living room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If the kitten was going to grow up to be a mouser like it's mother, where should it spend most of it's time?\nA. shelter\nB. floor\nC. warm place\nD. farmhouse\nE. living room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If the kitten was going to grow up to be a mouser like it's mother, where should it spend most of it's time?\nA. shelter\nB. floor\nC. warm place\nD. farmhouse\nE. living room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If the kitten was going to grow up to be a mouser like it's mother, where should it spend most of it's time?\nA. shelter\nB. floor\nC. warm place\nD. farmhouse\nE. living room\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7204484939575195", "False"]], [["-2.9704484939575195", "False"]], [["-1.22044837474823", "True"]], [["-6.4704484939575195", "False"]], [["-3.7204484939575195", "False"]]], "filtered_resps": [["-2.7204484939575195", "False"], ["-2.9704484939575195", "False"], ["-1.22044837474823", "True"], ["-6.4704484939575195", "False"], ["-3.7204484939575195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "50ff77115d261d15341f0ad5632676f7e4f8d1debc52d0bbedc62cfa60e78cf7", "prompt_hash": "3a42fcfafe3349c8c6c3271975553e87e3749bf064c79365d3868208412d8323", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 609, "doc": {"id": "fad197409a977126c9587eccd240ceea", "question": "Where is that man buying silk from?", "question_concept": "human", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["space shuttle", "theater", "china", "indian resteraunt", "bar"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where is that man buying silk from?\nA. space shuttle\nB. theater\nC. china\nD. indian resteraunt\nE. bar\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is that man buying silk from?\nA. space shuttle\nB. theater\nC. china\nD. indian resteraunt\nE. bar\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is that man buying silk from?\nA. space shuttle\nB. theater\nC. china\nD. indian resteraunt\nE. bar\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is that man buying silk from?\nA. space shuttle\nB. theater\nC. china\nD. indian resteraunt\nE. bar\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is that man buying silk from?\nA. space shuttle\nB. theater\nC. china\nD. indian resteraunt\nE. bar\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.314528465270996", "False"]], [["-5.314528465270996", "False"]], [["-2.064528465270996", "False"]], [["-3.564528465270996", "False"]], [["-7.814528465270996", "False"]]], "filtered_resps": [["-4.314528465270996", "False"], ["-5.314528465270996", "False"], ["-2.064528465270996", "False"], ["-3.564528465270996", "False"], ["-7.814528465270996", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0cc485dc553885cc89fc275fb62b0fa9501a852f0cb98a3098ea97c003fd396d", "prompt_hash": "e644ca1b9e80dd7442bb87cdc2a85e2377d64def6fe886329a0c919d1703e4a5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 610, "doc": {"id": "f09038444aeb1a048f04dedd5b97b769", "question": "Where is a teacher likely to keep her clavichord?", "question_concept": "clavichord", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["living room", "parlor", "music hall", "music room", "museum"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a teacher likely to keep her clavichord?\nA. living room\nB. parlor\nC. music hall\nD. music room\nE. museum\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a teacher likely to keep her clavichord?\nA. living room\nB. parlor\nC. music hall\nD. music room\nE. museum\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a teacher likely to keep her clavichord?\nA. living room\nB. parlor\nC. music hall\nD. music room\nE. museum\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a teacher likely to keep her clavichord?\nA. living room\nB. parlor\nC. music hall\nD. music room\nE. museum\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a teacher likely to keep her clavichord?\nA. living room\nB. parlor\nC. music hall\nD. music room\nE. museum\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.852531909942627", "False"]], [["-5.852531909942627", "False"]], [["-4.602531909942627", "False"]], [["-2.102531909942627", "False"]], [["-10.102531433105469", "False"]]], "filtered_resps": [["-1.852531909942627", "False"], ["-5.852531909942627", "False"], ["-4.602531909942627", "False"], ["-2.102531909942627", "False"], ["-10.102531433105469", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "27e36f960f48c5bb5247f790615dbf1b99f9e0ef104f2073f9082429f1ad1d3e", "prompt_hash": "bf58d0c6163d57bf072a46dd9e696842468819b5fadb0b919f950b4139860324", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 611, "doc": {"id": "0aa23ad1ba9f28bc3e0185237a7ce1cc", "question": "Where are you if your bieifcase is going through an x-ray machine?", "question_concept": "briefcase", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["luggage store", "courtroom", "airport", "office building", "hand"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where are you if your bieifcase is going through an x-ray machine?\nA. luggage store\nB. courtroom\nC. airport\nD. office building\nE. hand\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are you if your bieifcase is going through an x-ray machine?\nA. luggage store\nB. courtroom\nC. airport\nD. office building\nE. hand\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are you if your bieifcase is going through an x-ray machine?\nA. luggage store\nB. courtroom\nC. airport\nD. office building\nE. hand\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are you if your bieifcase is going through an x-ray machine?\nA. luggage store\nB. courtroom\nC. airport\nD. office building\nE. hand\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are you if your bieifcase is going through an x-ray machine?\nA. luggage store\nB. courtroom\nC. airport\nD. office building\nE. hand\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.14362096786499", "False"]], [["-5.89362096786499", "False"]], [["-0.893621027469635", "True"]], [["-8.143621444702148", "False"]], [["-8.143621444702148", "False"]]], "filtered_resps": [["-5.14362096786499", "False"], ["-5.89362096786499", "False"], ["-0.893621027469635", "True"], ["-8.143621444702148", "False"], ["-8.143621444702148", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c2e8236c300ea0d3b4f9e582c8655068ecc7eccb7663988c7945a14caf069c51", "prompt_hash": "ce2df0ec68820710e80da9e15826487e979a73df2feea62bf448071729dbe2cf", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 612, "doc": {"id": "06be29539ad3e1fbd7b53b05243f4bd7", "question": "They were kissing each other good bye, they had no worries because their relationship had a strong foundation of what?", "question_concept": "kissing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["partner", "trust", "cooperation", "bricks", "herpes"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: They were kissing each other good bye, they had no worries because their relationship had a strong foundation of what?\nA. partner\nB. trust\nC. cooperation\nD. bricks\nE. herpes\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They were kissing each other good bye, they had no worries because their relationship had a strong foundation of what?\nA. partner\nB. trust\nC. cooperation\nD. bricks\nE. herpes\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They were kissing each other good bye, they had no worries because their relationship had a strong foundation of what?\nA. partner\nB. trust\nC. cooperation\nD. bricks\nE. herpes\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They were kissing each other good bye, they had no worries because their relationship had a strong foundation of what?\nA. partner\nB. trust\nC. cooperation\nD. bricks\nE. herpes\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They were kissing each other good bye, they had no worries because their relationship had a strong foundation of what?\nA. partner\nB. trust\nC. cooperation\nD. bricks\nE. herpes\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.099597930908203", "False"]], [["-0.8495981097221375", "True"]], [["-9.349597930908203", "False"]], [["-7.349597930908203", "False"]], [["-8.599597930908203", "False"]]], "filtered_resps": [["-4.099597930908203", "False"], ["-0.8495981097221375", "True"], ["-9.349597930908203", "False"], ["-7.349597930908203", "False"], ["-8.599597930908203", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "37e16ca394b30818c8736b38bc5b74fe4eabe3c3558c8452f7df5168e02f9d8e", "prompt_hash": "ca899ad35fe662a5eeaee6aea9e88e1cdb11012bd2649c741749f2180ff6145b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 613, "doc": {"id": "bbe0a1ad733e5699f991ff91b3712a6f", "question": "Why would you take a bus to work?", "question_concept": "take bus", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["commute", "flying", "get somewhere", "travel", "go home"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Why would you take a bus to work?\nA. commute\nB. flying\nC. get somewhere\nD. travel\nE. go home\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would you take a bus to work?\nA. commute\nB. flying\nC. get somewhere\nD. travel\nE. go home\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would you take a bus to work?\nA. commute\nB. flying\nC. get somewhere\nD. travel\nE. go home\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would you take a bus to work?\nA. commute\nB. flying\nC. get somewhere\nD. travel\nE. go home\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would you take a bus to work?\nA. commute\nB. flying\nC. get somewhere\nD. travel\nE. go home\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.505517840385437", "True"]], [["-6.755517959594727", "False"]], [["-6.005517959594727", "False"]], [["-5.755517959594727", "False"]], [["-7.755517959594727", "False"]]], "filtered_resps": [["-0.505517840385437", "True"], ["-6.755517959594727", "False"], ["-6.005517959594727", "False"], ["-5.755517959594727", "False"], ["-7.755517959594727", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "35ac1fa56ddfec321a4020baf6ddf9d70edab942cd86cc5ca0de897d18c9148e", "prompt_hash": "9b0f060e8d9b08b1123eb3f14dac5dbb9cb276e414ee55e20d1cf22304c9c060", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 614, "doc": {"id": "9e5ce2b7d9eb404cdf8c7317dd0b5a59", "question": "If you are hungry and going fishing, why would you be going fishing?", "question_concept": "going fishing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["to see the fish", "have fun", "catching fish", "wet clothes", "killing"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If you are hungry and going fishing, why would you be going fishing?\nA. to see the fish\nB. have fun\nC. catching fish\nD. wet clothes\nE. killing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you are hungry and going fishing, why would you be going fishing?\nA. to see the fish\nB. have fun\nC. catching fish\nD. wet clothes\nE. killing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you are hungry and going fishing, why would you be going fishing?\nA. to see the fish\nB. have fun\nC. catching fish\nD. wet clothes\nE. killing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you are hungry and going fishing, why would you be going fishing?\nA. to see the fish\nB. have fun\nC. catching fish\nD. wet clothes\nE. killing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you are hungry and going fishing, why would you be going fishing?\nA. to see the fish\nB. have fun\nC. catching fish\nD. wet clothes\nE. killing\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2326736450195312", "False"]], [["-3.7326736450195312", "False"]], [["-1.4826735258102417", "True"]], [["-5.482673645019531", "False"]], [["-4.482673645019531", "False"]]], "filtered_resps": [["-3.2326736450195312", "False"], ["-3.7326736450195312", "False"], ["-1.4826735258102417", "True"], ["-5.482673645019531", "False"], ["-4.482673645019531", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c66385033a9cf397afb0d4403d1dedf5b305548871248d1e2ce7d0dbe8d71b8e", "prompt_hash": "726be2c836e3d82eda910ae812aa629f393ed0f7aed9c6a3c034b3c5f173065a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 615, "doc": {"id": "ffde211723f55e9744f94cbc14488a23", "question": "Dogs are very loyal if they have a good owner, they will always what them?", "question_concept": "dogs", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fleas", "eat cake", "attack", "defend", "run fast"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Dogs are very loyal if they have a good owner, they will always what them?\nA. fleas\nB. eat cake\nC. attack\nD. defend\nE. run fast\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Dogs are very loyal if they have a good owner, they will always what them?\nA. fleas\nB. eat cake\nC. attack\nD. defend\nE. run fast\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Dogs are very loyal if they have a good owner, they will always what them?\nA. fleas\nB. eat cake\nC. attack\nD. defend\nE. run fast\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Dogs are very loyal if they have a good owner, they will always what them?\nA. fleas\nB. eat cake\nC. attack\nD. defend\nE. run fast\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Dogs are very loyal if they have a good owner, they will always what them?\nA. fleas\nB. eat cake\nC. attack\nD. defend\nE. run fast\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.156937122344971", "False"]], [["-6.156937122344971", "False"]], [["-7.156937122344971", "False"]], [["-0.6569371819496155", "True"]], [["-9.656937599182129", "False"]]], "filtered_resps": [["-4.156937122344971", "False"], ["-6.156937122344971", "False"], ["-7.156937122344971", "False"], ["-0.6569371819496155", "True"], ["-9.656937599182129", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "425ad1d8ce2162820efbf08f94fd291147813e178b995a73f3caf6e9e480cf0f", "prompt_hash": "b306b652aa5d8c368e1d0008f66da75e26019a0c4bf9751c2fc347fa3138b4ae", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 616, "doc": {"id": "5ff8b0deed53b9ff91d58bd5b6f85bdf", "question": "What does a farmer need to do to make  a maze on his farm in the fall?", "question_concept": "farmer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["seed plants", "plant seeds", "garden", "grow corn", "produce food"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What does a farmer need to do to make  a maze on his farm in the fall?\nA. seed plants\nB. plant seeds\nC. garden\nD. grow corn\nE. produce food\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a farmer need to do to make  a maze on his farm in the fall?\nA. seed plants\nB. plant seeds\nC. garden\nD. grow corn\nE. produce food\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a farmer need to do to make  a maze on his farm in the fall?\nA. seed plants\nB. plant seeds\nC. garden\nD. grow corn\nE. produce food\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a farmer need to do to make  a maze on his farm in the fall?\nA. seed plants\nB. plant seeds\nC. garden\nD. grow corn\nE. produce food\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a farmer need to do to make  a maze on his farm in the fall?\nA. seed plants\nB. plant seeds\nC. garden\nD. grow corn\nE. produce food\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.08308744430542", "False"]], [["-1.0830875635147095", "True"]], [["-5.58308744430542", "False"]], [["-6.83308744430542", "False"]], [["-8.833087921142578", "False"]]], "filtered_resps": [["-3.08308744430542", "False"], ["-1.0830875635147095", "True"], ["-5.58308744430542", "False"], ["-6.83308744430542", "False"], ["-8.833087921142578", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e156bd7df14937a1314a50fea6a0acf1070594f0ab9e9d0fb2faf3a8be4868cf", "prompt_hash": "10d26654e6a7c8c3efcc82c275faeb90db421ca04df408dad739f3c31ae3c1f1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 617, "doc": {"id": "36f1ceeecde7abf99dab635239e12442", "question": "For many males hair is a concern as they get older, it begins to what, causing a receding hairline?", "question_concept": "hair", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["thin out", "grow in ear", "fall out", "bulge", "composted"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: For many males hair is a concern as they get older, it begins to what, causing a receding hairline?\nA. thin out\nB. grow in ear\nC. fall out\nD. bulge\nE. composted\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: For many males hair is a concern as they get older, it begins to what, causing a receding hairline?\nA. thin out\nB. grow in ear\nC. fall out\nD. bulge\nE. composted\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: For many males hair is a concern as they get older, it begins to what, causing a receding hairline?\nA. thin out\nB. grow in ear\nC. fall out\nD. bulge\nE. composted\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: For many males hair is a concern as they get older, it begins to what, causing a receding hairline?\nA. thin out\nB. grow in ear\nC. fall out\nD. bulge\nE. composted\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: For many males hair is a concern as they get older, it begins to what, causing a receding hairline?\nA. thin out\nB. grow in ear\nC. fall out\nD. bulge\nE. composted\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6419658660888672", "False"]], [["-5.891965866088867", "False"]], [["-1.3919658660888672", "True"]], [["-7.391965866088867", "False"]], [["-8.641965866088867", "False"]]], "filtered_resps": [["-1.6419658660888672", "False"], ["-5.891965866088867", "False"], ["-1.3919658660888672", "True"], ["-7.391965866088867", "False"], ["-8.641965866088867", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "025f4ac57c50dd77604499ddf5be41291d778e304f0acb5b46ad39c7cf3a4732", "prompt_hash": "4c5fe80250b7dc424d5f16adfa7db3fa345bf299df65cee90b024bc268f25f44", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 618, "doc": {"id": "e3c9e83c0c62d842de2dfe229f5e6d41", "question": "What happens someone who is bad play poker?", "question_concept": "play poker", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["think", "ante up", "drink", "win money", "losing money"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What happens someone who is bad play poker?\nA. think\nB. ante up\nC. drink\nD. win money\nE. losing money\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens someone who is bad play poker?\nA. think\nB. ante up\nC. drink\nD. win money\nE. losing money\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens someone who is bad play poker?\nA. think\nB. ante up\nC. drink\nD. win money\nE. losing money\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens someone who is bad play poker?\nA. think\nB. ante up\nC. drink\nD. win money\nE. losing money\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens someone who is bad play poker?\nA. think\nB. ante up\nC. drink\nD. win money\nE. losing money\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.316651344299316", "False"]], [["-4.816651344299316", "False"]], [["-7.816651344299316", "False"]], [["-7.816651344299316", "False"]], [["-1.3166513442993164", "True"]]], "filtered_resps": [["-5.316651344299316", "False"], ["-4.816651344299316", "False"], ["-7.816651344299316", "False"], ["-7.816651344299316", "False"], ["-1.3166513442993164", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2df47724726e1cb4abbc0d533b21e88c4148a2cbd15f3e8040422e095aecf88e", "prompt_hash": "4aaab0b176f8727b1f30e1278fee0e6901db0da368badb9067cc79ca0153c273", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 619, "doc": {"id": "c0e4d0118c9cdfe2edc49ef954572b31", "question": "John loved his snake.  It was the only ting he loved. He hated everyone else and was abrasive to most people, but he loved his snake.   How might you describe the snake?", "question_concept": "snake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sun itself", "tropical forest", "pet", "rude", "sharp"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: John loved his snake.  It was the only ting he loved. He hated everyone else and was abrasive to most people, but he loved his snake.   How might you describe the snake?\nA. sun itself\nB. tropical forest\nC. pet\nD. rude\nE. sharp\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John loved his snake.  It was the only ting he loved. He hated everyone else and was abrasive to most people, but he loved his snake.   How might you describe the snake?\nA. sun itself\nB. tropical forest\nC. pet\nD. rude\nE. sharp\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John loved his snake.  It was the only ting he loved. He hated everyone else and was abrasive to most people, but he loved his snake.   How might you describe the snake?\nA. sun itself\nB. tropical forest\nC. pet\nD. rude\nE. sharp\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John loved his snake.  It was the only ting he loved. He hated everyone else and was abrasive to most people, but he loved his snake.   How might you describe the snake?\nA. sun itself\nB. tropical forest\nC. pet\nD. rude\nE. sharp\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John loved his snake.  It was the only ting he loved. He hated everyone else and was abrasive to most people, but he loved his snake.   How might you describe the snake?\nA. sun itself\nB. tropical forest\nC. pet\nD. rude\nE. sharp\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7516732215881348", "False"]], [["-6.751673221588135", "False"]], [["-0.7516731023788452", "True"]], [["-8.251672744750977", "False"]], [["-6.751673221588135", "False"]]], "filtered_resps": [["-3.7516732215881348", "False"], ["-6.751673221588135", "False"], ["-0.7516731023788452", "True"], ["-8.251672744750977", "False"], ["-6.751673221588135", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "803e17f1efd6d4d5e2e8633b258115af977c2ae50a8f65ccf4110cead6e09217", "prompt_hash": "ce9a285861e13f7f8d27e67ec806a03df634b74cd1aedb4648b8a5d4c0555182", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 620, "doc": {"id": "4423c006f2a43f222d4c4e97360c25d3", "question": "The fresh herbs, flowers, and vegetables will shrivel up if people don't do this?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["water plants", "believe in god", "drive to the nearest pool", "speaking english", "raise children"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The fresh herbs, flowers, and vegetables will shrivel up if people don't do this?\nA. water plants\nB. believe in god\nC. drive to the nearest pool\nD. speaking english\nE. raise children\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The fresh herbs, flowers, and vegetables will shrivel up if people don't do this?\nA. water plants\nB. believe in god\nC. drive to the nearest pool\nD. speaking english\nE. raise children\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The fresh herbs, flowers, and vegetables will shrivel up if people don't do this?\nA. water plants\nB. believe in god\nC. drive to the nearest pool\nD. speaking english\nE. raise children\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The fresh herbs, flowers, and vegetables will shrivel up if people don't do this?\nA. water plants\nB. believe in god\nC. drive to the nearest pool\nD. speaking english\nE. raise children\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The fresh herbs, flowers, and vegetables will shrivel up if people don't do this?\nA. water plants\nB. believe in god\nC. drive to the nearest pool\nD. speaking english\nE. raise children\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.41831544041633606", "True"]], [["-7.168315410614014", "False"]], [["-8.168315887451172", "False"]], [["-8.668315887451172", "False"]], [["-8.918315887451172", "False"]]], "filtered_resps": [["-0.41831544041633606", "True"], ["-7.168315410614014", "False"], ["-8.168315887451172", "False"], ["-8.668315887451172", "False"], ["-8.918315887451172", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b091a2e903cab3ce9ea2a517fcf2f112e2a61914bda1223f0641a9b3186c0321", "prompt_hash": "d9783a752c5136c4bb414a1c6601553e6a3885a3450e25f3ee664d63d1982b9f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 621, "doc": {"id": "9382bc51ba092f55a494eff8615899de", "question": "I picked from an apple tree outside of Fort Wayne, where am I?", "question_concept": "apple tree", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["woods", "illinois", "indiana", "washington state", "tampa"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: I picked from an apple tree outside of Fort Wayne, where am I?\nA. woods\nB. illinois\nC. indiana\nD. washington state\nE. tampa\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I picked from an apple tree outside of Fort Wayne, where am I?\nA. woods\nB. illinois\nC. indiana\nD. washington state\nE. tampa\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I picked from an apple tree outside of Fort Wayne, where am I?\nA. woods\nB. illinois\nC. indiana\nD. washington state\nE. tampa\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I picked from an apple tree outside of Fort Wayne, where am I?\nA. woods\nB. illinois\nC. indiana\nD. washington state\nE. tampa\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I picked from an apple tree outside of Fort Wayne, where am I?\nA. woods\nB. illinois\nC. indiana\nD. washington state\nE. tampa\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.709326267242432", "False"]], [["-7.209326267242432", "False"]], [["-1.2093263864517212", "True"]], [["-8.45932674407959", "False"]], [["-7.709326267242432", "False"]]], "filtered_resps": [["-4.709326267242432", "False"], ["-7.209326267242432", "False"], ["-1.2093263864517212", "True"], ["-8.45932674407959", "False"], ["-7.709326267242432", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d301ae8e07b0a47b8731630e158de374914d3e252c64c8a7e712185d11f919d5", "prompt_hash": "32e36274c1256a6d2a59fae1b4b9e2e8825b19f8861492d49b513cdcceb8822b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 622, "doc": {"id": "dec1c42628a7448aa364cdada6e82f98", "question": "The janitor never had much to clean after services, but there was still always a paper or two to pick up where?", "question_concept": "paper", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["synagogue", "front porch", "classroom", "obesity", "grocery store"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The janitor never had much to clean after services, but there was still always a paper or two to pick up where?\nA. synagogue\nB. front porch\nC. classroom\nD. obesity\nE. grocery store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The janitor never had much to clean after services, but there was still always a paper or two to pick up where?\nA. synagogue\nB. front porch\nC. classroom\nD. obesity\nE. grocery store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The janitor never had much to clean after services, but there was still always a paper or two to pick up where?\nA. synagogue\nB. front porch\nC. classroom\nD. obesity\nE. grocery store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The janitor never had much to clean after services, but there was still always a paper or two to pick up where?\nA. synagogue\nB. front porch\nC. classroom\nD. obesity\nE. grocery store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The janitor never had much to clean after services, but there was still always a paper or two to pick up where?\nA. synagogue\nB. front porch\nC. classroom\nD. obesity\nE. grocery store\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6295629739761353", "False"]], [["-2.3795628547668457", "False"]], [["-2.6295628547668457", "False"]], [["-4.129562854766846", "False"]], [["-7.629562854766846", "False"]]], "filtered_resps": [["-1.6295629739761353", "False"], ["-2.3795628547668457", "False"], ["-2.6295628547668457", "False"], ["-4.129562854766846", "False"], ["-7.629562854766846", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "394092950b9fabf113e7c05386a7540396852743ec5554113adfcbe59f26940e", "prompt_hash": "670e2b7a66df13a23bbdd93103af086e470e177491904736748de0d8c038240e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 623, "doc": {"id": "07ea8ff6ee916f2bf9aceab1e19ff99a", "question": "If you're celebrating with too many cocktails what may you have in the morning?", "question_concept": "celebrating", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["drunkenness", "have fun", "headache", "hang over", "intimacy"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If you're celebrating with too many cocktails what may you have in the morning?\nA. drunkenness\nB. have fun\nC. headache\nD. hang over\nE. intimacy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're celebrating with too many cocktails what may you have in the morning?\nA. drunkenness\nB. have fun\nC. headache\nD. hang over\nE. intimacy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're celebrating with too many cocktails what may you have in the morning?\nA. drunkenness\nB. have fun\nC. headache\nD. hang over\nE. intimacy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're celebrating with too many cocktails what may you have in the morning?\nA. drunkenness\nB. have fun\nC. headache\nD. hang over\nE. intimacy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're celebrating with too many cocktails what may you have in the morning?\nA. drunkenness\nB. have fun\nC. headache\nD. hang over\nE. intimacy\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.576174259185791", "False"]], [["-6.076174259185791", "False"]], [["-2.326174259185791", "False"]], [["-2.076174259185791", "False"]], [["-6.826174259185791", "False"]]], "filtered_resps": [["-2.576174259185791", "False"], ["-6.076174259185791", "False"], ["-2.326174259185791", "False"], ["-2.076174259185791", "False"], ["-6.826174259185791", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b4e4ec54a110bcc247d24bf9239be9c04d86958907f6fc0f1c893b3714d37e3c", "prompt_hash": "4ae3e213b319dc383184cb7fc7890cd5cd532082c4f08c5d4134eb25251578e7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 624, "doc": {"id": "a328285c6212c899e335c45db3c49ffd", "question": "Danny found an old film in a sealed what?", "question_concept": "film", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["clingfilm", "disneyland", "cave", "cabinet", "movie"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Danny found an old film in a sealed what?\nA. clingfilm\nB. disneyland\nC. cave\nD. cabinet\nE. movie\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Danny found an old film in a sealed what?\nA. clingfilm\nB. disneyland\nC. cave\nD. cabinet\nE. movie\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Danny found an old film in a sealed what?\nA. clingfilm\nB. disneyland\nC. cave\nD. cabinet\nE. movie\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Danny found an old film in a sealed what?\nA. clingfilm\nB. disneyland\nC. cave\nD. cabinet\nE. movie\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Danny found an old film in a sealed what?\nA. clingfilm\nB. disneyland\nC. cave\nD. cabinet\nE. movie\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7878360748291016", "False"]], [["-6.787836074829102", "False"]], [["-3.7878360748291016", "False"]], [["-1.5378360748291016", "True"]], [["-5.787836074829102", "False"]]], "filtered_resps": [["-1.7878360748291016", "False"], ["-6.787836074829102", "False"], ["-3.7878360748291016", "False"], ["-1.5378360748291016", "True"], ["-5.787836074829102", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eaacb4d4f5b8f0bbc777db905b82f0d0272e33c4901f4ec812b9ec08d742a710", "prompt_hash": "ecef00a1baaa57c88b4631dc26cb37012a078f77bec06640675da023264c8cac", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 625, "doc": {"id": "e248968fec422e1fab0f0561fedff76e", "question": "Where are you likely to find much more than a drop of blood on the floor?", "question_concept": "drop of blood", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["crime scene", "vein", "blood bank", "slaughter house", "needle"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where are you likely to find much more than a drop of blood on the floor?\nA. crime scene\nB. vein\nC. blood bank\nD. slaughter house\nE. needle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are you likely to find much more than a drop of blood on the floor?\nA. crime scene\nB. vein\nC. blood bank\nD. slaughter house\nE. needle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are you likely to find much more than a drop of blood on the floor?\nA. crime scene\nB. vein\nC. blood bank\nD. slaughter house\nE. needle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are you likely to find much more than a drop of blood on the floor?\nA. crime scene\nB. vein\nC. blood bank\nD. slaughter house\nE. needle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are you likely to find much more than a drop of blood on the floor?\nA. crime scene\nB. vein\nC. blood bank\nD. slaughter house\nE. needle\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.162155270576477", "True"]], [["-6.6621551513671875", "False"]], [["-5.9121551513671875", "False"]], [["-2.1621551513671875", "False"]], [["-7.4121551513671875", "False"]]], "filtered_resps": [["-1.162155270576477", "True"], ["-6.6621551513671875", "False"], ["-5.9121551513671875", "False"], ["-2.1621551513671875", "False"], ["-7.4121551513671875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fcd31b0f1b7e838c18899174ee1bc18f7877cd611b24c91922834c527c116550", "prompt_hash": "5394e033e994bd0b3e145b0f7464e3d66463fd1e1cd50d6d9666ad9b87da25f5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 626, "doc": {"id": "2067720531fc03c017af941cec2f6f40", "question": "Where is the first place someone leaving the planet ends up?", "question_concept": "planet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pay debts", "galaxy", "outer space", "orbit", "universe"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where is the first place someone leaving the planet ends up?\nA. pay debts\nB. galaxy\nC. outer space\nD. orbit\nE. universe\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is the first place someone leaving the planet ends up?\nA. pay debts\nB. galaxy\nC. outer space\nD. orbit\nE. universe\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is the first place someone leaving the planet ends up?\nA. pay debts\nB. galaxy\nC. outer space\nD. orbit\nE. universe\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is the first place someone leaving the planet ends up?\nA. pay debts\nB. galaxy\nC. outer space\nD. orbit\nE. universe\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is the first place someone leaving the planet ends up?\nA. pay debts\nB. galaxy\nC. outer space\nD. orbit\nE. universe\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.336931228637695", "False"]], [["-6.086931228637695", "False"]], [["-1.8369314670562744", "True"]], [["-5.086931228637695", "False"]], [["-4.086931228637695", "False"]]], "filtered_resps": [["-4.336931228637695", "False"], ["-6.086931228637695", "False"], ["-1.8369314670562744", "True"], ["-5.086931228637695", "False"], ["-4.086931228637695", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4084d615f9a638425e683bdf5a15dfa6bbca5333afd88952d432cd491a73dc00", "prompt_hash": "b7b2e40462d332d422ce9b127d35564e289b8cd0c48e3ee7e271ed3628de94e0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 627, "doc": {"id": "70d3ebc00b165d9d08f9491a1dd85034", "question": "The town house went right to the curb, a slot effectively made a mailbox of the what?", "question_concept": "mailbox", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["apartment building", "front door", "back door", "street corner", "porch"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The town house went right to the curb, a slot effectively made a mailbox of the what?\nA. apartment building\nB. front door\nC. back door\nD. street corner\nE. porch\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The town house went right to the curb, a slot effectively made a mailbox of the what?\nA. apartment building\nB. front door\nC. back door\nD. street corner\nE. porch\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The town house went right to the curb, a slot effectively made a mailbox of the what?\nA. apartment building\nB. front door\nC. back door\nD. street corner\nE. porch\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The town house went right to the curb, a slot effectively made a mailbox of the what?\nA. apartment building\nB. front door\nC. back door\nD. street corner\nE. porch\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The town house went right to the curb, a slot effectively made a mailbox of the what?\nA. apartment building\nB. front door\nC. back door\nD. street corner\nE. porch\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.6414198875427246", "False"]], [["-2.1414198875427246", "False"]], [["-5.641419887542725", "False"]], [["-3.8914198875427246", "False"]], [["-1.6414198875427246", "True"]]], "filtered_resps": [["-2.6414198875427246", "False"], ["-2.1414198875427246", "False"], ["-5.641419887542725", "False"], ["-3.8914198875427246", "False"], ["-1.6414198875427246", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "286e6446629e7908dc576b89292c0d1219bfbc381b314a18dda889d947f7500c", "prompt_hash": "202be3bd9ade6738ad8c64f7b1547ca4ec0c0bb0d59f586b0ef1d23478e6f610", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 628, "doc": {"id": "41bab71fea3fa04e5a4e10a2f86996df", "question": "The architect thought that a mezzanine would look good, but the planning committee rejected it.  They told the architect that they felt it was a potential hazard given the ages of the people who would be using it.  What might they be designing?", "question_concept": "mezzanine", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["actors", "theater", "concert hall", "floors", "school"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The architect thought that a mezzanine would look good, but the planning committee rejected it.  They told the architect that they felt it was a potential hazard given the ages of the people who would be using it.  What might they be designing?\nA. actors\nB. theater\nC. concert hall\nD. floors\nE. school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The architect thought that a mezzanine would look good, but the planning committee rejected it.  They told the architect that they felt it was a potential hazard given the ages of the people who would be using it.  What might they be designing?\nA. actors\nB. theater\nC. concert hall\nD. floors\nE. school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The architect thought that a mezzanine would look good, but the planning committee rejected it.  They told the architect that they felt it was a potential hazard given the ages of the people who would be using it.  What might they be designing?\nA. actors\nB. theater\nC. concert hall\nD. floors\nE. school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The architect thought that a mezzanine would look good, but the planning committee rejected it.  They told the architect that they felt it was a potential hazard given the ages of the people who would be using it.  What might they be designing?\nA. actors\nB. theater\nC. concert hall\nD. floors\nE. school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The architect thought that a mezzanine would look good, but the planning committee rejected it.  They told the architect that they felt it was a potential hazard given the ages of the people who would be using it.  What might they be designing?\nA. actors\nB. theater\nC. concert hall\nD. floors\nE. school\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.564739227294922", "False"]], [["-3.814739227294922", "False"]], [["-6.564739227294922", "False"]], [["-4.064739227294922", "False"]], [["-1.8147391080856323", "False"]]], "filtered_resps": [["-3.564739227294922", "False"], ["-3.814739227294922", "False"], ["-6.564739227294922", "False"], ["-4.064739227294922", "False"], ["-1.8147391080856323", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1900e70af76391fe095974739a25a258b30e88668fa0233e16e23b67af742b86", "prompt_hash": "6ad15bd03f9fcab31eb6c7a4da1671867fe64fc63cac2a6da2a873b961fdef63", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 629, "doc": {"id": "e18dd9ffc7b7934c39f2b5e9dee5a8c2", "question": "The person wasn't bothered by the weather, she had remembered to bring her what?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["read book", "own house", "apartment", "more rice", "warm coat"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The person wasn't bothered by the weather, she had remembered to bring her what?\nA. read book\nB. own house\nC. apartment\nD. more rice\nE. warm coat\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The person wasn't bothered by the weather, she had remembered to bring her what?\nA. read book\nB. own house\nC. apartment\nD. more rice\nE. warm coat\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The person wasn't bothered by the weather, she had remembered to bring her what?\nA. read book\nB. own house\nC. apartment\nD. more rice\nE. warm coat\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The person wasn't bothered by the weather, she had remembered to bring her what?\nA. read book\nB. own house\nC. apartment\nD. more rice\nE. warm coat\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The person wasn't bothered by the weather, she had remembered to bring her what?\nA. read book\nB. own house\nC. apartment\nD. more rice\nE. warm coat\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6648247241973877", "False"]], [["-7.164824485778809", "False"]], [["-8.414824485778809", "False"]], [["-9.414824485778809", "False"]], [["-1.1648247241973877", "True"]]], "filtered_resps": [["-3.6648247241973877", "False"], ["-7.164824485778809", "False"], ["-8.414824485778809", "False"], ["-9.414824485778809", "False"], ["-1.1648247241973877", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "845ed9ef007f14332dca554f770d396c25bf30c4773ff10d14d9ce130df97f71", "prompt_hash": "e026e88ab17e1b794ad85203bcf213deaf934997b4b7466baeef91ca228669a3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 630, "doc": {"id": "449de58e919975867255218484a9fc89", "question": "If you want to learn about the world and understand the real reasons behind cultural norms and mores, you have achieved a sense of what?", "question_concept": "learning about world", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["enlightenment", "open mind", "confusion", "smartness", "anger"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If you want to learn about the world and understand the real reasons behind cultural norms and mores, you have achieved a sense of what?\nA. enlightenment\nB. open mind\nC. confusion\nD. smartness\nE. anger\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you want to learn about the world and understand the real reasons behind cultural norms and mores, you have achieved a sense of what?\nA. enlightenment\nB. open mind\nC. confusion\nD. smartness\nE. anger\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you want to learn about the world and understand the real reasons behind cultural norms and mores, you have achieved a sense of what?\nA. enlightenment\nB. open mind\nC. confusion\nD. smartness\nE. anger\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you want to learn about the world and understand the real reasons behind cultural norms and mores, you have achieved a sense of what?\nA. enlightenment\nB. open mind\nC. confusion\nD. smartness\nE. anger\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you want to learn about the world and understand the real reasons behind cultural norms and mores, you have achieved a sense of what?\nA. enlightenment\nB. open mind\nC. confusion\nD. smartness\nE. anger\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3122862577438354", "True"]], [["-2.312286376953125", "False"]], [["-6.812286376953125", "False"]], [["-7.312286376953125", "False"]], [["-9.312286376953125", "False"]]], "filtered_resps": [["-1.3122862577438354", "True"], ["-2.312286376953125", "False"], ["-6.812286376953125", "False"], ["-7.312286376953125", "False"], ["-9.312286376953125", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "73f3277c8e84aedb52d4a2169875602bb70cff5b4b40329e92f3dad41e97dadd", "prompt_hash": "065abd16f5da8126dd8c64d9ecab7ef36d9d49e1767bd3e542651ae37fb67f2d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 631, "doc": {"id": "9698232e3599157431c9dc8f2fe179cd", "question": "What is the hopeful result of going to see a play?", "question_concept": "going to play", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sit", "being entertained", "jobless", "meet", "laugh"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is the hopeful result of going to see a play?\nA. sit\nB. being entertained\nC. jobless\nD. meet\nE. laugh\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the hopeful result of going to see a play?\nA. sit\nB. being entertained\nC. jobless\nD. meet\nE. laugh\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the hopeful result of going to see a play?\nA. sit\nB. being entertained\nC. jobless\nD. meet\nE. laugh\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the hopeful result of going to see a play?\nA. sit\nB. being entertained\nC. jobless\nD. meet\nE. laugh\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the hopeful result of going to see a play?\nA. sit\nB. being entertained\nC. jobless\nD. meet\nE. laugh\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.028927326202393", "False"]], [["-1.2789273262023926", "True"]], [["-7.778927326202393", "False"]], [["-7.028927326202393", "False"]], [["-5.278927326202393", "False"]]], "filtered_resps": [["-4.028927326202393", "False"], ["-1.2789273262023926", "True"], ["-7.778927326202393", "False"], ["-7.028927326202393", "False"], ["-5.278927326202393", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "505ed83f64a26668f6730f5af3c77d400751243acf1119532a64687c84cf7222", "prompt_hash": "aa9d2746cee5139faecae375f71fc9e3f26c565e7156aba3f82971b608183513", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 632, "doc": {"id": "0b5d0c3bafbe06dd5334c20cd8ea7fe2", "question": "A person would join a trade school for finding information related to what?", "question_concept": "finding information", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ulcers", "degree", "understanding of", "gaining knowledge", "happiness"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: A person would join a trade school for finding information related to what?\nA. ulcers\nB. degree\nC. understanding of\nD. gaining knowledge\nE. happiness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A person would join a trade school for finding information related to what?\nA. ulcers\nB. degree\nC. understanding of\nD. gaining knowledge\nE. happiness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A person would join a trade school for finding information related to what?\nA. ulcers\nB. degree\nC. understanding of\nD. gaining knowledge\nE. happiness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A person would join a trade school for finding information related to what?\nA. ulcers\nB. degree\nC. understanding of\nD. gaining knowledge\nE. happiness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A person would join a trade school for finding information related to what?\nA. ulcers\nB. degree\nC. understanding of\nD. gaining knowledge\nE. happiness\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4775254726409912", "True"]], [["-4.22752571105957", "False"]], [["-4.22752571105957", "False"]], [["-1.4775254726409912", "True"]], [["-7.72752571105957", "False"]]], "filtered_resps": [["-1.4775254726409912", "True"], ["-4.22752571105957", "False"], ["-4.22752571105957", "False"], ["-1.4775254726409912", "True"], ["-7.72752571105957", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2ba66d5e2d9b9d4d1866b8537756a369981c37bd2052960ffc217d6b58b6e8ab", "prompt_hash": "1424fb93010ea67b49adb5c5193f6e281946ef8b334d8dd3cb255d653eda3f55", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 633, "doc": {"id": "7fe53bf68ec57a52a508611acf5b279e", "question": "Joan was a baby, so there were many things she couldn't do, which caused problems for her parents.  Name one thing that makes raising a baby difficult.", "question_concept": "baby", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["arrive early", "learn to walk", "boy or girl", "bring joy", "talk nonsense"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Joan was a baby, so there were many things she couldn't do, which caused problems for her parents.  Name one thing that makes raising a baby difficult.\nA. arrive early\nB. learn to walk\nC. boy or girl\nD. bring joy\nE. talk nonsense\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joan was a baby, so there were many things she couldn't do, which caused problems for her parents.  Name one thing that makes raising a baby difficult.\nA. arrive early\nB. learn to walk\nC. boy or girl\nD. bring joy\nE. talk nonsense\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joan was a baby, so there were many things she couldn't do, which caused problems for her parents.  Name one thing that makes raising a baby difficult.\nA. arrive early\nB. learn to walk\nC. boy or girl\nD. bring joy\nE. talk nonsense\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joan was a baby, so there were many things she couldn't do, which caused problems for her parents.  Name one thing that makes raising a baby difficult.\nA. arrive early\nB. learn to walk\nC. boy or girl\nD. bring joy\nE. talk nonsense\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joan was a baby, so there were many things she couldn't do, which caused problems for her parents.  Name one thing that makes raising a baby difficult.\nA. arrive early\nB. learn to walk\nC. boy or girl\nD. bring joy\nE. talk nonsense\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.969567775726318", "False"]], [["-0.7195676565170288", "True"]], [["-5.719567775726318", "False"]], [["-6.469567775726318", "False"]], [["-5.719567775726318", "False"]]], "filtered_resps": [["-4.969567775726318", "False"], ["-0.7195676565170288", "True"], ["-5.719567775726318", "False"], ["-6.469567775726318", "False"], ["-5.719567775726318", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b88dd134999a99abe703bbc0b8b955ebbfd9c45e3c536675b20067a0679d3ae0", "prompt_hash": "fba52a119c552b38f40f0607466913e8213ed4fb332365dbd31e196cb93cb2c3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 634, "doc": {"id": "68c41ec8415eab50620eb9ecf6f35a6a", "question": "Where would you put some ham if you want to cook it?", "question_concept": "ham", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hamshackle", "pizza", "fridge", "refrigerator", "part of meal"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put some ham if you want to cook it?\nA. hamshackle\nB. pizza\nC. fridge\nD. refrigerator\nE. part of meal\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put some ham if you want to cook it?\nA. hamshackle\nB. pizza\nC. fridge\nD. refrigerator\nE. part of meal\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put some ham if you want to cook it?\nA. hamshackle\nB. pizza\nC. fridge\nD. refrigerator\nE. part of meal\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put some ham if you want to cook it?\nA. hamshackle\nB. pizza\nC. fridge\nD. refrigerator\nE. part of meal\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put some ham if you want to cook it?\nA. hamshackle\nB. pizza\nC. fridge\nD. refrigerator\nE. part of meal\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.318741798400879", "False"]], [["-7.318741798400879", "False"]], [["-2.818741798400879", "False"]], [["-2.068741798400879", "False"]], [["-6.318741798400879", "False"]]], "filtered_resps": [["-5.318741798400879", "False"], ["-7.318741798400879", "False"], ["-2.818741798400879", "False"], ["-2.068741798400879", "False"], ["-6.318741798400879", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a55811ca85e623bd25ca07601088b45f28c34e1af60ac5973f2fe36345a35370", "prompt_hash": "a9dfc5716b74ea6f1a368fe877a6aca04d57e5157eca0b36926ecc20df8fee33", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 635, "doc": {"id": "6c4b2c93a4bdafb6cbf2b2ef2439b06f", "question": "Running errands with screaming kids will likely cause what?", "question_concept": "running errands", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["efficiency", "insanity", "aggravation", "tiredness", "stress"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Running errands with screaming kids will likely cause what?\nA. efficiency\nB. insanity\nC. aggravation\nD. tiredness\nE. stress\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Running errands with screaming kids will likely cause what?\nA. efficiency\nB. insanity\nC. aggravation\nD. tiredness\nE. stress\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Running errands with screaming kids will likely cause what?\nA. efficiency\nB. insanity\nC. aggravation\nD. tiredness\nE. stress\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Running errands with screaming kids will likely cause what?\nA. efficiency\nB. insanity\nC. aggravation\nD. tiredness\nE. stress\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Running errands with screaming kids will likely cause what?\nA. efficiency\nB. insanity\nC. aggravation\nD. tiredness\nE. stress\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9919800758361816", "False"]], [["-2.9919800758361816", "False"]], [["-2.2419800758361816", "False"]], [["-3.4919800758361816", "False"]], [["-3.9919800758361816", "False"]]], "filtered_resps": [["-3.9919800758361816", "False"], ["-2.9919800758361816", "False"], ["-2.2419800758361816", "False"], ["-3.4919800758361816", "False"], ["-3.9919800758361816", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2eee9a8567db28ac05d021c3a1e19b8f2d98c65c1672987aae3d044e7fe64538", "prompt_hash": "ca398616b875e0174249c2c04785056158cbde8cb1685d7a644033d394bf1945", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 636, "doc": {"id": "51e2da7396ab7045533e885dbb98a424", "question": "Sam wasn't lying, but he left out important details. He was being what?", "question_concept": "lying", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dishonesty", "deceitful", "imagination", "deceptive", "poker face"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Sam wasn't lying, but he left out important details. He was being what?\nA. dishonesty\nB. deceitful\nC. imagination\nD. deceptive\nE. poker face\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sam wasn't lying, but he left out important details. He was being what?\nA. dishonesty\nB. deceitful\nC. imagination\nD. deceptive\nE. poker face\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sam wasn't lying, but he left out important details. He was being what?\nA. dishonesty\nB. deceitful\nC. imagination\nD. deceptive\nE. poker face\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sam wasn't lying, but he left out important details. He was being what?\nA. dishonesty\nB. deceitful\nC. imagination\nD. deceptive\nE. poker face\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sam wasn't lying, but he left out important details. He was being what?\nA. dishonesty\nB. deceitful\nC. imagination\nD. deceptive\nE. poker face\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.274514675140381", "False"]], [["-3.024514675140381", "False"]], [["-5.274514675140381", "False"]], [["-1.2745147943496704", "True"]], [["-8.024515151977539", "False"]]], "filtered_resps": [["-4.274514675140381", "False"], ["-3.024514675140381", "False"], ["-5.274514675140381", "False"], ["-1.2745147943496704", "True"], ["-8.024515151977539", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e9f0aca13f4431ad973610d5a5bda4f6b8b009ce26792d07604ac3264f7b9eba", "prompt_hash": "cc58abc171d4c81d07b4fabaef8e14a6001dd587dd6d02182e751ffdc90147b5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 637, "doc": {"id": "3f6157968fcf50d257ec3d8c729b7443", "question": "what does someone have that causes them committing murder?", "question_concept": "committing murder", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["problems", "distress", "fear", "go to jail", "killer"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: what does someone have that causes them committing murder?\nA. problems\nB. distress\nC. fear\nD. go to jail\nE. killer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: what does someone have that causes them committing murder?\nA. problems\nB. distress\nC. fear\nD. go to jail\nE. killer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: what does someone have that causes them committing murder?\nA. problems\nB. distress\nC. fear\nD. go to jail\nE. killer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: what does someone have that causes them committing murder?\nA. problems\nB. distress\nC. fear\nD. go to jail\nE. killer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: what does someone have that causes them committing murder?\nA. problems\nB. distress\nC. fear\nD. go to jail\nE. killer\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2432602643966675", "True"]], [["-3.243260383605957", "False"]], [["-5.243260383605957", "False"]], [["-5.243260383605957", "False"]], [["-5.743260383605957", "False"]]], "filtered_resps": [["-1.2432602643966675", "True"], ["-3.243260383605957", "False"], ["-5.243260383605957", "False"], ["-5.243260383605957", "False"], ["-5.743260383605957", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b834ec4b833a0e3b9ece3bab7a248861a06c6fdb36d7f961ce4610b6aacd0d4d", "prompt_hash": "d587e44231bcd89e7c770c1b03d06cb7a6828a0099468f61afb70bef33f26032", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 638, "doc": {"id": "4768aa28fa14569d830f8947565296c1", "question": "What kind of place has a leader?", "question_concept": "leader", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["army", "battle", "wildlife", "country", "organization"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of place has a leader?\nA. army\nB. battle\nC. wildlife\nD. country\nE. organization\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of place has a leader?\nA. army\nB. battle\nC. wildlife\nD. country\nE. organization\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of place has a leader?\nA. army\nB. battle\nC. wildlife\nD. country\nE. organization\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of place has a leader?\nA. army\nB. battle\nC. wildlife\nD. country\nE. organization\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of place has a leader?\nA. army\nB. battle\nC. wildlife\nD. country\nE. organization\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.54839289188385", "True"]], [["-7.0483927726745605", "False"]], [["-7.5483927726745605", "False"]], [["-1.79839289188385", "False"]], [["-4.2983927726745605", "False"]]], "filtered_resps": [["-1.54839289188385", "True"], ["-7.0483927726745605", "False"], ["-7.5483927726745605", "False"], ["-1.79839289188385", "False"], ["-4.2983927726745605", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ba83f0ddad686c33b4e4864478feee7fa88b7620f3948cec4027d748fe0fedb4", "prompt_hash": "72d8f1ed6d12de368ed634bc53b9fc4c762596093bde261771bf4ac2a9615be0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 639, "doc": {"id": "5516b1c93f94aaa0bf9a4c7b124788d4", "question": "How is a person likely to communicatewith others?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["say words", "meet friends", "open mouth", "thank god", "die of cancer"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: How is a person likely to communicatewith others?\nA. say words\nB. meet friends\nC. open mouth\nD. thank god\nE. die of cancer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How is a person likely to communicatewith others?\nA. say words\nB. meet friends\nC. open mouth\nD. thank god\nE. die of cancer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How is a person likely to communicatewith others?\nA. say words\nB. meet friends\nC. open mouth\nD. thank god\nE. die of cancer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How is a person likely to communicatewith others?\nA. say words\nB. meet friends\nC. open mouth\nD. thank god\nE. die of cancer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How is a person likely to communicatewith others?\nA. say words\nB. meet friends\nC. open mouth\nD. thank god\nE. die of cancer\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6343028545379639", "True"]], [["-6.384303092956543", "False"]], [["-6.634303092956543", "False"]], [["-8.384303092956543", "False"]], [["-9.134303092956543", "False"]]], "filtered_resps": [["-0.6343028545379639", "True"], ["-6.384303092956543", "False"], ["-6.634303092956543", "False"], ["-8.384303092956543", "False"], ["-9.134303092956543", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6288556b288d3d57f60e55629ee0c4bb27117e25852fb9ae85df0403f4908392", "prompt_hash": "fb9431d33d7300a24e3254a96ec4cb895901bd52cdfeddfb2195972d66c86f31", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 640, "doc": {"id": "96ea2c3174229c4a6a0e2ffaed2df378", "question": "Where may you be if you're buying pork chops at a corner shop?", "question_concept": "corner shop", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["england", "town", "desert", "kentucky", "iowa"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where may you be if you're buying pork chops at a corner shop?\nA. england\nB. town\nC. desert\nD. kentucky\nE. iowa\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where may you be if you're buying pork chops at a corner shop?\nA. england\nB. town\nC. desert\nD. kentucky\nE. iowa\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where may you be if you're buying pork chops at a corner shop?\nA. england\nB. town\nC. desert\nD. kentucky\nE. iowa\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where may you be if you're buying pork chops at a corner shop?\nA. england\nB. town\nC. desert\nD. kentucky\nE. iowa\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where may you be if you're buying pork chops at a corner shop?\nA. england\nB. town\nC. desert\nD. kentucky\nE. iowa\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.759340763092041", "False"]], [["-1.7593408823013306", "False"]], [["-5.259340763092041", "False"]], [["-5.009340763092041", "False"]], [["-2.509340763092041", "False"]]], "filtered_resps": [["-3.759340763092041", "False"], ["-1.7593408823013306", "False"], ["-5.259340763092041", "False"], ["-5.009340763092041", "False"], ["-2.509340763092041", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "64d61f74ecf9569d415d9f63713dbe58e5b393e04a4d7c0e4d8a06247b0049df", "prompt_hash": "fc61e58f210ae154e2eb2e2828f50f6e9c126c07682c1adb959d6d7b4856f475", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 641, "doc": {"id": "7905b9f4ba503b0ce13b576808e99c42", "question": "Where is a well used toy car likely to be found?", "question_concept": "toy car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["child's room", "boy's bedroom", "own home", "toy store", "house"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a well used toy car likely to be found?\nA. child's room\nB. boy's bedroom\nC. own home\nD. toy store\nE. house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a well used toy car likely to be found?\nA. child's room\nB. boy's bedroom\nC. own home\nD. toy store\nE. house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a well used toy car likely to be found?\nA. child's room\nB. boy's bedroom\nC. own home\nD. toy store\nE. house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a well used toy car likely to be found?\nA. child's room\nB. boy's bedroom\nC. own home\nD. toy store\nE. house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a well used toy car likely to be found?\nA. child's room\nB. boy's bedroom\nC. own home\nD. toy store\nE. house\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2388936281204224", "True"]], [["-4.238893508911133", "False"]], [["-2.738893508911133", "False"]], [["-5.738893508911133", "False"]], [["-2.988893508911133", "False"]]], "filtered_resps": [["-1.2388936281204224", "True"], ["-4.238893508911133", "False"], ["-2.738893508911133", "False"], ["-5.738893508911133", "False"], ["-2.988893508911133", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3529de8013928860d47c34c2ed1b92d5c46f197bbc40df621731c20af6d09f73", "prompt_hash": "85b579db7e1fafd10452ab007646ffb9459861a8bec923ee412b231ad1c68e43", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 642, "doc": {"id": "e0a7d1df3ce14b27888e785e6636d5f0", "question": "Where can fisherman store their rods when on a fishing trip?", "question_concept": "rod", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hardware store", "engine", "fishing camp", "lake", "sporting goods store"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where can fisherman store their rods when on a fishing trip?\nA. hardware store\nB. engine\nC. fishing camp\nD. lake\nE. sporting goods store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can fisherman store their rods when on a fishing trip?\nA. hardware store\nB. engine\nC. fishing camp\nD. lake\nE. sporting goods store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can fisherman store their rods when on a fishing trip?\nA. hardware store\nB. engine\nC. fishing camp\nD. lake\nE. sporting goods store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can fisherman store their rods when on a fishing trip?\nA. hardware store\nB. engine\nC. fishing camp\nD. lake\nE. sporting goods store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can fisherman store their rods when on a fishing trip?\nA. hardware store\nB. engine\nC. fishing camp\nD. lake\nE. sporting goods store\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.684492349624634", "False"]], [["-4.934492111206055", "False"]], [["-1.4344923496246338", "False"]], [["-6.934492111206055", "False"]], [["-4.934492111206055", "False"]]], "filtered_resps": [["-3.684492349624634", "False"], ["-4.934492111206055", "False"], ["-1.4344923496246338", "False"], ["-6.934492111206055", "False"], ["-4.934492111206055", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "59c47a0005d845f95e548f2984b2b1601ca4b51526240fb8da9292246f9bf280", "prompt_hash": "07693f0d4986faadc772cdb5b1ce2e29b3a1409c7d792b98ac627c2861ba8d93", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 643, "doc": {"id": "3eb397b96b6c3a245c81ab30205943f1", "question": "Danny is having fun just dancing and singing with his friends. He wasn't concerned with things that weren't fun. For him having fun is the same as what?", "question_concept": "having fun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["injuries", "smiling", "being happy", "glee", "jump"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Danny is having fun just dancing and singing with his friends. He wasn't concerned with things that weren't fun. For him having fun is the same as what?\nA. injuries\nB. smiling\nC. being happy\nD. glee\nE. jump\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Danny is having fun just dancing and singing with his friends. He wasn't concerned with things that weren't fun. For him having fun is the same as what?\nA. injuries\nB. smiling\nC. being happy\nD. glee\nE. jump\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Danny is having fun just dancing and singing with his friends. He wasn't concerned with things that weren't fun. For him having fun is the same as what?\nA. injuries\nB. smiling\nC. being happy\nD. glee\nE. jump\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Danny is having fun just dancing and singing with his friends. He wasn't concerned with things that weren't fun. For him having fun is the same as what?\nA. injuries\nB. smiling\nC. being happy\nD. glee\nE. jump\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Danny is having fun just dancing and singing with his friends. He wasn't concerned with things that weren't fun. For him having fun is the same as what?\nA. injuries\nB. smiling\nC. being happy\nD. glee\nE. jump\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.78781795501709", "False"]], [["-4.78781795501709", "False"]], [["-1.287818193435669", "True"]], [["-2.037818193435669", "False"]], [["-7.78781795501709", "False"]]], "filtered_resps": [["-5.78781795501709", "False"], ["-4.78781795501709", "False"], ["-1.287818193435669", "True"], ["-2.037818193435669", "False"], ["-7.78781795501709", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d66b5e358a96c6959d5e767e52b57dee5cd3ad99f476c7af47b632fce8b944cc", "prompt_hash": "f01d5bb093b83093da8c2c9a67bb071cbce116e28f2780ea13efb508cdf9025e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 644, "doc": {"id": "536c9af0fae0aa75b32874dfcac66353", "question": "Where would you find an office worker gossiping with their colleagues?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["water cooler", "space shuttle", "baby shower", "bus stop", "family"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find an office worker gossiping with their colleagues?\nA. water cooler\nB. space shuttle\nC. baby shower\nD. bus stop\nE. family\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find an office worker gossiping with their colleagues?\nA. water cooler\nB. space shuttle\nC. baby shower\nD. bus stop\nE. family\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find an office worker gossiping with their colleagues?\nA. water cooler\nB. space shuttle\nC. baby shower\nD. bus stop\nE. family\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find an office worker gossiping with their colleagues?\nA. water cooler\nB. space shuttle\nC. baby shower\nD. bus stop\nE. family\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find an office worker gossiping with their colleagues?\nA. water cooler\nB. space shuttle\nC. baby shower\nD. bus stop\nE. family\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7729510068893433", "True"]], [["-7.272951126098633", "False"]], [["-7.272951126098633", "False"]], [["-8.022951126098633", "False"]], [["-7.772951126098633", "False"]]], "filtered_resps": [["-0.7729510068893433", "True"], ["-7.272951126098633", "False"], ["-7.272951126098633", "False"], ["-8.022951126098633", "False"], ["-7.772951126098633", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "43c67e3db87325af4f01bba4b32975d0eac9647707b8cafc681429093e444c8d", "prompt_hash": "ca45de7ff5c8a73e8d783edd72d98048f6e3d1a1a693768d017b787f6a000a61", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 645, "doc": {"id": "dc36293f603cf230f8059fc6f2e5660d", "question": "Where would you put nails if they are already packaged?", "question_concept": "nails", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pocket", "container", "cabinet", "jar", "store"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put nails if they are already packaged?\nA. pocket\nB. container\nC. cabinet\nD. jar\nE. store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put nails if they are already packaged?\nA. pocket\nB. container\nC. cabinet\nD. jar\nE. store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put nails if they are already packaged?\nA. pocket\nB. container\nC. cabinet\nD. jar\nE. store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put nails if they are already packaged?\nA. pocket\nB. container\nC. cabinet\nD. jar\nE. store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put nails if they are already packaged?\nA. pocket\nB. container\nC. cabinet\nD. jar\nE. store\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.448084831237793", "False"]], [["-1.4480847120285034", "False"]], [["-4.698084831237793", "False"]], [["-3.698084831237793", "False"]], [["-5.698084831237793", "False"]]], "filtered_resps": [["-3.448084831237793", "False"], ["-1.4480847120285034", "False"], ["-4.698084831237793", "False"], ["-3.698084831237793", "False"], ["-5.698084831237793", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "042bcc3c69879b2f7aa9d7e8d297b7bf9a0a86c30b2b4319ea4d4612dfe63e50", "prompt_hash": "3d8c1702bc9c4a8833ee50191bb443d172059f0f649cd9154c79bae5d1b27f91", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 646, "doc": {"id": "1510f5183095466e4fe41b82501a9dd0", "question": "What is a person who is good at sports considered?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lazy", "own house", "talented", "affluent", "reproduce"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is a person who is good at sports considered?\nA. lazy\nB. own house\nC. talented\nD. affluent\nE. reproduce\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a person who is good at sports considered?\nA. lazy\nB. own house\nC. talented\nD. affluent\nE. reproduce\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a person who is good at sports considered?\nA. lazy\nB. own house\nC. talented\nD. affluent\nE. reproduce\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a person who is good at sports considered?\nA. lazy\nB. own house\nC. talented\nD. affluent\nE. reproduce\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a person who is good at sports considered?\nA. lazy\nB. own house\nC. talented\nD. affluent\nE. reproduce\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8028604984283447", "False"]], [["-9.302860260009766", "False"]], [["-1.0528604984283447", "True"]], [["-9.552860260009766", "False"]], [["-10.302860260009766", "False"]]], "filtered_resps": [["-1.8028604984283447", "False"], ["-9.302860260009766", "False"], ["-1.0528604984283447", "True"], ["-9.552860260009766", "False"], ["-10.302860260009766", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7ede268450275b0615c884a673e5902ac282d320dd8d44a930749b4565aa2843", "prompt_hash": "b0e459a11fea8ebc9c5fd616407e8795b58d5fca02b3a7b0817898c971b15af9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 647, "doc": {"id": "1fcc547e4e6813afc1a66717248d6c62", "question": "The man acted ridiculous at the funeral, what attitude should he have taken?", "question_concept": "ridiculous", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["straightforward", "serious", "solemn", "somber", "funny"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The man acted ridiculous at the funeral, what attitude should he have taken?\nA. straightforward\nB. serious\nC. solemn\nD. somber\nE. funny\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man acted ridiculous at the funeral, what attitude should he have taken?\nA. straightforward\nB. serious\nC. solemn\nD. somber\nE. funny\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man acted ridiculous at the funeral, what attitude should he have taken?\nA. straightforward\nB. serious\nC. solemn\nD. somber\nE. funny\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man acted ridiculous at the funeral, what attitude should he have taken?\nA. straightforward\nB. serious\nC. solemn\nD. somber\nE. funny\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man acted ridiculous at the funeral, what attitude should he have taken?\nA. straightforward\nB. serious\nC. solemn\nD. somber\nE. funny\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.513204574584961", "False"]], [["-2.513204574584961", "False"]], [["-2.013204574584961", "False"]], [["-4.263204574584961", "False"]], [["-7.263204574584961", "False"]]], "filtered_resps": [["-3.513204574584961", "False"], ["-2.513204574584961", "False"], ["-2.013204574584961", "False"], ["-4.263204574584961", "False"], ["-7.263204574584961", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b392c0271f9a92b8b6d16a4c36a6fd757d2a1f78559c7a2d0d551f812394acba", "prompt_hash": "515a230dee36e19f7f30a175b2064d4f33001715dbd9236654e27d7ccc05c590", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 648, "doc": {"id": "68a911b64dc943b5f81c0f8dec7faed7", "question": "The pencil sharpener was broken in the classroom, where did the teacher recommend the student go?", "question_concept": "pencil sharpener", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["home", "library", "stationery store", "cabinet", "desk drawer"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The pencil sharpener was broken in the classroom, where did the teacher recommend the student go?\nA. home\nB. library\nC. stationery store\nD. cabinet\nE. desk drawer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The pencil sharpener was broken in the classroom, where did the teacher recommend the student go?\nA. home\nB. library\nC. stationery store\nD. cabinet\nE. desk drawer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The pencil sharpener was broken in the classroom, where did the teacher recommend the student go?\nA. home\nB. library\nC. stationery store\nD. cabinet\nE. desk drawer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The pencil sharpener was broken in the classroom, where did the teacher recommend the student go?\nA. home\nB. library\nC. stationery store\nD. cabinet\nE. desk drawer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The pencil sharpener was broken in the classroom, where did the teacher recommend the student go?\nA. home\nB. library\nC. stationery store\nD. cabinet\nE. desk drawer\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.492062568664551", "False"]], [["-4.742062568664551", "False"]], [["-1.9920624494552612", "False"]], [["-4.742062568664551", "False"]], [["-4.742062568664551", "False"]]], "filtered_resps": [["-3.492062568664551", "False"], ["-4.742062568664551", "False"], ["-1.9920624494552612", "False"], ["-4.742062568664551", "False"], ["-4.742062568664551", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f7d30a6d68b9bbb662ef4e7b5cab281c0d64ee3f7fb22268cd16af5c06051614", "prompt_hash": "fccaf9ffc971f7fca0ae05190bad6d3d22b591ec2f36e5395e402c362ef117ed", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 649, "doc": {"id": "92f423de9a556a66c3eb73e9ddf9399a", "question": "Where does a child likely sit at a desk?", "question_concept": "desk", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["furniture store", "schoolroom", "patio", "office building", "library"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where does a child likely sit at a desk?\nA. furniture store\nB. schoolroom\nC. patio\nD. office building\nE. library\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where does a child likely sit at a desk?\nA. furniture store\nB. schoolroom\nC. patio\nD. office building\nE. library\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where does a child likely sit at a desk?\nA. furniture store\nB. schoolroom\nC. patio\nD. office building\nE. library\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where does a child likely sit at a desk?\nA. furniture store\nB. schoolroom\nC. patio\nD. office building\nE. library\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where does a child likely sit at a desk?\nA. furniture store\nB. schoolroom\nC. patio\nD. office building\nE. library\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3745508193969727", "False"]], [["-0.8745507597923279", "True"]], [["-7.374550819396973", "False"]], [["-8.874550819396973", "False"]], [["-8.124550819396973", "False"]]], "filtered_resps": [["-2.3745508193969727", "False"], ["-0.8745507597923279", "True"], ["-7.374550819396973", "False"], ["-8.874550819396973", "False"], ["-8.124550819396973", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7f9fcb5c02df66fab3cee0faaa90905440477ff8ffc1d191a8ecba1e722b875f", "prompt_hash": "463c0b83ae58b7b80cb7ba36c0b81ba7862dea90796fd902bfd7fa1b558a76fd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 650, "doc": {"id": "1cd94405124031e8681cd12bd25e2d61", "question": "He was trying to procreate with many individuals, this led to a what?", "question_concept": "procreate", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["moaning", "die", "kiss", "std", "sanity"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He was trying to procreate with many individuals, this led to a what?\nA. moaning\nB. die\nC. kiss\nD. std\nE. sanity\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was trying to procreate with many individuals, this led to a what?\nA. moaning\nB. die\nC. kiss\nD. std\nE. sanity\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was trying to procreate with many individuals, this led to a what?\nA. moaning\nB. die\nC. kiss\nD. std\nE. sanity\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was trying to procreate with many individuals, this led to a what?\nA. moaning\nB. die\nC. kiss\nD. std\nE. sanity\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was trying to procreate with many individuals, this led to a what?\nA. moaning\nB. die\nC. kiss\nD. std\nE. sanity\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.928351402282715", "False"]], [["-5.178351402282715", "False"]], [["-5.678351402282715", "False"]], [["-1.4283515214920044", "True"]], [["-6.428351402282715", "False"]]], "filtered_resps": [["-3.928351402282715", "False"], ["-5.178351402282715", "False"], ["-5.678351402282715", "False"], ["-1.4283515214920044", "True"], ["-6.428351402282715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "07287556d38cc76c8962577cefe4a34235e3f0847cc17f04738e519221b390b2", "prompt_hash": "96a9bc4735573c454f3558efeea75cabd203004877f63be6ee73e719f9918e03", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 651, "doc": {"id": "64ab884bd870f6f68146636b4cce921c", "question": "What does playing soccer and winning lead to?", "question_concept": "playing soccer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["excitement", "getting tired", "overtime", "anger", "fights"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What does playing soccer and winning lead to?\nA. excitement\nB. getting tired\nC. overtime\nD. anger\nE. fights\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does playing soccer and winning lead to?\nA. excitement\nB. getting tired\nC. overtime\nD. anger\nE. fights\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does playing soccer and winning lead to?\nA. excitement\nB. getting tired\nC. overtime\nD. anger\nE. fights\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does playing soccer and winning lead to?\nA. excitement\nB. getting tired\nC. overtime\nD. anger\nE. fights\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does playing soccer and winning lead to?\nA. excitement\nB. getting tired\nC. overtime\nD. anger\nE. fights\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.118821620941162", "True"]], [["-5.368821620941162", "False"]], [["-4.368821620941162", "False"]], [["-6.868821620941162", "False"]], [["-7.868821620941162", "False"]]], "filtered_resps": [["-1.118821620941162", "True"], ["-5.368821620941162", "False"], ["-4.368821620941162", "False"], ["-6.868821620941162", "False"], ["-7.868821620941162", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cc5b5a4f3aae014021acebaa1882ba31c4e750fc275cb63a47dc16843474830a", "prompt_hash": "704073c61971e388973b89a982cd80aecc06dea0a99ef70144fefe9a19dd43d7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 652, "doc": {"id": "66275550d64d16339c944e6a6d63eb5b", "question": "What attraction is sometimes so large that you need a map to find your way around?", "question_concept": "map", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["amusement park", "truck stop", "mcdonalds", "backpack", "classroom"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What attraction is sometimes so large that you need a map to find your way around?\nA. amusement park\nB. truck stop\nC. mcdonalds\nD. backpack\nE. classroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What attraction is sometimes so large that you need a map to find your way around?\nA. amusement park\nB. truck stop\nC. mcdonalds\nD. backpack\nE. classroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What attraction is sometimes so large that you need a map to find your way around?\nA. amusement park\nB. truck stop\nC. mcdonalds\nD. backpack\nE. classroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What attraction is sometimes so large that you need a map to find your way around?\nA. amusement park\nB. truck stop\nC. mcdonalds\nD. backpack\nE. classroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What attraction is sometimes so large that you need a map to find your way around?\nA. amusement park\nB. truck stop\nC. mcdonalds\nD. backpack\nE. classroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7900579571723938", "True"]], [["-7.040058135986328", "False"]], [["-7.040058135986328", "False"]], [["-8.290058135986328", "False"]], [["-9.290058135986328", "False"]]], "filtered_resps": [["-0.7900579571723938", "True"], ["-7.040058135986328", "False"], ["-7.040058135986328", "False"], ["-8.290058135986328", "False"], ["-9.290058135986328", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c3d4f671c675bb6235ce9f96414ffa2dd1f9974be1811dabad61379e62a3e1bf", "prompt_hash": "7e0ba87fe7481bc9b159e6d3337f39ff8d4e76841bb01a19f173d7a23637e44c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 653, "doc": {"id": "9b26329d74a6159ab9af4f899303de39", "question": "If my husband never helps me doing housework, what might that lead to?", "question_concept": "doing housework", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boredom", "arguments", "headache", "exhaustion", "park"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If my husband never helps me doing housework, what might that lead to?\nA. boredom\nB. arguments\nC. headache\nD. exhaustion\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If my husband never helps me doing housework, what might that lead to?\nA. boredom\nB. arguments\nC. headache\nD. exhaustion\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If my husband never helps me doing housework, what might that lead to?\nA. boredom\nB. arguments\nC. headache\nD. exhaustion\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If my husband never helps me doing housework, what might that lead to?\nA. boredom\nB. arguments\nC. headache\nD. exhaustion\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If my husband never helps me doing housework, what might that lead to?\nA. boredom\nB. arguments\nC. headache\nD. exhaustion\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.800510406494141", "False"]], [["-1.8005106449127197", "False"]], [["-7.300510406494141", "False"]], [["-5.800510406494141", "False"]], [["-9.05051040649414", "False"]]], "filtered_resps": [["-4.800510406494141", "False"], ["-1.8005106449127197", "False"], ["-7.300510406494141", "False"], ["-5.800510406494141", "False"], ["-9.05051040649414", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2fa9faaac68b4df5754d3b537443bcf910bd03281a1b8c906a794d9e53afa4ec", "prompt_hash": "d515ec6f7b433a81bfbce6283dc6db11d4552ec8adea68d70d3813a0d8232cd9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 654, "doc": {"id": "f74b7f268d3c190a13f99ede6d2359e1", "question": "The advertisement came in the form of a pop-up, where did it appear?", "question_concept": "advertisement", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["web page", "store", "la ville", "bus", "email"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The advertisement came in the form of a pop-up, where did it appear?\nA. web page\nB. store\nC. la ville\nD. bus\nE. email\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The advertisement came in the form of a pop-up, where did it appear?\nA. web page\nB. store\nC. la ville\nD. bus\nE. email\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The advertisement came in the form of a pop-up, where did it appear?\nA. web page\nB. store\nC. la ville\nD. bus\nE. email\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The advertisement came in the form of a pop-up, where did it appear?\nA. web page\nB. store\nC. la ville\nD. bus\nE. email\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The advertisement came in the form of a pop-up, where did it appear?\nA. web page\nB. store\nC. la ville\nD. bus\nE. email\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9372670650482178", "True"]], [["-5.437267303466797", "False"]], [["-4.937267303466797", "False"]], [["-7.937267303466797", "False"]], [["-5.937267303466797", "False"]]], "filtered_resps": [["-0.9372670650482178", "True"], ["-5.437267303466797", "False"], ["-4.937267303466797", "False"], ["-7.937267303466797", "False"], ["-5.937267303466797", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cb13d2300ad00b78d124b0d19dd479e0c8e474454233113000fae76cc7d95463", "prompt_hash": "f28a298d2c7e982ae85a2f9a0f3890cc8b4388ed7934328d182cea42fed4dd9d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 655, "doc": {"id": "22458fdcead20e2def0df0d92d5806f6", "question": "WHere do people live?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["apartment", "eat cake", "bus depot", "football stadium", "surface of earth"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: WHere do people live?\nA. apartment\nB. eat cake\nC. bus depot\nD. football stadium\nE. surface of earth\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: WHere do people live?\nA. apartment\nB. eat cake\nC. bus depot\nD. football stadium\nE. surface of earth\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: WHere do people live?\nA. apartment\nB. eat cake\nC. bus depot\nD. football stadium\nE. surface of earth\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: WHere do people live?\nA. apartment\nB. eat cake\nC. bus depot\nD. football stadium\nE. surface of earth\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: WHere do people live?\nA. apartment\nB. eat cake\nC. bus depot\nD. football stadium\nE. surface of earth\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8988327980041504", "False"]], [["-7.14883279800415", "False"]], [["-8.648833274841309", "False"]], [["-8.898833274841309", "False"]], [["-0.8988329172134399", "True"]]], "filtered_resps": [["-2.8988327980041504", "False"], ["-7.14883279800415", "False"], ["-8.648833274841309", "False"], ["-8.898833274841309", "False"], ["-0.8988329172134399", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "22aa58d4359e7c11a5a62a7f6a54725ed98a40573a8b0d6d59cae8f1b4918564", "prompt_hash": "422eeb1c1b0a2d4b942f7bf71931a5deb460a600fe24b1251ea6d99f14bca842", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 656, "doc": {"id": "f7b96f195a7adfe0c74924a165cfd055", "question": "People are what when you're a stranger?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["train", "strange", "human", "stupid", "dangerous"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: People are what when you're a stranger?\nA. train\nB. strange\nC. human\nD. stupid\nE. dangerous\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: People are what when you're a stranger?\nA. train\nB. strange\nC. human\nD. stupid\nE. dangerous\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: People are what when you're a stranger?\nA. train\nB. strange\nC. human\nD. stupid\nE. dangerous\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: People are what when you're a stranger?\nA. train\nB. strange\nC. human\nD. stupid\nE. dangerous\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: People are what when you're a stranger?\nA. train\nB. strange\nC. human\nD. stupid\nE. dangerous\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5569450855255127", "False"]], [["-3.5569450855255127", "False"]], [["-1.3069450855255127", "True"]], [["-6.056944847106934", "False"]], [["-6.306944847106934", "False"]]], "filtered_resps": [["-3.5569450855255127", "False"], ["-3.5569450855255127", "False"], ["-1.3069450855255127", "True"], ["-6.056944847106934", "False"], ["-6.306944847106934", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "264d5f505c73f7ca19f6c9b58693c090dc9c77886ece8e2ba59782044dc699c2", "prompt_hash": "260f0e5aa88420cacffd6fbccdf594e17bc8e57b016f8cbe00ddd7b40adee6ff", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 657, "doc": {"id": "9b631734e72a0e559da153492c1e7894", "question": "The juror was quite bored and zoning out but wanted to convey he was hearing testimony, so he just sat there doing what?", "question_concept": "hearing testimony", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["take notes", "nodding", "change of heart", "writing down", "listening"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The juror was quite bored and zoning out but wanted to convey he was hearing testimony, so he just sat there doing what?\nA. take notes\nB. nodding\nC. change of heart\nD. writing down\nE. listening\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The juror was quite bored and zoning out but wanted to convey he was hearing testimony, so he just sat there doing what?\nA. take notes\nB. nodding\nC. change of heart\nD. writing down\nE. listening\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The juror was quite bored and zoning out but wanted to convey he was hearing testimony, so he just sat there doing what?\nA. take notes\nB. nodding\nC. change of heart\nD. writing down\nE. listening\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The juror was quite bored and zoning out but wanted to convey he was hearing testimony, so he just sat there doing what?\nA. take notes\nB. nodding\nC. change of heart\nD. writing down\nE. listening\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The juror was quite bored and zoning out but wanted to convey he was hearing testimony, so he just sat there doing what?\nA. take notes\nB. nodding\nC. change of heart\nD. writing down\nE. listening\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.380908489227295", "False"]], [["-0.8809083700180054", "True"]], [["-6.130908489227295", "False"]], [["-6.630908489227295", "False"]], [["-4.380908489227295", "False"]]], "filtered_resps": [["-3.380908489227295", "False"], ["-0.8809083700180054", "True"], ["-6.130908489227295", "False"], ["-6.630908489227295", "False"], ["-4.380908489227295", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6be464a025ef3c42eb811379aa3f816945d3d3188009f45b3ae8d8e2da668aaf", "prompt_hash": "d7e8b543df12b5962ad12bfc0c467c1a8ba47cdd163321853da456772d2ba6c3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 658, "doc": {"id": "caccaa51ee960a92d44e5b949fc35a66", "question": "They wanted to try blowfish, so they went to get some where?", "question_concept": "blowfish", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["atlantic ocean", "books", "france", "aquarium", "fish market"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: They wanted to try blowfish, so they went to get some where?\nA. atlantic ocean\nB. books\nC. france\nD. aquarium\nE. fish market\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They wanted to try blowfish, so they went to get some where?\nA. atlantic ocean\nB. books\nC. france\nD. aquarium\nE. fish market\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They wanted to try blowfish, so they went to get some where?\nA. atlantic ocean\nB. books\nC. france\nD. aquarium\nE. fish market\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They wanted to try blowfish, so they went to get some where?\nA. atlantic ocean\nB. books\nC. france\nD. aquarium\nE. fish market\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They wanted to try blowfish, so they went to get some where?\nA. atlantic ocean\nB. books\nC. france\nD. aquarium\nE. fish market\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.209568977355957", "False"]], [["-5.459568977355957", "False"]], [["-5.709568977355957", "False"]], [["-1.9595690965652466", "False"]], [["-3.209568977355957", "False"]]], "filtered_resps": [["-2.209568977355957", "False"], ["-5.459568977355957", "False"], ["-5.709568977355957", "False"], ["-1.9595690965652466", "False"], ["-3.209568977355957", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6a72b0893c6652edbc5f9630e11dd6eaac754ff3a3025a26793ca75d2df5d08b", "prompt_hash": "9a6deb9230ba0326fe1b9635a1088722189320de9e841b60bf8fd4f21dc800cb", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 659, "doc": {"id": "def936fda9f6ccee01f57c0f804fabd0", "question": "When a main artery is used to expedite travel what would it be referred to as?", "question_concept": "main artery", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["neck", "busy city", "own brain", "thruway", "food"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When a main artery is used to expedite travel what would it be referred to as?\nA. neck\nB. busy city\nC. own brain\nD. thruway\nE. food\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When a main artery is used to expedite travel what would it be referred to as?\nA. neck\nB. busy city\nC. own brain\nD. thruway\nE. food\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When a main artery is used to expedite travel what would it be referred to as?\nA. neck\nB. busy city\nC. own brain\nD. thruway\nE. food\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When a main artery is used to expedite travel what would it be referred to as?\nA. neck\nB. busy city\nC. own brain\nD. thruway\nE. food\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When a main artery is used to expedite travel what would it be referred to as?\nA. neck\nB. busy city\nC. own brain\nD. thruway\nE. food\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.0061216354370117", "False"]], [["-5.256121635437012", "False"]], [["-6.506121635437012", "False"]], [["-2.0061216354370117", "False"]], [["-8.506121635437012", "False"]]], "filtered_resps": [["-2.0061216354370117", "False"], ["-5.256121635437012", "False"], ["-6.506121635437012", "False"], ["-2.0061216354370117", "False"], ["-8.506121635437012", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "44c76609d1f609d37ddcae930a5d63b3225284345fcc27ea306500616e4713dc", "prompt_hash": "5ccf461d9c9351e313dbffcb2b4fd9e7cc002d9d24e7f26c88ac98793920b760", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 660, "doc": {"id": "761b0f6c68b1540949b70f76a9e67c78", "question": "If someone rules the universe of what are they in charge?", "question_concept": "rule", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["classroom", "football game", "everything", "text book", "lawbook"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If someone rules the universe of what are they in charge?\nA. classroom\nB. football game\nC. everything\nD. text book\nE. lawbook\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If someone rules the universe of what are they in charge?\nA. classroom\nB. football game\nC. everything\nD. text book\nE. lawbook\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If someone rules the universe of what are they in charge?\nA. classroom\nB. football game\nC. everything\nD. text book\nE. lawbook\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If someone rules the universe of what are they in charge?\nA. classroom\nB. football game\nC. everything\nD. text book\nE. lawbook\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If someone rules the universe of what are they in charge?\nA. classroom\nB. football game\nC. everything\nD. text book\nE. lawbook\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.692318916320801", "False"]], [["-6.692318916320801", "False"]], [["-1.1923191547393799", "True"]], [["-7.942318916320801", "False"]], [["-5.442318916320801", "False"]]], "filtered_resps": [["-4.692318916320801", "False"], ["-6.692318916320801", "False"], ["-1.1923191547393799", "True"], ["-7.942318916320801", "False"], ["-5.442318916320801", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7598ab63ed6217efeecacaed8c05598a4d8966c9a9d3a4cdb907bd7b03b2cdcc", "prompt_hash": "3e426fc45296ce7d0d17ef28ef7c9ff904d008ec0e016cafb907d14cf123fda9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 661, "doc": {"id": "8c11546468a2595b29a1297e73334fc4", "question": "The butt was bare, and Sam couldn't stop staring at it.  It was very what?", "question_concept": "bare", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["full", "ample", "covered", "bareword", "ample"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The butt was bare, and Sam couldn't stop staring at it.  It was very what?\nA. full\nB. ample\nC. covered\nD. bareword\nE. ample\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The butt was bare, and Sam couldn't stop staring at it.  It was very what?\nA. full\nB. ample\nC. covered\nD. bareword\nE. ample\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The butt was bare, and Sam couldn't stop staring at it.  It was very what?\nA. full\nB. ample\nC. covered\nD. bareword\nE. ample\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The butt was bare, and Sam couldn't stop staring at it.  It was very what?\nA. full\nB. ample\nC. covered\nD. bareword\nE. ample\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The butt was bare, and Sam couldn't stop staring at it.  It was very what?\nA. full\nB. ample\nC. covered\nD. bareword\nE. ample\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.653439998626709", "False"]], [["-2.903439998626709", "False"]], [["-1.6534401178359985", "False"]], [["-5.153439998626709", "False"]], [["-5.403439998626709", "False"]]], "filtered_resps": [["-4.653439998626709", "False"], ["-2.903439998626709", "False"], ["-1.6534401178359985", "False"], ["-5.153439998626709", "False"], ["-5.403439998626709", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "14f48f1c326bbfe6f62fb13df033e704b2322b1dfaa0305acc15c6ae1c8bb630", "prompt_hash": "05cba5feabbb409be30692d551e8c150e8d28308484d31ec78bfde8b74e7fffc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 662, "doc": {"id": "a5dcac512870e79f5aa2b22dbd662404", "question": "Where can many stores with clothing be found?", "question_concept": "clothing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shop", "mall", "department store", "drawer", "library"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where can many stores with clothing be found?\nA. shop\nB. mall\nC. department store\nD. drawer\nE. library\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can many stores with clothing be found?\nA. shop\nB. mall\nC. department store\nD. drawer\nE. library\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can many stores with clothing be found?\nA. shop\nB. mall\nC. department store\nD. drawer\nE. library\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can many stores with clothing be found?\nA. shop\nB. mall\nC. department store\nD. drawer\nE. library\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can many stores with clothing be found?\nA. shop\nB. mall\nC. department store\nD. drawer\nE. library\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3518805503845215", "False"]], [["-1.8518805503845215", "False"]], [["-2.1018805503845215", "False"]], [["-7.8518805503845215", "False"]], [["-9.85188102722168", "False"]]], "filtered_resps": [["-3.3518805503845215", "False"], ["-1.8518805503845215", "False"], ["-2.1018805503845215", "False"], ["-7.8518805503845215", "False"], ["-9.85188102722168", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cd33a62619cb8c31c75309cdd731d339fac7fcd9c545f52683de1e469c26ea75", "prompt_hash": "485bb61ea9bfd6937e2ce80344ae98b5a6879f8f6c6eac12e2eb43eea4c4fb39", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 663, "doc": {"id": "870b07a1c5af2e956673a9680da99852", "question": "After working on the car, what did it end up doing?", "question_concept": "car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["going too fast", "last several years", "honk the horn", "go fast", "start running"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: After working on the car, what did it end up doing?\nA. going too fast\nB. last several years\nC. honk the horn\nD. go fast\nE. start running\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: After working on the car, what did it end up doing?\nA. going too fast\nB. last several years\nC. honk the horn\nD. go fast\nE. start running\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: After working on the car, what did it end up doing?\nA. going too fast\nB. last several years\nC. honk the horn\nD. go fast\nE. start running\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: After working on the car, what did it end up doing?\nA. going too fast\nB. last several years\nC. honk the horn\nD. go fast\nE. start running\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: After working on the car, what did it end up doing?\nA. going too fast\nB. last several years\nC. honk the horn\nD. go fast\nE. start running\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.645972728729248", "False"]], [["-2.145972728729248", "True"]], [["-4.645972728729248", "False"]], [["-3.145972728729248", "False"]], [["-2.395972728729248", "False"]]], "filtered_resps": [["-3.645972728729248", "False"], ["-2.145972728729248", "True"], ["-4.645972728729248", "False"], ["-3.145972728729248", "False"], ["-2.395972728729248", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ce304a0dde8ab0286a37746dddcb53f45ebd988fdd21d3cba6d482acbb4aa83f", "prompt_hash": "ac2f01c2c09dedcced69b311bbf04c30d4c3f088a73d8838dba3e3fd7265e6b1", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 664, "doc": {"id": "f48528156632b9c5b18af9ce2095509b", "question": "When an elderly person needs help performing daily tasks, who might they call?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["creativity", "hatred", "caregiver", "own house", "much money"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: When an elderly person needs help performing daily tasks, who might they call?\nA. creativity\nB. hatred\nC. caregiver\nD. own house\nE. much money\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When an elderly person needs help performing daily tasks, who might they call?\nA. creativity\nB. hatred\nC. caregiver\nD. own house\nE. much money\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When an elderly person needs help performing daily tasks, who might they call?\nA. creativity\nB. hatred\nC. caregiver\nD. own house\nE. much money\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When an elderly person needs help performing daily tasks, who might they call?\nA. creativity\nB. hatred\nC. caregiver\nD. own house\nE. much money\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When an elderly person needs help performing daily tasks, who might they call?\nA. creativity\nB. hatred\nC. caregiver\nD. own house\nE. much money\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6337034702301025", "False"]], [["-9.633703231811523", "False"]], [["-0.8837035298347473", "True"]], [["-7.883703708648682", "False"]], [["-6.133703708648682", "False"]]], "filtered_resps": [["-3.6337034702301025", "False"], ["-9.633703231811523", "False"], ["-0.8837035298347473", "True"], ["-7.883703708648682", "False"], ["-6.133703708648682", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c3f92beda8189f35e15619febda93984797dbb81434c1a84580453385a9bd56c", "prompt_hash": "034d296df2f08642387ea34d62558708f17e935eaac3889abd1e6ebd803554a1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 665, "doc": {"id": "5496c7293f653120e5a5213db2d7b103", "question": "Where is beer drank by people watching sticks and pucks?", "question_concept": "beer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bottle", "refrigerator", "hockey game", "casino", "bar"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where is beer drank by people watching sticks and pucks?\nA. bottle\nB. refrigerator\nC. hockey game\nD. casino\nE. bar\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is beer drank by people watching sticks and pucks?\nA. bottle\nB. refrigerator\nC. hockey game\nD. casino\nE. bar\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is beer drank by people watching sticks and pucks?\nA. bottle\nB. refrigerator\nC. hockey game\nD. casino\nE. bar\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is beer drank by people watching sticks and pucks?\nA. bottle\nB. refrigerator\nC. hockey game\nD. casino\nE. bar\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is beer drank by people watching sticks and pucks?\nA. bottle\nB. refrigerator\nC. hockey game\nD. casino\nE. bar\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.356269836425781", "False"]], [["-5.356269836425781", "False"]], [["-1.3562700748443604", "True"]], [["-7.606269836425781", "False"]], [["-2.3562700748443604", "False"]]], "filtered_resps": [["-4.356269836425781", "False"], ["-5.356269836425781", "False"], ["-1.3562700748443604", "True"], ["-7.606269836425781", "False"], ["-2.3562700748443604", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e8930892322b458c0307ae60834214cec2c81d7d046b29c4551220218e9560ca", "prompt_hash": "1e6d2bebb90f6dd2abe4c1d98affcdd8cb27e294d2b8956784adbd6facae88de", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 666, "doc": {"id": "9d97e2bb458d93a8bafe4380b08727e3", "question": "Where is there a telephone book in almost every room?", "question_concept": "telephone book", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["at hotel", "house", "library", "bedsit", "closet"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is there a telephone book in almost every room?\nA. at hotel\nB. house\nC. library\nD. bedsit\nE. closet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is there a telephone book in almost every room?\nA. at hotel\nB. house\nC. library\nD. bedsit\nE. closet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is there a telephone book in almost every room?\nA. at hotel\nB. house\nC. library\nD. bedsit\nE. closet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is there a telephone book in almost every room?\nA. at hotel\nB. house\nC. library\nD. bedsit\nE. closet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is there a telephone book in almost every room?\nA. at hotel\nB. house\nC. library\nD. bedsit\nE. closet\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2427312135696411", "True"]], [["-4.242731094360352", "False"]], [["-3.7427310943603516", "False"]], [["-6.742731094360352", "False"]], [["-7.492731094360352", "False"]]], "filtered_resps": [["-1.2427312135696411", "True"], ["-4.242731094360352", "False"], ["-3.7427310943603516", "False"], ["-6.742731094360352", "False"], ["-7.492731094360352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cf935ba62b78222462e1f74453bc40a1e5837f8cb1b441898ce6477d1b44c625", "prompt_hash": "da4660a4269299c545aae2ce0eccf88ca6939c938794f0d4554d8a7007cd2a5c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 667, "doc": {"id": "26d7d59ef7b9f2e0c2d47419fa5bca91", "question": "Where might you see a green field while driving?", "question_concept": "field", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kansas", "meadow", "farmland", "countryside", "rural area"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where might you see a green field while driving?\nA. kansas\nB. meadow\nC. farmland\nD. countryside\nE. rural area\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where might you see a green field while driving?\nA. kansas\nB. meadow\nC. farmland\nD. countryside\nE. rural area\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where might you see a green field while driving?\nA. kansas\nB. meadow\nC. farmland\nD. countryside\nE. rural area\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where might you see a green field while driving?\nA. kansas\nB. meadow\nC. farmland\nD. countryside\nE. rural area\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where might you see a green field while driving?\nA. kansas\nB. meadow\nC. farmland\nD. countryside\nE. rural area\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1534647941589355", "False"]], [["-2.9034647941589355", "False"]], [["-1.6534647941589355", "True"]], [["-3.1534647941589355", "False"]], [["-7.1534647941589355", "False"]]], "filtered_resps": [["-2.1534647941589355", "False"], ["-2.9034647941589355", "False"], ["-1.6534647941589355", "True"], ["-3.1534647941589355", "False"], ["-7.1534647941589355", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b1a07fad5eb6eb3e06109ce1596b62820f05c7091d222cf2c8d969863b16d1e7", "prompt_hash": "1788e192e2b0d94f9810df24605c71644cad86b52d41a42f95b9c69dd3511b06", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 668, "doc": {"id": "c6f10fd07348bf2cf5488b0d9f38d806", "question": "Some people got escorted out of the library, they were probably what?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["state facts", "talking loudly", "making money", "amount to nothing", "believe in god"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Some people got escorted out of the library, they were probably what?\nA. state facts\nB. talking loudly\nC. making money\nD. amount to nothing\nE. believe in god\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Some people got escorted out of the library, they were probably what?\nA. state facts\nB. talking loudly\nC. making money\nD. amount to nothing\nE. believe in god\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Some people got escorted out of the library, they were probably what?\nA. state facts\nB. talking loudly\nC. making money\nD. amount to nothing\nE. believe in god\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Some people got escorted out of the library, they were probably what?\nA. state facts\nB. talking loudly\nC. making money\nD. amount to nothing\nE. believe in god\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Some people got escorted out of the library, they were probably what?\nA. state facts\nB. talking loudly\nC. making money\nD. amount to nothing\nE. believe in god\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.338059902191162", "False"]], [["-1.0880600214004517", "True"]], [["-7.838059902191162", "False"]], [["-6.338059902191162", "False"]], [["-7.588059902191162", "False"]]], "filtered_resps": [["-5.338059902191162", "False"], ["-1.0880600214004517", "True"], ["-7.838059902191162", "False"], ["-6.338059902191162", "False"], ["-7.588059902191162", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ae5c88f3e57f80897efcddcf317c2ae1a0eda4193005fefca7915edf7ae63eda", "prompt_hash": "bbfc6a0e639f760baa8cc0f1ac61d522c76a41bcb16b2f625b7a9c2b9cc56ae7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 669, "doc": {"id": "8ebf9d24719649a0b041aea02a6e46af", "question": "If there is a pond with trees around it, where it it likely located?", "question_concept": "pond", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ground", "bathroom", "forest", "countryside", "rural area"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If there is a pond with trees around it, where it it likely located?\nA. ground\nB. bathroom\nC. forest\nD. countryside\nE. rural area\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If there is a pond with trees around it, where it it likely located?\nA. ground\nB. bathroom\nC. forest\nD. countryside\nE. rural area\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If there is a pond with trees around it, where it it likely located?\nA. ground\nB. bathroom\nC. forest\nD. countryside\nE. rural area\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If there is a pond with trees around it, where it it likely located?\nA. ground\nB. bathroom\nC. forest\nD. countryside\nE. rural area\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If there is a pond with trees around it, where it it likely located?\nA. ground\nB. bathroom\nC. forest\nD. countryside\nE. rural area\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.2720251083374023", "False"]], [["-6.772025108337402", "False"]], [["-3.7720251083374023", "False"]], [["-2.5220251083374023", "False"]], [["-2.2720251083374023", "False"]]], "filtered_resps": [["-2.2720251083374023", "False"], ["-6.772025108337402", "False"], ["-3.7720251083374023", "False"], ["-2.5220251083374023", "False"], ["-2.2720251083374023", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "aa047dd47d1c45205693aaae8751ee50ce10e01772d241652c5b5bd92d01e949", "prompt_hash": "28fc259d46680eaed3f1d113f02005bf3bb04bfe3fa85a81bd0cd8a01f764283", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 670, "doc": {"id": "c961578f4c5768b67b843e5d2ce18452", "question": "Blowfish require what specific thing to live?", "question_concept": "blowfish", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sea water", "hatred", "fish market", "body of water", "jungle"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Blowfish require what specific thing to live?\nA. sea water\nB. hatred\nC. fish market\nD. body of water\nE. jungle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Blowfish require what specific thing to live?\nA. sea water\nB. hatred\nC. fish market\nD. body of water\nE. jungle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Blowfish require what specific thing to live?\nA. sea water\nB. hatred\nC. fish market\nD. body of water\nE. jungle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Blowfish require what specific thing to live?\nA. sea water\nB. hatred\nC. fish market\nD. body of water\nE. jungle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Blowfish require what specific thing to live?\nA. sea water\nB. hatred\nC. fish market\nD. body of water\nE. jungle\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.212430477142334", "False"]], [["-1.2124303579330444", "True"]], [["-6.212430477142334", "False"]], [["-1.7124303579330444", "False"]], [["-6.712430477142334", "False"]]], "filtered_resps": [["-2.212430477142334", "False"], ["-1.2124303579330444", "True"], ["-6.212430477142334", "False"], ["-1.7124303579330444", "False"], ["-6.712430477142334", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fb26b07ed3ef6c74f329f1273544db3f3724b5b4e221740b92f7b1f1b9ee21fd", "prompt_hash": "8bdde8fc0109503c9b69cdc8c0f76e6e3183cddecd2343d6afc692e00b509ee7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 671, "doc": {"id": "cce1b59f7c4f540a84a1a7d6d88548c4", "question": "What is the least likely immediate side effect of eating hamburger?", "question_concept": "eating hamburger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["nausea", "death", "illness", "health problems", "gain weight"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is the least likely immediate side effect of eating hamburger?\nA. nausea\nB. death\nC. illness\nD. health problems\nE. gain weight\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the least likely immediate side effect of eating hamburger?\nA. nausea\nB. death\nC. illness\nD. health problems\nE. gain weight\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the least likely immediate side effect of eating hamburger?\nA. nausea\nB. death\nC. illness\nD. health problems\nE. gain weight\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the least likely immediate side effect of eating hamburger?\nA. nausea\nB. death\nC. illness\nD. health problems\nE. gain weight\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the least likely immediate side effect of eating hamburger?\nA. nausea\nB. death\nC. illness\nD. health problems\nE. gain weight\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.35910177230835", "False"]], [["-3.3591017723083496", "False"]], [["-5.85910177230835", "False"]], [["-6.60910177230835", "False"]], [["-1.35910165309906", "True"]]], "filtered_resps": [["-4.35910177230835", "False"], ["-3.3591017723083496", "False"], ["-5.85910177230835", "False"], ["-6.60910177230835", "False"], ["-1.35910165309906", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "32e2f0606edcef54229cdc2b30a52ebd20e025798fadd61570a1c791ec0cb513", "prompt_hash": "6e9ffd1fc9c58e0fb5bba763671d4f673af11b102e9c21aa86386683d39a35aa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 672, "doc": {"id": "60848ce50295fc745756fbe960e78b88", "question": "What would I be doing while going to work and walking?", "question_concept": "going to work", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["listen to radio", "solve problems", "driving", "walk", "being late"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What would I be doing while going to work and walking?\nA. listen to radio\nB. solve problems\nC. driving\nD. walk\nE. being late\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would I be doing while going to work and walking?\nA. listen to radio\nB. solve problems\nC. driving\nD. walk\nE. being late\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would I be doing while going to work and walking?\nA. listen to radio\nB. solve problems\nC. driving\nD. walk\nE. being late\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would I be doing while going to work and walking?\nA. listen to radio\nB. solve problems\nC. driving\nD. walk\nE. being late\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would I be doing while going to work and walking?\nA. listen to radio\nB. solve problems\nC. driving\nD. walk\nE. being late\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3262062072753906", "False"]], [["-5.576206207275391", "False"]], [["-7.076206207275391", "False"]], [["-1.576206088066101", "False"]], [["-8.07620620727539", "False"]]], "filtered_resps": [["-3.3262062072753906", "False"], ["-5.576206207275391", "False"], ["-7.076206207275391", "False"], ["-1.576206088066101", "False"], ["-8.07620620727539", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b53a8c57892a52be63b18a74fea6320db542e48e116ceaff41d4bd13dc4e839e", "prompt_hash": "9d8960ee4fd9ca3328309e2854c672e5a9109590e47d4875aedb2a7754ff0125", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 673, "doc": {"id": "3fdc0c422c524c994b9911a17f1f1834", "question": "A showroom feature washers and refrigerators, where is this showroom located?", "question_concept": "showroom", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["appliance store", "vegas", "electronics store", "car dealership", "kitchen"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A showroom feature washers and refrigerators, where is this showroom located?\nA. appliance store\nB. vegas\nC. electronics store\nD. car dealership\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A showroom feature washers and refrigerators, where is this showroom located?\nA. appliance store\nB. vegas\nC. electronics store\nD. car dealership\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A showroom feature washers and refrigerators, where is this showroom located?\nA. appliance store\nB. vegas\nC. electronics store\nD. car dealership\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A showroom feature washers and refrigerators, where is this showroom located?\nA. appliance store\nB. vegas\nC. electronics store\nD. car dealership\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A showroom feature washers and refrigerators, where is this showroom located?\nA. appliance store\nB. vegas\nC. electronics store\nD. car dealership\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1283540725708008", "True"]], [["-5.378354072570801", "False"]], [["-6.128354072570801", "False"]], [["-7.628354072570801", "False"]], [["-6.378354072570801", "False"]]], "filtered_resps": [["-1.1283540725708008", "True"], ["-5.378354072570801", "False"], ["-6.128354072570801", "False"], ["-7.628354072570801", "False"], ["-6.378354072570801", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8aad668735f7ed6004d650d14cf5c182c7861fa384d12b4ba5113ed5c0b13e52", "prompt_hash": "9490647a972777582da9c309d107b4a1fac37b06685e80d309a9ca6663d35867", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 674, "doc": {"id": "cc8eac9956f645533b8d7b99702e3507", "question": "The man often made smart remarks, like that any restaurant is a mexican restaurant where?", "question_concept": "mexican restaurant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["city", "mexica", "san diego", "spain", "mexico"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The man often made smart remarks, like that any restaurant is a mexican restaurant where?\nA. city\nB. mexica\nC. san diego\nD. spain\nE. mexico\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man often made smart remarks, like that any restaurant is a mexican restaurant where?\nA. city\nB. mexica\nC. san diego\nD. spain\nE. mexico\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man often made smart remarks, like that any restaurant is a mexican restaurant where?\nA. city\nB. mexica\nC. san diego\nD. spain\nE. mexico\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man often made smart remarks, like that any restaurant is a mexican restaurant where?\nA. city\nB. mexica\nC. san diego\nD. spain\nE. mexico\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man often made smart remarks, like that any restaurant is a mexican restaurant where?\nA. city\nB. mexica\nC. san diego\nD. spain\nE. mexico\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6894798278808594", "False"]], [["-3.6894798278808594", "False"]], [["-6.689479827880859", "False"]], [["-7.939479827880859", "False"]], [["-2.1894798278808594", "False"]]], "filtered_resps": [["-3.6894798278808594", "False"], ["-3.6894798278808594", "False"], ["-6.689479827880859", "False"], ["-7.939479827880859", "False"], ["-2.1894798278808594", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "823882ef6b46553e0047e0cab6f3b9f2f9bacd7625766630ff0872f47094f9b2", "prompt_hash": "a76c6598943cbab6356efb9dec4dbf061da0ae3bc00e9f2b68a533bd9e35a203", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 675, "doc": {"id": "c0e7fa3e39a2d9af2c323416015729dc", "question": "I am looking for honey right from the source, where should I look?", "question_concept": "honey", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["last all night", "beehive", "farmer's market", "jar", "honeyful"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: I am looking for honey right from the source, where should I look?\nA. last all night\nB. beehive\nC. farmer's market\nD. jar\nE. honeyful\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I am looking for honey right from the source, where should I look?\nA. last all night\nB. beehive\nC. farmer's market\nD. jar\nE. honeyful\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I am looking for honey right from the source, where should I look?\nA. last all night\nB. beehive\nC. farmer's market\nD. jar\nE. honeyful\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I am looking for honey right from the source, where should I look?\nA. last all night\nB. beehive\nC. farmer's market\nD. jar\nE. honeyful\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I am looking for honey right from the source, where should I look?\nA. last all night\nB. beehive\nC. farmer's market\nD. jar\nE. honeyful\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.256072998046875", "False"]], [["-0.7560727596282959", "True"]], [["-5.506072998046875", "False"]], [["-8.006072998046875", "False"]], [["-9.006072998046875", "False"]]], "filtered_resps": [["-5.256072998046875", "False"], ["-0.7560727596282959", "True"], ["-5.506072998046875", "False"], ["-8.006072998046875", "False"], ["-9.006072998046875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ca0b250fc0c56b8ada41d002026b928957db86d3697300810138ed4b301dfdff", "prompt_hash": "4ae6984fe19c8d7b9dd47fc2a42c5b5fe19db2eeacc2cd1aeb5596db227e7372", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 676, "doc": {"id": "335b51bd3a8ada014bbe6754dcbd425f", "question": "Where are there likely to be a variety of flats to choose from?", "question_concept": "flat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["london", "apartment building", "city", "falling down", "town"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where are there likely to be a variety of flats to choose from?\nA. london\nB. apartment building\nC. city\nD. falling down\nE. town\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are there likely to be a variety of flats to choose from?\nA. london\nB. apartment building\nC. city\nD. falling down\nE. town\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are there likely to be a variety of flats to choose from?\nA. london\nB. apartment building\nC. city\nD. falling down\nE. town\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are there likely to be a variety of flats to choose from?\nA. london\nB. apartment building\nC. city\nD. falling down\nE. town\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are there likely to be a variety of flats to choose from?\nA. london\nB. apartment building\nC. city\nD. falling down\nE. town\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.122692584991455", "True"]], [["-3.122692584991455", "False"]], [["-1.872692584991455", "False"]], [["-7.372692584991455", "False"]], [["-6.372692584991455", "False"]]], "filtered_resps": [["-1.122692584991455", "True"], ["-3.122692584991455", "False"], ["-1.872692584991455", "False"], ["-7.372692584991455", "False"], ["-6.372692584991455", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c211df5933ed0968dab8be3221136b2a138c6087da0ae910a985578f49485193", "prompt_hash": "2db1c180c6d93f7137c3e7aede58f2d88bde39750720d87b48e6fc7c462af126", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 677, "doc": {"id": "c7327a1a7d12b6cc0740fc9446270e02", "question": "A weasel has a thin body and short legs to easier burrow after prey in a what?", "question_concept": "weasel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tree", "mulberry bush", "chicken coop", "viking ship", "rabbit warren"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: A weasel has a thin body and short legs to easier burrow after prey in a what?\nA. tree\nB. mulberry bush\nC. chicken coop\nD. viking ship\nE. rabbit warren\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A weasel has a thin body and short legs to easier burrow after prey in a what?\nA. tree\nB. mulberry bush\nC. chicken coop\nD. viking ship\nE. rabbit warren\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A weasel has a thin body and short legs to easier burrow after prey in a what?\nA. tree\nB. mulberry bush\nC. chicken coop\nD. viking ship\nE. rabbit warren\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A weasel has a thin body and short legs to easier burrow after prey in a what?\nA. tree\nB. mulberry bush\nC. chicken coop\nD. viking ship\nE. rabbit warren\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A weasel has a thin body and short legs to easier burrow after prey in a what?\nA. tree\nB. mulberry bush\nC. chicken coop\nD. viking ship\nE. rabbit warren\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.819733142852783", "False"]], [["-6.569733142852783", "False"]], [["-6.819733142852783", "False"]], [["-8.319732666015625", "False"]], [["-0.8197330832481384", "True"]]], "filtered_resps": [["-2.819733142852783", "False"], ["-6.569733142852783", "False"], ["-6.819733142852783", "False"], ["-8.319732666015625", "False"], ["-0.8197330832481384", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6d032a584d711652d8934945bca03ab6ad60f3938a7dd1b29ea5dd00c4984672", "prompt_hash": "725b3c09620a203c1a630532baf14a1fd2e224f2d447d6437d0cf3692083cccd", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 678, "doc": {"id": "2729d8502208c25d8e9293cd4e8ecbb5", "question": "What can disease destroy?", "question_concept": "disease", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rug", "third world country", "human body", "hospital", "building"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What can disease destroy?\nA. rug\nB. third world country\nC. human body\nD. hospital\nE. building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can disease destroy?\nA. rug\nB. third world country\nC. human body\nD. hospital\nE. building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can disease destroy?\nA. rug\nB. third world country\nC. human body\nD. hospital\nE. building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can disease destroy?\nA. rug\nB. third world country\nC. human body\nD. hospital\nE. building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can disease destroy?\nA. rug\nB. third world country\nC. human body\nD. hospital\nE. building\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.974109172821045", "False"]], [["-4.974109172821045", "False"]], [["-0.7241091132164001", "True"]], [["-2.974109172821045", "False"]], [["-8.474108695983887", "False"]]], "filtered_resps": [["-3.974109172821045", "False"], ["-4.974109172821045", "False"], ["-0.7241091132164001", "True"], ["-2.974109172821045", "False"], ["-8.474108695983887", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2d485d2dfd7bfdf734ae5e09b966da43624f0ed8cc1acaf35036d56a1660dca3", "prompt_hash": "85b262a0fb1c84bb859cbd3d6ffc20591fad98a69e09e7b12d2b3dd9e8ce7513", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 679, "doc": {"id": "7ea57ee4580042b0a6a40479c8ace3e4", "question": "What does a person from Avalon live in?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pain", "meaningful work", "english house", "cotton candy", "headache"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What does a person from Avalon live in?\nA. pain\nB. meaningful work\nC. english house\nD. cotton candy\nE. headache\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a person from Avalon live in?\nA. pain\nB. meaningful work\nC. english house\nD. cotton candy\nE. headache\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a person from Avalon live in?\nA. pain\nB. meaningful work\nC. english house\nD. cotton candy\nE. headache\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a person from Avalon live in?\nA. pain\nB. meaningful work\nC. english house\nD. cotton candy\nE. headache\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a person from Avalon live in?\nA. pain\nB. meaningful work\nC. english house\nD. cotton candy\nE. headache\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5399065017700195", "True"]], [["-1.5399065017700195", "True"]], [["-3.5399065017700195", "False"]], [["-6.0399065017700195", "False"]], [["-8.28990650177002", "False"]]], "filtered_resps": [["-1.5399065017700195", "True"], ["-1.5399065017700195", "True"], ["-3.5399065017700195", "False"], ["-6.0399065017700195", "False"], ["-8.28990650177002", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c1d47a586da0b69238332040d4fc4a037a6f767824fb8e98ccf730eace8dae58", "prompt_hash": "bf49a164dc1260284c5a1d52a9d277ca922e4efa4731347f379812fdf7fa9389", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 680, "doc": {"id": "65432eb6e617514d863a465f38865fde", "question": "Where is one likely to find a fan for their stove?", "question_concept": "fan", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["appliance store", "sports stadium", "dress emporium", "hot room", "football stadium"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is one likely to find a fan for their stove?\nA. appliance store\nB. sports stadium\nC. dress emporium\nD. hot room\nE. football stadium\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is one likely to find a fan for their stove?\nA. appliance store\nB. sports stadium\nC. dress emporium\nD. hot room\nE. football stadium\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is one likely to find a fan for their stove?\nA. appliance store\nB. sports stadium\nC. dress emporium\nD. hot room\nE. football stadium\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is one likely to find a fan for their stove?\nA. appliance store\nB. sports stadium\nC. dress emporium\nD. hot room\nE. football stadium\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is one likely to find a fan for their stove?\nA. appliance store\nB. sports stadium\nC. dress emporium\nD. hot room\nE. football stadium\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6631757020950317", "True"]], [["-7.913175582885742", "False"]], [["-8.163175582885742", "False"]], [["-8.663175582885742", "False"]], [["-9.163175582885742", "False"]]], "filtered_resps": [["-0.6631757020950317", "True"], ["-7.913175582885742", "False"], ["-8.163175582885742", "False"], ["-8.663175582885742", "False"], ["-9.163175582885742", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fd8ad8e89b89878890c06d784c5faf188ed0fa70bf720fc1698ec13f7890bdf5", "prompt_hash": "dccf16915d79b6c7c41268bfe6dd5626bc6df5601c9b3a9d41c2767dcdf92b6f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 681, "doc": {"id": "316a8dee8a4dde7d95cf503a715104be", "question": "Jodie felt a tightness in her chest. She was worried but didn't want to go to the hospital. Where might she go instead?", "question_concept": "chest", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["istanbul", "concert", "bedroom", "antique shop", "human being"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Jodie felt a tightness in her chest. She was worried but didn't want to go to the hospital. Where might she go instead?\nA. istanbul\nB. concert\nC. bedroom\nD. antique shop\nE. human being\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Jodie felt a tightness in her chest. She was worried but didn't want to go to the hospital. Where might she go instead?\nA. istanbul\nB. concert\nC. bedroom\nD. antique shop\nE. human being\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Jodie felt a tightness in her chest. She was worried but didn't want to go to the hospital. Where might she go instead?\nA. istanbul\nB. concert\nC. bedroom\nD. antique shop\nE. human being\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Jodie felt a tightness in her chest. She was worried but didn't want to go to the hospital. Where might she go instead?\nA. istanbul\nB. concert\nC. bedroom\nD. antique shop\nE. human being\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Jodie felt a tightness in her chest. She was worried but didn't want to go to the hospital. Where might she go instead?\nA. istanbul\nB. concert\nC. bedroom\nD. antique shop\nE. human being\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.4328372478485107", "False"]], [["-3.9328372478485107", "False"]], [["-1.6828372478485107", "True"]], [["-6.43283748626709", "False"]], [["-4.18283748626709", "False"]]], "filtered_resps": [["-3.4328372478485107", "False"], ["-3.9328372478485107", "False"], ["-1.6828372478485107", "True"], ["-6.43283748626709", "False"], ["-4.18283748626709", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b035672c6fdad1d1d2c2f24aacc09b48bf940f3070e1d18f538edde2648c2671", "prompt_hash": "dfeee43d5d91c1563bca02147eba8fc39def3c082ceb980af299b9221f647485", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 682, "doc": {"id": "520972425aed0e532fa28a91c9b55b30", "question": "If you're buying beer for a float trip what are you preparing to do?", "question_concept": "buying beer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get arrested", "have fun", "get sick", "spend money", "stupidity"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you're buying beer for a float trip what are you preparing to do?\nA. get arrested\nB. have fun\nC. get sick\nD. spend money\nE. stupidity\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're buying beer for a float trip what are you preparing to do?\nA. get arrested\nB. have fun\nC. get sick\nD. spend money\nE. stupidity\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're buying beer for a float trip what are you preparing to do?\nA. get arrested\nB. have fun\nC. get sick\nD. spend money\nE. stupidity\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're buying beer for a float trip what are you preparing to do?\nA. get arrested\nB. have fun\nC. get sick\nD. spend money\nE. stupidity\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're buying beer for a float trip what are you preparing to do?\nA. get arrested\nB. have fun\nC. get sick\nD. spend money\nE. stupidity\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.951767921447754", "False"]], [["-0.4517679810523987", "True"]], [["-6.701767921447754", "False"]], [["-5.701767921447754", "False"]], [["-8.201767921447754", "False"]]], "filtered_resps": [["-4.951767921447754", "False"], ["-0.4517679810523987", "True"], ["-6.701767921447754", "False"], ["-5.701767921447754", "False"], ["-8.201767921447754", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6b139ec30adc477567982a483b23c2071a77ba8c0860afbd31d785ab1ce5ce71", "prompt_hash": "df16b55355964611d7a8fa00657cecb2febeea49db51372aa5c6fe1698a39a84", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 683, "doc": {"id": "4d67cdb4ba1b0058e383c212303a9f4e", "question": "Piece of land in Canada where you can find marmot?", "question_concept": "marmot", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["north america", "united states", "vancouver island", "american", "cage"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Piece of land in Canada where you can find marmot?\nA. north america\nB. united states\nC. vancouver island\nD. american\nE. cage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Piece of land in Canada where you can find marmot?\nA. north america\nB. united states\nC. vancouver island\nD. american\nE. cage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Piece of land in Canada where you can find marmot?\nA. north america\nB. united states\nC. vancouver island\nD. american\nE. cage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Piece of land in Canada where you can find marmot?\nA. north america\nB. united states\nC. vancouver island\nD. american\nE. cage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Piece of land in Canada where you can find marmot?\nA. north america\nB. united states\nC. vancouver island\nD. american\nE. cage\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.608900547027588", "False"]], [["-6.608900547027588", "False"]], [["-1.608900547027588", "False"]], [["-7.858900547027588", "False"]], [["-8.10890007019043", "False"]]], "filtered_resps": [["-3.608900547027588", "False"], ["-6.608900547027588", "False"], ["-1.608900547027588", "False"], ["-7.858900547027588", "False"], ["-8.10890007019043", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "53be17504025c49a95d9b4eb88d5ff8d073461cd2dd3530f9668eca725a155c2", "prompt_hash": "20e6b7203c1755f326702a1fe641fec45e3ac688feb93aebf594d49842583188", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 684, "doc": {"id": "95d1d968ee66b6054cbb16b58a7c6455", "question": "The surgeon's clients had begun to reduce, it seemed girls no longer want to what?", "question_concept": "reduce", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["reduction", "make larger", "augment", "gain weight", "expand"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The surgeon's clients had begun to reduce, it seemed girls no longer want to what?\nA. reduction\nB. make larger\nC. augment\nD. gain weight\nE. expand\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The surgeon's clients had begun to reduce, it seemed girls no longer want to what?\nA. reduction\nB. make larger\nC. augment\nD. gain weight\nE. expand\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The surgeon's clients had begun to reduce, it seemed girls no longer want to what?\nA. reduction\nB. make larger\nC. augment\nD. gain weight\nE. expand\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The surgeon's clients had begun to reduce, it seemed girls no longer want to what?\nA. reduction\nB. make larger\nC. augment\nD. gain weight\nE. expand\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The surgeon's clients had begun to reduce, it seemed girls no longer want to what?\nA. reduction\nB. make larger\nC. augment\nD. gain weight\nE. expand\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.225185394287109", "False"]], [["-3.7251853942871094", "False"]], [["-1.2251853942871094", "True"]], [["-5.225185394287109", "False"]], [["-5.225185394287109", "False"]]], "filtered_resps": [["-4.225185394287109", "False"], ["-3.7251853942871094", "False"], ["-1.2251853942871094", "True"], ["-5.225185394287109", "False"], ["-5.225185394287109", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b1d06eb3933ec77949c5caba8ca56522eb67c2cc2d6803976d66da89b726a26d", "prompt_hash": "5884fcf4cfc7915dccf1259c7decee6f16cc4bd470dc552b14d58a81d3ad1f73", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 685, "doc": {"id": "c43b60be106662de1863097ee3ddb4d2", "question": "While waiting for this appointment, people often read magazines.", "question_concept": "magazines", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["doctor", "train station", "newsagent", "market", "table"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: While waiting for this appointment, people often read magazines.\nA. doctor\nB. train station\nC. newsagent\nD. market\nE. table\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: While waiting for this appointment, people often read magazines.\nA. doctor\nB. train station\nC. newsagent\nD. market\nE. table\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: While waiting for this appointment, people often read magazines.\nA. doctor\nB. train station\nC. newsagent\nD. market\nE. table\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: While waiting for this appointment, people often read magazines.\nA. doctor\nB. train station\nC. newsagent\nD. market\nE. table\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: While waiting for this appointment, people often read magazines.\nA. doctor\nB. train station\nC. newsagent\nD. market\nE. table\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.035841941833496", "False"]], [["-3.785841941833496", "False"]], [["-2.535841941833496", "False"]], [["-6.535841941833496", "False"]], [["-5.535841941833496", "False"]]], "filtered_resps": [["-4.035841941833496", "False"], ["-3.785841941833496", "False"], ["-2.535841941833496", "False"], ["-6.535841941833496", "False"], ["-5.535841941833496", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fdf0a9cd7cf29bc15fb20a76b54357fd82da07eb28169fd8a7dc566a10188c26", "prompt_hash": "0548419c142539249adef8cd0426769720e98bf09a1eb3aa7845c6e93dc9e0f5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 686, "doc": {"id": "456f2fb41cac8c028dcfe2f48637e473", "question": "Where would you find a fox that is made up?", "question_concept": "fox", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["storybook", "woods", "hen house", "natural habitat", "back yard"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find a fox that is made up?\nA. storybook\nB. woods\nC. hen house\nD. natural habitat\nE. back yard\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find a fox that is made up?\nA. storybook\nB. woods\nC. hen house\nD. natural habitat\nE. back yard\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find a fox that is made up?\nA. storybook\nB. woods\nC. hen house\nD. natural habitat\nE. back yard\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find a fox that is made up?\nA. storybook\nB. woods\nC. hen house\nD. natural habitat\nE. back yard\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find a fox that is made up?\nA. storybook\nB. woods\nC. hen house\nD. natural habitat\nE. back yard\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8382396101951599", "True"]], [["-3.8382396697998047", "False"]], [["-4.838239669799805", "False"]], [["-5.588239669799805", "False"]], [["-6.838239669799805", "False"]]], "filtered_resps": [["-0.8382396101951599", "True"], ["-3.8382396697998047", "False"], ["-4.838239669799805", "False"], ["-5.588239669799805", "False"], ["-6.838239669799805", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dd5c63d4a1266f8f6eccc245825f02ae59782f03d3b61981125f347f9d0b391b", "prompt_hash": "30237a0ef464ca2a8772f77fe61477209c6e97f2b29319bb96ae742c6dfa0160", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 687, "doc": {"id": "a5d853d1c2fb3ef160218fb91110fbe5", "question": "In basic training they build you up only to do what, all in hopes of building you up even stronger the next time?", "question_concept": "build", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["destroying", "tear down", "raze", "mutilate", "demolition"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: In basic training they build you up only to do what, all in hopes of building you up even stronger the next time?\nA. destroying\nB. tear down\nC. raze\nD. mutilate\nE. demolition\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: In basic training they build you up only to do what, all in hopes of building you up even stronger the next time?\nA. destroying\nB. tear down\nC. raze\nD. mutilate\nE. demolition\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: In basic training they build you up only to do what, all in hopes of building you up even stronger the next time?\nA. destroying\nB. tear down\nC. raze\nD. mutilate\nE. demolition\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: In basic training they build you up only to do what, all in hopes of building you up even stronger the next time?\nA. destroying\nB. tear down\nC. raze\nD. mutilate\nE. demolition\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: In basic training they build you up only to do what, all in hopes of building you up even stronger the next time?\nA. destroying\nB. tear down\nC. raze\nD. mutilate\nE. demolition\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.998988628387451", "False"]], [["-1.4989887475967407", "True"]], [["-6.498988628387451", "False"]], [["-7.498988628387451", "False"]], [["-8.99898910522461", "False"]]], "filtered_resps": [["-6.998988628387451", "False"], ["-1.4989887475967407", "True"], ["-6.498988628387451", "False"], ["-7.498988628387451", "False"], ["-8.99898910522461", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b5cce16198eee51b61d9906111b8d3e0b1a80150b311d49e3b3dbbab7fc79b55", "prompt_hash": "ed3376c1da62fd1d38178c19c96ecc5602e7a5370a63467e8ce6a4ed762982ee", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 688, "doc": {"id": "3df1b88da6a90c9526be2c8a6cc736dc", "question": "Billy saw a dog running from him and did his best to get away from it.  The leaped up to where the dog couldn't reach and was stuck.  Where might he have been trapped?", "question_concept": "dog", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kennel", "table", "porch", "backyard", "park"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Billy saw a dog running from him and did his best to get away from it.  The leaped up to where the dog couldn't reach and was stuck.  Where might he have been trapped?\nA. kennel\nB. table\nC. porch\nD. backyard\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Billy saw a dog running from him and did his best to get away from it.  The leaped up to where the dog couldn't reach and was stuck.  Where might he have been trapped?\nA. kennel\nB. table\nC. porch\nD. backyard\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Billy saw a dog running from him and did his best to get away from it.  The leaped up to where the dog couldn't reach and was stuck.  Where might he have been trapped?\nA. kennel\nB. table\nC. porch\nD. backyard\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Billy saw a dog running from him and did his best to get away from it.  The leaped up to where the dog couldn't reach and was stuck.  Where might he have been trapped?\nA. kennel\nB. table\nC. porch\nD. backyard\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Billy saw a dog running from him and did his best to get away from it.  The leaped up to where the dog couldn't reach and was stuck.  Where might he have been trapped?\nA. kennel\nB. table\nC. porch\nD. backyard\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.721829652786255", "False"]], [["-1.9718296527862549", "False"]], [["-3.971829652786255", "False"]], [["-5.721829414367676", "False"]], [["-6.721829414367676", "False"]]], "filtered_resps": [["-3.721829652786255", "False"], ["-1.9718296527862549", "False"], ["-3.971829652786255", "False"], ["-5.721829414367676", "False"], ["-6.721829414367676", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "593122d8d801cbc2bd36398b88a0c3c0f189cbf9f044dd91219c74bbca4d2ac4", "prompt_hash": "3f369b69addfe9b9d9eea79c5e4606de47e54a27f7bce0a39b9008b4730e6b26", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 689, "doc": {"id": "f912bcd7479b76db9b1c57a612b90f00", "question": "John and Judy were parents.  They had two wonderful kids who weren't always well behaved.  They were light tough, though.  They felt it was a parent's job to do what?", "question_concept": "parents", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["control children", "guide children", "speak freely", "cry", "understand children"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John and Judy were parents.  They had two wonderful kids who weren't always well behaved.  They were light tough, though.  They felt it was a parent's job to do what?\nA. control children\nB. guide children\nC. speak freely\nD. cry\nE. understand children\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John and Judy were parents.  They had two wonderful kids who weren't always well behaved.  They were light tough, though.  They felt it was a parent's job to do what?\nA. control children\nB. guide children\nC. speak freely\nD. cry\nE. understand children\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John and Judy were parents.  They had two wonderful kids who weren't always well behaved.  They were light tough, though.  They felt it was a parent's job to do what?\nA. control children\nB. guide children\nC. speak freely\nD. cry\nE. understand children\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John and Judy were parents.  They had two wonderful kids who weren't always well behaved.  They were light tough, though.  They felt it was a parent's job to do what?\nA. control children\nB. guide children\nC. speak freely\nD. cry\nE. understand children\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John and Judy were parents.  They had two wonderful kids who weren't always well behaved.  They were light tough, though.  They felt it was a parent's job to do what?\nA. control children\nB. guide children\nC. speak freely\nD. cry\nE. understand children\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.85584831237793", "False"]], [["-0.8558485507965088", "True"]], [["-7.85584831237793", "False"]], [["-8.85584831237793", "False"]], [["-4.10584831237793", "False"]]], "filtered_resps": [["-5.85584831237793", "False"], ["-0.8558485507965088", "True"], ["-7.85584831237793", "False"], ["-8.85584831237793", "False"], ["-4.10584831237793", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c36a2ce8a56773897b772b315d7eb0c1b498cada55a27d4ce43141ee7a5b0373", "prompt_hash": "16659e55468d20d778bfaaec9b3a1fa28ae5650cb643cfc97d569a0844370f1b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 690, "doc": {"id": "94f34cc1e6aa9eefe06563cce8225658", "question": "What are you playing if you're fiddling on a violin?", "question_concept": "fiddling", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bluegrass music", "make music", "drop", "string instrument", "troubles"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What are you playing if you're fiddling on a violin?\nA. bluegrass music\nB. make music\nC. drop\nD. string instrument\nE. troubles\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are you playing if you're fiddling on a violin?\nA. bluegrass music\nB. make music\nC. drop\nD. string instrument\nE. troubles\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are you playing if you're fiddling on a violin?\nA. bluegrass music\nB. make music\nC. drop\nD. string instrument\nE. troubles\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are you playing if you're fiddling on a violin?\nA. bluegrass music\nB. make music\nC. drop\nD. string instrument\nE. troubles\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are you playing if you're fiddling on a violin?\nA. bluegrass music\nB. make music\nC. drop\nD. string instrument\nE. troubles\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.748149871826172", "False"]], [["-2.748149871826172", "False"]], [["-5.248149871826172", "False"]], [["-1.4981498718261719", "True"]], [["-5.498149871826172", "False"]]], "filtered_resps": [["-2.748149871826172", "False"], ["-2.748149871826172", "False"], ["-5.248149871826172", "False"], ["-1.4981498718261719", "True"], ["-5.498149871826172", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "38a2bd5e948dcaa394f399585b808afe265995f765767cd532d117332d931a95", "prompt_hash": "ef707e983cee1c902b97d2e6cef6d16fc15afbe83dd0bd39276764a277b94bd6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 691, "doc": {"id": "bb503ece4eac41dfe608a1dcb654e6bf", "question": "If somebody buys something and gives it to me as a free gift, what is the cost status of the gift?", "question_concept": "free", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["deadly", "imprisoned", "paid for", "expensive", "in prison"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If somebody buys something and gives it to me as a free gift, what is the cost status of the gift?\nA. deadly\nB. imprisoned\nC. paid for\nD. expensive\nE. in prison\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If somebody buys something and gives it to me as a free gift, what is the cost status of the gift?\nA. deadly\nB. imprisoned\nC. paid for\nD. expensive\nE. in prison\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If somebody buys something and gives it to me as a free gift, what is the cost status of the gift?\nA. deadly\nB. imprisoned\nC. paid for\nD. expensive\nE. in prison\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If somebody buys something and gives it to me as a free gift, what is the cost status of the gift?\nA. deadly\nB. imprisoned\nC. paid for\nD. expensive\nE. in prison\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If somebody buys something and gives it to me as a free gift, what is the cost status of the gift?\nA. deadly\nB. imprisoned\nC. paid for\nD. expensive\nE. in prison\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7696735858917236", "False"]], [["-5.0196733474731445", "False"]], [["-1.7696735858917236", "False"]], [["-6.2696733474731445", "False"]], [["-5.0196733474731445", "False"]]], "filtered_resps": [["-3.7696735858917236", "False"], ["-5.0196733474731445", "False"], ["-1.7696735858917236", "False"], ["-6.2696733474731445", "False"], ["-5.0196733474731445", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "55a98b47a31ae16d345cacc6661610acfb4dfe99487381358a54de4277f645f8", "prompt_hash": "4e4ca011b7a654783fa38d7562761421a4c19575007e1ff8a817008d0fae5eb4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 692, "doc": {"id": "5502dc807d4921679ae1abd0dc9570d6", "question": "Why does someone flirt with many people at once?", "question_concept": "flirt", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have sex", "get laid", "were lonely", "attract attention", "dance"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Why does someone flirt with many people at once?\nA. have sex\nB. get laid\nC. were lonely\nD. attract attention\nE. dance\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why does someone flirt with many people at once?\nA. have sex\nB. get laid\nC. were lonely\nD. attract attention\nE. dance\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why does someone flirt with many people at once?\nA. have sex\nB. get laid\nC. were lonely\nD. attract attention\nE. dance\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why does someone flirt with many people at once?\nA. have sex\nB. get laid\nC. were lonely\nD. attract attention\nE. dance\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why does someone flirt with many people at once?\nA. have sex\nB. get laid\nC. were lonely\nD. attract attention\nE. dance\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.773984909057617", "False"]], [["-6.023984909057617", "False"]], [["-3.773984909057617", "False"]], [["-1.7739850282669067", "False"]], [["-8.523984909057617", "False"]]], "filtered_resps": [["-4.773984909057617", "False"], ["-6.023984909057617", "False"], ["-3.773984909057617", "False"], ["-1.7739850282669067", "False"], ["-8.523984909057617", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6ac11398273ac9baf82329164c29effb9eb8bab27f015bfedc6a18a0ee79bf37", "prompt_hash": "2e37b7bc0ee5e2b950de49ae7f1314578a83655934eb71f75731132b9a7bc9ee", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 693, "doc": {"id": "a7e3de0719fe30e7048f67426e29fdd1", "question": "James tore the antenna off of his boat due to bad reception as he was crossing the channel from France.  Where was he going?", "question_concept": "channel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["river", "television", "india", "england", "europe"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: James tore the antenna off of his boat due to bad reception as he was crossing the channel from France.  Where was he going?\nA. river\nB. television\nC. india\nD. england\nE. europe\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James tore the antenna off of his boat due to bad reception as he was crossing the channel from France.  Where was he going?\nA. river\nB. television\nC. india\nD. england\nE. europe\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James tore the antenna off of his boat due to bad reception as he was crossing the channel from France.  Where was he going?\nA. river\nB. television\nC. india\nD. england\nE. europe\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James tore the antenna off of his boat due to bad reception as he was crossing the channel from France.  Where was he going?\nA. river\nB. television\nC. india\nD. england\nE. europe\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James tore the antenna off of his boat due to bad reception as he was crossing the channel from France.  Where was he going?\nA. river\nB. television\nC. india\nD. england\nE. europe\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.427892684936523", "False"]], [["-5.927892684936523", "False"]], [["-5.177892684936523", "False"]], [["-2.4278926849365234", "False"]], [["-4.927892684936523", "False"]]], "filtered_resps": [["-4.427892684936523", "False"], ["-5.927892684936523", "False"], ["-5.177892684936523", "False"], ["-2.4278926849365234", "False"], ["-4.927892684936523", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "214d93e1d8fd749515495d20903ec223b044bcc76b3242682a5d425a68df1b8b", "prompt_hash": "0ac637014a93f4720a9ae7d2d615ab82c4ee7d4763bc79178f9e0da583056080", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 694, "doc": {"id": "d6107d454181b701ddcaa449a1e422a3", "question": "Why would a band be performing when there are no people nearby?", "question_concept": "band", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["record album", "play music", "hold concert", "blaring", "practice"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Why would a band be performing when there are no people nearby?\nA. record album\nB. play music\nC. hold concert\nD. blaring\nE. practice\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would a band be performing when there are no people nearby?\nA. record album\nB. play music\nC. hold concert\nD. blaring\nE. practice\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would a band be performing when there are no people nearby?\nA. record album\nB. play music\nC. hold concert\nD. blaring\nE. practice\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would a band be performing when there are no people nearby?\nA. record album\nB. play music\nC. hold concert\nD. blaring\nE. practice\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would a band be performing when there are no people nearby?\nA. record album\nB. play music\nC. hold concert\nD. blaring\nE. practice\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.677504062652588", "False"]], [["-2.177504062652588", "False"]], [["-3.427504062652588", "False"]], [["-5.427504062652588", "False"]], [["-2.177504062652588", "False"]]], "filtered_resps": [["-2.677504062652588", "False"], ["-2.177504062652588", "False"], ["-3.427504062652588", "False"], ["-5.427504062652588", "False"], ["-2.177504062652588", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a2b3e048af6eb92c34cbb2fa1f582427a8c3ea3d7c22557ca5d4ea560c11c2cd", "prompt_hash": "68ff80238a2ede8cda1e2ab169a4e28a44875dea07d0e33eafbfb3534a0e20c2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 695, "doc": {"id": "ab2eb930b29bb6d5e94a6cd3b04ba01e", "question": "The dogs were protecting their own when they decided to what the bad man?", "question_concept": "dogs", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bad breath", "defend", "run fast", "ocean", "attack"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The dogs were protecting their own when they decided to what the bad man?\nA. bad breath\nB. defend\nC. run fast\nD. ocean\nE. attack\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The dogs were protecting their own when they decided to what the bad man?\nA. bad breath\nB. defend\nC. run fast\nD. ocean\nE. attack\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The dogs were protecting their own when they decided to what the bad man?\nA. bad breath\nB. defend\nC. run fast\nD. ocean\nE. attack\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The dogs were protecting their own when they decided to what the bad man?\nA. bad breath\nB. defend\nC. run fast\nD. ocean\nE. attack\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The dogs were protecting their own when they decided to what the bad man?\nA. bad breath\nB. defend\nC. run fast\nD. ocean\nE. attack\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.1176252365112305", "False"]], [["-1.36762535572052", "True"]], [["-8.11762523651123", "False"]], [["-6.8676252365112305", "False"]], [["-3.3676252365112305", "False"]]], "filtered_resps": [["-6.1176252365112305", "False"], ["-1.36762535572052", "True"], ["-8.11762523651123", "False"], ["-6.8676252365112305", "False"], ["-3.3676252365112305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2534b2bc6a0d79e484932a4bd6a81de4103a937feae9872ae4b1a91f61b8a707", "prompt_hash": "a08c1f0ff4d203f4d40b585ee49b319d6fa961c2610b2479347dfb9cc674814c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 696, "doc": {"id": "92869fc0be5dc45f407700692ffd80a0", "question": "What is used to grind wheat for bread?", "question_concept": "wheat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["farmer's field", "countryside", "cereal packets", "bread", "mill"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is used to grind wheat for bread?\nA. farmer's field\nB. countryside\nC. cereal packets\nD. bread\nE. mill\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is used to grind wheat for bread?\nA. farmer's field\nB. countryside\nC. cereal packets\nD. bread\nE. mill\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is used to grind wheat for bread?\nA. farmer's field\nB. countryside\nC. cereal packets\nD. bread\nE. mill\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is used to grind wheat for bread?\nA. farmer's field\nB. countryside\nC. cereal packets\nD. bread\nE. mill\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is used to grind wheat for bread?\nA. farmer's field\nB. countryside\nC. cereal packets\nD. bread\nE. mill\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2642743587493896", "False"]], [["-5.764274597167969", "False"]], [["-7.514274597167969", "False"]], [["-8.514274597167969", "False"]], [["-1.2642743587493896", "False"]]], "filtered_resps": [["-3.2642743587493896", "False"], ["-5.764274597167969", "False"], ["-7.514274597167969", "False"], ["-8.514274597167969", "False"], ["-1.2642743587493896", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e5d1501910a8ce300dd7282819e062be1c9123c56571d822e7980f60f088cab9", "prompt_hash": "b3c52a94e0d07b9247c1a8f3daad78ce4b3ae0e7ac8f7f43c76ba461d71f8e0b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 697, "doc": {"id": "6a0177586d506cb7b741f4207b428e42", "question": "If you have a large satchel with you when you fly you'll be asked to store it where?", "question_concept": "satchel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["airport", "luggage compartment", "with the pilot", "room", "clothing store"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you have a large satchel with you when you fly you'll be asked to store it where?\nA. airport\nB. luggage compartment\nC. with the pilot\nD. room\nE. clothing store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you have a large satchel with you when you fly you'll be asked to store it where?\nA. airport\nB. luggage compartment\nC. with the pilot\nD. room\nE. clothing store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you have a large satchel with you when you fly you'll be asked to store it where?\nA. airport\nB. luggage compartment\nC. with the pilot\nD. room\nE. clothing store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you have a large satchel with you when you fly you'll be asked to store it where?\nA. airport\nB. luggage compartment\nC. with the pilot\nD. room\nE. clothing store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you have a large satchel with you when you fly you'll be asked to store it where?\nA. airport\nB. luggage compartment\nC. with the pilot\nD. room\nE. clothing store\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.3886542320251465", "False"]], [["-1.1386542320251465", "True"]], [["-5.8886542320251465", "False"]], [["-8.138654708862305", "False"]], [["-8.138654708862305", "False"]]], "filtered_resps": [["-4.3886542320251465", "False"], ["-1.1386542320251465", "True"], ["-5.8886542320251465", "False"], ["-8.138654708862305", "False"], ["-8.138654708862305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5612c689bd35b8ec1be9750e9aaf090156442c6f5662d6b7e83b82615e9a2540", "prompt_hash": "444570da876e41989748fd60452b0a0be41b902f04037d5317fff5cb3f47e4e9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 698, "doc": {"id": "584188da9a429f1bc319abda5e5c7a76", "question": "Where would someone keep their nylon leggings?", "question_concept": "nylon", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stockings", "rope", "car", "clothing", "drawer"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would someone keep their nylon leggings?\nA. stockings\nB. rope\nC. car\nD. clothing\nE. drawer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would someone keep their nylon leggings?\nA. stockings\nB. rope\nC. car\nD. clothing\nE. drawer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would someone keep their nylon leggings?\nA. stockings\nB. rope\nC. car\nD. clothing\nE. drawer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would someone keep their nylon leggings?\nA. stockings\nB. rope\nC. car\nD. clothing\nE. drawer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would someone keep their nylon leggings?\nA. stockings\nB. rope\nC. car\nD. clothing\nE. drawer\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9668831825256348", "False"]], [["-7.966883182525635", "False"]], [["-6.716883182525635", "False"]], [["-1.9668831825256348", "True"]], [["-1.9668831825256348", "True"]]], "filtered_resps": [["-3.9668831825256348", "False"], ["-7.966883182525635", "False"], ["-6.716883182525635", "False"], ["-1.9668831825256348", "True"], ["-1.9668831825256348", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "060b47853dbc18b97873d2652dc05849b84bf4998073ae245715afefa94a61cf", "prompt_hash": "a67ea705271a17b2f480b94ad3df608a6233fb862ae3ea6be42d11ca5804c68f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 699, "doc": {"id": "e480d4a672af0194e0a6ccdb8c37499b", "question": "If you spend a long time running after a ball how are you likely to feel?", "question_concept": "running after ball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["laughter", "sweating", "embarrassed", "breathing heavily", "tiredness"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If you spend a long time running after a ball how are you likely to feel?\nA. laughter\nB. sweating\nC. embarrassed\nD. breathing heavily\nE. tiredness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you spend a long time running after a ball how are you likely to feel?\nA. laughter\nB. sweating\nC. embarrassed\nD. breathing heavily\nE. tiredness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you spend a long time running after a ball how are you likely to feel?\nA. laughter\nB. sweating\nC. embarrassed\nD. breathing heavily\nE. tiredness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you spend a long time running after a ball how are you likely to feel?\nA. laughter\nB. sweating\nC. embarrassed\nD. breathing heavily\nE. tiredness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you spend a long time running after a ball how are you likely to feel?\nA. laughter\nB. sweating\nC. embarrassed\nD. breathing heavily\nE. tiredness\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.4081621170043945", "False"]], [["-2.6581623554229736", "False"]], [["-7.4081621170043945", "False"]], [["-3.6581623554229736", "False"]], [["-3.4081623554229736", "False"]]], "filtered_resps": [["-5.4081621170043945", "False"], ["-2.6581623554229736", "False"], ["-7.4081621170043945", "False"], ["-3.6581623554229736", "False"], ["-3.4081623554229736", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7d0d9722728653ba383d488c710e74ac12e689474e1edcc0dce73f6eaa35b9d7", "prompt_hash": "64e6f7f7d8672fca0ac2c42a9c394454fc01ea3178eea77aadb5fec696cd35ae", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 700, "doc": {"id": "275c859994f7d3acd3c8863be591ab2c", "question": "When you need to rest it's often because you have been doing what?", "question_concept": "rest", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["action", "sleep", "sleeping", "in motion", "using energy"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: When you need to rest it's often because you have been doing what?\nA. action\nB. sleep\nC. sleeping\nD. in motion\nE. using energy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you need to rest it's often because you have been doing what?\nA. action\nB. sleep\nC. sleeping\nD. in motion\nE. using energy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you need to rest it's often because you have been doing what?\nA. action\nB. sleep\nC. sleeping\nD. in motion\nE. using energy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you need to rest it's often because you have been doing what?\nA. action\nB. sleep\nC. sleeping\nD. in motion\nE. using energy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you need to rest it's often because you have been doing what?\nA. action\nB. sleep\nC. sleeping\nD. in motion\nE. using energy\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0604069232940674", "False"]], [["-2.5604069232940674", "False"]], [["-3.5604069232940674", "False"]], [["-4.310406684875488", "False"]], [["-2.0604069232940674", "False"]]], "filtered_resps": [["-3.0604069232940674", "False"], ["-2.5604069232940674", "False"], ["-3.5604069232940674", "False"], ["-4.310406684875488", "False"], ["-2.0604069232940674", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1a86f0b81e5add8f07027977945ae1a6500e50dc73a1f9e0157fcb099243e4c1", "prompt_hash": "59ca73829e0b7ab0f01417fc7b04b2ce14f009b092261b5813d675233289c38a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 701, "doc": {"id": "32758ab86d888be680845b0dfe7de35e", "question": "Boredom and hunger led to a wandering waste of time and a cart full of unhealthy snacks during her trip to where?", "question_concept": "boredom", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["new moon", "play cards", "read book", "see art", "grocery shop"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Boredom and hunger led to a wandering waste of time and a cart full of unhealthy snacks during her trip to where?\nA. new moon\nB. play cards\nC. read book\nD. see art\nE. grocery shop\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Boredom and hunger led to a wandering waste of time and a cart full of unhealthy snacks during her trip to where?\nA. new moon\nB. play cards\nC. read book\nD. see art\nE. grocery shop\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Boredom and hunger led to a wandering waste of time and a cart full of unhealthy snacks during her trip to where?\nA. new moon\nB. play cards\nC. read book\nD. see art\nE. grocery shop\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Boredom and hunger led to a wandering waste of time and a cart full of unhealthy snacks during her trip to where?\nA. new moon\nB. play cards\nC. read book\nD. see art\nE. grocery shop\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Boredom and hunger led to a wandering waste of time and a cart full of unhealthy snacks during her trip to where?\nA. new moon\nB. play cards\nC. read book\nD. see art\nE. grocery shop\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.2341532707214355", "False"]], [["-2.4841532707214355", "False"]], [["-6.7341532707214355", "False"]], [["-6.4841532707214355", "False"]], [["-0.984153151512146", "True"]]], "filtered_resps": [["-4.2341532707214355", "False"], ["-2.4841532707214355", "False"], ["-6.7341532707214355", "False"], ["-6.4841532707214355", "False"], ["-0.984153151512146", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8b311f6a76778d328f682c598a0063c5cfbf3012424b38170143c2ddd625cbb0", "prompt_hash": "d81ad8b266027bd5d62a43cbd8bb0d69ccb9ebd495abff10400e5d50d480b38e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 702, "doc": {"id": "69335eb9bc5b7b5df840c38a086bf8b2", "question": "He was beginning to worry they wouldn't get on the ride before closing, they had been standing in queue for a long what?", "question_concept": "standing in queue", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["frustration", "waiting", "hair", "time", "patience"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He was beginning to worry they wouldn't get on the ride before closing, they had been standing in queue for a long what?\nA. frustration\nB. waiting\nC. hair\nD. time\nE. patience\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was beginning to worry they wouldn't get on the ride before closing, they had been standing in queue for a long what?\nA. frustration\nB. waiting\nC. hair\nD. time\nE. patience\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was beginning to worry they wouldn't get on the ride before closing, they had been standing in queue for a long what?\nA. frustration\nB. waiting\nC. hair\nD. time\nE. patience\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was beginning to worry they wouldn't get on the ride before closing, they had been standing in queue for a long what?\nA. frustration\nB. waiting\nC. hair\nD. time\nE. patience\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was beginning to worry they wouldn't get on the ride before closing, they had been standing in queue for a long what?\nA. frustration\nB. waiting\nC. hair\nD. time\nE. patience\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.095546722412109", "False"]], [["-1.3455464839935303", "True"]], [["-6.595546722412109", "False"]], [["-2.5955464839935303", "False"]], [["-7.595546722412109", "False"]]], "filtered_resps": [["-5.095546722412109", "False"], ["-1.3455464839935303", "True"], ["-6.595546722412109", "False"], ["-2.5955464839935303", "False"], ["-7.595546722412109", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e1a475b350b6a827e3f0d54681b2bb4fe719a7146cd6ca5cf17d5cd263978805", "prompt_hash": "e48c3f9638568a797a5d72b0d599c81269ff9b57e6be7738a1bd256d751dd49c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 703, "doc": {"id": "4396cb65629672723c7b184424e139bb", "question": "This is an unavoidable physiological consequence of running.  What is it?", "question_concept": "running", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["breathlessness", "increased heart rate", "falling down", "muscle bulk", "calluses"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: This is an unavoidable physiological consequence of running.  What is it?\nA. breathlessness\nB. increased heart rate\nC. falling down\nD. muscle bulk\nE. calluses\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: This is an unavoidable physiological consequence of running.  What is it?\nA. breathlessness\nB. increased heart rate\nC. falling down\nD. muscle bulk\nE. calluses\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: This is an unavoidable physiological consequence of running.  What is it?\nA. breathlessness\nB. increased heart rate\nC. falling down\nD. muscle bulk\nE. calluses\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: This is an unavoidable physiological consequence of running.  What is it?\nA. breathlessness\nB. increased heart rate\nC. falling down\nD. muscle bulk\nE. calluses\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: This is an unavoidable physiological consequence of running.  What is it?\nA. breathlessness\nB. increased heart rate\nC. falling down\nD. muscle bulk\nE. calluses\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7461822032928467", "True"]], [["-1.9961822032928467", "False"]], [["-5.996182441711426", "False"]], [["-5.746182441711426", "False"]], [["-4.246182441711426", "False"]]], "filtered_resps": [["-1.7461822032928467", "True"], ["-1.9961822032928467", "False"], ["-5.996182441711426", "False"], ["-5.746182441711426", "False"], ["-4.246182441711426", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1340b88530178ff23f8b5583d3ded101e2cb03bb7985df8f5bf265f4f9aec09f", "prompt_hash": "b9fd47526cac2ccb37bef83f2940c9a2a3bce17e2f9c3982373b4b9ddead0934", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 704, "doc": {"id": "2a58e81a9c4ce095d099e0d785fc2da4", "question": "Sometimes a person has a fear of water or a dislike of being wet, it is still important to make sure they are having a bath why?", "question_concept": "having bath", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["flooding", "drowning", "wet skin", "get wet", "rash"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Sometimes a person has a fear of water or a dislike of being wet, it is still important to make sure they are having a bath why?\nA. flooding\nB. drowning\nC. wet skin\nD. get wet\nE. rash\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sometimes a person has a fear of water or a dislike of being wet, it is still important to make sure they are having a bath why?\nA. flooding\nB. drowning\nC. wet skin\nD. get wet\nE. rash\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sometimes a person has a fear of water or a dislike of being wet, it is still important to make sure they are having a bath why?\nA. flooding\nB. drowning\nC. wet skin\nD. get wet\nE. rash\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sometimes a person has a fear of water or a dislike of being wet, it is still important to make sure they are having a bath why?\nA. flooding\nB. drowning\nC. wet skin\nD. get wet\nE. rash\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sometimes a person has a fear of water or a dislike of being wet, it is still important to make sure they are having a bath why?\nA. flooding\nB. drowning\nC. wet skin\nD. get wet\nE. rash\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.849036455154419", "False"]], [["-1.349036455154419", "True"]], [["-3.349036455154419", "False"]], [["-2.599036455154419", "False"]], [["-4.84903621673584", "False"]]], "filtered_resps": [["-3.849036455154419", "False"], ["-1.349036455154419", "True"], ["-3.349036455154419", "False"], ["-2.599036455154419", "False"], ["-4.84903621673584", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "be9bd8ee2a859c655d0470861dd6307fb3fb107b2e54125c1fc3387bedd8e68b", "prompt_hash": "25f8d48b28029495ba8fa5c5a6696de6afb4c316e402866d4fd07c81f58e29ce", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 705, "doc": {"id": "07f108d5321a66f460685f5c7499ecb2", "question": "Where would there be an auditorium with only a single person speaking?", "question_concept": "auditorium", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lights", "crowd", "university campus", "theater", "park"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would there be an auditorium with only a single person speaking?\nA. lights\nB. crowd\nC. university campus\nD. theater\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would there be an auditorium with only a single person speaking?\nA. lights\nB. crowd\nC. university campus\nD. theater\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would there be an auditorium with only a single person speaking?\nA. lights\nB. crowd\nC. university campus\nD. theater\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would there be an auditorium with only a single person speaking?\nA. lights\nB. crowd\nC. university campus\nD. theater\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would there be an auditorium with only a single person speaking?\nA. lights\nB. crowd\nC. university campus\nD. theater\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.828547477722168", "False"]], [["-4.578547477722168", "False"]], [["-3.828547477722168", "False"]], [["-1.078547477722168", "True"]], [["-4.078547477722168", "False"]]], "filtered_resps": [["-1.828547477722168", "False"], ["-4.578547477722168", "False"], ["-3.828547477722168", "False"], ["-1.078547477722168", "True"], ["-4.078547477722168", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "301847cb3f74214a5df67f70b33c32892179a16a16197cc95150419ee4edfac5", "prompt_hash": "f7766ed8356e96ce05abcb40861c9e834e6523a65a78fe1cae5be3090853c9fa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 706, "doc": {"id": "69bef3eb55463d040bdf98e2c97bfe1f", "question": "To get out of there the person had to keep on walking, they had to keep on what?", "question_concept": "walking", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["moving forward", "locomotion", "blisters", "rollerskate", "exercise"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: To get out of there the person had to keep on walking, they had to keep on what?\nA. moving forward\nB. locomotion\nC. blisters\nD. rollerskate\nE. exercise\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: To get out of there the person had to keep on walking, they had to keep on what?\nA. moving forward\nB. locomotion\nC. blisters\nD. rollerskate\nE. exercise\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: To get out of there the person had to keep on walking, they had to keep on what?\nA. moving forward\nB. locomotion\nC. blisters\nD. rollerskate\nE. exercise\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: To get out of there the person had to keep on walking, they had to keep on what?\nA. moving forward\nB. locomotion\nC. blisters\nD. rollerskate\nE. exercise\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: To get out of there the person had to keep on walking, they had to keep on what?\nA. moving forward\nB. locomotion\nC. blisters\nD. rollerskate\nE. exercise\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5653773546218872", "False"]], [["-6.065377235412598", "False"]], [["-8.815377235412598", "False"]], [["-10.065377235412598", "False"]], [["-10.565377235412598", "False"]]], "filtered_resps": [["-1.5653773546218872", "False"], ["-6.065377235412598", "False"], ["-8.815377235412598", "False"], ["-10.065377235412598", "False"], ["-10.565377235412598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d296497d75c2f5599eeea7c13eb02cc5ce4d830f54330d9a75f7da769d095db8", "prompt_hash": "87e1b99c7b213e8774fc4ea8a882fb9f6628b042001d09db00dce1d2a6781128", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 707, "doc": {"id": "912676495cceefadccbbf8c604486f97", "question": "What very large group of western citizens has bees everywhere?", "question_concept": "bee", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["united states", "space station", "trash can", "field of flowers", "bouquet of flowers"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What very large group of western citizens has bees everywhere?\nA. united states\nB. space station\nC. trash can\nD. field of flowers\nE. bouquet of flowers\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What very large group of western citizens has bees everywhere?\nA. united states\nB. space station\nC. trash can\nD. field of flowers\nE. bouquet of flowers\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What very large group of western citizens has bees everywhere?\nA. united states\nB. space station\nC. trash can\nD. field of flowers\nE. bouquet of flowers\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What very large group of western citizens has bees everywhere?\nA. united states\nB. space station\nC. trash can\nD. field of flowers\nE. bouquet of flowers\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What very large group of western citizens has bees everywhere?\nA. united states\nB. space station\nC. trash can\nD. field of flowers\nE. bouquet of flowers\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1774630546569824", "True"]], [["-3.1774630546569824", "False"]], [["-5.927463054656982", "False"]], [["-3.6774630546569824", "False"]], [["-5.177463054656982", "False"]]], "filtered_resps": [["-1.1774630546569824", "True"], ["-3.1774630546569824", "False"], ["-5.927463054656982", "False"], ["-3.6774630546569824", "False"], ["-5.177463054656982", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "303533a895936b9051966695a691280cb187d1507b7c7a8a60d15f69c93c124d", "prompt_hash": "eebe82ab1ca5bf4e00a8d4fad0ceba89fd05b8d399dd28c87365062446f81e9f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 708, "doc": {"id": "bdf92566f14599f1606109677206001f", "question": "Miss Grady took a stick from Bob because he was playing with it during class.  She wanted to make sure that he couldn't get to it so she put it where?", "question_concept": "glue stick", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desk drawer", "kitchen drawer", "classroom", "pocket", "office"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Miss Grady took a stick from Bob because he was playing with it during class.  She wanted to make sure that he couldn't get to it so she put it where?\nA. desk drawer\nB. kitchen drawer\nC. classroom\nD. pocket\nE. office\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Miss Grady took a stick from Bob because he was playing with it during class.  She wanted to make sure that he couldn't get to it so she put it where?\nA. desk drawer\nB. kitchen drawer\nC. classroom\nD. pocket\nE. office\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Miss Grady took a stick from Bob because he was playing with it during class.  She wanted to make sure that he couldn't get to it so she put it where?\nA. desk drawer\nB. kitchen drawer\nC. classroom\nD. pocket\nE. office\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Miss Grady took a stick from Bob because he was playing with it during class.  She wanted to make sure that he couldn't get to it so she put it where?\nA. desk drawer\nB. kitchen drawer\nC. classroom\nD. pocket\nE. office\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Miss Grady took a stick from Bob because he was playing with it during class.  She wanted to make sure that he couldn't get to it so she put it where?\nA. desk drawer\nB. kitchen drawer\nC. classroom\nD. pocket\nE. office\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4462119340896606", "True"]], [["-3.696211814880371", "False"]], [["-3.196211814880371", "False"]], [["-4.946211814880371", "False"]], [["-6.946211814880371", "False"]]], "filtered_resps": [["-1.4462119340896606", "True"], ["-3.696211814880371", "False"], ["-3.196211814880371", "False"], ["-4.946211814880371", "False"], ["-6.946211814880371", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4cd4a1ab3a8f55980d4dfd4e2205370a9bf1a4554753c5ff07b1d8cd1d6ebf2b", "prompt_hash": "d95d761b7fcb419978710e680728dade766362d3e478b61c3b265d0ad83f0f0a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 709, "doc": {"id": "0df042743128b57e874bd5d79b7aae7a", "question": "How does a person begin reproducing?", "question_concept": "reproducing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["genetic mutation", "have sex", "kiss", "flirting", "going on a date"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: How does a person begin reproducing?\nA. genetic mutation\nB. have sex\nC. kiss\nD. flirting\nE. going on a date\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How does a person begin reproducing?\nA. genetic mutation\nB. have sex\nC. kiss\nD. flirting\nE. going on a date\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How does a person begin reproducing?\nA. genetic mutation\nB. have sex\nC. kiss\nD. flirting\nE. going on a date\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How does a person begin reproducing?\nA. genetic mutation\nB. have sex\nC. kiss\nD. flirting\nE. going on a date\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How does a person begin reproducing?\nA. genetic mutation\nB. have sex\nC. kiss\nD. flirting\nE. going on a date\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8356616497039795", "False"]], [["-0.5856617093086243", "True"]], [["-7.585661888122559", "False"]], [["-8.335661888122559", "False"]], [["-8.835661888122559", "False"]]], "filtered_resps": [["-3.8356616497039795", "False"], ["-0.5856617093086243", "True"], ["-7.585661888122559", "False"], ["-8.335661888122559", "False"], ["-8.835661888122559", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f8b7836aba5c1e7ae1ae966bfcdeda87c10d4edb2487a67974fc1906f252dafd", "prompt_hash": "516ec7da644d840bb3293db342e4e0a13d5351ae6a8341eb97fd7e7450df6ca8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 710, "doc": {"id": "866ef7266d34c11e5a1b3df49fab96a4", "question": "Joe and Jill didn't want their children to be sedentary.  They might limit the time they children spend doing what?", "question_concept": "children", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["play sports", "throw things", "reading", "watch tv", "play with toys"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Joe and Jill didn't want their children to be sedentary.  They might limit the time they children spend doing what?\nA. play sports\nB. throw things\nC. reading\nD. watch tv\nE. play with toys\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe and Jill didn't want their children to be sedentary.  They might limit the time they children spend doing what?\nA. play sports\nB. throw things\nC. reading\nD. watch tv\nE. play with toys\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe and Jill didn't want their children to be sedentary.  They might limit the time they children spend doing what?\nA. play sports\nB. throw things\nC. reading\nD. watch tv\nE. play with toys\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe and Jill didn't want their children to be sedentary.  They might limit the time they children spend doing what?\nA. play sports\nB. throw things\nC. reading\nD. watch tv\nE. play with toys\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe and Jill didn't want their children to be sedentary.  They might limit the time they children spend doing what?\nA. play sports\nB. throw things\nC. reading\nD. watch tv\nE. play with toys\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.3919830322265625", "False"]], [["-5.6419830322265625", "False"]], [["-6.6419830322265625", "False"]], [["-1.641982913017273", "False"]], [["-9.891983032226562", "False"]]], "filtered_resps": [["-5.3919830322265625", "False"], ["-5.6419830322265625", "False"], ["-6.6419830322265625", "False"], ["-1.641982913017273", "False"], ["-9.891983032226562", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1a192ec3450e7892e933abcc15e373e09d417201feebe5406c4caa93077a796a", "prompt_hash": "ed6ad9128f63d587d3e3d5d8b7a5f7a2eccb8594d22897231eb06d05971214fc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 711, "doc": {"id": "67ffcb4c3f2c6a1155e356f8a15ed250", "question": "They were making sauerkraut, the instructor explained the liquid should be above the cabbage in the what?", "question_concept": "liquid", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["jar", "drinking glass", "pot", "container", "can"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: They were making sauerkraut, the instructor explained the liquid should be above the cabbage in the what?\nA. jar\nB. drinking glass\nC. pot\nD. container\nE. can\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They were making sauerkraut, the instructor explained the liquid should be above the cabbage in the what?\nA. jar\nB. drinking glass\nC. pot\nD. container\nE. can\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They were making sauerkraut, the instructor explained the liquid should be above the cabbage in the what?\nA. jar\nB. drinking glass\nC. pot\nD. container\nE. can\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They were making sauerkraut, the instructor explained the liquid should be above the cabbage in the what?\nA. jar\nB. drinking glass\nC. pot\nD. container\nE. can\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They were making sauerkraut, the instructor explained the liquid should be above the cabbage in the what?\nA. jar\nB. drinking glass\nC. pot\nD. container\nE. can\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1904915571212769", "True"]], [["-4.940491676330566", "False"]], [["-6.440491676330566", "False"]], [["-3.1904916763305664", "False"]], [["-8.940491676330566", "False"]]], "filtered_resps": [["-1.1904915571212769", "True"], ["-4.940491676330566", "False"], ["-6.440491676330566", "False"], ["-3.1904916763305664", "False"], ["-8.940491676330566", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d4107eb3c8c191264ee9677b57b57639f9a475a1ab8fce6306a792e5e2c381d3", "prompt_hash": "240933f954b3c4a97bc3727ebf22081ec1d99ef2100bff40d71a2d813fafdc4a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 712, "doc": {"id": "87a133afae5d9d29d634f3384f28ef24", "question": "From where would you normally take a cup when you're about to get a drink?", "question_concept": "cup", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dishwasher", "water fountain", "sand box", "toilet", "kitchen cabinet"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: From where would you normally take a cup when you're about to get a drink?\nA. dishwasher\nB. water fountain\nC. sand box\nD. toilet\nE. kitchen cabinet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: From where would you normally take a cup when you're about to get a drink?\nA. dishwasher\nB. water fountain\nC. sand box\nD. toilet\nE. kitchen cabinet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: From where would you normally take a cup when you're about to get a drink?\nA. dishwasher\nB. water fountain\nC. sand box\nD. toilet\nE. kitchen cabinet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: From where would you normally take a cup when you're about to get a drink?\nA. dishwasher\nB. water fountain\nC. sand box\nD. toilet\nE. kitchen cabinet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: From where would you normally take a cup when you're about to get a drink?\nA. dishwasher\nB. water fountain\nC. sand box\nD. toilet\nE. kitchen cabinet\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.590707302093506", "False"]], [["-1.5907071828842163", "False"]], [["-7.090707302093506", "False"]], [["-8.840706825256348", "False"]], [["-1.3407071828842163", "True"]]], "filtered_resps": [["-4.590707302093506", "False"], ["-1.5907071828842163", "False"], ["-7.090707302093506", "False"], ["-8.840706825256348", "False"], ["-1.3407071828842163", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "22c454fc513c6475bf0157fe73195a9bdfcd228fa7c796e3768099895edc86d3", "prompt_hash": "831056518731cd829919f36303eea068039b3405b9d214fb1e7b66726ca2c704", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 713, "doc": {"id": "4779be55f47a301debfc472e4fc2c7b6", "question": "What are you using if there are speakers strapped on your ears?", "question_concept": "speakers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["take it all in", "headphones", "desktop", "conference", "concert"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What are you using if there are speakers strapped on your ears?\nA. take it all in\nB. headphones\nC. desktop\nD. conference\nE. concert\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are you using if there are speakers strapped on your ears?\nA. take it all in\nB. headphones\nC. desktop\nD. conference\nE. concert\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are you using if there are speakers strapped on your ears?\nA. take it all in\nB. headphones\nC. desktop\nD. conference\nE. concert\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are you using if there are speakers strapped on your ears?\nA. take it all in\nB. headphones\nC. desktop\nD. conference\nE. concert\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are you using if there are speakers strapped on your ears?\nA. take it all in\nB. headphones\nC. desktop\nD. conference\nE. concert\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.721926689147949", "False"]], [["-0.7219266295433044", "True"]], [["-8.22192668914795", "False"]], [["-8.72192668914795", "False"]], [["-9.47192668914795", "False"]]], "filtered_resps": [["-5.721926689147949", "False"], ["-0.7219266295433044", "True"], ["-8.22192668914795", "False"], ["-8.72192668914795", "False"], ["-9.47192668914795", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4126dba463ed6b52ba59820efd5f03a8ee13c1c1e1429ec79da57d9bd72bb864", "prompt_hash": "1daab9dab4137924b33acb7ee7151fc0da855c6323af477e09f451db59565220", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 714, "doc": {"id": "7a28d31e66d870370642de3be47b9ef9", "question": "Because of his anger he couldn't clearly explain or what?", "question_concept": "anger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cool off", "write letter", "get mad", "illustrate point", "destroy enemy"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Because of his anger he couldn't clearly explain or what?\nA. cool off\nB. write letter\nC. get mad\nD. illustrate point\nE. destroy enemy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Because of his anger he couldn't clearly explain or what?\nA. cool off\nB. write letter\nC. get mad\nD. illustrate point\nE. destroy enemy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Because of his anger he couldn't clearly explain or what?\nA. cool off\nB. write letter\nC. get mad\nD. illustrate point\nE. destroy enemy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Because of his anger he couldn't clearly explain or what?\nA. cool off\nB. write letter\nC. get mad\nD. illustrate point\nE. destroy enemy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Because of his anger he couldn't clearly explain or what?\nA. cool off\nB. write letter\nC. get mad\nD. illustrate point\nE. destroy enemy\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.67118239402771", "False"]], [["-3.92118239402771", "False"]], [["-5.921182632446289", "False"]], [["-1.17118239402771", "True"]], [["-8.421182632446289", "False"]]], "filtered_resps": [["-2.67118239402771", "False"], ["-3.92118239402771", "False"], ["-5.921182632446289", "False"], ["-1.17118239402771", "True"], ["-8.421182632446289", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "25d321d7019dec992c520d476da99a6e67e36d1ee5893dd504e7265a6a6b360a", "prompt_hash": "b8a483888038ed3f195095c19175d0b1a00267206dafe227abf62754e515356e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 715, "doc": {"id": "042898e0c71adac5d123aaa6221c9754", "question": "Where is likely to not just have a kosher restaurant?", "question_concept": "kosher restaurant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["jerusalem", "jewish neighborhoods", "dining in", "new york city", "dining"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is likely to not just have a kosher restaurant?\nA. jerusalem\nB. jewish neighborhoods\nC. dining in\nD. new york city\nE. dining\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is likely to not just have a kosher restaurant?\nA. jerusalem\nB. jewish neighborhoods\nC. dining in\nD. new york city\nE. dining\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is likely to not just have a kosher restaurant?\nA. jerusalem\nB. jewish neighborhoods\nC. dining in\nD. new york city\nE. dining\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is likely to not just have a kosher restaurant?\nA. jerusalem\nB. jewish neighborhoods\nC. dining in\nD. new york city\nE. dining\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is likely to not just have a kosher restaurant?\nA. jerusalem\nB. jewish neighborhoods\nC. dining in\nD. new york city\nE. dining\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7456631660461426", "False"]], [["-4.995663166046143", "False"]], [["-4.245663166046143", "False"]], [["-2.4956631660461426", "False"]], [["-6.995663166046143", "False"]]], "filtered_resps": [["-1.7456631660461426", "False"], ["-4.995663166046143", "False"], ["-4.245663166046143", "False"], ["-2.4956631660461426", "False"], ["-6.995663166046143", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a165838627c81b6406cab6aa854dd857944005cbeb448d7e3ae3235a8cde2082", "prompt_hash": "4ab82fca90b86533aa797ee1c26402f01bca1da292b4bb388136fc49ad7e8b1f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 716, "doc": {"id": "93bbaccb1c46d22124a846b8514de5bc", "question": "The bald eagle flew from Mount St Helen's to the Puget Sound and all over what?", "question_concept": "bald eagle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["washington state", "utah", "pacific northwest", "northern california", "the desert"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The bald eagle flew from Mount St Helen's to the Puget Sound and all over what?\nA. washington state\nB. utah\nC. pacific northwest\nD. northern california\nE. the desert\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The bald eagle flew from Mount St Helen's to the Puget Sound and all over what?\nA. washington state\nB. utah\nC. pacific northwest\nD. northern california\nE. the desert\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The bald eagle flew from Mount St Helen's to the Puget Sound and all over what?\nA. washington state\nB. utah\nC. pacific northwest\nD. northern california\nE. the desert\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The bald eagle flew from Mount St Helen's to the Puget Sound and all over what?\nA. washington state\nB. utah\nC. pacific northwest\nD. northern california\nE. the desert\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The bald eagle flew from Mount St Helen's to the Puget Sound and all over what?\nA. washington state\nB. utah\nC. pacific northwest\nD. northern california\nE. the desert\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8110287189483643", "False"]], [["-5.311028480529785", "False"]], [["-1.5610287189483643", "False"]], [["-8.061028480529785", "False"]], [["-8.311028480529785", "False"]]], "filtered_resps": [["-3.8110287189483643", "False"], ["-5.311028480529785", "False"], ["-1.5610287189483643", "False"], ["-8.061028480529785", "False"], ["-8.311028480529785", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5ef696a47e044571308e04ed9944579b138420a3e3cb9339ef1213fb0f8684c7", "prompt_hash": "20251022d8fc6bb97877ff1a3f03e51572e6c634732eafe56f97c7a24595919d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 717, "doc": {"id": "ef889edd1b57d8d0c81e43f73c98c8e9", "question": "Where could you get some knives if you are planning to bring them outside with you?", "question_concept": "knives", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sporting goods store", "backpack", "kitchen", "sharp edges", "dog house"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where could you get some knives if you are planning to bring them outside with you?\nA. sporting goods store\nB. backpack\nC. kitchen\nD. sharp edges\nE. dog house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could you get some knives if you are planning to bring them outside with you?\nA. sporting goods store\nB. backpack\nC. kitchen\nD. sharp edges\nE. dog house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could you get some knives if you are planning to bring them outside with you?\nA. sporting goods store\nB. backpack\nC. kitchen\nD. sharp edges\nE. dog house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could you get some knives if you are planning to bring them outside with you?\nA. sporting goods store\nB. backpack\nC. kitchen\nD. sharp edges\nE. dog house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could you get some knives if you are planning to bring them outside with you?\nA. sporting goods store\nB. backpack\nC. kitchen\nD. sharp edges\nE. dog house\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5153526067733765", "True"]], [["-5.515352725982666", "False"]], [["-4.765352725982666", "False"]], [["-7.765352725982666", "False"]], [["-8.265352249145508", "False"]]], "filtered_resps": [["-0.5153526067733765", "True"], ["-5.515352725982666", "False"], ["-4.765352725982666", "False"], ["-7.765352725982666", "False"], ["-8.265352249145508", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8b9cc1e16fbcaa496fb41ee579cacec250842dc396aebcc445e08f770757296d", "prompt_hash": "65ff2605eae10a4385325f9bfd99d7912b828e99d35f46eae2bce3ae49d7a75a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 718, "doc": {"id": "f4bb8ecacb9ce89e040f5f76bc79afb3", "question": "How can people fulfill their own calorie requirements?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["compete with each other", "feed themselves", "feel lonely", "talk to each other", "ask a doctor"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: How can people fulfill their own calorie requirements?\nA. compete with each other\nB. feed themselves\nC. feel lonely\nD. talk to each other\nE. ask a doctor\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How can people fulfill their own calorie requirements?\nA. compete with each other\nB. feed themselves\nC. feel lonely\nD. talk to each other\nE. ask a doctor\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How can people fulfill their own calorie requirements?\nA. compete with each other\nB. feed themselves\nC. feel lonely\nD. talk to each other\nE. ask a doctor\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How can people fulfill their own calorie requirements?\nA. compete with each other\nB. feed themselves\nC. feel lonely\nD. talk to each other\nE. ask a doctor\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How can people fulfill their own calorie requirements?\nA. compete with each other\nB. feed themselves\nC. feel lonely\nD. talk to each other\nE. ask a doctor\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.242379188537598", "False"]], [["-0.9923793077468872", "True"]], [["-7.742379188537598", "False"]], [["-8.492379188537598", "False"]], [["-4.492379188537598", "False"]]], "filtered_resps": [["-5.242379188537598", "False"], ["-0.9923793077468872", "True"], ["-7.742379188537598", "False"], ["-8.492379188537598", "False"], ["-4.492379188537598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "601f584a07173b13890d22e4861191d76d88cb93782dd7f3e19df6eb3589ef91", "prompt_hash": "6f2d00f9aaf46b67aa5b3e4c74da01c5480840e20a6a5fae3ab2b41f09b85c9d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 719, "doc": {"id": "ec2e18fd8c18a4ebe5a091e0c8b94462", "question": "What does a stove do to the place that it's in?", "question_concept": "stove", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cool house", "warm room", "gas or electric", "burn child", "brown meat"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What does a stove do to the place that it's in?\nA. cool house\nB. warm room\nC. gas or electric\nD. burn child\nE. brown meat\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a stove do to the place that it's in?\nA. cool house\nB. warm room\nC. gas or electric\nD. burn child\nE. brown meat\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a stove do to the place that it's in?\nA. cool house\nB. warm room\nC. gas or electric\nD. burn child\nE. brown meat\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a stove do to the place that it's in?\nA. cool house\nB. warm room\nC. gas or electric\nD. burn child\nE. brown meat\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a stove do to the place that it's in?\nA. cool house\nB. warm room\nC. gas or electric\nD. burn child\nE. brown meat\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0492807626724243", "True"]], [["-1.5492807626724243", "False"]], [["-7.049280643463135", "False"]], [["-8.799281120300293", "False"]], [["-10.049281120300293", "False"]]], "filtered_resps": [["-1.0492807626724243", "True"], ["-1.5492807626724243", "False"], ["-7.049280643463135", "False"], ["-8.799281120300293", "False"], ["-10.049281120300293", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c5d87201edd3ce9bb414d2102d22b2a42d7ed330432c78286605b9a9de0438a7", "prompt_hash": "ed6ccd377aaa7d8e7ed30631cafe77d81f55a3aa0d70c44931f5831a78f28dd0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 720, "doc": {"id": "07b51b231a9d6a143d8a73e69121e1b1", "question": "What is the best way to begin going into trance?", "question_concept": "going into trance", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["religious experience", "closed eyes", "loss of control", "sleep", "hallucination"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is the best way to begin going into trance?\nA. religious experience\nB. closed eyes\nC. loss of control\nD. sleep\nE. hallucination\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the best way to begin going into trance?\nA. religious experience\nB. closed eyes\nC. loss of control\nD. sleep\nE. hallucination\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the best way to begin going into trance?\nA. religious experience\nB. closed eyes\nC. loss of control\nD. sleep\nE. hallucination\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the best way to begin going into trance?\nA. religious experience\nB. closed eyes\nC. loss of control\nD. sleep\nE. hallucination\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the best way to begin going into trance?\nA. religious experience\nB. closed eyes\nC. loss of control\nD. sleep\nE. hallucination\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.717592716217041", "False"]], [["-1.717592716217041", "True"]], [["-5.967592716217041", "False"]], [["-4.717592716217041", "False"]], [["-6.217592716217041", "False"]]], "filtered_resps": [["-4.717592716217041", "False"], ["-1.717592716217041", "True"], ["-5.967592716217041", "False"], ["-4.717592716217041", "False"], ["-6.217592716217041", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6cb5f0f190ae0213121d4a9bd0fd58751a1a73374603d3ab4dc56faf5978f89a", "prompt_hash": "1d45f3b27317ba0ae8367a544b2596652e6d5ace943a76629b20be001e6b499b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 721, "doc": {"id": "e1744fc698cffb574e5cf4b29a81ce76", "question": "A computer user working on an important work assignment is located in what structure?", "question_concept": "computer user", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["office building", "internet cafe", "house", "school", "internet cafe"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A computer user working on an important work assignment is located in what structure?\nA. office building\nB. internet cafe\nC. house\nD. school\nE. internet cafe\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A computer user working on an important work assignment is located in what structure?\nA. office building\nB. internet cafe\nC. house\nD. school\nE. internet cafe\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A computer user working on an important work assignment is located in what structure?\nA. office building\nB. internet cafe\nC. house\nD. school\nE. internet cafe\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A computer user working on an important work assignment is located in what structure?\nA. office building\nB. internet cafe\nC. house\nD. school\nE. internet cafe\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A computer user working on an important work assignment is located in what structure?\nA. office building\nB. internet cafe\nC. house\nD. school\nE. internet cafe\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4539633989334106", "True"]], [["-5.453963279724121", "False"]], [["-1.9539633989334106", "False"]], [["-6.203963279724121", "False"]], [["-3.703963279724121", "False"]]], "filtered_resps": [["-1.4539633989334106", "True"], ["-5.453963279724121", "False"], ["-1.9539633989334106", "False"], ["-6.203963279724121", "False"], ["-3.703963279724121", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bc61bbd7b813088dc6e10335496dd4cf1aa983b23050cf841ece2c00678d6e99", "prompt_hash": "b0207e8bee3ede1866881308654401761f909f5ea4d11fa7cdcdf0c731893558", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 722, "doc": {"id": "27604394ccee83e089f9ffae1883cf07", "question": "The music was festive but why are the horses dancing in circles", "question_concept": "music", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["carnival", "night club", "theatre", "opera", "ringmaster"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The music was festive but why are the horses dancing in circles\nA. carnival\nB. night club\nC. theatre\nD. opera\nE. ringmaster\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The music was festive but why are the horses dancing in circles\nA. carnival\nB. night club\nC. theatre\nD. opera\nE. ringmaster\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The music was festive but why are the horses dancing in circles\nA. carnival\nB. night club\nC. theatre\nD. opera\nE. ringmaster\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The music was festive but why are the horses dancing in circles\nA. carnival\nB. night club\nC. theatre\nD. opera\nE. ringmaster\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The music was festive but why are the horses dancing in circles\nA. carnival\nB. night club\nC. theatre\nD. opera\nE. ringmaster\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1558313369750977", "True"]], [["-6.655831336975098", "False"]], [["-6.905831336975098", "False"]], [["-6.655831336975098", "False"]], [["-3.4058313369750977", "False"]]], "filtered_resps": [["-1.1558313369750977", "True"], ["-6.655831336975098", "False"], ["-6.905831336975098", "False"], ["-6.655831336975098", "False"], ["-3.4058313369750977", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7333d55c0f55ac0e6951d73dfe7c3fc08c7a447c7859d004a8bd71a50acf419c", "prompt_hash": "305585ad5eca3d49e5303b94eb1348b3af3cb84b1455e5ce730eb7edbbf7f347", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 723, "doc": {"id": "1272e693cf9152e7ac71095c643676b5", "question": "In the building where James worked there was a small mezzanine in the auditorium to make more space for seats.  Where might James work?", "question_concept": "mezzanine", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["theater", "floors", "concert hall", "education", "school"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: In the building where James worked there was a small mezzanine in the auditorium to make more space for seats.  Where might James work?\nA. theater\nB. floors\nC. concert hall\nD. education\nE. school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: In the building where James worked there was a small mezzanine in the auditorium to make more space for seats.  Where might James work?\nA. theater\nB. floors\nC. concert hall\nD. education\nE. school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: In the building where James worked there was a small mezzanine in the auditorium to make more space for seats.  Where might James work?\nA. theater\nB. floors\nC. concert hall\nD. education\nE. school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: In the building where James worked there was a small mezzanine in the auditorium to make more space for seats.  Where might James work?\nA. theater\nB. floors\nC. concert hall\nD. education\nE. school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: In the building where James worked there was a small mezzanine in the auditorium to make more space for seats.  Where might James work?\nA. theater\nB. floors\nC. concert hall\nD. education\nE. school\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1926307678222656", "False"]], [["-5.442630767822266", "False"]], [["-4.942630767822266", "False"]], [["-1.9426307678222656", "False"]], [["-4.442630767822266", "False"]]], "filtered_resps": [["-3.1926307678222656", "False"], ["-5.442630767822266", "False"], ["-4.942630767822266", "False"], ["-1.9426307678222656", "False"], ["-4.442630767822266", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e83a25bb108189e10d2ca8611832b1ac36f3d22bb02fc795ca59682d11b64e82", "prompt_hash": "6dda010d6dac52303f3107ece2f467b3d39bdfd335b8f4708dd4cb71b5983330", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 724, "doc": {"id": "7bff23f6c12e9136f0961514bebb8cd3", "question": "If you aren't well rested and it's a rainy day what might you do?", "question_concept": "rainy day", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sleep", "write", "make bread", "stay in bed", "enjoy film"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If you aren't well rested and it's a rainy day what might you do?\nA. sleep\nB. write\nC. make bread\nD. stay in bed\nE. enjoy film\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you aren't well rested and it's a rainy day what might you do?\nA. sleep\nB. write\nC. make bread\nD. stay in bed\nE. enjoy film\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you aren't well rested and it's a rainy day what might you do?\nA. sleep\nB. write\nC. make bread\nD. stay in bed\nE. enjoy film\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you aren't well rested and it's a rainy day what might you do?\nA. sleep\nB. write\nC. make bread\nD. stay in bed\nE. enjoy film\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you aren't well rested and it's a rainy day what might you do?\nA. sleep\nB. write\nC. make bread\nD. stay in bed\nE. enjoy film\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.2276649475097656", "False"]], [["-1.9776649475097656", "True"]], [["-3.4776649475097656", "False"]], [["-3.9776649475097656", "False"]], [["-2.2276649475097656", "False"]]], "filtered_resps": [["-2.2276649475097656", "False"], ["-1.9776649475097656", "True"], ["-3.4776649475097656", "False"], ["-3.9776649475097656", "False"], ["-2.2276649475097656", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8e78bb32988a7547f4237f88adf5e64b9dc8f1a3893843d44666b2d0df22b166", "prompt_hash": "299e0b5242c093db7d7df19df9e49827f342772be2afa81e4b27ca3bacc2e10c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 725, "doc": {"id": "20ae70b9b157b298569cd761787833e7", "question": "Where would you have a stove if you don't live in a detached dwelling?", "question_concept": "stove", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tent", "car", "living room", "friend's house", "apartment"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you have a stove if you don't live in a detached dwelling?\nA. tent\nB. car\nC. living room\nD. friend's house\nE. apartment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you have a stove if you don't live in a detached dwelling?\nA. tent\nB. car\nC. living room\nD. friend's house\nE. apartment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you have a stove if you don't live in a detached dwelling?\nA. tent\nB. car\nC. living room\nD. friend's house\nE. apartment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you have a stove if you don't live in a detached dwelling?\nA. tent\nB. car\nC. living room\nD. friend's house\nE. apartment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you have a stove if you don't live in a detached dwelling?\nA. tent\nB. car\nC. living room\nD. friend's house\nE. apartment\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1802523136138916", "False"]], [["-5.1802520751953125", "False"]], [["-4.6802520751953125", "False"]], [["-7.1802520751953125", "False"]], [["-1.1802523136138916", "True"]]], "filtered_resps": [["-3.1802523136138916", "False"], ["-5.1802520751953125", "False"], ["-4.6802520751953125", "False"], ["-7.1802520751953125", "False"], ["-1.1802523136138916", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c02764b046b71a9be8dad4e00d0a3138803120265e906d65c64a7e33be88a269", "prompt_hash": "8e7b09b3e5c7ebc5ae3d23e66796838c2979d5c3428955ffe2ffe25c8085f5ee", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 726, "doc": {"id": "bdd29d7c12e3d795b78ffc048631e7e7", "question": "What kind of place has a revolving door and has things to buy in it?", "question_concept": "revolving door", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["new york", "public place", "bank", "mall", "supermarket door"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of place has a revolving door and has things to buy in it?\nA. new york\nB. public place\nC. bank\nD. mall\nE. supermarket door\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of place has a revolving door and has things to buy in it?\nA. new york\nB. public place\nC. bank\nD. mall\nE. supermarket door\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of place has a revolving door and has things to buy in it?\nA. new york\nB. public place\nC. bank\nD. mall\nE. supermarket door\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of place has a revolving door and has things to buy in it?\nA. new york\nB. public place\nC. bank\nD. mall\nE. supermarket door\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of place has a revolving door and has things to buy in it?\nA. new york\nB. public place\nC. bank\nD. mall\nE. supermarket door\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.594742774963379", "False"]], [["-4.844742774963379", "False"]], [["-6.844742774963379", "False"]], [["-2.344742774963379", "False"]], [["-10.094742774963379", "False"]]], "filtered_resps": [["-1.594742774963379", "False"], ["-4.844742774963379", "False"], ["-6.844742774963379", "False"], ["-2.344742774963379", "False"], ["-10.094742774963379", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "23cadbf41881beda23192cff715206b625290fd9f4fcecd7a084ce6b2aa572e1", "prompt_hash": "55502bbe851373d539e427296eb9a84c6ee9ceca32ab42971678db691a1666f4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 727, "doc": {"id": "cc1a547bdfdcc95e4d632453af14bc96", "question": "Where can books be read?", "question_concept": "books", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cabinet", "backpack", "table", "shelf", "sink"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where can books be read?\nA. cabinet\nB. backpack\nC. table\nD. shelf\nE. sink\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can books be read?\nA. cabinet\nB. backpack\nC. table\nD. shelf\nE. sink\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can books be read?\nA. cabinet\nB. backpack\nC. table\nD. shelf\nE. sink\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can books be read?\nA. cabinet\nB. backpack\nC. table\nD. shelf\nE. sink\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can books be read?\nA. cabinet\nB. backpack\nC. table\nD. shelf\nE. sink\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.158179759979248", "False"]], [["-5.408179759979248", "False"]], [["-2.658179759979248", "False"]], [["-1.908179759979248", "False"]], [["-8.408180236816406", "False"]]], "filtered_resps": [["-4.158179759979248", "False"], ["-5.408179759979248", "False"], ["-2.658179759979248", "False"], ["-1.908179759979248", "False"], ["-8.408180236816406", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e3da2be0fd6514e9ffad6284cb19ce2898da148547217976abed2bbc5b359a94", "prompt_hash": "60cb3a8eac213ed3bbd41f5d58f95702a83627d6d25ad79040f5a5fa4c60671f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 728, "doc": {"id": "896b25dc41f84357add1c798d4a96cd8", "question": "Where is seaweed usually found alive?", "question_concept": "seaweed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ocean", "found in ocean", "water", "found in sea", "beach"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where is seaweed usually found alive?\nA. ocean\nB. found in ocean\nC. water\nD. found in sea\nE. beach\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is seaweed usually found alive?\nA. ocean\nB. found in ocean\nC. water\nD. found in sea\nE. beach\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is seaweed usually found alive?\nA. ocean\nB. found in ocean\nC. water\nD. found in sea\nE. beach\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is seaweed usually found alive?\nA. ocean\nB. found in ocean\nC. water\nD. found in sea\nE. beach\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is seaweed usually found alive?\nA. ocean\nB. found in ocean\nC. water\nD. found in sea\nE. beach\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.696965217590332", "False"]], [["-4.196965217590332", "False"]], [["-5.696965217590332", "False"]], [["-6.696965217590332", "False"]], [["-7.696965217590332", "False"]]], "filtered_resps": [["-1.696965217590332", "False"], ["-4.196965217590332", "False"], ["-5.696965217590332", "False"], ["-6.696965217590332", "False"], ["-7.696965217590332", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dc04d3d7106e5a3ffd6ede1a44f26fc578662df4a4b20543d0f5dd9eaddfa6fc", "prompt_hash": "c9a719009a162ef367aea41b23aeb1f3ab9716d5745294cdd540476367abb268", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 729, "doc": {"id": "1ca3cd9475d7e9da2ddb74911ee2bb68", "question": "If a lizard is fed by people every day, what has happened to it?", "question_concept": "lizard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["documentary", "costa rica", "garden", "encouragement", "captivity"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If a lizard is fed by people every day, what has happened to it?\nA. documentary\nB. costa rica\nC. garden\nD. encouragement\nE. captivity\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a lizard is fed by people every day, what has happened to it?\nA. documentary\nB. costa rica\nC. garden\nD. encouragement\nE. captivity\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a lizard is fed by people every day, what has happened to it?\nA. documentary\nB. costa rica\nC. garden\nD. encouragement\nE. captivity\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a lizard is fed by people every day, what has happened to it?\nA. documentary\nB. costa rica\nC. garden\nD. encouragement\nE. captivity\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a lizard is fed by people every day, what has happened to it?\nA. documentary\nB. costa rica\nC. garden\nD. encouragement\nE. captivity\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.426513671875", "False"]], [["-8.926513671875", "False"]], [["-7.926513671875", "False"]], [["-9.426513671875", "False"]], [["-0.926513671875", "True"]]], "filtered_resps": [["-4.426513671875", "False"], ["-8.926513671875", "False"], ["-7.926513671875", "False"], ["-9.426513671875", "False"], ["-0.926513671875", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e87a67b7696775553741dc992d3fd87538e46ac892bc043f7611a7e8228abc1d", "prompt_hash": "184707373e3ffba08b962f39540edba66072d7c9f6ee7dd8569f2c5beaefbdfc", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 730, "doc": {"id": "129ec46cc2541b73198d774ee632c8d7", "question": "What will happen to someone if his or her spirits cannot elevate?", "question_concept": "elevate", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sadden", "demote", "depress", "drop", "decrease"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What will happen to someone if his or her spirits cannot elevate?\nA. sadden\nB. demote\nC. depress\nD. drop\nE. decrease\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will happen to someone if his or her spirits cannot elevate?\nA. sadden\nB. demote\nC. depress\nD. drop\nE. decrease\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will happen to someone if his or her spirits cannot elevate?\nA. sadden\nB. demote\nC. depress\nD. drop\nE. decrease\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will happen to someone if his or her spirits cannot elevate?\nA. sadden\nB. demote\nC. depress\nD. drop\nE. decrease\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will happen to someone if his or her spirits cannot elevate?\nA. sadden\nB. demote\nC. depress\nD. drop\nE. decrease\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.790231943130493", "False"]], [["-6.540231704711914", "False"]], [["-2.540231943130493", "False"]], [["-4.540231704711914", "False"]], [["-5.040231704711914", "False"]]], "filtered_resps": [["-3.790231943130493", "False"], ["-6.540231704711914", "False"], ["-2.540231943130493", "False"], ["-4.540231704711914", "False"], ["-5.040231704711914", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "245a9803fd1eca471252f104fecb3278fe1b68938f892e7c63c9908e32afaf5b", "prompt_hash": "26f3cffdc5327e8e14ac6a70329abea43075c25d38ea98442a2c929a4be6c938", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 731, "doc": {"id": "0e5c7c0cec5b693e52f74f5f879d84fb", "question": "If you wanted a license to catch crabs, what government office would you go to?", "question_concept": "crab", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["most offices", "fish department", "fancy restaurant", "government submarine", "chesapeake bay"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you wanted a license to catch crabs, what government office would you go to?\nA. most offices\nB. fish department\nC. fancy restaurant\nD. government submarine\nE. chesapeake bay\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you wanted a license to catch crabs, what government office would you go to?\nA. most offices\nB. fish department\nC. fancy restaurant\nD. government submarine\nE. chesapeake bay\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you wanted a license to catch crabs, what government office would you go to?\nA. most offices\nB. fish department\nC. fancy restaurant\nD. government submarine\nE. chesapeake bay\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you wanted a license to catch crabs, what government office would you go to?\nA. most offices\nB. fish department\nC. fancy restaurant\nD. government submarine\nE. chesapeake bay\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you wanted a license to catch crabs, what government office would you go to?\nA. most offices\nB. fish department\nC. fancy restaurant\nD. government submarine\nE. chesapeake bay\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.72144079208374", "False"]], [["-0.9714409708976746", "True"]], [["-7.22144079208374", "False"]], [["-8.971441268920898", "False"]], [["-6.97144079208374", "False"]]], "filtered_resps": [["-4.72144079208374", "False"], ["-0.9714409708976746", "True"], ["-7.22144079208374", "False"], ["-8.971441268920898", "False"], ["-6.97144079208374", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4755ac1e96b2861760364748733e30e91a5654b09904df9f037360e70cd63924", "prompt_hash": "12a9f32cb5f113a4751b81ab129bd87cd2d3a4cde51b390341ff0ac633bb7ea3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 732, "doc": {"id": "af035b75b6f7a1927b1648963f281c5e", "question": "What furniture will you normally find near a side chair?", "question_concept": "side chair", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bedroom", "table", "wheel barrow", "building", "office"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What furniture will you normally find near a side chair?\nA. bedroom\nB. table\nC. wheel barrow\nD. building\nE. office\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What furniture will you normally find near a side chair?\nA. bedroom\nB. table\nC. wheel barrow\nD. building\nE. office\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What furniture will you normally find near a side chair?\nA. bedroom\nB. table\nC. wheel barrow\nD. building\nE. office\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What furniture will you normally find near a side chair?\nA. bedroom\nB. table\nC. wheel barrow\nD. building\nE. office\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What furniture will you normally find near a side chair?\nA. bedroom\nB. table\nC. wheel barrow\nD. building\nE. office\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6559882164001465", "False"]], [["-0.6559882164001465", "True"]], [["-6.9059882164001465", "False"]], [["-8.155988693237305", "False"]], [["-8.655988693237305", "False"]]], "filtered_resps": [["-3.6559882164001465", "False"], ["-0.6559882164001465", "True"], ["-6.9059882164001465", "False"], ["-8.155988693237305", "False"], ["-8.655988693237305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bd51c2485aa2c910cece0f5a9a26341f1a6b1d05edde62116cb10d8853b09c20", "prompt_hash": "f850e07186f72bced012878832bd649066110427f133184300968fc6f5a00641", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 733, "doc": {"id": "32d5b7fcae24f0d4871cfb219c5a4b47", "question": "Metal is used to make what?", "question_concept": "metal", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["junkyard", "ore", "instruments", "metal fabrication shop", "bowls"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Metal is used to make what?\nA. junkyard\nB. ore\nC. instruments\nD. metal fabrication shop\nE. bowls\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Metal is used to make what?\nA. junkyard\nB. ore\nC. instruments\nD. metal fabrication shop\nE. bowls\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Metal is used to make what?\nA. junkyard\nB. ore\nC. instruments\nD. metal fabrication shop\nE. bowls\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Metal is used to make what?\nA. junkyard\nB. ore\nC. instruments\nD. metal fabrication shop\nE. bowls\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Metal is used to make what?\nA. junkyard\nB. ore\nC. instruments\nD. metal fabrication shop\nE. bowls\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.66884183883667", "False"]], [["-4.66884183883667", "False"]], [["-2.16884183883667", "False"]], [["-5.91884183883667", "False"]], [["-3.91884183883667", "False"]]], "filtered_resps": [["-4.66884183883667", "False"], ["-4.66884183883667", "False"], ["-2.16884183883667", "False"], ["-5.91884183883667", "False"], ["-3.91884183883667", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2d42f0b1a8d1d59cc2960db0d0bd3b8ac86efbd06c6a7947febbdd386cd72203", "prompt_hash": "54bf9d0a642081a9bfa909952fb5f0d3c17f59a12546392b37cf8e01ff0b5113", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 734, "doc": {"id": "87505da761eaa5c3c4703d02a12d46bc", "question": "What is the word added to Manchester that signifies what county it is in?", "question_concept": "manchester", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["england", "united kingdome", "lancashire", "greater manchester", "cheshire"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is the word added to Manchester that signifies what county it is in?\nA. england\nB. united kingdome\nC. lancashire\nD. greater manchester\nE. cheshire\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the word added to Manchester that signifies what county it is in?\nA. england\nB. united kingdome\nC. lancashire\nD. greater manchester\nE. cheshire\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the word added to Manchester that signifies what county it is in?\nA. england\nB. united kingdome\nC. lancashire\nD. greater manchester\nE. cheshire\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the word added to Manchester that signifies what county it is in?\nA. england\nB. united kingdome\nC. lancashire\nD. greater manchester\nE. cheshire\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the word added to Manchester that signifies what county it is in?\nA. england\nB. united kingdome\nC. lancashire\nD. greater manchester\nE. cheshire\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.093620777130127", "False"]], [["-7.093620777130127", "False"]], [["-1.8436208963394165", "False"]], [["-7.093620777130127", "False"]], [["-8.843621253967285", "False"]]], "filtered_resps": [["-5.093620777130127", "False"], ["-7.093620777130127", "False"], ["-1.8436208963394165", "False"], ["-7.093620777130127", "False"], ["-8.843621253967285", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "48ec66eebee83444b6e3d20b966c6d6204c5c6ed09ab031ab5b503edb4ca5922", "prompt_hash": "a4a258a585555b53d04b2b3cfc109a070d2f13338930d15028ae6ae2529ee1b6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 735, "doc": {"id": "ef3d5d35128678937c36438466e0fc93", "question": "The program kept getting errors, the amateur end user began to what?", "question_concept": "program", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get mad", "compile", "debug", "write code", "get frustrated"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The program kept getting errors, the amateur end user began to what?\nA. get mad\nB. compile\nC. debug\nD. write code\nE. get frustrated\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The program kept getting errors, the amateur end user began to what?\nA. get mad\nB. compile\nC. debug\nD. write code\nE. get frustrated\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The program kept getting errors, the amateur end user began to what?\nA. get mad\nB. compile\nC. debug\nD. write code\nE. get frustrated\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The program kept getting errors, the amateur end user began to what?\nA. get mad\nB. compile\nC. debug\nD. write code\nE. get frustrated\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The program kept getting errors, the amateur end user began to what?\nA. get mad\nB. compile\nC. debug\nD. write code\nE. get frustrated\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.355198383331299", "False"]], [["-5.105198383331299", "False"]], [["-5.355198383331299", "False"]], [["-7.105198383331299", "False"]], [["-1.6051983833312988", "True"]]], "filtered_resps": [["-2.355198383331299", "False"], ["-5.105198383331299", "False"], ["-5.355198383331299", "False"], ["-7.105198383331299", "False"], ["-1.6051983833312988", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "974d8d35c6d16eb55b656de64e8ec1deadd13c0944e2f5b70871b4a7d051870d", "prompt_hash": "44827d3392c3571894ddd743bf073f96ee82dd4afa7a97aad52a92b96281a422", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 736, "doc": {"id": "4f1d8007b446b0e10f07fd63cbd31b6f", "question": "John knew that the sun produced a massive amount of energy in two forms.  If you were on the surface of the sun, what would kill you first?", "question_concept": "sun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ocean", "heat", "life on earth", "wrinkles", "light"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: John knew that the sun produced a massive amount of energy in two forms.  If you were on the surface of the sun, what would kill you first?\nA. ocean\nB. heat\nC. life on earth\nD. wrinkles\nE. light\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John knew that the sun produced a massive amount of energy in two forms.  If you were on the surface of the sun, what would kill you first?\nA. ocean\nB. heat\nC. life on earth\nD. wrinkles\nE. light\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John knew that the sun produced a massive amount of energy in two forms.  If you were on the surface of the sun, what would kill you first?\nA. ocean\nB. heat\nC. life on earth\nD. wrinkles\nE. light\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John knew that the sun produced a massive amount of energy in two forms.  If you were on the surface of the sun, what would kill you first?\nA. ocean\nB. heat\nC. life on earth\nD. wrinkles\nE. light\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John knew that the sun produced a massive amount of energy in two forms.  If you were on the surface of the sun, what would kill you first?\nA. ocean\nB. heat\nC. life on earth\nD. wrinkles\nE. light\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.063747406005859", "False"]], [["-1.0637471675872803", "True"]], [["-7.563747406005859", "False"]], [["-6.563747406005859", "False"]], [["-7.313747406005859", "False"]]], "filtered_resps": [["-6.063747406005859", "False"], ["-1.0637471675872803", "True"], ["-7.563747406005859", "False"], ["-6.563747406005859", "False"], ["-7.313747406005859", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "47fe2e31e7a8da845ef366cdd518b4a796857291c4b1889de4d2bc5956827005", "prompt_hash": "7061216ce338d94dc50b625a0c725b44216933ec548ce3edf00c6ab781f31098", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 737, "doc": {"id": "4c30d5eed4137cba89747510973f37a3", "question": "Lawyers often talk in front of an audience where?", "question_concept": "lawyers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["work", "courtroom", "office building", "press charges", "theatre"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Lawyers often talk in front of an audience where?\nA. work\nB. courtroom\nC. office building\nD. press charges\nE. theatre\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Lawyers often talk in front of an audience where?\nA. work\nB. courtroom\nC. office building\nD. press charges\nE. theatre\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Lawyers often talk in front of an audience where?\nA. work\nB. courtroom\nC. office building\nD. press charges\nE. theatre\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Lawyers often talk in front of an audience where?\nA. work\nB. courtroom\nC. office building\nD. press charges\nE. theatre\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Lawyers often talk in front of an audience where?\nA. work\nB. courtroom\nC. office building\nD. press charges\nE. theatre\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.777743816375732", "False"]], [["-1.2777438163757324", "True"]], [["-7.527743816375732", "False"]], [["-8.52774429321289", "False"]], [["-7.777743816375732", "False"]]], "filtered_resps": [["-4.777743816375732", "False"], ["-1.2777438163757324", "True"], ["-7.527743816375732", "False"], ["-8.52774429321289", "False"], ["-7.777743816375732", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "47ab811758a1ff448b3a50affe91db75e43c03c64ba73922b425e1097e3ea74e", "prompt_hash": "fd696cb04e7b842807d958838d0a4d7a578c2fa1b5ffe0cfef923e9619a3575c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 738, "doc": {"id": "515834727e23e30ab7c8fe5ba7e9a765", "question": "James bought a new set of tire chains and put them somewhere he could find them.  Where would he put them?", "question_concept": "chain", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["gear shift", "garage", "kitchen", "jewelry store", "hardware store"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: James bought a new set of tire chains and put them somewhere he could find them.  Where would he put them?\nA. gear shift\nB. garage\nC. kitchen\nD. jewelry store\nE. hardware store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James bought a new set of tire chains and put them somewhere he could find them.  Where would he put them?\nA. gear shift\nB. garage\nC. kitchen\nD. jewelry store\nE. hardware store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James bought a new set of tire chains and put them somewhere he could find them.  Where would he put them?\nA. gear shift\nB. garage\nC. kitchen\nD. jewelry store\nE. hardware store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James bought a new set of tire chains and put them somewhere he could find them.  Where would he put them?\nA. gear shift\nB. garage\nC. kitchen\nD. jewelry store\nE. hardware store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James bought a new set of tire chains and put them somewhere he could find them.  Where would he put them?\nA. gear shift\nB. garage\nC. kitchen\nD. jewelry store\nE. hardware store\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.537035942077637", "False"]], [["-1.5370357036590576", "False"]], [["-6.787035942077637", "False"]], [["-8.287035942077637", "False"]], [["-7.787035942077637", "False"]]], "filtered_resps": [["-4.537035942077637", "False"], ["-1.5370357036590576", "False"], ["-6.787035942077637", "False"], ["-8.287035942077637", "False"], ["-7.787035942077637", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "68425ff4de124e6ac0b22485922df1b0be52adb7dcfc92a6cdbcacd2a3e5a832", "prompt_hash": "e2fef188f05a3ffb076c4031091d55f440248a98478dca88ac639d2d8b0b4f37", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 739, "doc": {"id": "34ec6393db5a01f689c11fac153e31c1", "question": "If I wanted to eat something that is made from plants and needs to be washed, what would it be?", "question_concept": "plants", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["roots", "millions of cells", "see work", "leaves to gather light", "flowers on"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If I wanted to eat something that is made from plants and needs to be washed, what would it be?\nA. roots\nB. millions of cells\nC. see work\nD. leaves to gather light\nE. flowers on\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I wanted to eat something that is made from plants and needs to be washed, what would it be?\nA. roots\nB. millions of cells\nC. see work\nD. leaves to gather light\nE. flowers on\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I wanted to eat something that is made from plants and needs to be washed, what would it be?\nA. roots\nB. millions of cells\nC. see work\nD. leaves to gather light\nE. flowers on\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I wanted to eat something that is made from plants and needs to be washed, what would it be?\nA. roots\nB. millions of cells\nC. see work\nD. leaves to gather light\nE. flowers on\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I wanted to eat something that is made from plants and needs to be washed, what would it be?\nA. roots\nB. millions of cells\nC. see work\nD. leaves to gather light\nE. flowers on\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1721153259277344", "True"]], [["-4.422115325927734", "False"]], [["-3.9221153259277344", "False"]], [["-3.9221153259277344", "False"]], [["-4.172115325927734", "False"]]], "filtered_resps": [["-1.1721153259277344", "True"], ["-4.422115325927734", "False"], ["-3.9221153259277344", "False"], ["-3.9221153259277344", "False"], ["-4.172115325927734", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "34a35284f2904a022ed9300a6ac048d10a6a24e06c39e38281749516e334ec5f", "prompt_hash": "412fbe1fb19c919e41e14b74f3109258359d62d61af7bddb400dd310fb8f6595", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 740, "doc": {"id": "0f0e339412f719a019bf373e6daf2530", "question": "Ficus can be planted in a yard to make summer more bearable, what sort of areas do they create?", "question_concept": "ficus", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shady places", "screened porch", "pots", "ceramics", "clay pot"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Ficus can be planted in a yard to make summer more bearable, what sort of areas do they create?\nA. shady places\nB. screened porch\nC. pots\nD. ceramics\nE. clay pot\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Ficus can be planted in a yard to make summer more bearable, what sort of areas do they create?\nA. shady places\nB. screened porch\nC. pots\nD. ceramics\nE. clay pot\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Ficus can be planted in a yard to make summer more bearable, what sort of areas do they create?\nA. shady places\nB. screened porch\nC. pots\nD. ceramics\nE. clay pot\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Ficus can be planted in a yard to make summer more bearable, what sort of areas do they create?\nA. shady places\nB. screened porch\nC. pots\nD. ceramics\nE. clay pot\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Ficus can be planted in a yard to make summer more bearable, what sort of areas do they create?\nA. shady places\nB. screened porch\nC. pots\nD. ceramics\nE. clay pot\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6040065288543701", "False"]], [["-2.85400652885437", "False"]], [["-3.85400652885437", "False"]], [["-7.104006767272949", "False"]], [["-7.604006767272949", "False"]]], "filtered_resps": [["-1.6040065288543701", "False"], ["-2.85400652885437", "False"], ["-3.85400652885437", "False"], ["-7.104006767272949", "False"], ["-7.604006767272949", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "45ed8c50ecfa30e1d4eef2dbca1929b0c6a8c57ec12848f37e3c4ed65dd1c374", "prompt_hash": "59b3c1d22445c6d5d9a923d7341e4de1a5fc861b4c8f6661d33b2122b13a290f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 741, "doc": {"id": "489a082aab43dd1a53f3f1f89c2365ed", "question": "Children's behavior is a direct reflection of their what?", "question_concept": "children", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["parents", "old people", "play ball", "many adults", "grown ups"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Children's behavior is a direct reflection of their what?\nA. parents\nB. old people\nC. play ball\nD. many adults\nE. grown ups\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Children's behavior is a direct reflection of their what?\nA. parents\nB. old people\nC. play ball\nD. many adults\nE. grown ups\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Children's behavior is a direct reflection of their what?\nA. parents\nB. old people\nC. play ball\nD. many adults\nE. grown ups\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Children's behavior is a direct reflection of their what?\nA. parents\nB. old people\nC. play ball\nD. many adults\nE. grown ups\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Children's behavior is a direct reflection of their what?\nA. parents\nB. old people\nC. play ball\nD. many adults\nE. grown ups\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2206940650939941", "True"]], [["-4.470694065093994", "False"]], [["-4.470694065093994", "False"]], [["-5.970694065093994", "False"]], [["-3.470694065093994", "False"]]], "filtered_resps": [["-1.2206940650939941", "True"], ["-4.470694065093994", "False"], ["-4.470694065093994", "False"], ["-5.970694065093994", "False"], ["-3.470694065093994", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3d0b72471460add3265f156c6ece8071c5d13bd6106f62dd2559ea5fa8664c14", "prompt_hash": "4386c5029efc5cefca46e2bdbb8c6cdc98d5f70730ba998be273370c850d4f9e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 742, "doc": {"id": "7c45033e9fd9f1a759923971b14390ed", "question": "Most people who are family like to greet each other with a what?", "question_concept": "most people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["apartments", "listen to music", "have friends", "know what ophiolites", "hug"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Most people who are family like to greet each other with a what?\nA. apartments\nB. listen to music\nC. have friends\nD. know what ophiolites\nE. hug\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Most people who are family like to greet each other with a what?\nA. apartments\nB. listen to music\nC. have friends\nD. know what ophiolites\nE. hug\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Most people who are family like to greet each other with a what?\nA. apartments\nB. listen to music\nC. have friends\nD. know what ophiolites\nE. hug\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Most people who are family like to greet each other with a what?\nA. apartments\nB. listen to music\nC. have friends\nD. know what ophiolites\nE. hug\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Most people who are family like to greet each other with a what?\nA. apartments\nB. listen to music\nC. have friends\nD. know what ophiolites\nE. hug\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.22544527053833", "False"]], [["-6.47544527053833", "False"]], [["-7.72544527053833", "False"]], [["-7.72544527053833", "False"]], [["-0.9754452705383301", "True"]]], "filtered_resps": [["-4.22544527053833", "False"], ["-6.47544527053833", "False"], ["-7.72544527053833", "False"], ["-7.72544527053833", "False"], ["-0.9754452705383301", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5da49251ffc7463b1b5c644a67aa5940e8650cb2cd185f4b05b3b3f151465a49", "prompt_hash": "508e70b32d8e81b193ef202ed57bd46681b529a2d92fee742af80a6b1590973d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 743, "doc": {"id": "061f326d2a87a10da6316b55bd5522bd", "question": "John bought a new water hose.  But he found his old one near his car.  Where did he find the old one?", "question_concept": "hose", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["garden shed", "hardware store", "greenhouse", "garage", "in a van"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: John bought a new water hose.  But he found his old one near his car.  Where did he find the old one?\nA. garden shed\nB. hardware store\nC. greenhouse\nD. garage\nE. in a van\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John bought a new water hose.  But he found his old one near his car.  Where did he find the old one?\nA. garden shed\nB. hardware store\nC. greenhouse\nD. garage\nE. in a van\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John bought a new water hose.  But he found his old one near his car.  Where did he find the old one?\nA. garden shed\nB. hardware store\nC. greenhouse\nD. garage\nE. in a van\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John bought a new water hose.  But he found his old one near his car.  Where did he find the old one?\nA. garden shed\nB. hardware store\nC. greenhouse\nD. garage\nE. in a van\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John bought a new water hose.  But he found his old one near his car.  Where did he find the old one?\nA. garden shed\nB. hardware store\nC. greenhouse\nD. garage\nE. in a van\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.990337371826172", "False"]], [["-6.740337371826172", "False"]], [["-7.490337371826172", "False"]], [["-2.490337371826172", "False"]], [["-9.990337371826172", "False"]]], "filtered_resps": [["-2.990337371826172", "False"], ["-6.740337371826172", "False"], ["-7.490337371826172", "False"], ["-2.490337371826172", "False"], ["-9.990337371826172", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5a9fcc565478ade87e629bb1da0f1f21950942102a10d7fe0dff7e13b981436e", "prompt_hash": "77447cf1e3a85e7724188a989ce7a4a14c13ad448a682683280c2af32ab2934e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 744, "doc": {"id": "d747c4e463b80bfcc49b874063f9fae1", "question": "Where is a control room needed to prevent wide spread disaster?", "question_concept": "control room", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["airbase", "prison", "mill", "nuclear plant", "recording studio"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a control room needed to prevent wide spread disaster?\nA. airbase\nB. prison\nC. mill\nD. nuclear plant\nE. recording studio\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a control room needed to prevent wide spread disaster?\nA. airbase\nB. prison\nC. mill\nD. nuclear plant\nE. recording studio\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a control room needed to prevent wide spread disaster?\nA. airbase\nB. prison\nC. mill\nD. nuclear plant\nE. recording studio\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a control room needed to prevent wide spread disaster?\nA. airbase\nB. prison\nC. mill\nD. nuclear plant\nE. recording studio\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a control room needed to prevent wide spread disaster?\nA. airbase\nB. prison\nC. mill\nD. nuclear plant\nE. recording studio\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.9724431037902832", "False"]], [["-8.472443580627441", "False"]], [["-6.972443103790283", "False"]], [["-1.4724431037902832", "False"]], [["-9.972443580627441", "False"]]], "filtered_resps": [["-1.9724431037902832", "False"], ["-8.472443580627441", "False"], ["-6.972443103790283", "False"], ["-1.4724431037902832", "False"], ["-9.972443580627441", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bf2486bb97d89c8e07d5b8c2ac6f013a68248fb9d45505c7ddedb99a53d544ea", "prompt_hash": "1394d3ef1f80e453e29905abc32e447d385e1f01c628e01cbae4b54991292f60", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 745, "doc": {"id": "df3d27338bcf86b341b8b02d4309daf5", "question": "Where do you keep your pizza slice before you eat it?", "question_concept": "pizza", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["table", "plate", "restaurant", "oven", "popular"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you keep your pizza slice before you eat it?\nA. table\nB. plate\nC. restaurant\nD. oven\nE. popular\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you keep your pizza slice before you eat it?\nA. table\nB. plate\nC. restaurant\nD. oven\nE. popular\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you keep your pizza slice before you eat it?\nA. table\nB. plate\nC. restaurant\nD. oven\nE. popular\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you keep your pizza slice before you eat it?\nA. table\nB. plate\nC. restaurant\nD. oven\nE. popular\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you keep your pizza slice before you eat it?\nA. table\nB. plate\nC. restaurant\nD. oven\nE. popular\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.185110569000244", "False"]], [["-1.4351105690002441", "True"]], [["-7.935110569000244", "False"]], [["-7.685110569000244", "False"]], [["-8.685110092163086", "False"]]], "filtered_resps": [["-2.185110569000244", "False"], ["-1.4351105690002441", "True"], ["-7.935110569000244", "False"], ["-7.685110569000244", "False"], ["-8.685110092163086", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e723f184725da53bb47eef092cbe486d73f7e6336607850126bfd79115b22494", "prompt_hash": "1546598ef65458e6f5fe14e3ff41bb7aca7855a01576c12357afe4d7f9e54c28", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 746, "doc": {"id": "db63bf66a8bfd16e5103cbdd350f5202", "question": "Everybody was changing into costumes in the dressing room, it was almost time to take the what stage?", "question_concept": "dressing room", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["theater", "train", "bathhouse", "dwelling", "actors and actresses"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Everybody was changing into costumes in the dressing room, it was almost time to take the what stage?\nA. theater\nB. train\nC. bathhouse\nD. dwelling\nE. actors and actresses\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Everybody was changing into costumes in the dressing room, it was almost time to take the what stage?\nA. theater\nB. train\nC. bathhouse\nD. dwelling\nE. actors and actresses\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Everybody was changing into costumes in the dressing room, it was almost time to take the what stage?\nA. theater\nB. train\nC. bathhouse\nD. dwelling\nE. actors and actresses\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Everybody was changing into costumes in the dressing room, it was almost time to take the what stage?\nA. theater\nB. train\nC. bathhouse\nD. dwelling\nE. actors and actresses\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Everybody was changing into costumes in the dressing room, it was almost time to take the what stage?\nA. theater\nB. train\nC. bathhouse\nD. dwelling\nE. actors and actresses\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9661843776702881", "True"]], [["-5.966184616088867", "False"]], [["-6.216184616088867", "False"]], [["-6.966184616088867", "False"]], [["-5.966184616088867", "False"]]], "filtered_resps": [["-0.9661843776702881", "True"], ["-5.966184616088867", "False"], ["-6.216184616088867", "False"], ["-6.966184616088867", "False"], ["-5.966184616088867", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5293cf44524cdf35f707e53149669b5ceb9f8c834aba14635c667ad4eb3a8954", "prompt_hash": "081b920cc7efd5530145f3642a8b6f7a3c1182cf1d8488c7824875d9887defb3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 747, "doc": {"id": "f8a9208665a4f2d64986940456b4b964", "question": "The homeowner frowned at the price of gas, what did he have to do later?", "question_concept": "homeowner", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["own home", "mail property tax payments", "board windows", "cut grass", "receive mail"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The homeowner frowned at the price of gas, what did he have to do later?\nA. own home\nB. mail property tax payments\nC. board windows\nD. cut grass\nE. receive mail\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The homeowner frowned at the price of gas, what did he have to do later?\nA. own home\nB. mail property tax payments\nC. board windows\nD. cut grass\nE. receive mail\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The homeowner frowned at the price of gas, what did he have to do later?\nA. own home\nB. mail property tax payments\nC. board windows\nD. cut grass\nE. receive mail\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The homeowner frowned at the price of gas, what did he have to do later?\nA. own home\nB. mail property tax payments\nC. board windows\nD. cut grass\nE. receive mail\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The homeowner frowned at the price of gas, what did he have to do later?\nA. own home\nB. mail property tax payments\nC. board windows\nD. cut grass\nE. receive mail\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2317819595336914", "False"]], [["-1.731782078742981", "True"]], [["-3.4817819595336914", "False"]], [["-2.4817819595336914", "False"]], [["-4.231781959533691", "False"]]], "filtered_resps": [["-3.2317819595336914", "False"], ["-1.731782078742981", "True"], ["-3.4817819595336914", "False"], ["-2.4817819595336914", "False"], ["-4.231781959533691", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9429f0d5b16bc39c586adbb4615e705c860ffcafce13cda8703efa732bd5823b", "prompt_hash": "cd5317010b3b3018b3c15fcf73b8bdea3160273be1b88dfc21161451e65491c1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 748, "doc": {"id": "1bf4c6b5bd870b1a079106e1e97e5d09", "question": "A thoroughfare meandered through fields and woods, where was it passing though?", "question_concept": "thoroughfare", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["move about", "city", "country", "town", "new york city"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: A thoroughfare meandered through fields and woods, where was it passing though?\nA. move about\nB. city\nC. country\nD. town\nE. new york city\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A thoroughfare meandered through fields and woods, where was it passing though?\nA. move about\nB. city\nC. country\nD. town\nE. new york city\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A thoroughfare meandered through fields and woods, where was it passing though?\nA. move about\nB. city\nC. country\nD. town\nE. new york city\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A thoroughfare meandered through fields and woods, where was it passing though?\nA. move about\nB. city\nC. country\nD. town\nE. new york city\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A thoroughfare meandered through fields and woods, where was it passing though?\nA. move about\nB. city\nC. country\nD. town\nE. new york city\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.688052177429199", "False"]], [["-5.188052177429199", "False"]], [["-1.1880521774291992", "True"]], [["-4.438052177429199", "False"]], [["-5.438052177429199", "False"]]], "filtered_resps": [["-2.688052177429199", "False"], ["-5.188052177429199", "False"], ["-1.1880521774291992", "True"], ["-4.438052177429199", "False"], ["-5.438052177429199", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9a0755e8a872dbe82626a35cc772255759c8a81826911e6ccc346d9d779d16a0", "prompt_hash": "7da2bc3e63ac39779de0f9abbe53a307ca00e68a8eb85e2ab9a4ebec6ea9735a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 749, "doc": {"id": "c1c73ef0ff662a76cd42c3500240974a", "question": "If I want a new ottoman, where should I go?", "question_concept": "ottoman", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["furniture store", "parlor", "turkey", "living room", "den"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If I want a new ottoman, where should I go?\nA. furniture store\nB. parlor\nC. turkey\nD. living room\nE. den\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I want a new ottoman, where should I go?\nA. furniture store\nB. parlor\nC. turkey\nD. living room\nE. den\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I want a new ottoman, where should I go?\nA. furniture store\nB. parlor\nC. turkey\nD. living room\nE. den\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I want a new ottoman, where should I go?\nA. furniture store\nB. parlor\nC. turkey\nD. living room\nE. den\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I want a new ottoman, where should I go?\nA. furniture store\nB. parlor\nC. turkey\nD. living room\nE. den\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7779048681259155", "True"]], [["-6.527904987335205", "False"]], [["-8.027904510498047", "False"]], [["-9.277904510498047", "False"]], [["-10.277904510498047", "False"]]], "filtered_resps": [["-0.7779048681259155", "True"], ["-6.527904987335205", "False"], ["-8.027904510498047", "False"], ["-9.277904510498047", "False"], ["-10.277904510498047", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d0dc850bb832f64b41cf11c33ad29cb65b1cd286403c93efcb9639358010ae73", "prompt_hash": "51a32b75238caeebd72a2afce01ba782766139e55c691a6646d1e2f0691c1316", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 750, "doc": {"id": "aefa60233f3c5c4966f8ac22e901a1c7", "question": "Sean was leaving work and took the roadway that led to his what?", "question_concept": "roadway", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["neighborhood", "city", "fate", "countryside", "maps"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Sean was leaving work and took the roadway that led to his what?\nA. neighborhood\nB. city\nC. fate\nD. countryside\nE. maps\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sean was leaving work and took the roadway that led to his what?\nA. neighborhood\nB. city\nC. fate\nD. countryside\nE. maps\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sean was leaving work and took the roadway that led to his what?\nA. neighborhood\nB. city\nC. fate\nD. countryside\nE. maps\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sean was leaving work and took the roadway that led to his what?\nA. neighborhood\nB. city\nC. fate\nD. countryside\nE. maps\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sean was leaving work and took the roadway that led to his what?\nA. neighborhood\nB. city\nC. fate\nD. countryside\nE. maps\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6354427337646484", "True"]], [["-2.8854427337646484", "False"]], [["-4.635442733764648", "False"]], [["-4.885442733764648", "False"]], [["-6.385442733764648", "False"]]], "filtered_resps": [["-1.6354427337646484", "True"], ["-2.8854427337646484", "False"], ["-4.635442733764648", "False"], ["-4.885442733764648", "False"], ["-6.385442733764648", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9f659afd1e13592f86fc48d4e246258081ca60ecdee086669e7d52f15c1780fe", "prompt_hash": "fc2280fe3b70d4ea496a183f46c8ce689631ab1a2b6cef9546d72bbbd1f5b581", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 751, "doc": {"id": "9221962ed3a6094e5c8f33785ce048cd", "question": "What can you use to get a jellyfish?", "question_concept": "jellyfish", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["adriatic sea", "mediterranean sea", "hand", "see", "atlantic ocean"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What can you use to get a jellyfish?\nA. adriatic sea\nB. mediterranean sea\nC. hand\nD. see\nE. atlantic ocean\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can you use to get a jellyfish?\nA. adriatic sea\nB. mediterranean sea\nC. hand\nD. see\nE. atlantic ocean\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can you use to get a jellyfish?\nA. adriatic sea\nB. mediterranean sea\nC. hand\nD. see\nE. atlantic ocean\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can you use to get a jellyfish?\nA. adriatic sea\nB. mediterranean sea\nC. hand\nD. see\nE. atlantic ocean\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can you use to get a jellyfish?\nA. adriatic sea\nB. mediterranean sea\nC. hand\nD. see\nE. atlantic ocean\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.922330617904663", "False"]], [["-4.172330856323242", "False"]], [["-1.922330617904663", "True"]], [["-2.672330617904663", "False"]], [["-4.922330856323242", "False"]]], "filtered_resps": [["-2.922330617904663", "False"], ["-4.172330856323242", "False"], ["-1.922330617904663", "True"], ["-2.672330617904663", "False"], ["-4.922330856323242", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "42d9ae85e511da7b3999446eed97c39ce54c75dc847904a7a55e36ad4a9ba8d3", "prompt_hash": "2413db5b370924553976f63b1f176d708d67ac38eea385e59705be59ebe31b62", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 752, "doc": {"id": "8c8052980e357545398d27d1c3c832d8", "question": "What has a shelf that does not allow you to see what is inside of it?", "question_concept": "shelf", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["chest of drawers", "stove", "hold alcohol", "bookcase", "grocery store"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What has a shelf that does not allow you to see what is inside of it?\nA. chest of drawers\nB. stove\nC. hold alcohol\nD. bookcase\nE. grocery store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What has a shelf that does not allow you to see what is inside of it?\nA. chest of drawers\nB. stove\nC. hold alcohol\nD. bookcase\nE. grocery store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What has a shelf that does not allow you to see what is inside of it?\nA. chest of drawers\nB. stove\nC. hold alcohol\nD. bookcase\nE. grocery store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What has a shelf that does not allow you to see what is inside of it?\nA. chest of drawers\nB. stove\nC. hold alcohol\nD. bookcase\nE. grocery store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What has a shelf that does not allow you to see what is inside of it?\nA. chest of drawers\nB. stove\nC. hold alcohol\nD. bookcase\nE. grocery store\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8898640871047974", "True"]], [["-3.389863967895508", "False"]], [["-2.639863967895508", "False"]], [["-3.889863967895508", "False"]], [["-5.389863967895508", "False"]]], "filtered_resps": [["-0.8898640871047974", "True"], ["-3.389863967895508", "False"], ["-2.639863967895508", "False"], ["-3.889863967895508", "False"], ["-5.389863967895508", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4266e406ca1c3303733a6e53858ff26285e1bc38c12f1b262a18e9f9dce3eaea", "prompt_hash": "607fdb1cf3bd444caab95e7793130c56a293ae451376b91ac0707c6320f6e7d6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 753, "doc": {"id": "418913999c665ae9527fd14a6132da39", "question": "What will likely happen after stabbing to death a person?", "question_concept": "stabbing to death", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["gruesome", "being arrested", "pool of blood", "mess", "grisly"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What will likely happen after stabbing to death a person?\nA. gruesome\nB. being arrested\nC. pool of blood\nD. mess\nE. grisly\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will likely happen after stabbing to death a person?\nA. gruesome\nB. being arrested\nC. pool of blood\nD. mess\nE. grisly\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will likely happen after stabbing to death a person?\nA. gruesome\nB. being arrested\nC. pool of blood\nD. mess\nE. grisly\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will likely happen after stabbing to death a person?\nA. gruesome\nB. being arrested\nC. pool of blood\nD. mess\nE. grisly\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will likely happen after stabbing to death a person?\nA. gruesome\nB. being arrested\nC. pool of blood\nD. mess\nE. grisly\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.0681300163269043", "False"]], [["-3.0681300163269043", "False"]], [["-5.568130016326904", "False"]], [["-6.068130016326904", "False"]], [["-2.8181300163269043", "False"]]], "filtered_resps": [["-2.0681300163269043", "False"], ["-3.0681300163269043", "False"], ["-5.568130016326904", "False"], ["-6.068130016326904", "False"], ["-2.8181300163269043", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "01b3401a299deba3ae22db3fb38dc9425ae9160839fbd9fc30b99c9a70b1207e", "prompt_hash": "89e785425572a7082d7f17f7cbc2cf6e37474477d791bb470fd1a43dfb65f409", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 754, "doc": {"id": "2634468d21fa33a88cefe28a5d613f59", "question": "The boat passenger was explaining his fear of blowfish, but the captain figured he meant piranhas since they were on a river in the what?", "question_concept": "blowfish", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cuba", "styx", "atlantic ocean", "france", "jungle"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The boat passenger was explaining his fear of blowfish, but the captain figured he meant piranhas since they were on a river in the what?\nA. cuba\nB. styx\nC. atlantic ocean\nD. france\nE. jungle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The boat passenger was explaining his fear of blowfish, but the captain figured he meant piranhas since they were on a river in the what?\nA. cuba\nB. styx\nC. atlantic ocean\nD. france\nE. jungle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The boat passenger was explaining his fear of blowfish, but the captain figured he meant piranhas since they were on a river in the what?\nA. cuba\nB. styx\nC. atlantic ocean\nD. france\nE. jungle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The boat passenger was explaining his fear of blowfish, but the captain figured he meant piranhas since they were on a river in the what?\nA. cuba\nB. styx\nC. atlantic ocean\nD. france\nE. jungle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The boat passenger was explaining his fear of blowfish, but the captain figured he meant piranhas since they were on a river in the what?\nA. cuba\nB. styx\nC. atlantic ocean\nD. france\nE. jungle\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.394288063049316", "False"]], [["-2.3942883014678955", "False"]], [["-4.894288063049316", "False"]], [["-6.394288063049316", "False"]], [["-1.8942883014678955", "False"]]], "filtered_resps": [["-4.394288063049316", "False"], ["-2.3942883014678955", "False"], ["-4.894288063049316", "False"], ["-6.394288063049316", "False"], ["-1.8942883014678955", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "69b8ab5d932937e5954fdd230ecd28dde48c1df92e2e36c66f1a5ca1ef1d6076", "prompt_hash": "233a5e5b83170eb8c8e28d64d5a8ed843df2c578225cbca868c56f71887e2ffd", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 755, "doc": {"id": "66bfb6e209c94e6be5b0d04b0c7e2064", "question": "Where could you find only a few office?", "question_concept": "office", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["skyscraper", "new york", "school building", "city", "work"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where could you find only a few office?\nA. skyscraper\nB. new york\nC. school building\nD. city\nE. work\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could you find only a few office?\nA. skyscraper\nB. new york\nC. school building\nD. city\nE. work\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could you find only a few office?\nA. skyscraper\nB. new york\nC. school building\nD. city\nE. work\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could you find only a few office?\nA. skyscraper\nB. new york\nC. school building\nD. city\nE. work\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could you find only a few office?\nA. skyscraper\nB. new york\nC. school building\nD. city\nE. work\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.194711446762085", "True"]], [["-4.694711685180664", "False"]], [["-2.944711446762085", "False"]], [["-5.194711685180664", "False"]], [["-3.194711446762085", "False"]]], "filtered_resps": [["-1.194711446762085", "True"], ["-4.694711685180664", "False"], ["-2.944711446762085", "False"], ["-5.194711685180664", "False"], ["-3.194711446762085", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8569bf0daff6ef71f680046228cec577b58b5d815e83580e36bdfe53b9895cb9", "prompt_hash": "d916a4ebb5301e2730dc83318050e28a1395249ef36faa6781110d2e30277e34", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 756, "doc": {"id": "3163910d665c139a1f6f07d85803baba", "question": "Where can I go to be a religious gentleman?", "question_concept": "gentleman", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["club", "restaurant", "university", "pub", "church"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where can I go to be a religious gentleman?\nA. club\nB. restaurant\nC. university\nD. pub\nE. church\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can I go to be a religious gentleman?\nA. club\nB. restaurant\nC. university\nD. pub\nE. church\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can I go to be a religious gentleman?\nA. club\nB. restaurant\nC. university\nD. pub\nE. church\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can I go to be a religious gentleman?\nA. club\nB. restaurant\nC. university\nD. pub\nE. church\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can I go to be a religious gentleman?\nA. club\nB. restaurant\nC. university\nD. pub\nE. church\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7690482139587402", "False"]], [["-6.51904821395874", "False"]], [["-5.76904821395874", "False"]], [["-7.76904821395874", "False"]], [["-1.0190480947494507", "True"]]], "filtered_resps": [["-3.7690482139587402", "False"], ["-6.51904821395874", "False"], ["-5.76904821395874", "False"], ["-7.76904821395874", "False"], ["-1.0190480947494507", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "84b51f07a78c847c41484fded02332f41ae525726b177762560d7730a108039f", "prompt_hash": "09a537451aec6d84164ab2d24535b962d77b508058596fd2d42062d9a3ddfd81", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 757, "doc": {"id": "0e52659484f2f6d763cf0d38d4c5999d", "question": "I want to see a prepared slide up close, what would I use to help?", "question_concept": "lens", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["microscope", "abbreviate", "glasses", "camera", "telescope"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: I want to see a prepared slide up close, what would I use to help?\nA. microscope\nB. abbreviate\nC. glasses\nD. camera\nE. telescope\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I want to see a prepared slide up close, what would I use to help?\nA. microscope\nB. abbreviate\nC. glasses\nD. camera\nE. telescope\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I want to see a prepared slide up close, what would I use to help?\nA. microscope\nB. abbreviate\nC. glasses\nD. camera\nE. telescope\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I want to see a prepared slide up close, what would I use to help?\nA. microscope\nB. abbreviate\nC. glasses\nD. camera\nE. telescope\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I want to see a prepared slide up close, what would I use to help?\nA. microscope\nB. abbreviate\nC. glasses\nD. camera\nE. telescope\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5465265512466431", "True"]], [["-6.5465264320373535", "False"]], [["-6.5465264320373535", "False"]], [["-7.0465264320373535", "False"]], [["-9.046526908874512", "False"]]], "filtered_resps": [["-0.5465265512466431", "True"], ["-6.5465264320373535", "False"], ["-6.5465264320373535", "False"], ["-7.0465264320373535", "False"], ["-9.046526908874512", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "990c3d814177c9750f7ba8fcd486a96b0b207702ff12f8cb1a81b60ebb2b9f52", "prompt_hash": "3c95d4dfa4c92155bafa57b213ab94713dfaa0bbe7f3d04fe4863433c64ed83e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 758, "doc": {"id": "167d2cfa04bfaea0e0b5bac3598d5769", "question": "Where can you buy a magazine, paper or gum?", "question_concept": "magazine", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bank", "rack", "bed", "newsstand", "bus depot"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you buy a magazine, paper or gum?\nA. bank\nB. rack\nC. bed\nD. newsstand\nE. bus depot\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you buy a magazine, paper or gum?\nA. bank\nB. rack\nC. bed\nD. newsstand\nE. bus depot\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you buy a magazine, paper or gum?\nA. bank\nB. rack\nC. bed\nD. newsstand\nE. bus depot\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you buy a magazine, paper or gum?\nA. bank\nB. rack\nC. bed\nD. newsstand\nE. bus depot\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you buy a magazine, paper or gum?\nA. bank\nB. rack\nC. bed\nD. newsstand\nE. bus depot\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7021374702453613", "False"]], [["-4.452137470245361", "False"]], [["-8.452136993408203", "False"]], [["-1.7021374702453613", "False"]], [["-7.952137470245361", "False"]]], "filtered_resps": [["-2.7021374702453613", "False"], ["-4.452137470245361", "False"], ["-8.452136993408203", "False"], ["-1.7021374702453613", "False"], ["-7.952137470245361", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "175a5e77d0f8bfe2fd2f90966adc3b5de6b77b23a713c2abe470d208e774d070", "prompt_hash": "fd01732a67d209818b826cedb4f4b29ac2e8ee70609331ed24049e86b6a01180", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 759, "doc": {"id": "39572e0ba1db51fa74f7fc2d90c5ec7f", "question": "Where would you get some wood if you do not have any?", "question_concept": "wood", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["carpet", "boat", "river", "lumberyard", "synagogue"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you get some wood if you do not have any?\nA. carpet\nB. boat\nC. river\nD. lumberyard\nE. synagogue\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you get some wood if you do not have any?\nA. carpet\nB. boat\nC. river\nD. lumberyard\nE. synagogue\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you get some wood if you do not have any?\nA. carpet\nB. boat\nC. river\nD. lumberyard\nE. synagogue\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you get some wood if you do not have any?\nA. carpet\nB. boat\nC. river\nD. lumberyard\nE. synagogue\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you get some wood if you do not have any?\nA. carpet\nB. boat\nC. river\nD. lumberyard\nE. synagogue\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.13917875289917", "False"]], [["-5.88917875289917", "False"]], [["-4.13917875289917", "False"]], [["-0.8891786336898804", "True"]], [["-8.639178276062012", "False"]]], "filtered_resps": [["-3.13917875289917", "False"], ["-5.88917875289917", "False"], ["-4.13917875289917", "False"], ["-0.8891786336898804", "True"], ["-8.639178276062012", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "edb84ce72b60522a46e8737af77d648a8d17a51e1fce9940161583c4bc7b517f", "prompt_hash": "59d1dea2cfa28547658e93fe16278acffebdcea090abc9472ad547fc2556b8ef", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 760, "doc": {"id": "2a32b1e541b1daae04690d0d3a4b3310", "question": "The pitcher felt stress and tension on the mound, what did he feel like?", "question_concept": "mound", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desert", "baseball field", "hell", "baseball diamond", "baseball stadium"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The pitcher felt stress and tension on the mound, what did he feel like?\nA. desert\nB. baseball field\nC. hell\nD. baseball diamond\nE. baseball stadium\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The pitcher felt stress and tension on the mound, what did he feel like?\nA. desert\nB. baseball field\nC. hell\nD. baseball diamond\nE. baseball stadium\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The pitcher felt stress and tension on the mound, what did he feel like?\nA. desert\nB. baseball field\nC. hell\nD. baseball diamond\nE. baseball stadium\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The pitcher felt stress and tension on the mound, what did he feel like?\nA. desert\nB. baseball field\nC. hell\nD. baseball diamond\nE. baseball stadium\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The pitcher felt stress and tension on the mound, what did he feel like?\nA. desert\nB. baseball field\nC. hell\nD. baseball diamond\nE. baseball stadium\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.845329999923706", "False"]], [["-4.845330238342285", "False"]], [["-1.095329999923706", "True"]], [["-6.345330238342285", "False"]], [["-5.095330238342285", "False"]]], "filtered_resps": [["-3.845329999923706", "False"], ["-4.845330238342285", "False"], ["-1.095329999923706", "True"], ["-6.345330238342285", "False"], ["-5.095330238342285", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4702a85b1745b5e5e2d788136a058a250b0be86b9b5722cd6f92e6ee33c23194", "prompt_hash": "07ea96fbf58c653c2f828aa7a85ed1710e286ea23a51923493b40938e89232fc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 761, "doc": {"id": "71cbfeb995b06b21e890c91040722252", "question": "What negative effect can competing in a chess game on a cold day have?", "question_concept": "competing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["enemies", "perform better", "sweat", "tension", "frostbite"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What negative effect can competing in a chess game on a cold day have?\nA. enemies\nB. perform better\nC. sweat\nD. tension\nE. frostbite\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What negative effect can competing in a chess game on a cold day have?\nA. enemies\nB. perform better\nC. sweat\nD. tension\nE. frostbite\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What negative effect can competing in a chess game on a cold day have?\nA. enemies\nB. perform better\nC. sweat\nD. tension\nE. frostbite\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What negative effect can competing in a chess game on a cold day have?\nA. enemies\nB. perform better\nC. sweat\nD. tension\nE. frostbite\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What negative effect can competing in a chess game on a cold day have?\nA. enemies\nB. perform better\nC. sweat\nD. tension\nE. frostbite\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.653831958770752", "False"]], [["-6.153831958770752", "False"]], [["-3.653831958770752", "False"]], [["-4.403831958770752", "False"]], [["-1.153831958770752", "True"]]], "filtered_resps": [["-4.653831958770752", "False"], ["-6.153831958770752", "False"], ["-3.653831958770752", "False"], ["-4.403831958770752", "False"], ["-1.153831958770752", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f995d7fc3b19504e25d63c44d0a60c3ec0d91f588db9e7eaed668e31e0dfd47d", "prompt_hash": "0e43330844db7c0474c9f9b73e3d0fd6fec3d2c9b005b0604fe2eee39305f154", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 762, "doc": {"id": "a15d564d0be6996251b5d523ac62db2a", "question": "Why is it hard for a young child to read a long book?", "question_concept": "book", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["knowledge", "cover", "no pictures", "past", "many words"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Why is it hard for a young child to read a long book?\nA. knowledge\nB. cover\nC. no pictures\nD. past\nE. many words\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why is it hard for a young child to read a long book?\nA. knowledge\nB. cover\nC. no pictures\nD. past\nE. many words\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why is it hard for a young child to read a long book?\nA. knowledge\nB. cover\nC. no pictures\nD. past\nE. many words\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why is it hard for a young child to read a long book?\nA. knowledge\nB. cover\nC. no pictures\nD. past\nE. many words\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why is it hard for a young child to read a long book?\nA. knowledge\nB. cover\nC. no pictures\nD. past\nE. many words\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.584299325942993", "False"]], [["-6.334299087524414", "False"]], [["-6.834299087524414", "False"]], [["-7.334299087524414", "False"]], [["-0.8342993259429932", "True"]]], "filtered_resps": [["-3.584299325942993", "False"], ["-6.334299087524414", "False"], ["-6.834299087524414", "False"], ["-7.334299087524414", "False"], ["-0.8342993259429932", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "df31cdec0518165dc3c55398ccd1107852f8cd6a220da73b570e09f705b4bf5d", "prompt_hash": "7a22dfeeeae54ef2b46130a8bd1d427f96145bd8eb3ba69e3157e0e0f59f7eb3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 763, "doc": {"id": "6bd170c8d3d99d3c47b3e96427bacaeb", "question": "On a hot day what can you do to enjoy something cool and sweet?", "question_concept": "hot day", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dive", "cool off", "fresh cake", "go for swim", "eat ice cream"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: On a hot day what can you do to enjoy something cool and sweet?\nA. dive\nB. cool off\nC. fresh cake\nD. go for swim\nE. eat ice cream\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: On a hot day what can you do to enjoy something cool and sweet?\nA. dive\nB. cool off\nC. fresh cake\nD. go for swim\nE. eat ice cream\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: On a hot day what can you do to enjoy something cool and sweet?\nA. dive\nB. cool off\nC. fresh cake\nD. go for swim\nE. eat ice cream\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: On a hot day what can you do to enjoy something cool and sweet?\nA. dive\nB. cool off\nC. fresh cake\nD. go for swim\nE. eat ice cream\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: On a hot day what can you do to enjoy something cool and sweet?\nA. dive\nB. cool off\nC. fresh cake\nD. go for swim\nE. eat ice cream\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.659191608428955", "False"]], [["-3.909191608428955", "False"]], [["-5.909191608428955", "False"]], [["-7.159191608428955", "False"]], [["-0.6591915488243103", "True"]]], "filtered_resps": [["-4.659191608428955", "False"], ["-3.909191608428955", "False"], ["-5.909191608428955", "False"], ["-7.159191608428955", "False"], ["-0.6591915488243103", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cd467e77f5c0da7297ffb52cee6bfe163e9a08e901a8b26d20ddb5c92eb102a8", "prompt_hash": "a03ff2a86d72b2dc73a663039f339b4cbf9d4468a1d370d9764284e479847f61", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 764, "doc": {"id": "7bc1198664b376f79d584725ad7f874b", "question": "What is likely to be found in a book that is not a foreword?", "question_concept": "foreword", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["last word", "conclusion", "ikea instructions", "afterword", "epilogue"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is likely to be found in a book that is not a foreword?\nA. last word\nB. conclusion\nC. ikea instructions\nD. afterword\nE. epilogue\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is likely to be found in a book that is not a foreword?\nA. last word\nB. conclusion\nC. ikea instructions\nD. afterword\nE. epilogue\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is likely to be found in a book that is not a foreword?\nA. last word\nB. conclusion\nC. ikea instructions\nD. afterword\nE. epilogue\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is likely to be found in a book that is not a foreword?\nA. last word\nB. conclusion\nC. ikea instructions\nD. afterword\nE. epilogue\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is likely to be found in a book that is not a foreword?\nA. last word\nB. conclusion\nC. ikea instructions\nD. afterword\nE. epilogue\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7957122325897217", "True"]], [["-3.0457122325897217", "False"]], [["-5.045712471008301", "False"]], [["-3.2957122325897217", "False"]], [["-4.045712471008301", "False"]]], "filtered_resps": [["-1.7957122325897217", "True"], ["-3.0457122325897217", "False"], ["-5.045712471008301", "False"], ["-3.2957122325897217", "False"], ["-4.045712471008301", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0a260c280bb2665ab89d88f2dcfa03cce24a7e912fd785942b48fdec9f29e6c4", "prompt_hash": "dac8f4d99ea4e57f88d8c9b5ecff12b6156f4b1fd81ba33693f0cc68a94f37cc", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 765, "doc": {"id": "d6c002d46d9bfa466637cec4a134f332", "question": "How many hours are in a day?", "question_concept": "day", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["week", "bright", "night", "twenty four", "year"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: How many hours are in a day?\nA. week\nB. bright\nC. night\nD. twenty four\nE. year\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How many hours are in a day?\nA. week\nB. bright\nC. night\nD. twenty four\nE. year\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How many hours are in a day?\nA. week\nB. bright\nC. night\nD. twenty four\nE. year\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How many hours are in a day?\nA. week\nB. bright\nC. night\nD. twenty four\nE. year\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How many hours are in a day?\nA. week\nB. bright\nC. night\nD. twenty four\nE. year\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.39539909362793", "False"]], [["-6.89539909362793", "False"]], [["-8.39539909362793", "False"]], [["-0.8953990340232849", "True"]], [["-9.39539909362793", "False"]]], "filtered_resps": [["-4.39539909362793", "False"], ["-6.89539909362793", "False"], ["-8.39539909362793", "False"], ["-0.8953990340232849", "True"], ["-9.39539909362793", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f89f1e80bfb3f29a09ae57c891436a0b3f0f06cc2af819cb21f43e56ca727c05", "prompt_hash": "5aa81e67893365ad4921a8e4a25eb85cf38f56e570c48475129b528c2f8fec2f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 766, "doc": {"id": "8cb45b421375243e788cfc64bd77b051", "question": "Why is religion so hard to understand?", "question_concept": "religion", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["both positive and negative", "unknowable", "important to people", "ocean", "confusing"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Why is religion so hard to understand?\nA. both positive and negative\nB. unknowable\nC. important to people\nD. ocean\nE. confusing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why is religion so hard to understand?\nA. both positive and negative\nB. unknowable\nC. important to people\nD. ocean\nE. confusing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why is religion so hard to understand?\nA. both positive and negative\nB. unknowable\nC. important to people\nD. ocean\nE. confusing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why is religion so hard to understand?\nA. both positive and negative\nB. unknowable\nC. important to people\nD. ocean\nE. confusing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why is religion so hard to understand?\nA. both positive and negative\nB. unknowable\nC. important to people\nD. ocean\nE. confusing\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3809642791748047", "False"]], [["-4.130964279174805", "False"]], [["-2.3809642791748047", "False"]], [["-6.130964279174805", "False"]], [["-1.8809643983840942", "False"]]], "filtered_resps": [["-3.3809642791748047", "False"], ["-4.130964279174805", "False"], ["-2.3809642791748047", "False"], ["-6.130964279174805", "False"], ["-1.8809643983840942", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9d5bb997a10555b56e4df6858efe641888ed7b018f030fbbbf11cf678c9a82d0", "prompt_hash": "302395447bc0a2e94a0c5de2ced1682f17569da8c570366e5fe58dbc53843390", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 767, "doc": {"id": "d6ff2d749494d89e9c7a53f587c519f4", "question": "The couple explained they were having trouble communicating, it seemed every conversation took great what?", "question_concept": "communicating", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["thinking", "effort", "laugh", "force", "medium"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The couple explained they were having trouble communicating, it seemed every conversation took great what?\nA. thinking\nB. effort\nC. laugh\nD. force\nE. medium\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The couple explained they were having trouble communicating, it seemed every conversation took great what?\nA. thinking\nB. effort\nC. laugh\nD. force\nE. medium\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The couple explained they were having trouble communicating, it seemed every conversation took great what?\nA. thinking\nB. effort\nC. laugh\nD. force\nE. medium\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The couple explained they were having trouble communicating, it seemed every conversation took great what?\nA. thinking\nB. effort\nC. laugh\nD. force\nE. medium\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The couple explained they were having trouble communicating, it seemed every conversation took great what?\nA. thinking\nB. effort\nC. laugh\nD. force\nE. medium\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.747806549072266", "False"]], [["-1.2478065490722656", "True"]], [["-8.497806549072266", "False"]], [["-6.247806549072266", "False"]], [["-8.747806549072266", "False"]]], "filtered_resps": [["-4.747806549072266", "False"], ["-1.2478065490722656", "True"], ["-8.497806549072266", "False"], ["-6.247806549072266", "False"], ["-8.747806549072266", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65e52d3a3cbc8f3270d143ec3e2d85fa2f2df94dc5d5ed19513bb74d412eaff4", "prompt_hash": "1fd97c8eff65db5ee0efcc1f85a36d271b9f598b5b77527adc555665f1441542", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 768, "doc": {"id": "6974d215428a974641c1df18678522f5", "question": "What would a person need to do if his or her captain dies at sea?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cross street", "have a party", "experience life", "cross road", "man crew"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What would a person need to do if his or her captain dies at sea?\nA. cross street\nB. have a party\nC. experience life\nD. cross road\nE. man crew\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would a person need to do if his or her captain dies at sea?\nA. cross street\nB. have a party\nC. experience life\nD. cross road\nE. man crew\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would a person need to do if his or her captain dies at sea?\nA. cross street\nB. have a party\nC. experience life\nD. cross road\nE. man crew\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would a person need to do if his or her captain dies at sea?\nA. cross street\nB. have a party\nC. experience life\nD. cross road\nE. man crew\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would a person need to do if his or her captain dies at sea?\nA. cross street\nB. have a party\nC. experience life\nD. cross road\nE. man crew\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.06772518157959", "False"]], [["-6.81772518157959", "False"]], [["-6.06772518157959", "False"]], [["-5.56772518157959", "False"]], [["-1.3177253007888794", "True"]]], "filtered_resps": [["-3.06772518157959", "False"], ["-6.81772518157959", "False"], ["-6.06772518157959", "False"], ["-5.56772518157959", "False"], ["-1.3177253007888794", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "47fb408cd8753bebd518a891fbc402ced61b3d1782737d79a9374c468f26d6fd", "prompt_hash": "fdb3404ad1ef6b07421f397549835bcc59e20bac9f53752fec86040205e1e385", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 769, "doc": {"id": "b94a9764acff078b52a9cbae04661dc9", "question": "What do children require to grow up healthy?", "question_concept": "children", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["need care", "come home", "fast food", "watch television", "wash dishes"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do children require to grow up healthy?\nA. need care\nB. come home\nC. fast food\nD. watch television\nE. wash dishes\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do children require to grow up healthy?\nA. need care\nB. come home\nC. fast food\nD. watch television\nE. wash dishes\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do children require to grow up healthy?\nA. need care\nB. come home\nC. fast food\nD. watch television\nE. wash dishes\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do children require to grow up healthy?\nA. need care\nB. come home\nC. fast food\nD. watch television\nE. wash dishes\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do children require to grow up healthy?\nA. need care\nB. come home\nC. fast food\nD. watch television\nE. wash dishes\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6005348563194275", "True"]], [["-8.100534439086914", "False"]], [["-7.350534915924072", "False"]], [["-9.350534439086914", "False"]], [["-8.850534439086914", "False"]]], "filtered_resps": [["-0.6005348563194275", "True"], ["-8.100534439086914", "False"], ["-7.350534915924072", "False"], ["-9.350534439086914", "False"], ["-8.850534439086914", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1d5e5c5847671db11c067ad75e610ea5dfb2c8cc65a330a606ea656b0c7e652c", "prompt_hash": "02c869f9f99f82dbce110e6cd6a96356a00e6b75546402797a9e93c78740111c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 770, "doc": {"id": "80930e9df9ac4ad752749a54e7fc124f_1", "question": "I house outside the center of a community is said to be where?", "question_concept": "house", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["city", "subdivision", "newspaper", "residential area", "street"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: I house outside the center of a community is said to be where?\nA. city\nB. subdivision\nC. newspaper\nD. residential area\nE. street\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I house outside the center of a community is said to be where?\nA. city\nB. subdivision\nC. newspaper\nD. residential area\nE. street\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I house outside the center of a community is said to be where?\nA. city\nB. subdivision\nC. newspaper\nD. residential area\nE. street\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I house outside the center of a community is said to be where?\nA. city\nB. subdivision\nC. newspaper\nD. residential area\nE. street\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I house outside the center of a community is said to be where?\nA. city\nB. subdivision\nC. newspaper\nD. residential area\nE. street\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.804804265499115", "True"]], [["-2.8048043251037598", "False"]], [["-7.55480432510376", "False"]], [["-2.8048043251037598", "False"]], [["-6.05480432510376", "False"]]], "filtered_resps": [["-0.804804265499115", "True"], ["-2.8048043251037598", "False"], ["-7.55480432510376", "False"], ["-2.8048043251037598", "False"], ["-6.05480432510376", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0e8b58483c305373649a2c8e7b1d033943687aa0aca290192e9de50126e3bb8e", "prompt_hash": "95070e22251e8562c67de909baa3584f4fce5732287ca0ac9e874d6b4025a5b1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 771, "doc": {"id": "3310b5b24f03d67179fababf9ae95144", "question": "The field general began to write out a letter to the king, he was told to send what when the enemy was near?", "question_concept": "letter", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["syllable", "english alphabet", "word", "email", "invitation"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The field general began to write out a letter to the king, he was told to send what when the enemy was near?\nA. syllable\nB. english alphabet\nC. word\nD. email\nE. invitation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The field general began to write out a letter to the king, he was told to send what when the enemy was near?\nA. syllable\nB. english alphabet\nC. word\nD. email\nE. invitation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The field general began to write out a letter to the king, he was told to send what when the enemy was near?\nA. syllable\nB. english alphabet\nC. word\nD. email\nE. invitation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The field general began to write out a letter to the king, he was told to send what when the enemy was near?\nA. syllable\nB. english alphabet\nC. word\nD. email\nE. invitation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The field general began to write out a letter to the king, he was told to send what when the enemy was near?\nA. syllable\nB. english alphabet\nC. word\nD. email\nE. invitation\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3678414821624756", "False"]], [["-2.6178414821624756", "False"]], [["-1.8678414821624756", "True"]], [["-4.617841720581055", "False"]], [["-5.617841720581055", "False"]]], "filtered_resps": [["-2.3678414821624756", "False"], ["-2.6178414821624756", "False"], ["-1.8678414821624756", "True"], ["-4.617841720581055", "False"], ["-5.617841720581055", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4ab53a8bac6a40c37c97791e63c6dc733b14f96231801c6ef3e03156fe35c044", "prompt_hash": "4fe14c6b560b0ff0292fa9acfb097b325a98648325879f8f8476c63d6f6d1387", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 772, "doc": {"id": "846bc47ced7119ad2ee19a8780d7fe18", "question": "What will you put on a pen to prevent it from drying out?", "question_concept": "pens", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["write sentences on paper", "ink in", "ink cartridges", "caps", "cling film"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What will you put on a pen to prevent it from drying out?\nA. write sentences on paper\nB. ink in\nC. ink cartridges\nD. caps\nE. cling film\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will you put on a pen to prevent it from drying out?\nA. write sentences on paper\nB. ink in\nC. ink cartridges\nD. caps\nE. cling film\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will you put on a pen to prevent it from drying out?\nA. write sentences on paper\nB. ink in\nC. ink cartridges\nD. caps\nE. cling film\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will you put on a pen to prevent it from drying out?\nA. write sentences on paper\nB. ink in\nC. ink cartridges\nD. caps\nE. cling film\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will you put on a pen to prevent it from drying out?\nA. write sentences on paper\nB. ink in\nC. ink cartridges\nD. caps\nE. cling film\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.257812976837158", "False"]], [["-5.507812976837158", "False"]], [["-4.757812976837158", "False"]], [["-1.5078129768371582", "True"]], [["-8.2578125", "False"]]], "filtered_resps": [["-3.257812976837158", "False"], ["-5.507812976837158", "False"], ["-4.757812976837158", "False"], ["-1.5078129768371582", "True"], ["-8.2578125", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "98e87dc970eae824c47a1b169b15cb408477ac395b9276ad206c4b002e62ea56", "prompt_hash": "46a249ba598fe1998a421604d7f4edf8890f5baef02a404315cdabb01a7047cc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 773, "doc": {"id": "fd5a34e94303d7fd343de2a8f36943d5", "question": "After climbing the mountains, the explored found the cave, what was the general goegraphy of the region he found it in?", "question_concept": "cave", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["west virginia", "kentucky", "desert", "sea", "rocky hills"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: After climbing the mountains, the explored found the cave, what was the general goegraphy of the region he found it in?\nA. west virginia\nB. kentucky\nC. desert\nD. sea\nE. rocky hills\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: After climbing the mountains, the explored found the cave, what was the general goegraphy of the region he found it in?\nA. west virginia\nB. kentucky\nC. desert\nD. sea\nE. rocky hills\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: After climbing the mountains, the explored found the cave, what was the general goegraphy of the region he found it in?\nA. west virginia\nB. kentucky\nC. desert\nD. sea\nE. rocky hills\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: After climbing the mountains, the explored found the cave, what was the general goegraphy of the region he found it in?\nA. west virginia\nB. kentucky\nC. desert\nD. sea\nE. rocky hills\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: After climbing the mountains, the explored found the cave, what was the general goegraphy of the region he found it in?\nA. west virginia\nB. kentucky\nC. desert\nD. sea\nE. rocky hills\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.2736897468566895", "False"]], [["-5.0236897468566895", "False"]], [["-6.5236897468566895", "False"]], [["-7.5236897468566895", "False"]], [["-2.7736897468566895", "False"]]], "filtered_resps": [["-4.2736897468566895", "False"], ["-5.0236897468566895", "False"], ["-6.5236897468566895", "False"], ["-7.5236897468566895", "False"], ["-2.7736897468566895", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f631d49240616ea665ea4f26f30c8c697bc4801a964b480576c88ef597fbb189", "prompt_hash": "b311cd68f6d44452bdf6e1188244521fbe897ff382db176c9664d8594153938d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 774, "doc": {"id": "4e87db4771f2d6423034935446e3fff1", "question": "They dealt with combustible mixtures in their experiments, this is why they kept a fire extinguisher where?", "question_concept": "fire extinguisher", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hospital", "chemistry lab", "most businesses", "classroom", "public building"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: They dealt with combustible mixtures in their experiments, this is why they kept a fire extinguisher where?\nA. hospital\nB. chemistry lab\nC. most businesses\nD. classroom\nE. public building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They dealt with combustible mixtures in their experiments, this is why they kept a fire extinguisher where?\nA. hospital\nB. chemistry lab\nC. most businesses\nD. classroom\nE. public building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They dealt with combustible mixtures in their experiments, this is why they kept a fire extinguisher where?\nA. hospital\nB. chemistry lab\nC. most businesses\nD. classroom\nE. public building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They dealt with combustible mixtures in their experiments, this is why they kept a fire extinguisher where?\nA. hospital\nB. chemistry lab\nC. most businesses\nD. classroom\nE. public building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They dealt with combustible mixtures in their experiments, this is why they kept a fire extinguisher where?\nA. hospital\nB. chemistry lab\nC. most businesses\nD. classroom\nE. public building\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.9669189453125", "False"]], [["-0.9669189453125", "True"]], [["-7.4669189453125", "False"]], [["-8.4669189453125", "False"]], [["-10.4669189453125", "False"]]], "filtered_resps": [["-4.9669189453125", "False"], ["-0.9669189453125", "True"], ["-7.4669189453125", "False"], ["-8.4669189453125", "False"], ["-10.4669189453125", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "66e9b1d4731f413a36a7b862b03c8db5b53b940d61208790b60907d7556fdf6f", "prompt_hash": "c15dccaaa8bc7eb2b6a28f94ec54ca23349bf0cda48be0d4559cafaee5a1bb0d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 775, "doc": {"id": "a585df0818180ce3c06f963a4c3c810a", "question": "If someone mean wanted to insult somebody by calling them a fruit, where is probably not the smartest place to do it?", "question_concept": "fruit", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["gay bar", "market", "grocery store", "refrigerator", "container"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If someone mean wanted to insult somebody by calling them a fruit, where is probably not the smartest place to do it?\nA. gay bar\nB. market\nC. grocery store\nD. refrigerator\nE. container\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If someone mean wanted to insult somebody by calling them a fruit, where is probably not the smartest place to do it?\nA. gay bar\nB. market\nC. grocery store\nD. refrigerator\nE. container\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If someone mean wanted to insult somebody by calling them a fruit, where is probably not the smartest place to do it?\nA. gay bar\nB. market\nC. grocery store\nD. refrigerator\nE. container\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If someone mean wanted to insult somebody by calling them a fruit, where is probably not the smartest place to do it?\nA. gay bar\nB. market\nC. grocery store\nD. refrigerator\nE. container\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If someone mean wanted to insult somebody by calling them a fruit, where is probably not the smartest place to do it?\nA. gay bar\nB. market\nC. grocery store\nD. refrigerator\nE. container\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.9352831840515137", "False"]], [["-4.435283184051514", "False"]], [["-4.685283184051514", "False"]], [["-2.4352831840515137", "False"]], [["-7.185283184051514", "False"]]], "filtered_resps": [["-1.9352831840515137", "False"], ["-4.435283184051514", "False"], ["-4.685283184051514", "False"], ["-2.4352831840515137", "False"], ["-7.185283184051514", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "17a1e8750aa449c21352f6a0a285d09b41cdce19cd3b42f52a6a4b0eeecf74ba", "prompt_hash": "8afa0258f562ed1e357273bfd363409d89b31d4ec104db62c11db10ebb67c457", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 776, "doc": {"id": "c9f7d07e6d363a99f5fadd68a4dfa35a", "question": "Where would you get a toothpick if you do not have any?", "question_concept": "toothpick", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["box", "grocery store", "eyes", "chewing", "mouth"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you get a toothpick if you do not have any?\nA. box\nB. grocery store\nC. eyes\nD. chewing\nE. mouth\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you get a toothpick if you do not have any?\nA. box\nB. grocery store\nC. eyes\nD. chewing\nE. mouth\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you get a toothpick if you do not have any?\nA. box\nB. grocery store\nC. eyes\nD. chewing\nE. mouth\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you get a toothpick if you do not have any?\nA. box\nB. grocery store\nC. eyes\nD. chewing\nE. mouth\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you get a toothpick if you do not have any?\nA. box\nB. grocery store\nC. eyes\nD. chewing\nE. mouth\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.481313705444336", "False"]], [["-0.9813136458396912", "True"]], [["-6.731313705444336", "False"]], [["-8.231313705444336", "False"]], [["-8.231313705444336", "False"]]], "filtered_resps": [["-2.481313705444336", "False"], ["-0.9813136458396912", "True"], ["-6.731313705444336", "False"], ["-8.231313705444336", "False"], ["-8.231313705444336", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65026973264ae7532542f036e1e4c0d33f81202276eca846918a9874da5627bc", "prompt_hash": "9a4b6f15060b6ad7e15007edc0061f13aa09092b496625cb52068d32833142de", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 777, "doc": {"id": "c7cb327fa4c0008efaa7741081a365d4", "question": "What would you be building if you designed a place for an annoying critter to stay?", "question_concept": "mosquitoes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["spread disease", "swamp", "fly away", "cat condo", "bug campers"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What would you be building if you designed a place for an annoying critter to stay?\nA. spread disease\nB. swamp\nC. fly away\nD. cat condo\nE. bug campers\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you be building if you designed a place for an annoying critter to stay?\nA. spread disease\nB. swamp\nC. fly away\nD. cat condo\nE. bug campers\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you be building if you designed a place for an annoying critter to stay?\nA. spread disease\nB. swamp\nC. fly away\nD. cat condo\nE. bug campers\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you be building if you designed a place for an annoying critter to stay?\nA. spread disease\nB. swamp\nC. fly away\nD. cat condo\nE. bug campers\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you be building if you designed a place for an annoying critter to stay?\nA. spread disease\nB. swamp\nC. fly away\nD. cat condo\nE. bug campers\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0553297996520996", "False"]], [["-2.8053297996520996", "False"]], [["-5.0553297996521", "False"]], [["-2.0553297996520996", "False"]], [["-1.55532968044281", "True"]]], "filtered_resps": [["-3.0553297996520996", "False"], ["-2.8053297996520996", "False"], ["-5.0553297996521", "False"], ["-2.0553297996520996", "False"], ["-1.55532968044281", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "521d8652eaa3a228875e31916652b1c5040432cd1f86168ffe2322f27898cd3b", "prompt_hash": "94f156b57c92fc5f0e7fc5d461687b32d12f168110cb73ed2a5f030c09328bca", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 778, "doc": {"id": "c54ddc0f9d170ba65d9f4f2e0bb41d1c", "question": "The man working in the attic swatted away a bee, but soon the single bee was an entire what?", "question_concept": "bee", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["swarm", "pack", "countryside", "soft drink", "field of flowers"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The man working in the attic swatted away a bee, but soon the single bee was an entire what?\nA. swarm\nB. pack\nC. countryside\nD. soft drink\nE. field of flowers\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man working in the attic swatted away a bee, but soon the single bee was an entire what?\nA. swarm\nB. pack\nC. countryside\nD. soft drink\nE. field of flowers\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man working in the attic swatted away a bee, but soon the single bee was an entire what?\nA. swarm\nB. pack\nC. countryside\nD. soft drink\nE. field of flowers\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man working in the attic swatted away a bee, but soon the single bee was an entire what?\nA. swarm\nB. pack\nC. countryside\nD. soft drink\nE. field of flowers\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man working in the attic swatted away a bee, but soon the single bee was an entire what?\nA. swarm\nB. pack\nC. countryside\nD. soft drink\nE. field of flowers\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6539251804351807", "True"]], [["-6.16314172744751", "False"]], [["-7.153924942016602", "False"]], [["-8.903924942016602", "False"]], [["-8.403924942016602", "False"]]], "filtered_resps": [["-0.6539251804351807", "True"], ["-6.16314172744751", "False"], ["-7.153924942016602", "False"], ["-8.903924942016602", "False"], ["-8.403924942016602", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e69a41afa8762272d8fcfbc490707c8da73fc068f9915767492eb8b185e94e90", "prompt_hash": "8c0ff202305eb49f9876a6ebd6ab6f8378bce7afe76431f854fd7bf5998e8f83", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 779, "doc": {"id": "1729c737ff92cf558efecde2c6cafc5e", "question": "What do you need to wear when hiking?", "question_concept": "hiking", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cast iron stomach", "physical exertion", "shin splints", "adventure", "fatigue"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What do you need to wear when hiking?\nA. cast iron stomach\nB. physical exertion\nC. shin splints\nD. adventure\nE. fatigue\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you need to wear when hiking?\nA. cast iron stomach\nB. physical exertion\nC. shin splints\nD. adventure\nE. fatigue\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you need to wear when hiking?\nA. cast iron stomach\nB. physical exertion\nC. shin splints\nD. adventure\nE. fatigue\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you need to wear when hiking?\nA. cast iron stomach\nB. physical exertion\nC. shin splints\nD. adventure\nE. fatigue\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you need to wear when hiking?\nA. cast iron stomach\nB. physical exertion\nC. shin splints\nD. adventure\nE. fatigue\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.669968843460083", "False"]], [["-1.419968843460083", "True"]], [["-5.669968605041504", "False"]], [["-5.169968605041504", "False"]], [["-5.169968605041504", "False"]]], "filtered_resps": [["-2.669968843460083", "False"], ["-1.419968843460083", "True"], ["-5.669968605041504", "False"], ["-5.169968605041504", "False"], ["-5.169968605041504", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6f08a7e26f3d72fcbcd93c0906852bcaf3d0a4149298c94b6153a952671ae982", "prompt_hash": "da1f0338f39402b93fea499afb17f206886c02dc6b59a3da53db96ace303870a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 780, "doc": {"id": "19dfd55e967dacd6f5700a62c1e14eee", "question": "What type of store would have lots of sports equipment?", "question_concept": "sports equipment", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mall", "office supply store", "school", "sporting goods store", "sporting event"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What type of store would have lots of sports equipment?\nA. mall\nB. office supply store\nC. school\nD. sporting goods store\nE. sporting event\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What type of store would have lots of sports equipment?\nA. mall\nB. office supply store\nC. school\nD. sporting goods store\nE. sporting event\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What type of store would have lots of sports equipment?\nA. mall\nB. office supply store\nC. school\nD. sporting goods store\nE. sporting event\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What type of store would have lots of sports equipment?\nA. mall\nB. office supply store\nC. school\nD. sporting goods store\nE. sporting event\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What type of store would have lots of sports equipment?\nA. mall\nB. office supply store\nC. school\nD. sporting goods store\nE. sporting event\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8490993976593018", "False"]], [["-8.599099159240723", "False"]], [["-7.599099159240723", "False"]], [["-1.0990993976593018", "False"]], [["-10.099099159240723", "False"]]], "filtered_resps": [["-2.8490993976593018", "False"], ["-8.599099159240723", "False"], ["-7.599099159240723", "False"], ["-1.0990993976593018", "False"], ["-10.099099159240723", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fd49ba006a07525469d9f3e6a26de6002f40e1426cae2d77e44adea4120fea4a", "prompt_hash": "5756b4dec32140838df7c3cdf8afc2a3e43174e59c1f8cfd324cec8a4bc42f99", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 781, "doc": {"id": "b9bed83138901f4a45041b02c5b242c1", "question": "The business man was promoted recently, to celebrate he went where to buy an expensive wristwatch?", "question_concept": "wristwatch", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["case", "jewelry store", "shopping", "jewelery box", "hock"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The business man was promoted recently, to celebrate he went where to buy an expensive wristwatch?\nA. case\nB. jewelry store\nC. shopping\nD. jewelery box\nE. hock\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The business man was promoted recently, to celebrate he went where to buy an expensive wristwatch?\nA. case\nB. jewelry store\nC. shopping\nD. jewelery box\nE. hock\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The business man was promoted recently, to celebrate he went where to buy an expensive wristwatch?\nA. case\nB. jewelry store\nC. shopping\nD. jewelery box\nE. hock\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The business man was promoted recently, to celebrate he went where to buy an expensive wristwatch?\nA. case\nB. jewelry store\nC. shopping\nD. jewelery box\nE. hock\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The business man was promoted recently, to celebrate he went where to buy an expensive wristwatch?\nA. case\nB. jewelry store\nC. shopping\nD. jewelery box\nE. hock\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.525177478790283", "False"]], [["-1.7751774787902832", "False"]], [["-7.775177478790283", "False"]], [["-10.275177001953125", "False"]], [["-10.525177001953125", "False"]]], "filtered_resps": [["-2.525177478790283", "False"], ["-1.7751774787902832", "False"], ["-7.775177478790283", "False"], ["-10.275177001953125", "False"], ["-10.525177001953125", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "02647e39c579a136655bd75a809119c1f1a4fd68d26f2f6565abbe1c94a0b1a5", "prompt_hash": "060e834ba8a25c9036399d0b3e825e45f6046f75dd3d0ab1948f95254a53a188", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 782, "doc": {"id": "b9d22425a3d5810be9528a55245c8f09", "question": "How is a child eager to be going to play likely to get there?", "question_concept": "going to play", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["slowly", "rush", "being entertained", "have fun", "enjoyment"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: How is a child eager to be going to play likely to get there?\nA. slowly\nB. rush\nC. being entertained\nD. have fun\nE. enjoyment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How is a child eager to be going to play likely to get there?\nA. slowly\nB. rush\nC. being entertained\nD. have fun\nE. enjoyment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How is a child eager to be going to play likely to get there?\nA. slowly\nB. rush\nC. being entertained\nD. have fun\nE. enjoyment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How is a child eager to be going to play likely to get there?\nA. slowly\nB. rush\nC. being entertained\nD. have fun\nE. enjoyment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How is a child eager to be going to play likely to get there?\nA. slowly\nB. rush\nC. being entertained\nD. have fun\nE. enjoyment\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.758893370628357", "False"]], [["-1.008893370628357", "True"]], [["-6.5088934898376465", "False"]], [["-8.008893013000488", "False"]], [["-8.508893013000488", "False"]]], "filtered_resps": [["-1.758893370628357", "False"], ["-1.008893370628357", "True"], ["-6.5088934898376465", "False"], ["-8.008893013000488", "False"], ["-8.508893013000488", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "61cff2a489edce37984e0ffdade522a84d03f3a595669c821c73811569ff5482", "prompt_hash": "1cd0d256aeb26a04391ef0d99c9c8fbc95467fefb299e92f86fbd7bcec6c7db2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 783, "doc": {"id": "2af70107e04e61e3c7884bc743901c02", "question": "There's some new buying products designed to get you money if you have none. The first step is that it will show you how to declare what?", "question_concept": "buying products", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tax return", "bankruptcy", "pleasure", "debt", "spending money"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: There's some new buying products designed to get you money if you have none. The first step is that it will show you how to declare what?\nA. tax return\nB. bankruptcy\nC. pleasure\nD. debt\nE. spending money\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: There's some new buying products designed to get you money if you have none. The first step is that it will show you how to declare what?\nA. tax return\nB. bankruptcy\nC. pleasure\nD. debt\nE. spending money\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: There's some new buying products designed to get you money if you have none. The first step is that it will show you how to declare what?\nA. tax return\nB. bankruptcy\nC. pleasure\nD. debt\nE. spending money\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: There's some new buying products designed to get you money if you have none. The first step is that it will show you how to declare what?\nA. tax return\nB. bankruptcy\nC. pleasure\nD. debt\nE. spending money\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: There's some new buying products designed to get you money if you have none. The first step is that it will show you how to declare what?\nA. tax return\nB. bankruptcy\nC. pleasure\nD. debt\nE. spending money\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.650717854499817", "True"]], [["-2.6507177352905273", "False"]], [["-5.900717735290527", "False"]], [["-3.9007177352905273", "False"]], [["-6.650717735290527", "False"]]], "filtered_resps": [["-1.650717854499817", "True"], ["-2.6507177352905273", "False"], ["-5.900717735290527", "False"], ["-3.9007177352905273", "False"], ["-6.650717735290527", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "13289b9cb4319392ee1cc65437874a0e0579c37cb5098a13179ea203d72208f0", "prompt_hash": "d8d54f3d77c603d1155c4ffb9a50bd6394e6d22a7b1e8d52f97d45862ae4f3a7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 784, "doc": {"id": "be2cb9c96069ac355a7ccef262743d14", "question": "Where can you buy a replacement ax handle?", "question_concept": "handle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bathroom", "hardware store", "water fountain", "grocery store", "fridge"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you buy a replacement ax handle?\nA. bathroom\nB. hardware store\nC. water fountain\nD. grocery store\nE. fridge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you buy a replacement ax handle?\nA. bathroom\nB. hardware store\nC. water fountain\nD. grocery store\nE. fridge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you buy a replacement ax handle?\nA. bathroom\nB. hardware store\nC. water fountain\nD. grocery store\nE. fridge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you buy a replacement ax handle?\nA. bathroom\nB. hardware store\nC. water fountain\nD. grocery store\nE. fridge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you buy a replacement ax handle?\nA. bathroom\nB. hardware store\nC. water fountain\nD. grocery store\nE. fridge\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.441481590270996", "False"]], [["-0.9414817094802856", "True"]], [["-8.941481590270996", "False"]], [["-10.191481590270996", "False"]], [["-11.441481590270996", "False"]]], "filtered_resps": [["-2.441481590270996", "False"], ["-0.9414817094802856", "True"], ["-8.941481590270996", "False"], ["-10.191481590270996", "False"], ["-11.441481590270996", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ee24dc71678488c63ae8bd25ff26dadd841e31dcb765cced572deaaeefc4bb6c", "prompt_hash": "f90efe480b039257e2914f9a8185da3ce9aea12daa1b8be6d9b9e07455b4b496", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 785, "doc": {"id": "799e48ec7fb16415c8f82828c5761ed1", "question": "Is that person acting as silly as a clown?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["make mistakes", "ridiculous", "have no home", "mentally unhinged", "schizophrenia"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Is that person acting as silly as a clown?\nA. make mistakes\nB. ridiculous\nC. have no home\nD. mentally unhinged\nE. schizophrenia\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Is that person acting as silly as a clown?\nA. make mistakes\nB. ridiculous\nC. have no home\nD. mentally unhinged\nE. schizophrenia\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Is that person acting as silly as a clown?\nA. make mistakes\nB. ridiculous\nC. have no home\nD. mentally unhinged\nE. schizophrenia\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Is that person acting as silly as a clown?\nA. make mistakes\nB. ridiculous\nC. have no home\nD. mentally unhinged\nE. schizophrenia\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Is that person acting as silly as a clown?\nA. make mistakes\nB. ridiculous\nC. have no home\nD. mentally unhinged\nE. schizophrenia\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.372505187988281", "False"]], [["-1.1225054264068604", "True"]], [["-5.622505187988281", "False"]], [["-5.872505187988281", "False"]], [["-8.622505187988281", "False"]]], "filtered_resps": [["-4.372505187988281", "False"], ["-1.1225054264068604", "True"], ["-5.622505187988281", "False"], ["-5.872505187988281", "False"], ["-8.622505187988281", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1eec8713804668a4bc78e2122c534c4a3fe1d6882301b0e95daf423aa5c146cf", "prompt_hash": "22027cc9b8f451acc5a0e313f56bf2291a66bfaff7f01351935afb53e5924452", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 786, "doc": {"id": "a5db1e9677af118deb8e4add8bc18db2", "question": "Which group of states is Louisiana part of?", "question_concept": "louisiana", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["deep south", "98 of world's crayfish", "united states", "gulf states", "bible belt"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Which group of states is Louisiana part of?\nA. deep south\nB. 98 of world's crayfish\nC. united states\nD. gulf states\nE. bible belt\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Which group of states is Louisiana part of?\nA. deep south\nB. 98 of world's crayfish\nC. united states\nD. gulf states\nE. bible belt\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Which group of states is Louisiana part of?\nA. deep south\nB. 98 of world's crayfish\nC. united states\nD. gulf states\nE. bible belt\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Which group of states is Louisiana part of?\nA. deep south\nB. 98 of world's crayfish\nC. united states\nD. gulf states\nE. bible belt\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Which group of states is Louisiana part of?\nA. deep south\nB. 98 of world's crayfish\nC. united states\nD. gulf states\nE. bible belt\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.490240097045898", "False"]], [["-7.740240097045898", "False"]], [["-3.2402400970458984", "False"]], [["-3.2402400970458984", "False"]], [["-10.740240097045898", "False"]]], "filtered_resps": [["-4.490240097045898", "False"], ["-7.740240097045898", "False"], ["-3.2402400970458984", "False"], ["-3.2402400970458984", "False"], ["-10.740240097045898", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "16fd2a9eb339f422f86a6ac8f703d6470d380cd611324247462b82a1b2158dbc", "prompt_hash": "c891553bb778fba49c63b1193543b459c5cfd55a4962490a6e1ca4a3de080418", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 787, "doc": {"id": "28357ebf85f8bb82b6a3210c4397e0aa", "question": "Where would you put a plate immediately after eating from it?", "question_concept": "plate", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kitchen cupboard", "floor", "table", "dishwasher", "flea market"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put a plate immediately after eating from it?\nA. kitchen cupboard\nB. floor\nC. table\nD. dishwasher\nE. flea market\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put a plate immediately after eating from it?\nA. kitchen cupboard\nB. floor\nC. table\nD. dishwasher\nE. flea market\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put a plate immediately after eating from it?\nA. kitchen cupboard\nB. floor\nC. table\nD. dishwasher\nE. flea market\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put a plate immediately after eating from it?\nA. kitchen cupboard\nB. floor\nC. table\nD. dishwasher\nE. flea market\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put a plate immediately after eating from it?\nA. kitchen cupboard\nB. floor\nC. table\nD. dishwasher\nE. flea market\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9601950645446777", "False"]], [["-3.4601950645446777", "False"]], [["-1.4601949453353882", "True"]], [["-2.7101950645446777", "False"]], [["-6.210195064544678", "False"]]], "filtered_resps": [["-3.9601950645446777", "False"], ["-3.4601950645446777", "False"], ["-1.4601949453353882", "True"], ["-2.7101950645446777", "False"], ["-6.210195064544678", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0bc78b58cebee20d1059eb7fb644f29207c85658a9a7ffe0afe3baed1e8ec14e", "prompt_hash": "793916c32b5b31e706b00f6d386636ed2c93eabdd0335e5ccd236bcd47687e50", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 788, "doc": {"id": "7b95825a19d6930d6aed35c7c57a2d82", "question": "James couldn't get comfortable.  There was too much dirt.  He needed to clean out what?", "question_concept": "dirt", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ground", "subway", "bank", "bed", "street"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: James couldn't get comfortable.  There was too much dirt.  He needed to clean out what?\nA. ground\nB. subway\nC. bank\nD. bed\nE. street\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James couldn't get comfortable.  There was too much dirt.  He needed to clean out what?\nA. ground\nB. subway\nC. bank\nD. bed\nE. street\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James couldn't get comfortable.  There was too much dirt.  He needed to clean out what?\nA. ground\nB. subway\nC. bank\nD. bed\nE. street\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James couldn't get comfortable.  There was too much dirt.  He needed to clean out what?\nA. ground\nB. subway\nC. bank\nD. bed\nE. street\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James couldn't get comfortable.  There was too much dirt.  He needed to clean out what?\nA. ground\nB. subway\nC. bank\nD. bed\nE. street\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.907186985015869", "False"]], [["-4.407186985015869", "False"]], [["-8.657186508178711", "False"]], [["-1.4071869850158691", "False"]], [["-10.157186508178711", "False"]]], "filtered_resps": [["-4.907186985015869", "False"], ["-4.407186985015869", "False"], ["-8.657186508178711", "False"], ["-1.4071869850158691", "False"], ["-10.157186508178711", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "21caa9b1ff9e45bab2ea192c93c66e163debaf5dad6dca7645b1ddcd1611f666", "prompt_hash": "a52b8e4b0364f246cd997bd2b9bd1816b247f2be4d5b5fc96ca740fc886699a8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 789, "doc": {"id": "6b270159bd402ddd498a38153f9d1efe", "question": "The rats were hiding in the house, where were they?", "question_concept": "rats", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sewers", "laboratory", "basement", "clinic", "cellar"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The rats were hiding in the house, where were they?\nA. sewers\nB. laboratory\nC. basement\nD. clinic\nE. cellar\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The rats were hiding in the house, where were they?\nA. sewers\nB. laboratory\nC. basement\nD. clinic\nE. cellar\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The rats were hiding in the house, where were they?\nA. sewers\nB. laboratory\nC. basement\nD. clinic\nE. cellar\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The rats were hiding in the house, where were they?\nA. sewers\nB. laboratory\nC. basement\nD. clinic\nE. cellar\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The rats were hiding in the house, where were they?\nA. sewers\nB. laboratory\nC. basement\nD. clinic\nE. cellar\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7031314373016357", "False"]], [["-3.21452260017395", "False"]], [["-2.21452260017395", "False"]], [["-4.964522361755371", "False"]], [["-2.21452260017395", "False"]]], "filtered_resps": [["-3.7031314373016357", "False"], ["-3.21452260017395", "False"], ["-2.21452260017395", "False"], ["-4.964522361755371", "False"], ["-2.21452260017395", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b8a86815cfe4a36bdd97a8206e0c21cfff2bafb41931abdbe6e9d48ceb136dbd", "prompt_hash": "28cf0692d4e82339b56ee90adc8783d1105ebf2a3a031b61f335a955a9d6abfb", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 790, "doc": {"id": "eae0e03773365064ce915603c7addc91", "question": "What do people do when they don't understand something?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ask questions", "experience joy", "believe in god", "talk to each other", "get sick"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do people do when they don't understand something?\nA. ask questions\nB. experience joy\nC. believe in god\nD. talk to each other\nE. get sick\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people do when they don't understand something?\nA. ask questions\nB. experience joy\nC. believe in god\nD. talk to each other\nE. get sick\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people do when they don't understand something?\nA. ask questions\nB. experience joy\nC. believe in god\nD. talk to each other\nE. get sick\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people do when they don't understand something?\nA. ask questions\nB. experience joy\nC. believe in god\nD. talk to each other\nE. get sick\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people do when they don't understand something?\nA. ask questions\nB. experience joy\nC. believe in god\nD. talk to each other\nE. get sick\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7078463435173035", "True"]], [["-7.707846164703369", "False"]], [["-8.207846641540527", "False"]], [["-7.457846164703369", "False"]], [["-9.207846641540527", "False"]]], "filtered_resps": [["-0.7078463435173035", "True"], ["-7.707846164703369", "False"], ["-8.207846641540527", "False"], ["-7.457846164703369", "False"], ["-9.207846641540527", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3c2015fa3e60898197e1ce247b6b5860e17c4381a1eaca7b1097242a4de2bc3c", "prompt_hash": "7af92fd4b1b8220466fdc533750bc8a78daf3298cfee2917c40b1bb816dba128", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 791, "doc": {"id": "a5ca7c89196e54938b5827814d0071d4", "question": "James saw a kite flying in the sky.  He traced the string back to its origin and found it.  Where did the string begin?", "question_concept": "kite", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["end of line", "hobby shop", "his hand", "toy store", "child's hand"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: James saw a kite flying in the sky.  He traced the string back to its origin and found it.  Where did the string begin?\nA. end of line\nB. hobby shop\nC. his hand\nD. toy store\nE. child's hand\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James saw a kite flying in the sky.  He traced the string back to its origin and found it.  Where did the string begin?\nA. end of line\nB. hobby shop\nC. his hand\nD. toy store\nE. child's hand\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James saw a kite flying in the sky.  He traced the string back to its origin and found it.  Where did the string begin?\nA. end of line\nB. hobby shop\nC. his hand\nD. toy store\nE. child's hand\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James saw a kite flying in the sky.  He traced the string back to its origin and found it.  Where did the string begin?\nA. end of line\nB. hobby shop\nC. his hand\nD. toy store\nE. child's hand\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James saw a kite flying in the sky.  He traced the string back to its origin and found it.  Where did the string begin?\nA. end of line\nB. hobby shop\nC. his hand\nD. toy store\nE. child's hand\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.200455188751221", "False"]], [["-5.950455188751221", "False"]], [["-2.7004551887512207", "False"]], [["-7.950455188751221", "False"]], [["-3.2004551887512207", "False"]]], "filtered_resps": [["-5.200455188751221", "False"], ["-5.950455188751221", "False"], ["-2.7004551887512207", "False"], ["-7.950455188751221", "False"], ["-3.2004551887512207", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ad818fed1251f0858c4198ecb1f07196f47178af895cfd1c8ed48a4fdcdfffb6", "prompt_hash": "5f543372ff68e512fefc39cca361dfbff4a6fd9394d96c1da7ede3f4cb070cdb", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 792, "doc": {"id": "ffc3461d437a1c6c22d1c4f6439ebd9c", "question": "What rubber toy filled with helium will make a child happy?", "question_concept": "child", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["loved", "learn", "eat cake", "balloon", "become adult"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What rubber toy filled with helium will make a child happy?\nA. loved\nB. learn\nC. eat cake\nD. balloon\nE. become adult\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What rubber toy filled with helium will make a child happy?\nA. loved\nB. learn\nC. eat cake\nD. balloon\nE. become adult\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What rubber toy filled with helium will make a child happy?\nA. loved\nB. learn\nC. eat cake\nD. balloon\nE. become adult\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What rubber toy filled with helium will make a child happy?\nA. loved\nB. learn\nC. eat cake\nD. balloon\nE. become adult\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What rubber toy filled with helium will make a child happy?\nA. loved\nB. learn\nC. eat cake\nD. balloon\nE. become adult\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.402886152267456", "False"]], [["-5.402886390686035", "False"]], [["-6.652886390686035", "False"]], [["-1.152886152267456", "True"]], [["-10.152886390686035", "False"]]], "filtered_resps": [["-2.402886152267456", "False"], ["-5.402886390686035", "False"], ["-6.652886390686035", "False"], ["-1.152886152267456", "True"], ["-10.152886390686035", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bdf5003a0b23cdf98fb09274fed8993f90d87fcdcb4e57acd66de06ebdf42520", "prompt_hash": "160e99f9204baf104a937c22c2edbd840e0ed3a82df03c2a25548f917a31ba31", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 793, "doc": {"id": "aa2dcd9bcce5e4445bd3bacbf0bb11d3", "question": "Where do people get beer after a bit of gambling?", "question_concept": "beer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bottle", "grocery store", "casino", "spaceship", "hockey game"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where do people get beer after a bit of gambling?\nA. bottle\nB. grocery store\nC. casino\nD. spaceship\nE. hockey game\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do people get beer after a bit of gambling?\nA. bottle\nB. grocery store\nC. casino\nD. spaceship\nE. hockey game\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do people get beer after a bit of gambling?\nA. bottle\nB. grocery store\nC. casino\nD. spaceship\nE. hockey game\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do people get beer after a bit of gambling?\nA. bottle\nB. grocery store\nC. casino\nD. spaceship\nE. hockey game\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do people get beer after a bit of gambling?\nA. bottle\nB. grocery store\nC. casino\nD. spaceship\nE. hockey game\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.987635850906372", "False"]], [["-5.487635612487793", "False"]], [["-0.9876357913017273", "True"]], [["-7.487635612487793", "False"]], [["-6.237635612487793", "False"]]], "filtered_resps": [["-3.987635850906372", "False"], ["-5.487635612487793", "False"], ["-0.9876357913017273", "True"], ["-7.487635612487793", "False"], ["-6.237635612487793", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d6fe1a815d157de68ad0b9a7b4e8f410ce1ebe88b126a11a2564d274bdc38e94", "prompt_hash": "1c206888e922215f2e9123581160b6aa63b44538baa9f6cc4c66867ca8a4850d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 794, "doc": {"id": "6cc797ec148c1fc74592957a55bd0951", "question": "What can happen to you when you are using television and it is not interesting?", "question_concept": "using television", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["turn off", "functions", "turning off", "entertainment", "fall asleep"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What can happen to you when you are using television and it is not interesting?\nA. turn off\nB. functions\nC. turning off\nD. entertainment\nE. fall asleep\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can happen to you when you are using television and it is not interesting?\nA. turn off\nB. functions\nC. turning off\nD. entertainment\nE. fall asleep\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can happen to you when you are using television and it is not interesting?\nA. turn off\nB. functions\nC. turning off\nD. entertainment\nE. fall asleep\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can happen to you when you are using television and it is not interesting?\nA. turn off\nB. functions\nC. turning off\nD. entertainment\nE. fall asleep\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can happen to you when you are using television and it is not interesting?\nA. turn off\nB. functions\nC. turning off\nD. entertainment\nE. fall asleep\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.6712348461151123", "False"]], [["-3.4212348461151123", "False"]], [["-3.6712348461151123", "False"]], [["-5.421235084533691", "False"]], [["-1.9212348461151123", "False"]]], "filtered_resps": [["-2.6712348461151123", "False"], ["-3.4212348461151123", "False"], ["-3.6712348461151123", "False"], ["-5.421235084533691", "False"], ["-1.9212348461151123", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "42bcbcf2bf301c01efe3c2273c5fcdb9fc971efaba901e459d2dbfb4a1ad62a4", "prompt_hash": "d214c016779671e8a4ee892b9283d57ad4dbeda70b770f8de3e5f7267c7a09ca", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 795, "doc": {"id": "64dbe5cb840ef4f1d25f8b68db8d5fed", "question": "The business men left the discussion in the dressing room, now they just wanted to relax in the sauna of the what?", "question_concept": "dressing room", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["brush hair", "theater", "house", "dwelling", "bathhouse"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The business men left the discussion in the dressing room, now they just wanted to relax in the sauna of the what?\nA. brush hair\nB. theater\nC. house\nD. dwelling\nE. bathhouse\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The business men left the discussion in the dressing room, now they just wanted to relax in the sauna of the what?\nA. brush hair\nB. theater\nC. house\nD. dwelling\nE. bathhouse\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The business men left the discussion in the dressing room, now they just wanted to relax in the sauna of the what?\nA. brush hair\nB. theater\nC. house\nD. dwelling\nE. bathhouse\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The business men left the discussion in the dressing room, now they just wanted to relax in the sauna of the what?\nA. brush hair\nB. theater\nC. house\nD. dwelling\nE. bathhouse\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The business men left the discussion in the dressing room, now they just wanted to relax in the sauna of the what?\nA. brush hair\nB. theater\nC. house\nD. dwelling\nE. bathhouse\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.312296390533447", "False"]], [["-5.312296390533447", "False"]], [["-7.062296390533447", "False"]], [["-5.562296390533447", "False"]], [["-1.5622965097427368", "False"]]], "filtered_resps": [["-4.312296390533447", "False"], ["-5.312296390533447", "False"], ["-7.062296390533447", "False"], ["-5.562296390533447", "False"], ["-1.5622965097427368", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bbbf040bb438d23f0ba051c10347697fb4683fc851b3613508d6ada346e72ce3", "prompt_hash": "33c1e2b7e268fbe099eb08e0b1061df8cf43e607b02930091b656852ef457c88", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 796, "doc": {"id": "a74753bf249c1cbcff632c5c16b0397b", "question": "Where is a likely place for an ivy plant?", "question_concept": "plant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["flower pot", "shelf", "windowsill", "outside", "sill"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a likely place for an ivy plant?\nA. flower pot\nB. shelf\nC. windowsill\nD. outside\nE. sill\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a likely place for an ivy plant?\nA. flower pot\nB. shelf\nC. windowsill\nD. outside\nE. sill\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a likely place for an ivy plant?\nA. flower pot\nB. shelf\nC. windowsill\nD. outside\nE. sill\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a likely place for an ivy plant?\nA. flower pot\nB. shelf\nC. windowsill\nD. outside\nE. sill\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a likely place for an ivy plant?\nA. flower pot\nB. shelf\nC. windowsill\nD. outside\nE. sill\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1673712730407715", "False"]], [["-5.4173712730407715", "False"]], [["-1.917371392250061", "False"]], [["-3.1673712730407715", "False"]], [["-4.4173712730407715", "False"]]], "filtered_resps": [["-3.1673712730407715", "False"], ["-5.4173712730407715", "False"], ["-1.917371392250061", "False"], ["-3.1673712730407715", "False"], ["-4.4173712730407715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c7ca32f19b9434c402ad3e48aeb42dc877e30c7a92e22460b305db8adef56360", "prompt_hash": "15f2ea5e76b98613a5f2f3050fb7c0856b80d627266ab501b56cde14882b79b9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 797, "doc": {"id": "9190efbd77fe10b989fcaae35e208a0f", "question": "Where has the newest baseball stadium?", "question_concept": "baseball stadium", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["phoenix", "chicago", "antarctica", "san francisco", "urban areas"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where has the newest baseball stadium?\nA. phoenix\nB. chicago\nC. antarctica\nD. san francisco\nE. urban areas\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where has the newest baseball stadium?\nA. phoenix\nB. chicago\nC. antarctica\nD. san francisco\nE. urban areas\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where has the newest baseball stadium?\nA. phoenix\nB. chicago\nC. antarctica\nD. san francisco\nE. urban areas\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where has the newest baseball stadium?\nA. phoenix\nB. chicago\nC. antarctica\nD. san francisco\nE. urban areas\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where has the newest baseball stadium?\nA. phoenix\nB. chicago\nC. antarctica\nD. san francisco\nE. urban areas\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5917773246765137", "False"]], [["-2.5917773246765137", "False"]], [["-5.341777324676514", "False"]], [["-3.0917773246765137", "False"]], [["-4.841777324676514", "False"]]], "filtered_resps": [["-2.5917773246765137", "False"], ["-2.5917773246765137", "False"], ["-5.341777324676514", "False"], ["-3.0917773246765137", "False"], ["-4.841777324676514", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cdb98e785c2c93db421f13df6b78e49456916ca7bd7508bd66a588f98bb979c7", "prompt_hash": "a6152d4d031523849195b6a3c68a6b174afed7cc4cd22f9ea58bf5aa9d57ed85", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 798, "doc": {"id": "ff0303db294a823d4138fb81a6ee6438", "question": "What type of residence has a ground floor with a stoop?", "question_concept": "ground floor", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["brownstone", "hotel", "condominium", "entering building", "office building"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What type of residence has a ground floor with a stoop?\nA. brownstone\nB. hotel\nC. condominium\nD. entering building\nE. office building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What type of residence has a ground floor with a stoop?\nA. brownstone\nB. hotel\nC. condominium\nD. entering building\nE. office building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What type of residence has a ground floor with a stoop?\nA. brownstone\nB. hotel\nC. condominium\nD. entering building\nE. office building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What type of residence has a ground floor with a stoop?\nA. brownstone\nB. hotel\nC. condominium\nD. entering building\nE. office building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What type of residence has a ground floor with a stoop?\nA. brownstone\nB. hotel\nC. condominium\nD. entering building\nE. office building\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.628630518913269", "True"]], [["-5.1246538162231445", "False"]], [["-7.628630638122559", "False"]], [["-7.628630638122559", "False"]], [["-9.628630638122559", "False"]]], "filtered_resps": [["-0.628630518913269", "True"], ["-5.1246538162231445", "False"], ["-7.628630638122559", "False"], ["-7.628630638122559", "False"], ["-9.628630638122559", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8a1830fc2180ea94cad4b1f6466047feb24785eb81f2f89496f8b74787a3069e", "prompt_hash": "dcca7ae44012ee792a66831b137e1cff5a757bd147ca8d7ce35d07c63add2f9b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 799, "doc": {"id": "63963c9c15835d451aac2e1e0b116388", "question": "If the wood texture is not smooth it is what?", "question_concept": "wood", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["gilded", "porous", "solid", "painted", "less dense than water"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If the wood texture is not smooth it is what?\nA. gilded\nB. porous\nC. solid\nD. painted\nE. less dense than water\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If the wood texture is not smooth it is what?\nA. gilded\nB. porous\nC. solid\nD. painted\nE. less dense than water\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If the wood texture is not smooth it is what?\nA. gilded\nB. porous\nC. solid\nD. painted\nE. less dense than water\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If the wood texture is not smooth it is what?\nA. gilded\nB. porous\nC. solid\nD. painted\nE. less dense than water\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If the wood texture is not smooth it is what?\nA. gilded\nB. porous\nC. solid\nD. painted\nE. less dense than water\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.751559734344482", "False"]], [["-0.7515597343444824", "True"]], [["-7.251559734344482", "False"]], [["-6.751559734344482", "False"]], [["-8.25156021118164", "False"]]], "filtered_resps": [["-4.751559734344482", "False"], ["-0.7515597343444824", "True"], ["-7.251559734344482", "False"], ["-6.751559734344482", "False"], ["-8.25156021118164", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "80ba5de3e42d5c4e51879e28170bd1c665fae270e4e55c25c41ac72c6a70af5f", "prompt_hash": "459da2b449a132a33a05733a8505ab06300e8821c7a6d44db8a8ea44194066c6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 800, "doc": {"id": "cc8324b73ed9625e723ef041dfc77a37", "question": "What might happen if someone is not losing weight?", "question_concept": "losing weight", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["loose skin", "beauty", "miss universe", "death", "healthier"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What might happen if someone is not losing weight?\nA. loose skin\nB. beauty\nC. miss universe\nD. death\nE. healthier\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might happen if someone is not losing weight?\nA. loose skin\nB. beauty\nC. miss universe\nD. death\nE. healthier\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might happen if someone is not losing weight?\nA. loose skin\nB. beauty\nC. miss universe\nD. death\nE. healthier\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might happen if someone is not losing weight?\nA. loose skin\nB. beauty\nC. miss universe\nD. death\nE. healthier\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might happen if someone is not losing weight?\nA. loose skin\nB. beauty\nC. miss universe\nD. death\nE. healthier\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4791569709777832", "True"]], [["-6.229156970977783", "False"]], [["-5.479156970977783", "False"]], [["-4.979156970977783", "False"]], [["-3.229156970977783", "False"]]], "filtered_resps": [["-1.4791569709777832", "True"], ["-6.229156970977783", "False"], ["-5.479156970977783", "False"], ["-4.979156970977783", "False"], ["-3.229156970977783", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "61d84122d4fbe67f38dccb55a4151422a83474434f0a8fe5a714ecae6295f9ba", "prompt_hash": "76c4390a7047da1a8466788a01a99698c525127a74d514092ab4e7b6b35b83e7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 801, "doc": {"id": "684dbde19719e8224113433981d6e01e", "question": "Billy lived in the capital of his country, then he moved.  Where might he move to?", "question_concept": "capital", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["small town", "jail", "lower case", "contain governmental activities", "lowercase"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Billy lived in the capital of his country, then he moved.  Where might he move to?\nA. small town\nB. jail\nC. lower case\nD. contain governmental activities\nE. lowercase\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Billy lived in the capital of his country, then he moved.  Where might he move to?\nA. small town\nB. jail\nC. lower case\nD. contain governmental activities\nE. lowercase\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Billy lived in the capital of his country, then he moved.  Where might he move to?\nA. small town\nB. jail\nC. lower case\nD. contain governmental activities\nE. lowercase\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Billy lived in the capital of his country, then he moved.  Where might he move to?\nA. small town\nB. jail\nC. lower case\nD. contain governmental activities\nE. lowercase\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Billy lived in the capital of his country, then he moved.  Where might he move to?\nA. small town\nB. jail\nC. lower case\nD. contain governmental activities\nE. lowercase\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3485841751098633", "False"]], [["-5.848584175109863", "False"]], [["-8.348584175109863", "False"]], [["-3.0985841751098633", "False"]], [["-9.598584175109863", "False"]]], "filtered_resps": [["-3.3485841751098633", "False"], ["-5.848584175109863", "False"], ["-8.348584175109863", "False"], ["-3.0985841751098633", "False"], ["-9.598584175109863", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "953d664277f684cb82a945645ba7169509c596d452e531058d80df3d58efd958", "prompt_hash": "af4d3f97902c84db3a320e893f2f18d72ac7d47f8ec23079852f70351d6a7c44", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 802, "doc": {"id": "21450618657881d8c5af73691f3423a7_1", "question": "Making a schedule was easy to pick, the major called for knowledge that required a certain what?", "question_concept": "knowledge", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["color", "class", "meeting", "university", "encyclopedia"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Making a schedule was easy to pick, the major called for knowledge that required a certain what?\nA. color\nB. class\nC. meeting\nD. university\nE. encyclopedia\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Making a schedule was easy to pick, the major called for knowledge that required a certain what?\nA. color\nB. class\nC. meeting\nD. university\nE. encyclopedia\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Making a schedule was easy to pick, the major called for knowledge that required a certain what?\nA. color\nB. class\nC. meeting\nD. university\nE. encyclopedia\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Making a schedule was easy to pick, the major called for knowledge that required a certain what?\nA. color\nB. class\nC. meeting\nD. university\nE. encyclopedia\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Making a schedule was easy to pick, the major called for knowledge that required a certain what?\nA. color\nB. class\nC. meeting\nD. university\nE. encyclopedia\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.056792259216309", "False"]], [["-2.3067920207977295", "False"]], [["-6.306792259216309", "False"]], [["-5.306792259216309", "False"]], [["-1.8067920207977295", "True"]]], "filtered_resps": [["-4.056792259216309", "False"], ["-2.3067920207977295", "False"], ["-6.306792259216309", "False"], ["-5.306792259216309", "False"], ["-1.8067920207977295", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fdb45ddde79e88e1900d211971209e3c9abdde777abb78a8eae85571d122291b", "prompt_hash": "1d963428dc711b10252b43c93d0494dea3c4736b8b228d0803bd018b43ce322b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 803, "doc": {"id": "8b94b61b604ec0d7508804033eec6d23", "question": "When getting in shape, this is something that does wonders?", "question_concept": "getting in shape", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat more", "starve", "give up", "period of recovery", "jogging"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: When getting in shape, this is something that does wonders?\nA. eat more\nB. starve\nC. give up\nD. period of recovery\nE. jogging\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When getting in shape, this is something that does wonders?\nA. eat more\nB. starve\nC. give up\nD. period of recovery\nE. jogging\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When getting in shape, this is something that does wonders?\nA. eat more\nB. starve\nC. give up\nD. period of recovery\nE. jogging\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When getting in shape, this is something that does wonders?\nA. eat more\nB. starve\nC. give up\nD. period of recovery\nE. jogging\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When getting in shape, this is something that does wonders?\nA. eat more\nB. starve\nC. give up\nD. period of recovery\nE. jogging\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.031458854675293", "False"]], [["-4.781458854675293", "False"]], [["-7.031458854675293", "False"]], [["-2.0153608322143555", "False"]], [["-1.5314586162567139", "True"]]], "filtered_resps": [["-4.031458854675293", "False"], ["-4.781458854675293", "False"], ["-7.031458854675293", "False"], ["-2.0153608322143555", "False"], ["-1.5314586162567139", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d0e7e5345f25236e719c5e813caaa30568bd71370714aeab9c9586d8a53022b2", "prompt_hash": "8b6d7302cc10eb9d9c18c1f9715186ff089fec4e4452122692e11b3df95c4f16", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 804, "doc": {"id": "52ecf169febc95a7f5ccb048fc85857d", "question": "What could prevent a driving car from continuing to drive?", "question_concept": "driving car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["automobile accidents", "backache", "pollution", "smoke", "low fuel tank"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What could prevent a driving car from continuing to drive?\nA. automobile accidents\nB. backache\nC. pollution\nD. smoke\nE. low fuel tank\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could prevent a driving car from continuing to drive?\nA. automobile accidents\nB. backache\nC. pollution\nD. smoke\nE. low fuel tank\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could prevent a driving car from continuing to drive?\nA. automobile accidents\nB. backache\nC. pollution\nD. smoke\nE. low fuel tank\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could prevent a driving car from continuing to drive?\nA. automobile accidents\nB. backache\nC. pollution\nD. smoke\nE. low fuel tank\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could prevent a driving car from continuing to drive?\nA. automobile accidents\nB. backache\nC. pollution\nD. smoke\nE. low fuel tank\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1738572120666504", "False"]], [["-5.17385721206665", "False"]], [["-5.92385721206665", "False"]], [["-4.42385721206665", "False"]], [["-1.1738570928573608", "True"]]], "filtered_resps": [["-2.1738572120666504", "False"], ["-5.17385721206665", "False"], ["-5.92385721206665", "False"], ["-4.42385721206665", "False"], ["-1.1738570928573608", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ea048c9bce45d99ffd15a041e44e849dffd1c10199f025fbbc4a5e4f3f00b80a", "prompt_hash": "c6db0e1de6a2960fd6db24b69bf8d3eb88c9ed102a7b9e8fd4c3bbd4e1e21e7e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 805, "doc": {"id": "e408a5a031caec33782cb3b3a005eecc", "question": "Where do you store a large container?", "question_concept": "large container", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["supermarket", "factory", "juice", "hostel", "cabinet"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you store a large container?\nA. supermarket\nB. factory\nC. juice\nD. hostel\nE. cabinet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you store a large container?\nA. supermarket\nB. factory\nC. juice\nD. hostel\nE. cabinet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you store a large container?\nA. supermarket\nB. factory\nC. juice\nD. hostel\nE. cabinet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you store a large container?\nA. supermarket\nB. factory\nC. juice\nD. hostel\nE. cabinet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you store a large container?\nA. supermarket\nB. factory\nC. juice\nD. hostel\nE. cabinet\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.056856632232666", "False"]], [["-1.8068565130233765", "False"]], [["-6.556856632232666", "False"]], [["-7.056856632232666", "False"]], [["-4.806856632232666", "False"]]], "filtered_resps": [["-3.056856632232666", "False"], ["-1.8068565130233765", "False"], ["-6.556856632232666", "False"], ["-7.056856632232666", "False"], ["-4.806856632232666", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6994b65d92bb6f3839f3b7fc781b553aa571a549dcda9bc29809815652ab1c32", "prompt_hash": "20eb27c73fd8c99af1df63d2f971a5e171ab8921e40675e403824f5f2eabbd04", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 806, "doc": {"id": "31bd05ba62a16ee35217224b98c6baea", "question": "What is a person likely to experience after they stop being married to a mean person?", "question_concept": "stopping being married to", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["isolation", "grief", "happiness", "relief", "angry"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is a person likely to experience after they stop being married to a mean person?\nA. isolation\nB. grief\nC. happiness\nD. relief\nE. angry\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a person likely to experience after they stop being married to a mean person?\nA. isolation\nB. grief\nC. happiness\nD. relief\nE. angry\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a person likely to experience after they stop being married to a mean person?\nA. isolation\nB. grief\nC. happiness\nD. relief\nE. angry\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a person likely to experience after they stop being married to a mean person?\nA. isolation\nB. grief\nC. happiness\nD. relief\nE. angry\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a person likely to experience after they stop being married to a mean person?\nA. isolation\nB. grief\nC. happiness\nD. relief\nE. angry\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1880011558532715", "False"]], [["-4.9380011558532715", "False"]], [["-3.1880011558532715", "False"]], [["-2.1880011558532715", "False"]], [["-9.68800163269043", "False"]]], "filtered_resps": [["-3.1880011558532715", "False"], ["-4.9380011558532715", "False"], ["-3.1880011558532715", "False"], ["-2.1880011558532715", "False"], ["-9.68800163269043", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e83dbd14be129dcb0594ce982e9580f720f874a4facfbf353014722b52206503", "prompt_hash": "496d72f934dd53a39018e246f80458a5c1a0a78eafdbe32208dd1bc5a1b71564", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 807, "doc": {"id": "b4043bd1f65a8ad088e62042eca259c2", "question": "Despite the large crowds, how did the depressed man feel?", "question_concept": "crowd", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["small group", "alone", "solitary", "solitude", "panic"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Despite the large crowds, how did the depressed man feel?\nA. small group\nB. alone\nC. solitary\nD. solitude\nE. panic\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Despite the large crowds, how did the depressed man feel?\nA. small group\nB. alone\nC. solitary\nD. solitude\nE. panic\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Despite the large crowds, how did the depressed man feel?\nA. small group\nB. alone\nC. solitary\nD. solitude\nE. panic\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Despite the large crowds, how did the depressed man feel?\nA. small group\nB. alone\nC. solitary\nD. solitude\nE. panic\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Despite the large crowds, how did the depressed man feel?\nA. small group\nB. alone\nC. solitary\nD. solitude\nE. panic\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.395633220672607", "False"]], [["-2.1456332206726074", "False"]], [["-3.3956332206726074", "False"]], [["-4.645633220672607", "False"]], [["-6.645633220672607", "False"]]], "filtered_resps": [["-4.395633220672607", "False"], ["-2.1456332206726074", "False"], ["-3.3956332206726074", "False"], ["-4.645633220672607", "False"], ["-6.645633220672607", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1571b2e0bf244e505d77df03123a17c9af76cf0b093659de9add235c957796b3", "prompt_hash": "94fe96bdf99590979bea12e9d457390bff516d53cbff3742a701b38a31c3ea76", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 808, "doc": {"id": "4302e727e47f464511d4d04f22bed0d2", "question": "Where does a maid empty a trash can?", "question_concept": "trash can", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bus stop", "corner", "hockey game", "motel", "alley"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where does a maid empty a trash can?\nA. bus stop\nB. corner\nC. hockey game\nD. motel\nE. alley\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where does a maid empty a trash can?\nA. bus stop\nB. corner\nC. hockey game\nD. motel\nE. alley\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where does a maid empty a trash can?\nA. bus stop\nB. corner\nC. hockey game\nD. motel\nE. alley\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where does a maid empty a trash can?\nA. bus stop\nB. corner\nC. hockey game\nD. motel\nE. alley\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where does a maid empty a trash can?\nA. bus stop\nB. corner\nC. hockey game\nD. motel\nE. alley\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.258415699005127", "False"]], [["-3.008415699005127", "False"]], [["-8.008415222167969", "False"]], [["-2.758415699005127", "False"]], [["-5.758415699005127", "False"]]], "filtered_resps": [["-2.258415699005127", "False"], ["-3.008415699005127", "False"], ["-8.008415222167969", "False"], ["-2.758415699005127", "False"], ["-5.758415699005127", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dc82d86455151c28ccac7e38afac3261f883aa0f403d34ad56e32d1afd39fe9a", "prompt_hash": "1fa343a17fb4dbe4917e6b1fcbd3b5c99b2c1ac730f3855b56063799d3bf6760", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 809, "doc": {"id": "f0d473701d52125dd055d23042de1b0d", "question": "The dog curled up for a nap, it was tuckered out because it had just been what?", "question_concept": "dog", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["walked", "petted", "affection", "go outside", "scratch"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The dog curled up for a nap, it was tuckered out because it had just been what?\nA. walked\nB. petted\nC. affection\nD. go outside\nE. scratch\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The dog curled up for a nap, it was tuckered out because it had just been what?\nA. walked\nB. petted\nC. affection\nD. go outside\nE. scratch\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The dog curled up for a nap, it was tuckered out because it had just been what?\nA. walked\nB. petted\nC. affection\nD. go outside\nE. scratch\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The dog curled up for a nap, it was tuckered out because it had just been what?\nA. walked\nB. petted\nC. affection\nD. go outside\nE. scratch\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The dog curled up for a nap, it was tuckered out because it had just been what?\nA. walked\nB. petted\nC. affection\nD. go outside\nE. scratch\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6351014375686646", "True"]], [["-5.635101318359375", "False"]], [["-6.635101318359375", "False"]], [["-5.135101318359375", "False"]], [["-9.385101318359375", "False"]]], "filtered_resps": [["-0.6351014375686646", "True"], ["-5.635101318359375", "False"], ["-6.635101318359375", "False"], ["-5.135101318359375", "False"], ["-9.385101318359375", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "38cc603ca35ac7285ca6923ca3b81fcd54d4dee78d1e5b6bd289dcd00d80671c", "prompt_hash": "99364fbbcfd7accc2c08226d079782b965e8eb12fb58eae0bb68b195e2ceb4bf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 810, "doc": {"id": "d35112a99ab3983fb51c3adae80bc2da", "question": "He used an umbrella while tanning, where was he likely?", "question_concept": "umbrella", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["waves", "seattle", "suitcase", "beach", "jacket closet"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He used an umbrella while tanning, where was he likely?\nA. waves\nB. seattle\nC. suitcase\nD. beach\nE. jacket closet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He used an umbrella while tanning, where was he likely?\nA. waves\nB. seattle\nC. suitcase\nD. beach\nE. jacket closet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He used an umbrella while tanning, where was he likely?\nA. waves\nB. seattle\nC. suitcase\nD. beach\nE. jacket closet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He used an umbrella while tanning, where was he likely?\nA. waves\nB. seattle\nC. suitcase\nD. beach\nE. jacket closet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He used an umbrella while tanning, where was he likely?\nA. waves\nB. seattle\nC. suitcase\nD. beach\nE. jacket closet\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.773563861846924", "False"]], [["-2.273563861846924", "False"]], [["-6.773563861846924", "False"]], [["-2.023563861846924", "False"]], [["-7.523563861846924", "False"]]], "filtered_resps": [["-3.773563861846924", "False"], ["-2.273563861846924", "False"], ["-6.773563861846924", "False"], ["-2.023563861846924", "False"], ["-7.523563861846924", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f5cbb8bf439e078b926207b423569cdd2003633cb2967e3ba0ed5d13aa1de6f7", "prompt_hash": "249a33d2b8dd1ddd0d2b98d284bd36cf630497552f1eced73080f83d6441377a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 811, "doc": {"id": "661474a1a0c29dd7a243b284535ac934", "question": "What do the feathers look like on birds found in the rainforest?", "question_concept": "birds", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pretty smart", "singing", "dark", "very colorful", "light"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What do the feathers look like on birds found in the rainforest?\nA. pretty smart\nB. singing\nC. dark\nD. very colorful\nE. light\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do the feathers look like on birds found in the rainforest?\nA. pretty smart\nB. singing\nC. dark\nD. very colorful\nE. light\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do the feathers look like on birds found in the rainforest?\nA. pretty smart\nB. singing\nC. dark\nD. very colorful\nE. light\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do the feathers look like on birds found in the rainforest?\nA. pretty smart\nB. singing\nC. dark\nD. very colorful\nE. light\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do the feathers look like on birds found in the rainforest?\nA. pretty smart\nB. singing\nC. dark\nD. very colorful\nE. light\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.815815448760986", "False"]], [["-4.315815448760986", "False"]], [["-4.315815448760986", "False"]], [["-1.3158155679702759", "True"]], [["-9.315815925598145", "False"]]], "filtered_resps": [["-4.815815448760986", "False"], ["-4.315815448760986", "False"], ["-4.315815448760986", "False"], ["-1.3158155679702759", "True"], ["-9.315815925598145", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fd24cd9fdefe68826d337fe40e160132b9d4e48c6fba875f6db3aea3496f6058", "prompt_hash": "6e71506af04792606994ab08c48c7f3c10a83951878a25a1d5baa9556d2b6fa9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 812, "doc": {"id": "6416dcdf9b8d7d2787f07e7426f86fe4", "question": "The ancient seafaring Norse tribesman brought pelts of weasel aboard his what?", "question_concept": "weasel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rabbit warren", "used car lot", "chicken coop", "cruise", "viking ship"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The ancient seafaring Norse tribesman brought pelts of weasel aboard his what?\nA. rabbit warren\nB. used car lot\nC. chicken coop\nD. cruise\nE. viking ship\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The ancient seafaring Norse tribesman brought pelts of weasel aboard his what?\nA. rabbit warren\nB. used car lot\nC. chicken coop\nD. cruise\nE. viking ship\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The ancient seafaring Norse tribesman brought pelts of weasel aboard his what?\nA. rabbit warren\nB. used car lot\nC. chicken coop\nD. cruise\nE. viking ship\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The ancient seafaring Norse tribesman brought pelts of weasel aboard his what?\nA. rabbit warren\nB. used car lot\nC. chicken coop\nD. cruise\nE. viking ship\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The ancient seafaring Norse tribesman brought pelts of weasel aboard his what?\nA. rabbit warren\nB. used car lot\nC. chicken coop\nD. cruise\nE. viking ship\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.367295265197754", "False"]], [["-7.617295265197754", "False"]], [["-8.867295265197754", "False"]], [["-8.117295265197754", "False"]], [["-1.3672953844070435", "True"]]], "filtered_resps": [["-5.367295265197754", "False"], ["-7.617295265197754", "False"], ["-8.867295265197754", "False"], ["-8.117295265197754", "False"], ["-1.3672953844070435", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b5e3a767b9f54fc6af7344584c03bcc84182e188e136d762a1da508957f3c359", "prompt_hash": "e3af9a0cea54ff66cf54221ddc35ea3e1cd0a7b13e76fd16de33acef36d90898", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 813, "doc": {"id": "0f54a1ee30a0034a3d2db1bfdef9ca85", "question": "What is the opposite of an area of elevation?", "question_concept": "elevation", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["disgust", "reduction", "depression", "demotion", "diminishment"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is the opposite of an area of elevation?\nA. disgust\nB. reduction\nC. depression\nD. demotion\nE. diminishment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the opposite of an area of elevation?\nA. disgust\nB. reduction\nC. depression\nD. demotion\nE. diminishment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the opposite of an area of elevation?\nA. disgust\nB. reduction\nC. depression\nD. demotion\nE. diminishment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the opposite of an area of elevation?\nA. disgust\nB. reduction\nC. depression\nD. demotion\nE. diminishment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the opposite of an area of elevation?\nA. disgust\nB. reduction\nC. depression\nD. demotion\nE. diminishment\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.10449743270874", "False"]], [["-8.104496955871582", "False"]], [["-1.8544973134994507", "False"]], [["-8.354496955871582", "False"]], [["-6.60449743270874", "False"]]], "filtered_resps": [["-4.10449743270874", "False"], ["-8.104496955871582", "False"], ["-1.8544973134994507", "False"], ["-8.354496955871582", "False"], ["-6.60449743270874", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "03e8ada65f6d49c038f349a397700d31103b17d76943543c0fed7dec12c4ea67", "prompt_hash": "44fe23afd415830f77411bf0a52072a18955e66840016799c4692d0de14939a8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 814, "doc": {"id": "7850beb1209c41fabe385cbedc96a61a", "question": "What do singers need to do before a show?", "question_concept": "singers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["warm up", "use microphones", "clear throats", "create music", "sound beautiful"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do singers need to do before a show?\nA. warm up\nB. use microphones\nC. clear throats\nD. create music\nE. sound beautiful\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do singers need to do before a show?\nA. warm up\nB. use microphones\nC. clear throats\nD. create music\nE. sound beautiful\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do singers need to do before a show?\nA. warm up\nB. use microphones\nC. clear throats\nD. create music\nE. sound beautiful\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do singers need to do before a show?\nA. warm up\nB. use microphones\nC. clear throats\nD. create music\nE. sound beautiful\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do singers need to do before a show?\nA. warm up\nB. use microphones\nC. clear throats\nD. create music\nE. sound beautiful\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6128774285316467", "True"]], [["-5.112877368927002", "False"]], [["-4.362877368927002", "False"]], [["-6.362877368927002", "False"]], [["-5.362877368927002", "False"]]], "filtered_resps": [["-0.6128774285316467", "True"], ["-5.112877368927002", "False"], ["-4.362877368927002", "False"], ["-6.362877368927002", "False"], ["-5.362877368927002", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "04caa05cb5015515f145cbbbd58cd6a8b9b935462e57f0103ea9494c2e38cb41", "prompt_hash": "97d8ecc9aaae4d53c37fd05f6767e145043f75eeef2d792ae0cd3a71996e9845", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 815, "doc": {"id": "cdb06b28b9c4e7ef7e880d1f096fd409", "question": "When a person with mental illness receives medication and therapy, what has happened?", "question_concept": "mental illness", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cause irrational behaviour", "recur", "effectively treated", "managed", "cause suffering"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: When a person with mental illness receives medication and therapy, what has happened?\nA. cause irrational behaviour\nB. recur\nC. effectively treated\nD. managed\nE. cause suffering\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When a person with mental illness receives medication and therapy, what has happened?\nA. cause irrational behaviour\nB. recur\nC. effectively treated\nD. managed\nE. cause suffering\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When a person with mental illness receives medication and therapy, what has happened?\nA. cause irrational behaviour\nB. recur\nC. effectively treated\nD. managed\nE. cause suffering\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When a person with mental illness receives medication and therapy, what has happened?\nA. cause irrational behaviour\nB. recur\nC. effectively treated\nD. managed\nE. cause suffering\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When a person with mental illness receives medication and therapy, what has happened?\nA. cause irrational behaviour\nB. recur\nC. effectively treated\nD. managed\nE. cause suffering\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.753952980041504", "False"]], [["-5.753952980041504", "False"]], [["-2.253952980041504", "False"]], [["-4.253952980041504", "False"]], [["-6.503952980041504", "False"]]], "filtered_resps": [["-4.753952980041504", "False"], ["-5.753952980041504", "False"], ["-2.253952980041504", "False"], ["-4.253952980041504", "False"], ["-6.503952980041504", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c481238d4af7acb16836dd3702c770d8dbff05e2ad8e28914c289f7e21ef909a", "prompt_hash": "7fc2ab71ae860c4508a6a3e1bc83f1cbfae081ee905d959812abdddd7e690cf5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 816, "doc": {"id": "14309d9bd3c13d1c0efb625198f6304a", "question": "What type of feeling is performing for the first time likely to produce?", "question_concept": "performing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["action", "butterflies", "happiness", "a sense of calm", "anxiety"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What type of feeling is performing for the first time likely to produce?\nA. action\nB. butterflies\nC. happiness\nD. a sense of calm\nE. anxiety\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What type of feeling is performing for the first time likely to produce?\nA. action\nB. butterflies\nC. happiness\nD. a sense of calm\nE. anxiety\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What type of feeling is performing for the first time likely to produce?\nA. action\nB. butterflies\nC. happiness\nD. a sense of calm\nE. anxiety\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What type of feeling is performing for the first time likely to produce?\nA. action\nB. butterflies\nC. happiness\nD. a sense of calm\nE. anxiety\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What type of feeling is performing for the first time likely to produce?\nA. action\nB. butterflies\nC. happiness\nD. a sense of calm\nE. anxiety\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.855193138122559", "False"]], [["-3.1051931381225586", "False"]], [["-6.855193138122559", "False"]], [["-8.105193138122559", "False"]], [["-1.605193018913269", "True"]]], "filtered_resps": [["-4.855193138122559", "False"], ["-3.1051931381225586", "False"], ["-6.855193138122559", "False"], ["-8.105193138122559", "False"], ["-1.605193018913269", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d50d30d5c00c5d42e084e9c6a4920f267c60c2785df87b5f0fbd84ea60fb94f6", "prompt_hash": "83a59230b0ef8ef1165ef85e71c4cba877b8e6e18fae8acb1a4c6affd978ed97", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 817, "doc": {"id": "a00276c6db928900772c0320aeff77c0", "question": "If someone is found to be committing murder, what did they do to someone?", "question_concept": "committing murder", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["misery", "kill", "distress", "tickel", "go to jail"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If someone is found to be committing murder, what did they do to someone?\nA. misery\nB. kill\nC. distress\nD. tickel\nE. go to jail\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If someone is found to be committing murder, what did they do to someone?\nA. misery\nB. kill\nC. distress\nD. tickel\nE. go to jail\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If someone is found to be committing murder, what did they do to someone?\nA. misery\nB. kill\nC. distress\nD. tickel\nE. go to jail\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If someone is found to be committing murder, what did they do to someone?\nA. misery\nB. kill\nC. distress\nD. tickel\nE. go to jail\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If someone is found to be committing murder, what did they do to someone?\nA. misery\nB. kill\nC. distress\nD. tickel\nE. go to jail\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.630118370056152", "False"]], [["-0.630118191242218", "True"]], [["-8.130118370056152", "False"]], [["-8.880118370056152", "False"]], [["-9.630118370056152", "False"]]], "filtered_resps": [["-5.630118370056152", "False"], ["-0.630118191242218", "True"], ["-8.130118370056152", "False"], ["-8.880118370056152", "False"], ["-9.630118370056152", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "67754e6c8f1ab9f7c98b520eada2e6110fbecae7c50c1c01c2aadc72d4bd9b1a", "prompt_hash": "2415e5ca86cf3cbe4d0eee7bc838ba68543d93668f157d9eb7a1e5384099d2eb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 818, "doc": {"id": "4706be6e24f1fafd9ff9fe63583acffd", "question": "The computer was hooked up to the internet, what could it do as a result?", "question_concept": "computer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["process information", "believe in god", "make decisions", "process information", "receive data"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The computer was hooked up to the internet, what could it do as a result?\nA. process information\nB. believe in god\nC. make decisions\nD. process information\nE. receive data\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The computer was hooked up to the internet, what could it do as a result?\nA. process information\nB. believe in god\nC. make decisions\nD. process information\nE. receive data\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The computer was hooked up to the internet, what could it do as a result?\nA. process information\nB. believe in god\nC. make decisions\nD. process information\nE. receive data\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The computer was hooked up to the internet, what could it do as a result?\nA. process information\nB. believe in god\nC. make decisions\nD. process information\nE. receive data\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The computer was hooked up to the internet, what could it do as a result?\nA. process information\nB. believe in god\nC. make decisions\nD. process information\nE. receive data\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.891870141029358", "False"]], [["-6.141870021820068", "False"]], [["-5.391870021820068", "False"]], [["-3.3918700218200684", "False"]], [["-3.3918700218200684", "False"]]], "filtered_resps": [["-1.891870141029358", "False"], ["-6.141870021820068", "False"], ["-5.391870021820068", "False"], ["-3.3918700218200684", "False"], ["-3.3918700218200684", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b7fe579d1e6ef7a97963d1564498c94b705544d1a36c87212954500c5a918a8e", "prompt_hash": "78da86ddfc3f582afb8b321424ee281150d77af0d4910838cde1d5f0bd1c2092", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 819, "doc": {"id": "ee8819b2da5453848c1cbb9d9c93403b", "question": "The planet Mercury is unsuitable for human life or what?", "question_concept": "mercury", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["toxic", "uninhabitable", "mercury sulphide", "poisonous", "jupiter"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The planet Mercury is unsuitable for human life or what?\nA. toxic\nB. uninhabitable\nC. mercury sulphide\nD. poisonous\nE. jupiter\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The planet Mercury is unsuitable for human life or what?\nA. toxic\nB. uninhabitable\nC. mercury sulphide\nD. poisonous\nE. jupiter\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The planet Mercury is unsuitable for human life or what?\nA. toxic\nB. uninhabitable\nC. mercury sulphide\nD. poisonous\nE. jupiter\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The planet Mercury is unsuitable for human life or what?\nA. toxic\nB. uninhabitable\nC. mercury sulphide\nD. poisonous\nE. jupiter\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The planet Mercury is unsuitable for human life or what?\nA. toxic\nB. uninhabitable\nC. mercury sulphide\nD. poisonous\nE. jupiter\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.623053073883057", "False"]], [["-0.8730530738830566", "True"]], [["-8.373052597045898", "False"]], [["-7.623053073883057", "False"]], [["-8.123052597045898", "False"]]], "filtered_resps": [["-5.623053073883057", "False"], ["-0.8730530738830566", "True"], ["-8.373052597045898", "False"], ["-7.623053073883057", "False"], ["-8.123052597045898", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "95a990e100693d714601462b836f52aa967d1ee6669980eb957ab886ef2e5770", "prompt_hash": "3afdf46a132b7b192256efcbb5797e93e18080e3ecd7914b184f2a473eabb5a6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 820, "doc": {"id": "84ea43b967259814d939c62131f74df0", "question": "Seeing idea become reality was a dream of hers for a long time, but as the time came to get on stage she had more what?", "question_concept": "seeing idea become reality", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["build", "anxiety", "celebrate", "very nice", "ocean"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Seeing idea become reality was a dream of hers for a long time, but as the time came to get on stage she had more what?\nA. build\nB. anxiety\nC. celebrate\nD. very nice\nE. ocean\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Seeing idea become reality was a dream of hers for a long time, but as the time came to get on stage she had more what?\nA. build\nB. anxiety\nC. celebrate\nD. very nice\nE. ocean\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Seeing idea become reality was a dream of hers for a long time, but as the time came to get on stage she had more what?\nA. build\nB. anxiety\nC. celebrate\nD. very nice\nE. ocean\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Seeing idea become reality was a dream of hers for a long time, but as the time came to get on stage she had more what?\nA. build\nB. anxiety\nC. celebrate\nD. very nice\nE. ocean\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Seeing idea become reality was a dream of hers for a long time, but as the time came to get on stage she had more what?\nA. build\nB. anxiety\nC. celebrate\nD. very nice\nE. ocean\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.6833343505859375", "False"]], [["-1.4333341121673584", "True"]], [["-8.183334350585938", "False"]], [["-7.4333343505859375", "False"]], [["-8.683334350585938", "False"]]], "filtered_resps": [["-5.6833343505859375", "False"], ["-1.4333341121673584", "True"], ["-8.183334350585938", "False"], ["-7.4333343505859375", "False"], ["-8.683334350585938", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a58689d7b7a2aa1c7aba6c4b9fe5f3ee7fd37e142d12b785f4cafaceb4e3e7c9", "prompt_hash": "de901e443b18e68d96c4837b0b648670849f63551414a2ca2235be5fbafd9dad", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 821, "doc": {"id": "60e7338e9e6bfc746a15a161eb12706c", "question": "A creek could be located in the opposite for the city which is called what?", "question_concept": "creek", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["meadow", "stick", "valley", "forest", "countryside"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: A creek could be located in the opposite for the city which is called what?\nA. meadow\nB. stick\nC. valley\nD. forest\nE. countryside\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A creek could be located in the opposite for the city which is called what?\nA. meadow\nB. stick\nC. valley\nD. forest\nE. countryside\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A creek could be located in the opposite for the city which is called what?\nA. meadow\nB. stick\nC. valley\nD. forest\nE. countryside\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A creek could be located in the opposite for the city which is called what?\nA. meadow\nB. stick\nC. valley\nD. forest\nE. countryside\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A creek could be located in the opposite for the city which is called what?\nA. meadow\nB. stick\nC. valley\nD. forest\nE. countryside\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.244154453277588", "True"]], [["-6.244154453277588", "False"]], [["-1.494154453277588", "False"]], [["-5.744154453277588", "False"]], [["-6.494154453277588", "False"]]], "filtered_resps": [["-1.244154453277588", "True"], ["-6.244154453277588", "False"], ["-1.494154453277588", "False"], ["-5.744154453277588", "False"], ["-6.494154453277588", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "95a9978253589a511662eff5d2ebe23bba50799beb4e62fa6ac55cc789157d5e", "prompt_hash": "b21b858f5128e5fc59ddcb0bf61ec6574a67852b4b7deba9c5740df1af7c178c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 822, "doc": {"id": "a0f5414bf98e094f4d807abee28861a4", "question": "Where off the eastern U.S. would you find an anemone?", "question_concept": "anemone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["flower bed", "tide pool", "florida keys", "coral sea", "aquarium"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where off the eastern U.S. would you find an anemone?\nA. flower bed\nB. tide pool\nC. florida keys\nD. coral sea\nE. aquarium\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where off the eastern U.S. would you find an anemone?\nA. flower bed\nB. tide pool\nC. florida keys\nD. coral sea\nE. aquarium\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where off the eastern U.S. would you find an anemone?\nA. flower bed\nB. tide pool\nC. florida keys\nD. coral sea\nE. aquarium\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where off the eastern U.S. would you find an anemone?\nA. flower bed\nB. tide pool\nC. florida keys\nD. coral sea\nE. aquarium\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where off the eastern U.S. would you find an anemone?\nA. flower bed\nB. tide pool\nC. florida keys\nD. coral sea\nE. aquarium\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.899409294128418", "False"]], [["-2.149409294128418", "False"]], [["-1.8994094133377075", "False"]], [["-4.399409294128418", "False"]], [["-5.149409294128418", "False"]]], "filtered_resps": [["-3.899409294128418", "False"], ["-2.149409294128418", "False"], ["-1.8994094133377075", "False"], ["-4.399409294128418", "False"], ["-5.149409294128418", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6d7f4dfcc4442baefed9861848f7a1c9f2c7c979deeba24101e2ed8389b5eb81", "prompt_hash": "e31c80280d47f140a6e2dc2e07b31b9c0e148d23ef7c6a016de0d0d7b989e142", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 823, "doc": {"id": "44120a9443c619d98ce5bfe4bb219c43", "question": "Where are traveling clothes often kept?", "question_concept": "clothes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["suitcase", "bedroom", "closet", "draws", "dresser"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where are traveling clothes often kept?\nA. suitcase\nB. bedroom\nC. closet\nD. draws\nE. dresser\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are traveling clothes often kept?\nA. suitcase\nB. bedroom\nC. closet\nD. draws\nE. dresser\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are traveling clothes often kept?\nA. suitcase\nB. bedroom\nC. closet\nD. draws\nE. dresser\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are traveling clothes often kept?\nA. suitcase\nB. bedroom\nC. closet\nD. draws\nE. dresser\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are traveling clothes often kept?\nA. suitcase\nB. bedroom\nC. closet\nD. draws\nE. dresser\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.9336662292480469", "False"]], [["-5.683666229248047", "False"]], [["-4.183666229248047", "False"]], [["-4.933666229248047", "False"]], [["-4.433666229248047", "False"]]], "filtered_resps": [["-1.9336662292480469", "False"], ["-5.683666229248047", "False"], ["-4.183666229248047", "False"], ["-4.933666229248047", "False"], ["-4.433666229248047", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "20ca5cee579231eb2a5a94fb5487dec6a52cf28596b2bddd9cbfe4da3b4531e3", "prompt_hash": "0723617ab08524cb803dda01246414ccc6e424b247abfa669a81010366d152d9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 824, "doc": {"id": "38ab26e29a0984b212006d39185c43f3", "question": "If one needed the bathroom they needed a key, to get it they had to also buy something from the what?", "question_concept": "bathroom", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["school", "convenience store", "rest area", "mall", "theater"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If one needed the bathroom they needed a key, to get it they had to also buy something from the what?\nA. school\nB. convenience store\nC. rest area\nD. mall\nE. theater\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If one needed the bathroom they needed a key, to get it they had to also buy something from the what?\nA. school\nB. convenience store\nC. rest area\nD. mall\nE. theater\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If one needed the bathroom they needed a key, to get it they had to also buy something from the what?\nA. school\nB. convenience store\nC. rest area\nD. mall\nE. theater\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If one needed the bathroom they needed a key, to get it they had to also buy something from the what?\nA. school\nB. convenience store\nC. rest area\nD. mall\nE. theater\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If one needed the bathroom they needed a key, to get it they had to also buy something from the what?\nA. school\nB. convenience store\nC. rest area\nD. mall\nE. theater\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.236506462097168", "False"]], [["-1.486506462097168", "True"]], [["-4.986506462097168", "False"]], [["-6.986506462097168", "False"]], [["-8.736506462097168", "False"]]], "filtered_resps": [["-4.236506462097168", "False"], ["-1.486506462097168", "True"], ["-4.986506462097168", "False"], ["-6.986506462097168", "False"], ["-8.736506462097168", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "79e15f57dfcfd9db2f91af8d0e0b4fd8d6b32364ab60628384935ca84c0e6c4c", "prompt_hash": "f080e8adf8b45f6c9f2bd171829d807c40612c25a16474c7c5a97ec5c91c0cb2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 825, "doc": {"id": "a5e207803684eea8a43ca6670c50b354", "question": "Although the sun did rise, what did the pessimist warn everyone it would do?", "question_concept": "rise", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lay", "go down", "fall", "below", "sundown"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Although the sun did rise, what did the pessimist warn everyone it would do?\nA. lay\nB. go down\nC. fall\nD. below\nE. sundown\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Although the sun did rise, what did the pessimist warn everyone it would do?\nA. lay\nB. go down\nC. fall\nD. below\nE. sundown\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Although the sun did rise, what did the pessimist warn everyone it would do?\nA. lay\nB. go down\nC. fall\nD. below\nE. sundown\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Although the sun did rise, what did the pessimist warn everyone it would do?\nA. lay\nB. go down\nC. fall\nD. below\nE. sundown\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Although the sun did rise, what did the pessimist warn everyone it would do?\nA. lay\nB. go down\nC. fall\nD. below\nE. sundown\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7826426029205322", "False"]], [["-3.2826426029205322", "False"]], [["-2.7826426029205322", "False"]], [["-5.032642364501953", "False"]], [["-3.7826426029205322", "False"]]], "filtered_resps": [["-2.7826426029205322", "False"], ["-3.2826426029205322", "False"], ["-2.7826426029205322", "False"], ["-5.032642364501953", "False"], ["-3.7826426029205322", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5e9aec33406b4a5fa0ad73d019ddb8db663186b24c0f166801a4687044954304", "prompt_hash": "5c729a9d7ba4e8884b886b09e51f06c9cb6cb5a0f50627ec33f6538462d51ac0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 826, "doc": {"id": "af3b9a8b1962cd3bcd19e644d873e7bc", "question": "The hardcovers were especially tall, so he removed a shelf on the what to make room?", "question_concept": "shelf", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["chest of drawers", "grocery store", "hold alcohol", "nightstand", "bookcase"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The hardcovers were especially tall, so he removed a shelf on the what to make room?\nA. chest of drawers\nB. grocery store\nC. hold alcohol\nD. nightstand\nE. bookcase\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The hardcovers were especially tall, so he removed a shelf on the what to make room?\nA. chest of drawers\nB. grocery store\nC. hold alcohol\nD. nightstand\nE. bookcase\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The hardcovers were especially tall, so he removed a shelf on the what to make room?\nA. chest of drawers\nB. grocery store\nC. hold alcohol\nD. nightstand\nE. bookcase\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The hardcovers were especially tall, so he removed a shelf on the what to make room?\nA. chest of drawers\nB. grocery store\nC. hold alcohol\nD. nightstand\nE. bookcase\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The hardcovers were especially tall, so he removed a shelf on the what to make room?\nA. chest of drawers\nB. grocery store\nC. hold alcohol\nD. nightstand\nE. bookcase\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.5798187255859375", "False"]], [["-7.5798187255859375", "False"]], [["-8.329818725585938", "False"]], [["-5.8298187255859375", "False"]], [["-1.0798184871673584", "True"]]], "filtered_resps": [["-4.5798187255859375", "False"], ["-7.5798187255859375", "False"], ["-8.329818725585938", "False"], ["-5.8298187255859375", "False"], ["-1.0798184871673584", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9f7ece55619c015cc9f126f880774ca5dedfb0a0a5789fc4d8bd78c72a27bc24", "prompt_hash": "764170874af2a165332ece9117405ca63e7827e3a627ed2651c47031e57cafca", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 827, "doc": {"id": "43a91955fd0717997a16897c3324e095", "question": "If you're watching a comedy film what would you expect to hear from the audience?", "question_concept": "watching film", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["park", "insight", "being entertained", "laughter", "fear"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If you're watching a comedy film what would you expect to hear from the audience?\nA. park\nB. insight\nC. being entertained\nD. laughter\nE. fear\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're watching a comedy film what would you expect to hear from the audience?\nA. park\nB. insight\nC. being entertained\nD. laughter\nE. fear\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're watching a comedy film what would you expect to hear from the audience?\nA. park\nB. insight\nC. being entertained\nD. laughter\nE. fear\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're watching a comedy film what would you expect to hear from the audience?\nA. park\nB. insight\nC. being entertained\nD. laughter\nE. fear\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're watching a comedy film what would you expect to hear from the audience?\nA. park\nB. insight\nC. being entertained\nD. laughter\nE. fear\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.857997417449951", "False"]], [["-7.607997417449951", "False"]], [["-4.857997417449951", "False"]], [["-0.8579972982406616", "True"]], [["-10.107996940612793", "False"]]], "filtered_resps": [["-5.857997417449951", "False"], ["-7.607997417449951", "False"], ["-4.857997417449951", "False"], ["-0.8579972982406616", "True"], ["-10.107996940612793", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1ad6ef86e5e6d645d99848e28b8c922a30dc79569ee21e1e883dc9f2cbd79ac1", "prompt_hash": "f7b36c5f642b7b8236bcb67cc71e96e7ac0839f6598f8b43543fd928dc5a4fd5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 828, "doc": {"id": "7f7a6f2b3087bf37dadbe8aa8d358047", "question": "What can eating lunch cause that is painful?", "question_concept": "eating lunch", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["headache", "bad breath", "heartburn", "gain weight", "farts"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What can eating lunch cause that is painful?\nA. headache\nB. bad breath\nC. heartburn\nD. gain weight\nE. farts\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can eating lunch cause that is painful?\nA. headache\nB. bad breath\nC. heartburn\nD. gain weight\nE. farts\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can eating lunch cause that is painful?\nA. headache\nB. bad breath\nC. heartburn\nD. gain weight\nE. farts\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can eating lunch cause that is painful?\nA. headache\nB. bad breath\nC. heartburn\nD. gain weight\nE. farts\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can eating lunch cause that is painful?\nA. headache\nB. bad breath\nC. heartburn\nD. gain weight\nE. farts\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.379497528076172", "False"]], [["-6.879497528076172", "False"]], [["-1.1294972896575928", "True"]], [["-6.879497528076172", "False"]], [["-1.1294972896575928", "True"]]], "filtered_resps": [["-4.379497528076172", "False"], ["-6.879497528076172", "False"], ["-1.1294972896575928", "True"], ["-6.879497528076172", "False"], ["-1.1294972896575928", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ee5771775c915e178193dba21ae794440fdebe311996c906f829255b86101f03", "prompt_hash": "7117c8987f224f3f35d1cf21e91cdac769c3cd550f2263a83fb3fb95da0cf0c6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 829, "doc": {"id": "37d88a9bb24913c1973cc26d4ce3394f", "question": "The performer was ready to put on a show and stepped onto the launch platform, what was his job?", "question_concept": "launch platform", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cape canaveral florida", "nasa", "battleship", "ocean", "trapeze"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The performer was ready to put on a show and stepped onto the launch platform, what was his job?\nA. cape canaveral florida\nB. nasa\nC. battleship\nD. ocean\nE. trapeze\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The performer was ready to put on a show and stepped onto the launch platform, what was his job?\nA. cape canaveral florida\nB. nasa\nC. battleship\nD. ocean\nE. trapeze\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The performer was ready to put on a show and stepped onto the launch platform, what was his job?\nA. cape canaveral florida\nB. nasa\nC. battleship\nD. ocean\nE. trapeze\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The performer was ready to put on a show and stepped onto the launch platform, what was his job?\nA. cape canaveral florida\nB. nasa\nC. battleship\nD. ocean\nE. trapeze\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The performer was ready to put on a show and stepped onto the launch platform, what was his job?\nA. cape canaveral florida\nB. nasa\nC. battleship\nD. ocean\nE. trapeze\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.776614189147949", "False"]], [["-6.026614189147949", "False"]], [["-8.27661418914795", "False"]], [["-9.02661418914795", "False"]], [["-1.2766139507293701", "False"]]], "filtered_resps": [["-4.776614189147949", "False"], ["-6.026614189147949", "False"], ["-8.27661418914795", "False"], ["-9.02661418914795", "False"], ["-1.2766139507293701", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "337db16dc59a1ee97d49805718a91d6fb3b4825d7ce5ffa6bea299a8a4155427", "prompt_hash": "830864a29204ba569358154bf1448c639a4e9199e3eb8a4fc12203e8401e721b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 830, "doc": {"id": "001b0f5a841fd81d13fbe67c7c7179d6", "question": "Eating is part of living, but your body doesn't use it all and the next day you will be doing what?", "question_concept": "eating", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["reduced", "getting full", "becoming full", "chewing", "defecating"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Eating is part of living, but your body doesn't use it all and the next day you will be doing what?\nA. reduced\nB. getting full\nC. becoming full\nD. chewing\nE. defecating\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Eating is part of living, but your body doesn't use it all and the next day you will be doing what?\nA. reduced\nB. getting full\nC. becoming full\nD. chewing\nE. defecating\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Eating is part of living, but your body doesn't use it all and the next day you will be doing what?\nA. reduced\nB. getting full\nC. becoming full\nD. chewing\nE. defecating\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Eating is part of living, but your body doesn't use it all and the next day you will be doing what?\nA. reduced\nB. getting full\nC. becoming full\nD. chewing\nE. defecating\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Eating is part of living, but your body doesn't use it all and the next day you will be doing what?\nA. reduced\nB. getting full\nC. becoming full\nD. chewing\nE. defecating\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.176729679107666", "False"]], [["-3.176729679107666", "False"]], [["-3.176729679107666", "False"]], [["-6.426729679107666", "False"]], [["-1.1767295598983765", "True"]]], "filtered_resps": [["-4.176729679107666", "False"], ["-3.176729679107666", "False"], ["-3.176729679107666", "False"], ["-6.426729679107666", "False"], ["-1.1767295598983765", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5fa200293e6b304f3af8bd7e57c6df28b34a8e9d7aa46e127693b73fefb3215a", "prompt_hash": "fb3d69dc3222b84cbfaca29812089bfa184dec148c56faa5d3d65e23d7198c1f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 831, "doc": {"id": "9f9ca9bb06d6afc31b19c365fb29a1c9", "question": "Where are you if you've paid to get a pizza?", "question_concept": "pizza", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["popular", "baked in oven", "restaurant", "oven", "plate"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where are you if you've paid to get a pizza?\nA. popular\nB. baked in oven\nC. restaurant\nD. oven\nE. plate\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are you if you've paid to get a pizza?\nA. popular\nB. baked in oven\nC. restaurant\nD. oven\nE. plate\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are you if you've paid to get a pizza?\nA. popular\nB. baked in oven\nC. restaurant\nD. oven\nE. plate\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are you if you've paid to get a pizza?\nA. popular\nB. baked in oven\nC. restaurant\nD. oven\nE. plate\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are you if you've paid to get a pizza?\nA. popular\nB. baked in oven\nC. restaurant\nD. oven\nE. plate\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8865866661071777", "False"]], [["-5.136586666107178", "False"]], [["-0.8865867853164673", "True"]], [["-7.386586666107178", "False"]], [["-7.636586666107178", "False"]]], "filtered_resps": [["-3.8865866661071777", "False"], ["-5.136586666107178", "False"], ["-0.8865867853164673", "True"], ["-7.386586666107178", "False"], ["-7.636586666107178", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "de7546ce5618b674b9f1a3284a5b22dc5fc311347549caa680fbf430e23dccc8", "prompt_hash": "d98e61b199b99191d7b9b15fb83b29b9b517ee52bce70d8427e7b40f995837aa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 832, "doc": {"id": "d60c5a494539c66982c0f692afde9499", "question": "What would you use to find a place to stay?", "question_concept": "place to stay", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mexico", "phone book", "town", "city", "sun dial"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What would you use to find a place to stay?\nA. mexico\nB. phone book\nC. town\nD. city\nE. sun dial\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you use to find a place to stay?\nA. mexico\nB. phone book\nC. town\nD. city\nE. sun dial\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you use to find a place to stay?\nA. mexico\nB. phone book\nC. town\nD. city\nE. sun dial\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you use to find a place to stay?\nA. mexico\nB. phone book\nC. town\nD. city\nE. sun dial\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you use to find a place to stay?\nA. mexico\nB. phone book\nC. town\nD. city\nE. sun dial\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8085381984710693", "False"]], [["-1.0585381984710693", "True"]], [["-6.808538436889648", "False"]], [["-7.058538436889648", "False"]], [["-8.058538436889648", "False"]]], "filtered_resps": [["-2.8085381984710693", "False"], ["-1.0585381984710693", "True"], ["-6.808538436889648", "False"], ["-7.058538436889648", "False"], ["-8.058538436889648", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6465abe531532dccbf70f53cd4720dbd0717014681407e4649d241f280b29131", "prompt_hash": "a93471e9b4681ccc90e4d39425be8136afe4e7974357e5ea962a04a7cbac5a9a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 833, "doc": {"id": "a6d3a2cb250a6310b8cabd31dbe2138c", "question": "If you're seeking a connection for your laptop, what are you trying to hook up with?", "question_concept": "connection", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["computer network", "electrical circuit", "lineage", "company", "wall"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If you're seeking a connection for your laptop, what are you trying to hook up with?\nA. computer network\nB. electrical circuit\nC. lineage\nD. company\nE. wall\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're seeking a connection for your laptop, what are you trying to hook up with?\nA. computer network\nB. electrical circuit\nC. lineage\nD. company\nE. wall\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're seeking a connection for your laptop, what are you trying to hook up with?\nA. computer network\nB. electrical circuit\nC. lineage\nD. company\nE. wall\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're seeking a connection for your laptop, what are you trying to hook up with?\nA. computer network\nB. electrical circuit\nC. lineage\nD. company\nE. wall\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're seeking a connection for your laptop, what are you trying to hook up with?\nA. computer network\nB. electrical circuit\nC. lineage\nD. company\nE. wall\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7241330146789551", "True"]], [["-4.974133014678955", "False"]], [["-5.724133014678955", "False"]], [["-8.224132537841797", "False"]], [["-8.474132537841797", "False"]]], "filtered_resps": [["-0.7241330146789551", "True"], ["-4.974133014678955", "False"], ["-5.724133014678955", "False"], ["-8.224132537841797", "False"], ["-8.474132537841797", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eab4879a7ec59ae7f3ed7e536d8788e8f15b4e31c23bb696c09e8509d686b16a", "prompt_hash": "1988db0dad41c709db865fc4c7b387cc632c295af5e6cf154c4b650f899b0f37", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 834, "doc": {"id": "27c523eb9099d2eec66296558eb4448e", "question": "The child didn't know the problems his mother was going through, all he had was what for her?", "question_concept": "child", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["care", "balloon", "loved", "become adult", "learn"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The child didn't know the problems his mother was going through, all he had was what for her?\nA. care\nB. balloon\nC. loved\nD. become adult\nE. learn\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The child didn't know the problems his mother was going through, all he had was what for her?\nA. care\nB. balloon\nC. loved\nD. become adult\nE. learn\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The child didn't know the problems his mother was going through, all he had was what for her?\nA. care\nB. balloon\nC. loved\nD. become adult\nE. learn\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The child didn't know the problems his mother was going through, all he had was what for her?\nA. care\nB. balloon\nC. loved\nD. become adult\nE. learn\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The child didn't know the problems his mother was going through, all he had was what for her?\nA. care\nB. balloon\nC. loved\nD. become adult\nE. learn\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.9311977624893188", "False"]], [["-4.931197643280029", "False"]], [["-1.4311977624893188", "True"]], [["-6.931197643280029", "False"]], [["-6.681197643280029", "False"]]], "filtered_resps": [["-1.9311977624893188", "False"], ["-4.931197643280029", "False"], ["-1.4311977624893188", "True"], ["-6.931197643280029", "False"], ["-6.681197643280029", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e980744391f78b12000e32060813b53b93ec97c2dcbe3afe3fa0b646c5662be4", "prompt_hash": "91cd84274c22ac71ad439998500f52db4269be9af4efcc33ee2d073523ad7caf", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 835, "doc": {"id": "2509fdd7d94afe9d0c021654ce0ba93f", "question": "To see new films you must?", "question_concept": "see new", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["open eyes", "go to movies", "kick ball", "make art", "look for"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: To see new films you must?\nA. open eyes\nB. go to movies\nC. kick ball\nD. make art\nE. look for\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: To see new films you must?\nA. open eyes\nB. go to movies\nC. kick ball\nD. make art\nE. look for\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: To see new films you must?\nA. open eyes\nB. go to movies\nC. kick ball\nD. make art\nE. look for\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: To see new films you must?\nA. open eyes\nB. go to movies\nC. kick ball\nD. make art\nE. look for\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: To see new films you must?\nA. open eyes\nB. go to movies\nC. kick ball\nD. make art\nE. look for\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.110616207122803", "False"]], [["-1.1106162071228027", "True"]], [["-7.860616207122803", "False"]], [["-7.860616207122803", "False"]], [["-6.110616207122803", "False"]]], "filtered_resps": [["-5.110616207122803", "False"], ["-1.1106162071228027", "True"], ["-7.860616207122803", "False"], ["-7.860616207122803", "False"], ["-6.110616207122803", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "08475c07e08d0e99a9eced199f05e7f4a93a7abd6e42223242eb0ea91850fe11", "prompt_hash": "08c28f753cdb4195778debc872c49eae7f1015493528c72b992b8b608a3c6d65", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 836, "doc": {"id": "75b8195e23c6bada574f1e41471b8f23", "question": "What can happen when you contemplate alone for a long time?", "question_concept": "contemplate", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["daydream", "headache", "get ideas", "sleep", "become distracted"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What can happen when you contemplate alone for a long time?\nA. daydream\nB. headache\nC. get ideas\nD. sleep\nE. become distracted\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can happen when you contemplate alone for a long time?\nA. daydream\nB. headache\nC. get ideas\nD. sleep\nE. become distracted\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can happen when you contemplate alone for a long time?\nA. daydream\nB. headache\nC. get ideas\nD. sleep\nE. become distracted\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can happen when you contemplate alone for a long time?\nA. daydream\nB. headache\nC. get ideas\nD. sleep\nE. become distracted\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can happen when you contemplate alone for a long time?\nA. daydream\nB. headache\nC. get ideas\nD. sleep\nE. become distracted\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.508127450942993", "False"]], [["-4.258127212524414", "False"]], [["-2.008127450942993", "False"]], [["-5.258127212524414", "False"]], [["-7.008127212524414", "False"]]], "filtered_resps": [["-2.508127450942993", "False"], ["-4.258127212524414", "False"], ["-2.008127450942993", "False"], ["-5.258127212524414", "False"], ["-7.008127212524414", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "164a6613ae0b25004d0a0811b61d25bd9cf6226161e4a70f9a6290ae83a09b2b", "prompt_hash": "53af55adaaf85b84af82e506a057b612ded0f3dcc592bbc92c5656343df5be7d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 837, "doc": {"id": "df1bf6f3f87975aa0c1b6d6153d9ecef", "question": "The pioneer went to the general store for storage measures, what was he looking for?", "question_concept": "general store", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["checkers", "barrels", "baking soda", "buffalo", "salt"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The pioneer went to the general store for storage measures, what was he looking for?\nA. checkers\nB. barrels\nC. baking soda\nD. buffalo\nE. salt\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The pioneer went to the general store for storage measures, what was he looking for?\nA. checkers\nB. barrels\nC. baking soda\nD. buffalo\nE. salt\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The pioneer went to the general store for storage measures, what was he looking for?\nA. checkers\nB. barrels\nC. baking soda\nD. buffalo\nE. salt\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The pioneer went to the general store for storage measures, what was he looking for?\nA. checkers\nB. barrels\nC. baking soda\nD. buffalo\nE. salt\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The pioneer went to the general store for storage measures, what was he looking for?\nA. checkers\nB. barrels\nC. baking soda\nD. buffalo\nE. salt\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.68472957611084", "False"]], [["-1.4347295761108398", "False"]], [["-7.93472957611084", "False"]], [["-7.43472957611084", "False"]], [["-6.68472957611084", "False"]]], "filtered_resps": [["-3.68472957611084", "False"], ["-1.4347295761108398", "False"], ["-7.93472957611084", "False"], ["-7.43472957611084", "False"], ["-6.68472957611084", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "179aa0b0231aca7a2bc51a31e5b9fc4df01f8735f0d118197c87b3a1fe033a6c", "prompt_hash": "60cf8341ffb946814b4e6bb32c9ad9a414860afa00aad3131f669c77e5677c2f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 838, "doc": {"id": "e99d4cb2e69d3e020ee9e4e9a84ac45b", "question": "I was apprehensive to buy the expensive equipment to play a game with so much walking and swinging around in grass, but now I understand why people what?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["care less", "play golf", "shake hands", "believe in god", "trip over"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: I was apprehensive to buy the expensive equipment to play a game with so much walking and swinging around in grass, but now I understand why people what?\nA. care less\nB. play golf\nC. shake hands\nD. believe in god\nE. trip over\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I was apprehensive to buy the expensive equipment to play a game with so much walking and swinging around in grass, but now I understand why people what?\nA. care less\nB. play golf\nC. shake hands\nD. believe in god\nE. trip over\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I was apprehensive to buy the expensive equipment to play a game with so much walking and swinging around in grass, but now I understand why people what?\nA. care less\nB. play golf\nC. shake hands\nD. believe in god\nE. trip over\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I was apprehensive to buy the expensive equipment to play a game with so much walking and swinging around in grass, but now I understand why people what?\nA. care less\nB. play golf\nC. shake hands\nD. believe in god\nE. trip over\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I was apprehensive to buy the expensive equipment to play a game with so much walking and swinging around in grass, but now I understand why people what?\nA. care less\nB. play golf\nC. shake hands\nD. believe in god\nE. trip over\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.615205764770508", "False"]], [["-0.8652056455612183", "True"]], [["-7.115205764770508", "False"]], [["-7.365205764770508", "False"]], [["-5.615205764770508", "False"]]], "filtered_resps": [["-5.615205764770508", "False"], ["-0.8652056455612183", "True"], ["-7.115205764770508", "False"], ["-7.365205764770508", "False"], ["-5.615205764770508", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "58e6cab1dc3252835f3e56540b37f56a62c0c8afd1bc34dbd38790130a6eb8f3", "prompt_hash": "6dbd672dbdf81696af189f0c2d841d09d147fcfbd41597982407bfcf399c9e48", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 839, "doc": {"id": "b1274d6f5969dea4d46f43fbdc28fd97", "question": "What can a newspaper be used to do to an engagement?", "question_concept": "newspaper", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["announce", "communicate", "educate", "inform", "cancel"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What can a newspaper be used to do to an engagement?\nA. announce\nB. communicate\nC. educate\nD. inform\nE. cancel\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can a newspaper be used to do to an engagement?\nA. announce\nB. communicate\nC. educate\nD. inform\nE. cancel\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can a newspaper be used to do to an engagement?\nA. announce\nB. communicate\nC. educate\nD. inform\nE. cancel\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can a newspaper be used to do to an engagement?\nA. announce\nB. communicate\nC. educate\nD. inform\nE. cancel\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can a newspaper be used to do to an engagement?\nA. announce\nB. communicate\nC. educate\nD. inform\nE. cancel\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.750166654586792", "False"]], [["-3.500166654586792", "False"]], [["-6.000166893005371", "False"]], [["-6.250166893005371", "False"]], [["-1.000166654586792", "True"]]], "filtered_resps": [["-1.750166654586792", "False"], ["-3.500166654586792", "False"], ["-6.000166893005371", "False"], ["-6.250166893005371", "False"], ["-1.000166654586792", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "94ccb377c6624e648ee0b5146bd67513d1f1722e818010367691dd4168970ecc", "prompt_hash": "afc3045a2b15bf2b7acb916b61b2ce57930024a5bacbf69309796c400322db82", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 840, "doc": {"id": "001cb999a61a5c8b4031ff53cf261714", "question": "John needed a straight wire.  Unfortunately, this one had endured some abuse and had become what?", "question_concept": "straight", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bent", "bent", "crooked", "straightforth", "curved"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: John needed a straight wire.  Unfortunately, this one had endured some abuse and had become what?\nA. bent\nB. bent\nC. crooked\nD. straightforth\nE. curved\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John needed a straight wire.  Unfortunately, this one had endured some abuse and had become what?\nA. bent\nB. bent\nC. crooked\nD. straightforth\nE. curved\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John needed a straight wire.  Unfortunately, this one had endured some abuse and had become what?\nA. bent\nB. bent\nC. crooked\nD. straightforth\nE. curved\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John needed a straight wire.  Unfortunately, this one had endured some abuse and had become what?\nA. bent\nB. bent\nC. crooked\nD. straightforth\nE. curved\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John needed a straight wire.  Unfortunately, this one had endured some abuse and had become what?\nA. bent\nB. bent\nC. crooked\nD. straightforth\nE. curved\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.352555274963379", "True"]], [["-3.102555274963379", "False"]], [["-3.352555274963379", "False"]], [["-7.602555274963379", "False"]], [["-8.602555274963379", "False"]]], "filtered_resps": [["-1.352555274963379", "True"], ["-3.102555274963379", "False"], ["-3.352555274963379", "False"], ["-7.602555274963379", "False"], ["-8.602555274963379", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "12e8ee29f9cf05bbe0e8158a06cab8f3884a79c10a1c630f78588d27329ed92c", "prompt_hash": "ce6e7d2c49f43bad954f5db541cdae93383a178f5598e48052cb3bff2915bbfc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 841, "doc": {"id": "18ee7a93410a6b4c9cec5d4894775991_1", "question": "Metal is taken from what which is pulled from the ground?", "question_concept": "metal", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dirt", "instruments", "ore", "car", "junkyard"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Metal is taken from what which is pulled from the ground?\nA. dirt\nB. instruments\nC. ore\nD. car\nE. junkyard\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Metal is taken from what which is pulled from the ground?\nA. dirt\nB. instruments\nC. ore\nD. car\nE. junkyard\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Metal is taken from what which is pulled from the ground?\nA. dirt\nB. instruments\nC. ore\nD. car\nE. junkyard\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Metal is taken from what which is pulled from the ground?\nA. dirt\nB. instruments\nC. ore\nD. car\nE. junkyard\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Metal is taken from what which is pulled from the ground?\nA. dirt\nB. instruments\nC. ore\nD. car\nE. junkyard\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.3262553215026855", "False"]], [["-9.076254844665527", "False"]], [["-1.326255202293396", "True"]], [["-9.326254844665527", "False"]], [["-9.826254844665527", "False"]]], "filtered_resps": [["-4.3262553215026855", "False"], ["-9.076254844665527", "False"], ["-1.326255202293396", "True"], ["-9.326254844665527", "False"], ["-9.826254844665527", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "702b29953e928c53ceebba19fce4399672ad5a96a0e675f62d0669ea93ef2dc8", "prompt_hash": "0260c393e768ca63a8408d55bb97fb5a66e91fc8a765900edabf229b4783766c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 842, "doc": {"id": "3b8be90fdd8c67571d8d692eaa6dd87b", "question": "When not in use where on your property would you store you bucket?", "question_concept": "bucket", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["utility closet", "outside", "well", "garden shed", "garage"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When not in use where on your property would you store you bucket?\nA. utility closet\nB. outside\nC. well\nD. garden shed\nE. garage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When not in use where on your property would you store you bucket?\nA. utility closet\nB. outside\nC. well\nD. garden shed\nE. garage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When not in use where on your property would you store you bucket?\nA. utility closet\nB. outside\nC. well\nD. garden shed\nE. garage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When not in use where on your property would you store you bucket?\nA. utility closet\nB. outside\nC. well\nD. garden shed\nE. garage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When not in use where on your property would you store you bucket?\nA. utility closet\nB. outside\nC. well\nD. garden shed\nE. garage\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.052164316177368", "False"]], [["-4.052164077758789", "False"]], [["-6.052164077758789", "False"]], [["-1.8021643161773682", "False"]], [["-6.052164077758789", "False"]]], "filtered_resps": [["-3.052164316177368", "False"], ["-4.052164077758789", "False"], ["-6.052164077758789", "False"], ["-1.8021643161773682", "False"], ["-6.052164077758789", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8fd3ab99103dcbe147a8940a98d5fb7d1d89780355a7955326f4cdd01d991f26", "prompt_hash": "ff2dae5cdcbe3493d34cd980c91a02fbc89555be23a7adcf611ae093a484ffe5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 843, "doc": {"id": "300bd7704ae8c5fcef618902f18fd01d", "question": "What does someone do to relax at night?", "question_concept": "relax", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["find time", "check mail", "listen to music", "go to bed", "stop worrying"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What does someone do to relax at night?\nA. find time\nB. check mail\nC. listen to music\nD. go to bed\nE. stop worrying\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does someone do to relax at night?\nA. find time\nB. check mail\nC. listen to music\nD. go to bed\nE. stop worrying\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does someone do to relax at night?\nA. find time\nB. check mail\nC. listen to music\nD. go to bed\nE. stop worrying\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does someone do to relax at night?\nA. find time\nB. check mail\nC. listen to music\nD. go to bed\nE. stop worrying\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does someone do to relax at night?\nA. find time\nB. check mail\nC. listen to music\nD. go to bed\nE. stop worrying\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.463460922241211", "False"]], [["-4.463460922241211", "False"]], [["-1.463460922241211", "True"]], [["-2.963460922241211", "False"]], [["-3.963460922241211", "False"]]], "filtered_resps": [["-3.463460922241211", "False"], ["-4.463460922241211", "False"], ["-1.463460922241211", "True"], ["-2.963460922241211", "False"], ["-3.963460922241211", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f6647420d36beb1ab6fc1eedf91d04cc55da962d0c4d8cef46b00ca25d001ddc", "prompt_hash": "db99fa77e0fbdfd6b6bf56c65b175dbaf3a4a5c7bb2991aad545b3c6268a022b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 844, "doc": {"id": "f18833ace65a54709377134168b457a9", "question": "Where might the stapler be if I cannot find it?", "question_concept": "stapler", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["office building", "office supply store", "desk drawer", "with dwight", "desktop"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where might the stapler be if I cannot find it?\nA. office building\nB. office supply store\nC. desk drawer\nD. with dwight\nE. desktop\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where might the stapler be if I cannot find it?\nA. office building\nB. office supply store\nC. desk drawer\nD. with dwight\nE. desktop\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where might the stapler be if I cannot find it?\nA. office building\nB. office supply store\nC. desk drawer\nD. with dwight\nE. desktop\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where might the stapler be if I cannot find it?\nA. office building\nB. office supply store\nC. desk drawer\nD. with dwight\nE. desktop\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where might the stapler be if I cannot find it?\nA. office building\nB. office supply store\nC. desk drawer\nD. with dwight\nE. desktop\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.114645957946777", "False"]], [["-6.114645957946777", "False"]], [["-1.114646077156067", "True"]], [["-3.6146459579467773", "False"]], [["-3.6146459579467773", "False"]]], "filtered_resps": [["-4.114645957946777", "False"], ["-6.114645957946777", "False"], ["-1.114646077156067", "True"], ["-3.6146459579467773", "False"], ["-3.6146459579467773", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8caba15e0d5bb2503cb61e799bbd6f44cd98df0a6e3671d11392830a31e640e3", "prompt_hash": "8691466b85213d05fa0339e6a254fc1de1a3862a13a9480f252fc59e8789c052", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 845, "doc": {"id": "5bba03b425f5abc6e017f194cf074b06", "question": "Many homes in this country are built around a courtyard. Where is it?", "question_concept": "courtyard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["candidate", "spain", "lawn", "asshole", "office complex"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Many homes in this country are built around a courtyard. Where is it?\nA. candidate\nB. spain\nC. lawn\nD. asshole\nE. office complex\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Many homes in this country are built around a courtyard. Where is it?\nA. candidate\nB. spain\nC. lawn\nD. asshole\nE. office complex\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Many homes in this country are built around a courtyard. Where is it?\nA. candidate\nB. spain\nC. lawn\nD. asshole\nE. office complex\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Many homes in this country are built around a courtyard. Where is it?\nA. candidate\nB. spain\nC. lawn\nD. asshole\nE. office complex\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Many homes in this country are built around a courtyard. Where is it?\nA. candidate\nB. spain\nC. lawn\nD. asshole\nE. office complex\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.958404541015625", "False"]], [["-1.4584046602249146", "False"]], [["-8.458404541015625", "False"]], [["-9.208404541015625", "False"]], [["-9.458404541015625", "False"]]], "filtered_resps": [["-5.958404541015625", "False"], ["-1.4584046602249146", "False"], ["-8.458404541015625", "False"], ["-9.208404541015625", "False"], ["-9.458404541015625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "461886be37d3c803dd2ff8f042959c5eeba401d0e66010c63f3f709c0014ef36", "prompt_hash": "6ed3aa8838bd0900db2ce91a81c8cd92d31579568ba35bb44f5687966cd574b4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 846, "doc": {"id": "78276a4eab6e8d6b9ae3749211816977", "question": "Sean was a wreck.  He  loved to build houses, but in his current state, he couldn't do what?", "question_concept": "wreck", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stand up", "produce", "construct", "make", "build"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Sean was a wreck.  He  loved to build houses, but in his current state, he couldn't do what?\nA. stand up\nB. produce\nC. construct\nD. make\nE. build\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sean was a wreck.  He  loved to build houses, but in his current state, he couldn't do what?\nA. stand up\nB. produce\nC. construct\nD. make\nE. build\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sean was a wreck.  He  loved to build houses, but in his current state, he couldn't do what?\nA. stand up\nB. produce\nC. construct\nD. make\nE. build\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sean was a wreck.  He  loved to build houses, but in his current state, he couldn't do what?\nA. stand up\nB. produce\nC. construct\nD. make\nE. build\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sean was a wreck.  He  loved to build houses, but in his current state, he couldn't do what?\nA. stand up\nB. produce\nC. construct\nD. make\nE. build\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.913226842880249", "False"]], [["-4.913227081298828", "False"]], [["-2.413226842880249", "False"]], [["-7.163227081298828", "False"]], [["-2.163226842880249", "False"]]], "filtered_resps": [["-3.913226842880249", "False"], ["-4.913227081298828", "False"], ["-2.413226842880249", "False"], ["-7.163227081298828", "False"], ["-2.163226842880249", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0888176b42ec21bf94e5abab767c0476428f63b6296d7267862a9a9aeae3cfe7", "prompt_hash": "af493fd76042d6419125ebc734e7b4fede1e865d67f81b8a3a2a4b38d4f0568f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 847, "doc": {"id": "cf33e0f5891ce53a716432be06a46ee1", "question": "What would be happening if you are pretending to be a police officer?", "question_concept": "pretending", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fighting", "misunderstanding", "deception", "play", "distrust"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What would be happening if you are pretending to be a police officer?\nA. fighting\nB. misunderstanding\nC. deception\nD. play\nE. distrust\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would be happening if you are pretending to be a police officer?\nA. fighting\nB. misunderstanding\nC. deception\nD. play\nE. distrust\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would be happening if you are pretending to be a police officer?\nA. fighting\nB. misunderstanding\nC. deception\nD. play\nE. distrust\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would be happening if you are pretending to be a police officer?\nA. fighting\nB. misunderstanding\nC. deception\nD. play\nE. distrust\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would be happening if you are pretending to be a police officer?\nA. fighting\nB. misunderstanding\nC. deception\nD. play\nE. distrust\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.40925407409668", "False"]], [["-5.40925407409668", "False"]], [["-1.1592538356781006", "True"]], [["-2.6592538356781006", "False"]], [["-6.15925407409668", "False"]]], "filtered_resps": [["-5.40925407409668", "False"], ["-5.40925407409668", "False"], ["-1.1592538356781006", "True"], ["-2.6592538356781006", "False"], ["-6.15925407409668", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7f739ed5d31a11e9ece80d674ac9a2e4d28c500247a61d5461e2d6c4d5093a2e", "prompt_hash": "cf2a2c9b5d0256c32f99ac3270ff53b0da0e3e277fb5d8f2572901120bb4d150", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 848, "doc": {"id": "3938d6e50d38b1f8774b4f00a89bdb39", "question": "Where would you buy a finely crafted writing instrument?", "question_concept": "writing instrument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["nasa", "classroom", "stationery store", "purse", "office supply store"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you buy a finely crafted writing instrument?\nA. nasa\nB. classroom\nC. stationery store\nD. purse\nE. office supply store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you buy a finely crafted writing instrument?\nA. nasa\nB. classroom\nC. stationery store\nD. purse\nE. office supply store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you buy a finely crafted writing instrument?\nA. nasa\nB. classroom\nC. stationery store\nD. purse\nE. office supply store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you buy a finely crafted writing instrument?\nA. nasa\nB. classroom\nC. stationery store\nD. purse\nE. office supply store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you buy a finely crafted writing instrument?\nA. nasa\nB. classroom\nC. stationery store\nD. purse\nE. office supply store\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.852966785430908", "False"]], [["-7.602966785430908", "False"]], [["-1.1029669046401978", "True"]], [["-9.602967262268066", "False"]], [["-2.602966785430908", "False"]]], "filtered_resps": [["-2.852966785430908", "False"], ["-7.602966785430908", "False"], ["-1.1029669046401978", "True"], ["-9.602967262268066", "False"], ["-2.602966785430908", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "28b05c4f82c2b48fff3d6943e8c54494f2cba215733f3d92d29079f40ecfb413", "prompt_hash": "4dca3b9c91b659e6d2ec4e0ab5d1d163bc74a2949c647c49679671f1f54dca80", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 849, "doc": {"id": "cabefb7063a728e77abd44d97397a2a4", "question": "The detective was finding information from witnesses, why would he do that?", "question_concept": "finding information", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fun", "ulcers", "get answers", "happiness", "power"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The detective was finding information from witnesses, why would he do that?\nA. fun\nB. ulcers\nC. get answers\nD. happiness\nE. power\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The detective was finding information from witnesses, why would he do that?\nA. fun\nB. ulcers\nC. get answers\nD. happiness\nE. power\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The detective was finding information from witnesses, why would he do that?\nA. fun\nB. ulcers\nC. get answers\nD. happiness\nE. power\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The detective was finding information from witnesses, why would he do that?\nA. fun\nB. ulcers\nC. get answers\nD. happiness\nE. power\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The detective was finding information from witnesses, why would he do that?\nA. fun\nB. ulcers\nC. get answers\nD. happiness\nE. power\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.475679874420166", "False"]], [["-8.725679397583008", "False"]], [["-0.7256796956062317", "True"]], [["-9.225679397583008", "False"]], [["-7.975679874420166", "False"]]], "filtered_resps": [["-4.475679874420166", "False"], ["-8.725679397583008", "False"], ["-0.7256796956062317", "True"], ["-9.225679397583008", "False"], ["-7.975679874420166", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "52c28462b70bb29752750f4ac90298420bf526a049a25886d37a63e51815d275", "prompt_hash": "179defbed634a9800dfd8e85bcc8e4be5edd22a4e30b9a96d3a425251d68cc29", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 850, "doc": {"id": "60b909ad1d7956218a5d99954fdebecd", "question": "Joe found spiders in the place where he keeps his tools.  Where might that be?", "question_concept": "spiders", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cupboard", "toolbox", "closet", "garage", "mail box"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Joe found spiders in the place where he keeps his tools.  Where might that be?\nA. cupboard\nB. toolbox\nC. closet\nD. garage\nE. mail box\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe found spiders in the place where he keeps his tools.  Where might that be?\nA. cupboard\nB. toolbox\nC. closet\nD. garage\nE. mail box\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe found spiders in the place where he keeps his tools.  Where might that be?\nA. cupboard\nB. toolbox\nC. closet\nD. garage\nE. mail box\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe found spiders in the place where he keeps his tools.  Where might that be?\nA. cupboard\nB. toolbox\nC. closet\nD. garage\nE. mail box\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe found spiders in the place where he keeps his tools.  Where might that be?\nA. cupboard\nB. toolbox\nC. closet\nD. garage\nE. mail box\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.557206153869629", "False"]], [["-2.057206153869629", "False"]], [["-4.807206153869629", "False"]], [["-2.557206153869629", "False"]], [["-9.307206153869629", "False"]]], "filtered_resps": [["-3.557206153869629", "False"], ["-2.057206153869629", "False"], ["-4.807206153869629", "False"], ["-2.557206153869629", "False"], ["-9.307206153869629", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f17cd0491a23b0325a249b0f7cd9d3d8f26bb1fbbfdb6d99dfac1165b632bbff", "prompt_hash": "4a4c94513b170b332414cda829737d87a4ab3d96cc5b6c1a1a61afbd5332e246", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 851, "doc": {"id": "9fdebd1c2cf498f1d726a025b780a39a", "question": "While on the fan boat he thought he'd see swamps and gators, but he was surprised to spot a bald eagle in what nature area?", "question_concept": "bald eagle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["everglades", "high places", "natural habitat", "new york", "colorado"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: While on the fan boat he thought he'd see swamps and gators, but he was surprised to spot a bald eagle in what nature area?\nA. everglades\nB. high places\nC. natural habitat\nD. new york\nE. colorado\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: While on the fan boat he thought he'd see swamps and gators, but he was surprised to spot a bald eagle in what nature area?\nA. everglades\nB. high places\nC. natural habitat\nD. new york\nE. colorado\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: While on the fan boat he thought he'd see swamps and gators, but he was surprised to spot a bald eagle in what nature area?\nA. everglades\nB. high places\nC. natural habitat\nD. new york\nE. colorado\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: While on the fan boat he thought he'd see swamps and gators, but he was surprised to spot a bald eagle in what nature area?\nA. everglades\nB. high places\nC. natural habitat\nD. new york\nE. colorado\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: While on the fan boat he thought he'd see swamps and gators, but he was surprised to spot a bald eagle in what nature area?\nA. everglades\nB. high places\nC. natural habitat\nD. new york\nE. colorado\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9815194606781006", "True"]], [["-4.23151969909668", "False"]], [["-3.9815194606781006", "False"]], [["-5.73151969909668", "False"]], [["-7.48151969909668", "False"]]], "filtered_resps": [["-0.9815194606781006", "True"], ["-4.23151969909668", "False"], ["-3.9815194606781006", "False"], ["-5.73151969909668", "False"], ["-7.48151969909668", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3375d285afa920ac244a088a9307ba6d64bcb30bfedea507de0637f0dec3aa1a", "prompt_hash": "ac5528410f81017247fd9bf0f1326a2d146c84dbf3e2c164f3f2f46cef4359ff", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 852, "doc": {"id": "f36027954e43cfd926451bdf7cb0c3ac", "question": "Where are you likely to find a supermarket?", "question_concept": "supermarket", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["buy food for family", "city or town", "get supplies", "strip mall", "vermont"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where are you likely to find a supermarket?\nA. buy food for family\nB. city or town\nC. get supplies\nD. strip mall\nE. vermont\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are you likely to find a supermarket?\nA. buy food for family\nB. city or town\nC. get supplies\nD. strip mall\nE. vermont\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are you likely to find a supermarket?\nA. buy food for family\nB. city or town\nC. get supplies\nD. strip mall\nE. vermont\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are you likely to find a supermarket?\nA. buy food for family\nB. city or town\nC. get supplies\nD. strip mall\nE. vermont\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are you likely to find a supermarket?\nA. buy food for family\nB. city or town\nC. get supplies\nD. strip mall\nE. vermont\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.556365966796875", "False"]], [["-1.556365966796875", "True"]], [["-5.806365966796875", "False"]], [["-1.806365966796875", "False"]], [["-8.056365966796875", "False"]]], "filtered_resps": [["-2.556365966796875", "False"], ["-1.556365966796875", "True"], ["-5.806365966796875", "False"], ["-1.806365966796875", "False"], ["-8.056365966796875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7f6dc7f71bc7434fd1fe68797fb73c914d7617b8a7b2ea55ebc5b2c82d620826", "prompt_hash": "5164f8cb92c521a59121aa4ac6492403ca36f2c3cfb954f923cd9555b233368f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 853, "doc": {"id": "7ec14907622c6d5a6087cd59a22d8c9d", "question": "Where would you need to use a lantern?", "question_concept": "lantern", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["grocery store", "antique shop", "house", "dark place", "street"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you need to use a lantern?\nA. grocery store\nB. antique shop\nC. house\nD. dark place\nE. street\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you need to use a lantern?\nA. grocery store\nB. antique shop\nC. house\nD. dark place\nE. street\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you need to use a lantern?\nA. grocery store\nB. antique shop\nC. house\nD. dark place\nE. street\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you need to use a lantern?\nA. grocery store\nB. antique shop\nC. house\nD. dark place\nE. street\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you need to use a lantern?\nA. grocery store\nB. antique shop\nC. house\nD. dark place\nE. street\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8914339542388916", "False"]], [["-5.641434192657471", "False"]], [["-6.891434192657471", "False"]], [["-1.3914339542388916", "False"]], [["-8.891433715820312", "False"]]], "filtered_resps": [["-2.8914339542388916", "False"], ["-5.641434192657471", "False"], ["-6.891434192657471", "False"], ["-1.3914339542388916", "False"], ["-8.891433715820312", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "65ba162eb6ae94500ad57b3b107e0d5c72621ab84fc352b7c2845b6828a4f4b9", "prompt_hash": "decab9951adaa58d98ce9ece1473b956662210e57f4120fd3db189fd228f3110", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 854, "doc": {"id": "efe488f67b53a4b6e69782c01c84f06c", "question": "What area does a police officer patrol?", "question_concept": "police officer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["direct traffic", "city", "beat", "street", "park"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What area does a police officer patrol?\nA. direct traffic\nB. city\nC. beat\nD. street\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What area does a police officer patrol?\nA. direct traffic\nB. city\nC. beat\nD. street\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What area does a police officer patrol?\nA. direct traffic\nB. city\nC. beat\nD. street\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What area does a police officer patrol?\nA. direct traffic\nB. city\nC. beat\nD. street\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What area does a police officer patrol?\nA. direct traffic\nB. city\nC. beat\nD. street\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7252511978149414", "False"]], [["-5.975251197814941", "False"]], [["-0.9752511978149414", "True"]], [["-7.225251197814941", "False"]], [["-9.725251197814941", "False"]]], "filtered_resps": [["-1.7252511978149414", "False"], ["-5.975251197814941", "False"], ["-0.9752511978149414", "True"], ["-7.225251197814941", "False"], ["-9.725251197814941", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1909ab03ade5838c3c16b43fb156fb8fa417ffb4bde1495158c8eb90053a6013", "prompt_hash": "9b27bd79fc4c041d993eed228d617199aba4b98dba2fbd532a446403ac339d73", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 855, "doc": {"id": "7c62637437ad7515452886074010a438", "question": "Why would a woman kill a stranger she met in a dark alley?", "question_concept": "kill", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["being raped", "get rid of", "they didn't know the passcode", "get revenge", "were evil"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Why would a woman kill a stranger she met in a dark alley?\nA. being raped\nB. get rid of\nC. they didn't know the passcode\nD. get revenge\nE. were evil\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would a woman kill a stranger she met in a dark alley?\nA. being raped\nB. get rid of\nC. they didn't know the passcode\nD. get revenge\nE. were evil\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would a woman kill a stranger she met in a dark alley?\nA. being raped\nB. get rid of\nC. they didn't know the passcode\nD. get revenge\nE. were evil\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would a woman kill a stranger she met in a dark alley?\nA. being raped\nB. get rid of\nC. they didn't know the passcode\nD. get revenge\nE. were evil\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would a woman kill a stranger she met in a dark alley?\nA. being raped\nB. get rid of\nC. they didn't know the passcode\nD. get revenge\nE. were evil\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1333770751953125", "False"]], [["-5.8833770751953125", "False"]], [["-6.1333770751953125", "False"]], [["-2.8833770751953125", "False"]], [["-5.8833770751953125", "False"]]], "filtered_resps": [["-2.1333770751953125", "False"], ["-5.8833770751953125", "False"], ["-6.1333770751953125", "False"], ["-2.8833770751953125", "False"], ["-5.8833770751953125", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ec5a1bc4d75d0e3372d06c08b496a5afcd6bc7c7c2855a558feca86a5016c711", "prompt_hash": "a3ae93e5c191f0f3ee246cc4221362994a78c2fe4d978b7c4f26b6c0bd7504b9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 856, "doc": {"id": "4f7be1c68654e2924c161c8eca652928", "question": "The baby was cranky, it needed to eat breakfast but refused to what?", "question_concept": "eat breakfast", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["buy food", "open mouth", "get out of bed", "cry", "wake up"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The baby was cranky, it needed to eat breakfast but refused to what?\nA. buy food\nB. open mouth\nC. get out of bed\nD. cry\nE. wake up\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The baby was cranky, it needed to eat breakfast but refused to what?\nA. buy food\nB. open mouth\nC. get out of bed\nD. cry\nE. wake up\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The baby was cranky, it needed to eat breakfast but refused to what?\nA. buy food\nB. open mouth\nC. get out of bed\nD. cry\nE. wake up\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The baby was cranky, it needed to eat breakfast but refused to what?\nA. buy food\nB. open mouth\nC. get out of bed\nD. cry\nE. wake up\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The baby was cranky, it needed to eat breakfast but refused to what?\nA. buy food\nB. open mouth\nC. get out of bed\nD. cry\nE. wake up\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.981568813323975", "False"]], [["-0.9815686941146851", "True"]], [["-5.231568813323975", "False"]], [["-5.981568813323975", "False"]], [["-4.731568813323975", "False"]]], "filtered_resps": [["-4.981568813323975", "False"], ["-0.9815686941146851", "True"], ["-5.231568813323975", "False"], ["-5.981568813323975", "False"], ["-4.731568813323975", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e9fb4f89a2ff3d184e0ed4bfd0a24188f6ad21d5415235938ff242812debca4f", "prompt_hash": "a5927df0abcfe65ee1974ef3b5959514805dcdafd8994bea45bbb9a1b7a10a6c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 857, "doc": {"id": "e4976ee741cf4b28b8a42780ffb15774", "question": "What is made up of people?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["buildings", "audience", "apartment", "classroom", "falling down"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is made up of people?\nA. buildings\nB. audience\nC. apartment\nD. classroom\nE. falling down\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is made up of people?\nA. buildings\nB. audience\nC. apartment\nD. classroom\nE. falling down\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is made up of people?\nA. buildings\nB. audience\nC. apartment\nD. classroom\nE. falling down\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is made up of people?\nA. buildings\nB. audience\nC. apartment\nD. classroom\nE. falling down\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is made up of people?\nA. buildings\nB. audience\nC. apartment\nD. classroom\nE. falling down\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.2679154872894287", "False"]], [["-1.2679154872894287", "True"]], [["-8.517915725708008", "False"]], [["-8.017915725708008", "False"]], [["-8.267915725708008", "False"]]], "filtered_resps": [["-2.2679154872894287", "False"], ["-1.2679154872894287", "True"], ["-8.517915725708008", "False"], ["-8.017915725708008", "False"], ["-8.267915725708008", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "605a4f0e8d69c1b331864d21b8721d12e8ea9a755a448bea75bcd27ad223a3bf", "prompt_hash": "47a0c0fe4b7610b63446bcc62f48552b8052a420ee81f1b18142b9b9f046fe4a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 858, "doc": {"id": "14e75a42a416d32a24e2826cae34d2bf", "question": "He was afraid he would die from his cold, so he wisely decided to what?", "question_concept": "die", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ocean", "write will", "never want", "were shot", "seek help"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: He was afraid he would die from his cold, so he wisely decided to what?\nA. ocean\nB. write will\nC. never want\nD. were shot\nE. seek help\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was afraid he would die from his cold, so he wisely decided to what?\nA. ocean\nB. write will\nC. never want\nD. were shot\nE. seek help\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was afraid he would die from his cold, so he wisely decided to what?\nA. ocean\nB. write will\nC. never want\nD. were shot\nE. seek help\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was afraid he would die from his cold, so he wisely decided to what?\nA. ocean\nB. write will\nC. never want\nD. were shot\nE. seek help\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was afraid he would die from his cold, so he wisely decided to what?\nA. ocean\nB. write will\nC. never want\nD. were shot\nE. seek help\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.416372776031494", "False"]], [["-3.916372776031494", "False"]], [["-6.166372776031494", "False"]], [["-7.416372776031494", "False"]], [["-0.6663728356361389", "True"]]], "filtered_resps": [["-4.416372776031494", "False"], ["-3.916372776031494", "False"], ["-6.166372776031494", "False"], ["-7.416372776031494", "False"], ["-0.6663728356361389", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "31d841204e10c420eda84b1ce239dc83a2e5dea37327543d7e1bd6f661dc7262", "prompt_hash": "38fc1b8cc1f5e26d7b331356ab895fc43a637e0dff677950392a6f059387a0ab", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 859, "doc": {"id": "004607228ad49b69eac932c1005d6106", "question": "Where would you get a pen if you do not have one?", "question_concept": "pen", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["briefcase", "desk drawer", "friend's house", "pocket", "sidewalk"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you get a pen if you do not have one?\nA. briefcase\nB. desk drawer\nC. friend's house\nD. pocket\nE. sidewalk\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you get a pen if you do not have one?\nA. briefcase\nB. desk drawer\nC. friend's house\nD. pocket\nE. sidewalk\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you get a pen if you do not have one?\nA. briefcase\nB. desk drawer\nC. friend's house\nD. pocket\nE. sidewalk\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you get a pen if you do not have one?\nA. briefcase\nB. desk drawer\nC. friend's house\nD. pocket\nE. sidewalk\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you get a pen if you do not have one?\nA. briefcase\nB. desk drawer\nC. friend's house\nD. pocket\nE. sidewalk\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.538973569869995", "False"]], [["-2.538973569869995", "False"]], [["-1.2889735698699951", "True"]], [["-5.038973808288574", "False"]], [["-4.288973808288574", "False"]]], "filtered_resps": [["-3.538973569869995", "False"], ["-2.538973569869995", "False"], ["-1.2889735698699951", "True"], ["-5.038973808288574", "False"], ["-4.288973808288574", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "31b648b6d9e18730e428e78abbb946a4a3e5cc4f6738ff5c6277ed30c80727cf", "prompt_hash": "844f1421955ae4cea1e816a29669e8a3f90f83b38372629b573c6f6f37e417aa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 860, "doc": {"id": "a7f54ee1866d5db34eacf40efa53c93e", "question": "Why would a small dog pant if it's hot outside?", "question_concept": "small dog", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["outside", "europe", "heat", "wet", "dog show"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Why would a small dog pant if it's hot outside?\nA. outside\nB. europe\nC. heat\nD. wet\nE. dog show\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would a small dog pant if it's hot outside?\nA. outside\nB. europe\nC. heat\nD. wet\nE. dog show\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would a small dog pant if it's hot outside?\nA. outside\nB. europe\nC. heat\nD. wet\nE. dog show\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would a small dog pant if it's hot outside?\nA. outside\nB. europe\nC. heat\nD. wet\nE. dog show\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would a small dog pant if it's hot outside?\nA. outside\nB. europe\nC. heat\nD. wet\nE. dog show\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.76015567779541", "False"]], [["-6.76015567779541", "False"]], [["-1.0101555585861206", "True"]], [["-6.76015567779541", "False"]], [["-9.26015567779541", "False"]]], "filtered_resps": [["-2.76015567779541", "False"], ["-6.76015567779541", "False"], ["-1.0101555585861206", "True"], ["-6.76015567779541", "False"], ["-9.26015567779541", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bb4fa12695bd113dd9914213bf83258775f56c1c1908051c22b04fbdc3e0ff80", "prompt_hash": "01fe8b8fc82ea97988c6d5619d4cb33a47addae5d300890d676ef3e3ba2fad33", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 861, "doc": {"id": "e56c56c3cfe50ba0c787c2bd67255be8", "question": "She asked her little boy why, he replied that he didn't know and it was just what?", "question_concept": "why", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["case", "reason", "how", "because", "answer"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: She asked her little boy why, he replied that he didn't know and it was just what?\nA. case\nB. reason\nC. how\nD. because\nE. answer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: She asked her little boy why, he replied that he didn't know and it was just what?\nA. case\nB. reason\nC. how\nD. because\nE. answer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: She asked her little boy why, he replied that he didn't know and it was just what?\nA. case\nB. reason\nC. how\nD. because\nE. answer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: She asked her little boy why, he replied that he didn't know and it was just what?\nA. case\nB. reason\nC. how\nD. because\nE. answer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: She asked her little boy why, he replied that he didn't know and it was just what?\nA. case\nB. reason\nC. how\nD. because\nE. answer\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8735153675079346", "False"]], [["-1.6235153675079346", "True"]], [["-3.1235153675079346", "False"]], [["-2.8735153675079346", "False"]], [["-2.1235153675079346", "False"]]], "filtered_resps": [["-2.8735153675079346", "False"], ["-1.6235153675079346", "True"], ["-3.1235153675079346", "False"], ["-2.8735153675079346", "False"], ["-2.1235153675079346", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bf316f4d41d7a19627773a65a7780e86a190b3ee1086daf6801bbeaf13bc6967", "prompt_hash": "383bb0d151d9606b2ce194fad381a0481e6f61ae08342fcc0a5f36ca0d231b71", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 862, "doc": {"id": "6f48ee564a48293eb501cc0d8197bdd9", "question": "Where would you display a picture on a horizontal surface?", "question_concept": "picture", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["microwave", "desktop", "shelf", "art show", "wall"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you display a picture on a horizontal surface?\nA. microwave\nB. desktop\nC. shelf\nD. art show\nE. wall\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you display a picture on a horizontal surface?\nA. microwave\nB. desktop\nC. shelf\nD. art show\nE. wall\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you display a picture on a horizontal surface?\nA. microwave\nB. desktop\nC. shelf\nD. art show\nE. wall\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you display a picture on a horizontal surface?\nA. microwave\nB. desktop\nC. shelf\nD. art show\nE. wall\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you display a picture on a horizontal surface?\nA. microwave\nB. desktop\nC. shelf\nD. art show\nE. wall\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3052752017974854", "False"]], [["-1.3052752017974854", "True"]], [["-3.0552752017974854", "False"]], [["-6.055274963378906", "False"]], [["-2.3052752017974854", "False"]]], "filtered_resps": [["-3.3052752017974854", "False"], ["-1.3052752017974854", "True"], ["-3.0552752017974854", "False"], ["-6.055274963378906", "False"], ["-2.3052752017974854", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "51e8382b50f91707bfe5e6f9ef123b676fd19624d8498e432a551e442a76f172", "prompt_hash": "e88885c96c4c369f302d9bf3c2b598288c9e301ccc65e59175824140fad0f0fa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 863, "doc": {"id": "13d2a103abbed930cabc9567a1ba12f2", "question": "What skill is needed for riding a bike?", "question_concept": "riding bike", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wheels", "feet", "pedalling", "practice", "good balance"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What skill is needed for riding a bike?\nA. wheels\nB. feet\nC. pedalling\nD. practice\nE. good balance\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What skill is needed for riding a bike?\nA. wheels\nB. feet\nC. pedalling\nD. practice\nE. good balance\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What skill is needed for riding a bike?\nA. wheels\nB. feet\nC. pedalling\nD. practice\nE. good balance\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What skill is needed for riding a bike?\nA. wheels\nB. feet\nC. pedalling\nD. practice\nE. good balance\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What skill is needed for riding a bike?\nA. wheels\nB. feet\nC. pedalling\nD. practice\nE. good balance\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.312835693359375", "False"]], [["-3.312835931777954", "False"]], [["-4.562835693359375", "False"]], [["-2.562835931777954", "False"]], [["-1.562835931777954", "True"]]], "filtered_resps": [["-4.312835693359375", "False"], ["-3.312835931777954", "False"], ["-4.562835693359375", "False"], ["-2.562835931777954", "False"], ["-1.562835931777954", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ddba04241d669ffa09fa2857288e8491bb24dd7cf4dca11dd1907e033dc591ca", "prompt_hash": "2259c9de18784f56fc1dee64bcc6443241eb844e7ed731dc927a5428b2620769", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 864, "doc": {"id": "0c1efb38e023ee9725486fbec4f2d797", "question": "He looked at the field of pumps, all slowing churning oil out of the what?", "question_concept": "oil", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["manual", "street", "restaurant", "ground", "service station"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He looked at the field of pumps, all slowing churning oil out of the what?\nA. manual\nB. street\nC. restaurant\nD. ground\nE. service station\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He looked at the field of pumps, all slowing churning oil out of the what?\nA. manual\nB. street\nC. restaurant\nD. ground\nE. service station\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He looked at the field of pumps, all slowing churning oil out of the what?\nA. manual\nB. street\nC. restaurant\nD. ground\nE. service station\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He looked at the field of pumps, all slowing churning oil out of the what?\nA. manual\nB. street\nC. restaurant\nD. ground\nE. service station\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He looked at the field of pumps, all slowing churning oil out of the what?\nA. manual\nB. street\nC. restaurant\nD. ground\nE. service station\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5642337799072266", "False"]], [["-6.064233779907227", "False"]], [["-7.564233779907227", "False"]], [["-1.3142337799072266", "True"]], [["-4.064233779907227", "False"]]], "filtered_resps": [["-3.5642337799072266", "False"], ["-6.064233779907227", "False"], ["-7.564233779907227", "False"], ["-1.3142337799072266", "True"], ["-4.064233779907227", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "174fedaebb7f13f11efa07da46331390396664063bf6b60e6f3b35c9e419eb0b", "prompt_hash": "223d99540538a1caa696031406f5c2961adea6388e0c8294f8b68173c871e3d4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 865, "doc": {"id": "b7ab4a5e0c19a98f41cd1ba3176f2dff", "question": "The department to where vendors deliver goods for sale is called what?", "question_concept": "deliver", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["delivered", "take away", "receiving", "pick up", "keep"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The department to where vendors deliver goods for sale is called what?\nA. delivered\nB. take away\nC. receiving\nD. pick up\nE. keep\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The department to where vendors deliver goods for sale is called what?\nA. delivered\nB. take away\nC. receiving\nD. pick up\nE. keep\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The department to where vendors deliver goods for sale is called what?\nA. delivered\nB. take away\nC. receiving\nD. pick up\nE. keep\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The department to where vendors deliver goods for sale is called what?\nA. delivered\nB. take away\nC. receiving\nD. pick up\nE. keep\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The department to where vendors deliver goods for sale is called what?\nA. delivered\nB. take away\nC. receiving\nD. pick up\nE. keep\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.729206562042236", "False"]], [["-7.979206562042236", "False"]], [["-2.2292065620422363", "False"]], [["-6.729206562042236", "False"]], [["-8.229206085205078", "False"]]], "filtered_resps": [["-6.729206562042236", "False"], ["-7.979206562042236", "False"], ["-2.2292065620422363", "False"], ["-6.729206562042236", "False"], ["-8.229206085205078", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ff8ff6a41e8dfa725248089d6c1e65825fc8047d043dc05628fb13a340245684", "prompt_hash": "4c026326f0d5e48697ef14dbab55830d5351f5f7493c67579805699e4e796051", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 866, "doc": {"id": "8bcbb5098876940b2382db3a9a0b1beb", "question": "Where is the worst place to be in a ticket office?", "question_concept": "ticket office", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["at the top", "movie theaters", "train station", "end of line", "opera house"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is the worst place to be in a ticket office?\nA. at the top\nB. movie theaters\nC. train station\nD. end of line\nE. opera house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is the worst place to be in a ticket office?\nA. at the top\nB. movie theaters\nC. train station\nD. end of line\nE. opera house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is the worst place to be in a ticket office?\nA. at the top\nB. movie theaters\nC. train station\nD. end of line\nE. opera house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is the worst place to be in a ticket office?\nA. at the top\nB. movie theaters\nC. train station\nD. end of line\nE. opera house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is the worst place to be in a ticket office?\nA. at the top\nB. movie theaters\nC. train station\nD. end of line\nE. opera house\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3281307220458984", "False"]], [["-4.328130722045898", "False"]], [["-4.078130722045898", "False"]], [["-1.3281307220458984", "True"]], [["-5.078130722045898", "False"]]], "filtered_resps": [["-3.3281307220458984", "False"], ["-4.328130722045898", "False"], ["-4.078130722045898", "False"], ["-1.3281307220458984", "True"], ["-5.078130722045898", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0e887fdba9300b15fe74f0f8b1101a2b193c598a31e74ebffee438cc9263c6d8", "prompt_hash": "05f30892ff240bb55a16910c0c9fd3a1473a0585518f4265955d6def40eb426c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 867, "doc": {"id": "c7ce02d9365fe9275f88338ad51cbde6", "question": "Exercise is very good for you, for faster recovery you should always do what afterwards?", "question_concept": "exercise", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stretch", "lower cholesterol", "weigh", "track", "expend energy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Exercise is very good for you, for faster recovery you should always do what afterwards?\nA. stretch\nB. lower cholesterol\nC. weigh\nD. track\nE. expend energy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Exercise is very good for you, for faster recovery you should always do what afterwards?\nA. stretch\nB. lower cholesterol\nC. weigh\nD. track\nE. expend energy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Exercise is very good for you, for faster recovery you should always do what afterwards?\nA. stretch\nB. lower cholesterol\nC. weigh\nD. track\nE. expend energy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Exercise is very good for you, for faster recovery you should always do what afterwards?\nA. stretch\nB. lower cholesterol\nC. weigh\nD. track\nE. expend energy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Exercise is very good for you, for faster recovery you should always do what afterwards?\nA. stretch\nB. lower cholesterol\nC. weigh\nD. track\nE. expend energy\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9787899851799011", "True"]], [["-6.978789806365967", "False"]], [["-8.728790283203125", "False"]], [["-7.978789806365967", "False"]], [["-4.728789806365967", "False"]]], "filtered_resps": [["-0.9787899851799011", "True"], ["-6.978789806365967", "False"], ["-8.728790283203125", "False"], ["-7.978789806365967", "False"], ["-4.728789806365967", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1b4636dafc966f5229b5b3599f293474fb60973a097fdfa886e618408c05ea13", "prompt_hash": "3771997ec159c31cd5b44ab649830e65a7a563eec70aca1001211b468e4bfc9c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 868, "doc": {"id": "fb54a118d46b2776e435d411ae3dd9c8", "question": "What happens when you go somewhere and forget something at home?", "question_concept": "go somewhere", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["arriving", "arrive there", "turn around", "go back", "fart"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What happens when you go somewhere and forget something at home?\nA. arriving\nB. arrive there\nC. turn around\nD. go back\nE. fart\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens when you go somewhere and forget something at home?\nA. arriving\nB. arrive there\nC. turn around\nD. go back\nE. fart\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens when you go somewhere and forget something at home?\nA. arriving\nB. arrive there\nC. turn around\nD. go back\nE. fart\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens when you go somewhere and forget something at home?\nA. arriving\nB. arrive there\nC. turn around\nD. go back\nE. fart\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens when you go somewhere and forget something at home?\nA. arriving\nB. arrive there\nC. turn around\nD. go back\nE. fart\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.003551006317139", "False"]], [["-4.003551006317139", "False"]], [["-4.003551006317139", "False"]], [["-1.5035508871078491", "True"]], [["-9.00355052947998", "False"]]], "filtered_resps": [["-5.003551006317139", "False"], ["-4.003551006317139", "False"], ["-4.003551006317139", "False"], ["-1.5035508871078491", "True"], ["-9.00355052947998", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7135576dc713a368d57500db292bc045b427bccaff89d08d38a638dda2053a08", "prompt_hash": "4d7e16f7627c454d09b324c61a719d2e31aceb8889f43fa4b7b9018af4e5c1eb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 869, "doc": {"id": "2c13e6d61e3733db90a9fd22d72b3337", "question": "Where would you acquire a wind instrument for you own use?", "question_concept": "wind instrument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["band practice", "concert", "music store", "symphony", "music room"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you acquire a wind instrument for you own use?\nA. band practice\nB. concert\nC. music store\nD. symphony\nE. music room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you acquire a wind instrument for you own use?\nA. band practice\nB. concert\nC. music store\nD. symphony\nE. music room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you acquire a wind instrument for you own use?\nA. band practice\nB. concert\nC. music store\nD. symphony\nE. music room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you acquire a wind instrument for you own use?\nA. band practice\nB. concert\nC. music store\nD. symphony\nE. music room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you acquire a wind instrument for you own use?\nA. band practice\nB. concert\nC. music store\nD. symphony\nE. music room\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2458832263946533", "False"]], [["-6.495882987976074", "False"]], [["-1.2458832263946533", "False"]], [["-9.745882987976074", "False"]], [["-8.995882987976074", "False"]]], "filtered_resps": [["-3.2458832263946533", "False"], ["-6.495882987976074", "False"], ["-1.2458832263946533", "False"], ["-9.745882987976074", "False"], ["-8.995882987976074", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0078b55f244b2cac2ef6c5ecd2930f3d277f54a37a925ed68186595358ac5912", "prompt_hash": "f4bf4096d1bc299311b24fa927d23d78b73dd453c39658c1ca3e38f2c43472cb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 870, "doc": {"id": "350292ae429060a00ff2cf64d71558e4", "question": "Where would a person light alcohol on fire to observe the reaction?", "question_concept": "alcohol", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["supermarket", "bar", "pub", "restaurants", "chemistry lab"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would a person light alcohol on fire to observe the reaction?\nA. supermarket\nB. bar\nC. pub\nD. restaurants\nE. chemistry lab\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would a person light alcohol on fire to observe the reaction?\nA. supermarket\nB. bar\nC. pub\nD. restaurants\nE. chemistry lab\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would a person light alcohol on fire to observe the reaction?\nA. supermarket\nB. bar\nC. pub\nD. restaurants\nE. chemistry lab\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would a person light alcohol on fire to observe the reaction?\nA. supermarket\nB. bar\nC. pub\nD. restaurants\nE. chemistry lab\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would a person light alcohol on fire to observe the reaction?\nA. supermarket\nB. bar\nC. pub\nD. restaurants\nE. chemistry lab\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.4736573696136475", "False"]], [["-4.973657608032227", "False"]], [["-6.223657608032227", "False"]], [["-6.973657608032227", "False"]], [["-0.7236574292182922", "True"]]], "filtered_resps": [["-3.4736573696136475", "False"], ["-4.973657608032227", "False"], ["-6.223657608032227", "False"], ["-6.973657608032227", "False"], ["-0.7236574292182922", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "16739394d2c55a0abb4ed7b5812bf844502c2f1f1f7bd08e74ab472d28f59230", "prompt_hash": "3d4c861a03ae9a0afdcd2187364d4f36f7ec98adab7480dbef56b2e7b3930091", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 871, "doc": {"id": "179fff4b5928e5ac3d3ae3e1db782547", "question": "If a storey contained a panoramic view, what kind of structure would it be in?", "question_concept": "storey", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["horizontal room", "storey book", "mall", "tall building", "book of stories"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If a storey contained a panoramic view, what kind of structure would it be in?\nA. horizontal room\nB. storey book\nC. mall\nD. tall building\nE. book of stories\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a storey contained a panoramic view, what kind of structure would it be in?\nA. horizontal room\nB. storey book\nC. mall\nD. tall building\nE. book of stories\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a storey contained a panoramic view, what kind of structure would it be in?\nA. horizontal room\nB. storey book\nC. mall\nD. tall building\nE. book of stories\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a storey contained a panoramic view, what kind of structure would it be in?\nA. horizontal room\nB. storey book\nC. mall\nD. tall building\nE. book of stories\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a storey contained a panoramic view, what kind of structure would it be in?\nA. horizontal room\nB. storey book\nC. mall\nD. tall building\nE. book of stories\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0951975584030151", "True"]], [["-6.845197677612305", "False"]], [["-6.345197677612305", "False"]], [["-1.5951975584030151", "False"]], [["-8.095197677612305", "False"]]], "filtered_resps": [["-1.0951975584030151", "True"], ["-6.845197677612305", "False"], ["-6.345197677612305", "False"], ["-1.5951975584030151", "False"], ["-8.095197677612305", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "905dd36a091dc4d55cefd4ea4235a4491a643ed9b7c4ae01886334554c5c2d21", "prompt_hash": "704313dea82553e5391d1fed912a8b01dcd3d81148cd1179ac351523a8dd6680", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 872, "doc": {"id": "81cc0d320488c7bacafb285cf7db5fbd", "question": "Where does lettuce arrive by large trucks?", "question_concept": "lettuce", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kitchen", "supermarket", "farmer's market", "salad", "refrigerator"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where does lettuce arrive by large trucks?\nA. kitchen\nB. supermarket\nC. farmer's market\nD. salad\nE. refrigerator\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where does lettuce arrive by large trucks?\nA. kitchen\nB. supermarket\nC. farmer's market\nD. salad\nE. refrigerator\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where does lettuce arrive by large trucks?\nA. kitchen\nB. supermarket\nC. farmer's market\nD. salad\nE. refrigerator\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where does lettuce arrive by large trucks?\nA. kitchen\nB. supermarket\nC. farmer's market\nD. salad\nE. refrigerator\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where does lettuce arrive by large trucks?\nA. kitchen\nB. supermarket\nC. farmer's market\nD. salad\nE. refrigerator\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.972117900848389", "False"]], [["-0.9721180200576782", "True"]], [["-7.222117900848389", "False"]], [["-6.722117900848389", "False"]], [["-10.222118377685547", "False"]]], "filtered_resps": [["-5.972117900848389", "False"], ["-0.9721180200576782", "True"], ["-7.222117900848389", "False"], ["-6.722117900848389", "False"], ["-10.222118377685547", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "370e1652eae32a79a6814ec3124b0f02a720ef03eaabdca40a8cd16dabaea16d", "prompt_hash": "e9e31fafad2b5b73cabaa17065fcaf6fbeb6f0d62e42bbd984e0e178312009ba", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 873, "doc": {"id": "26c8a7165d0ed7250b9328f90d83ba83", "question": "Why do people who are dying receive social security payments?", "question_concept": "dying", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rejuvenation", "born again", "no longer exist", "unable to work", "change of color"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Why do people who are dying receive social security payments?\nA. rejuvenation\nB. born again\nC. no longer exist\nD. unable to work\nE. change of color\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why do people who are dying receive social security payments?\nA. rejuvenation\nB. born again\nC. no longer exist\nD. unable to work\nE. change of color\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why do people who are dying receive social security payments?\nA. rejuvenation\nB. born again\nC. no longer exist\nD. unable to work\nE. change of color\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why do people who are dying receive social security payments?\nA. rejuvenation\nB. born again\nC. no longer exist\nD. unable to work\nE. change of color\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why do people who are dying receive social security payments?\nA. rejuvenation\nB. born again\nC. no longer exist\nD. unable to work\nE. change of color\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.072933197021484", "False"]], [["-7.322933197021484", "False"]], [["-3.0729334354400635", "False"]], [["-1.5729334354400635", "True"]], [["-9.572933197021484", "False"]]], "filtered_resps": [["-6.072933197021484", "False"], ["-7.322933197021484", "False"], ["-3.0729334354400635", "False"], ["-1.5729334354400635", "True"], ["-9.572933197021484", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2ff2dd2253070c9f40e05320ae1c7d5c718b458c1b0b6a4825f4f30d576c6ae6", "prompt_hash": "acb25dd09bc8cfe40e03b29b12feb651e76a79fb36fab07f1a38cbedd91e5185", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 874, "doc": {"id": "636fc69dee35cd357b4191b47e64d0e5", "question": "What should I do with a jumping rope?", "question_concept": "jumping rope", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fatigue", "sweating", "get tired", "tiredness", "hopping"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What should I do with a jumping rope?\nA. fatigue\nB. sweating\nC. get tired\nD. tiredness\nE. hopping\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What should I do with a jumping rope?\nA. fatigue\nB. sweating\nC. get tired\nD. tiredness\nE. hopping\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What should I do with a jumping rope?\nA. fatigue\nB. sweating\nC. get tired\nD. tiredness\nE. hopping\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What should I do with a jumping rope?\nA. fatigue\nB. sweating\nC. get tired\nD. tiredness\nE. hopping\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What should I do with a jumping rope?\nA. fatigue\nB. sweating\nC. get tired\nD. tiredness\nE. hopping\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.272998571395874", "False"]], [["-3.022998571395874", "False"]], [["-4.772998809814453", "False"]], [["-5.772998809814453", "False"]], [["-1.272998571395874", "True"]]], "filtered_resps": [["-3.272998571395874", "False"], ["-3.022998571395874", "False"], ["-4.772998809814453", "False"], ["-5.772998809814453", "False"], ["-1.272998571395874", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "198b654574d24fc4785bd6a90a3c0b13b7f45ca41407fe199c94f4d359b7142e", "prompt_hash": "785e7fb991b0d5ce6d2abdbaf6096f4190e2cb5006f7c93d08143adb69fa104f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 875, "doc": {"id": "f0c4622a082eb9ad0690dd36dcf61297", "question": "What do geese do every fall in fields?", "question_concept": "geese", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["guard house", "fly", "eat", "follow ultralight airplane", "group together"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do geese do every fall in fields?\nA. guard house\nB. fly\nC. eat\nD. follow ultralight airplane\nE. group together\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do geese do every fall in fields?\nA. guard house\nB. fly\nC. eat\nD. follow ultralight airplane\nE. group together\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do geese do every fall in fields?\nA. guard house\nB. fly\nC. eat\nD. follow ultralight airplane\nE. group together\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do geese do every fall in fields?\nA. guard house\nB. fly\nC. eat\nD. follow ultralight airplane\nE. group together\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do geese do every fall in fields?\nA. guard house\nB. fly\nC. eat\nD. follow ultralight airplane\nE. group together\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.056708335876465", "False"]], [["-2.306708335876465", "False"]], [["-7.556708335876465", "False"]], [["-5.806708335876465", "False"]], [["-2.806708335876465", "False"]]], "filtered_resps": [["-6.056708335876465", "False"], ["-2.306708335876465", "False"], ["-7.556708335876465", "False"], ["-5.806708335876465", "False"], ["-2.806708335876465", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "adb5290c1ba69b3ef1ca87cabe24c6669a1849b2cfb1173f6721dd45a25c4425", "prompt_hash": "686ca756a129ed2c64a3fe44e7f0a917604e287862bcb1aa81c707a7c02809f2", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 876, "doc": {"id": "4499ebd5e8188b0d5fdef6afd893017a", "question": "I took my seat, the curtains drew back and I enjoyed the what?", "question_concept": "seat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["airplane", "movie", "auditorium", "theatre", "show"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: I took my seat, the curtains drew back and I enjoyed the what?\nA. airplane\nB. movie\nC. auditorium\nD. theatre\nE. show\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I took my seat, the curtains drew back and I enjoyed the what?\nA. airplane\nB. movie\nC. auditorium\nD. theatre\nE. show\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I took my seat, the curtains drew back and I enjoyed the what?\nA. airplane\nB. movie\nC. auditorium\nD. theatre\nE. show\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I took my seat, the curtains drew back and I enjoyed the what?\nA. airplane\nB. movie\nC. auditorium\nD. theatre\nE. show\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I took my seat, the curtains drew back and I enjoyed the what?\nA. airplane\nB. movie\nC. auditorium\nD. theatre\nE. show\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7462916374206543", "False"]], [["-1.7462916374206543", "True"]], [["-5.496291637420654", "False"]], [["-2.7462916374206543", "False"]], [["-2.9962916374206543", "False"]]], "filtered_resps": [["-3.7462916374206543", "False"], ["-1.7462916374206543", "True"], ["-5.496291637420654", "False"], ["-2.7462916374206543", "False"], ["-2.9962916374206543", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fa579bc5ef44fa51cd079ff363d4f61d6bc7dbf2902dade5e0e70665e9e10f58", "prompt_hash": "ff9e0df0929fba7e5d8c2d6f3299b5f812d3ed7dec47a84e251132add3440d2f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 877, "doc": {"id": "230cc491829307e8edb5423c8d09f945", "question": "What should everyone do who doesn't want to fight anymore?", "question_concept": "everyone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["explicate", "pay tribute to king", "hope for peace", "wear shoes", "do well"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What should everyone do who doesn't want to fight anymore?\nA. explicate\nB. pay tribute to king\nC. hope for peace\nD. wear shoes\nE. do well\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What should everyone do who doesn't want to fight anymore?\nA. explicate\nB. pay tribute to king\nC. hope for peace\nD. wear shoes\nE. do well\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What should everyone do who doesn't want to fight anymore?\nA. explicate\nB. pay tribute to king\nC. hope for peace\nD. wear shoes\nE. do well\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What should everyone do who doesn't want to fight anymore?\nA. explicate\nB. pay tribute to king\nC. hope for peace\nD. wear shoes\nE. do well\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What should everyone do who doesn't want to fight anymore?\nA. explicate\nB. pay tribute to king\nC. hope for peace\nD. wear shoes\nE. do well\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.654726982116699", "False"]], [["-6.904726982116699", "False"]], [["-1.1547271013259888", "True"]], [["-6.654726982116699", "False"]], [["-5.654726982116699", "False"]]], "filtered_resps": [["-4.654726982116699", "False"], ["-6.904726982116699", "False"], ["-1.1547271013259888", "True"], ["-6.654726982116699", "False"], ["-5.654726982116699", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fe1e0aeacc71c8b97506dbbaa5663be1c34f8897944902928528d8426add956c", "prompt_hash": "e90ff78449a65464a36ddb45f52a8d0b8b03b34c91f8693a180ec0ac81ac67c3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 878, "doc": {"id": "6163a897cd7eac1deddd4c002a1930ae", "question": "Where is the ideal location for a post office?", "question_concept": "post office", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["building", "business district", "above ground", "most towns", "center of town"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where is the ideal location for a post office?\nA. building\nB. business district\nC. above ground\nD. most towns\nE. center of town\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is the ideal location for a post office?\nA. building\nB. business district\nC. above ground\nD. most towns\nE. center of town\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is the ideal location for a post office?\nA. building\nB. business district\nC. above ground\nD. most towns\nE. center of town\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is the ideal location for a post office?\nA. building\nB. business district\nC. above ground\nD. most towns\nE. center of town\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is the ideal location for a post office?\nA. building\nB. business district\nC. above ground\nD. most towns\nE. center of town\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.479253768920898", "False"]], [["-4.479253768920898", "False"]], [["-6.729253768920898", "False"]], [["-2.2292535305023193", "False"]], [["-2.4792535305023193", "False"]]], "filtered_resps": [["-4.479253768920898", "False"], ["-4.479253768920898", "False"], ["-6.729253768920898", "False"], ["-2.2292535305023193", "False"], ["-2.4792535305023193", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9cdb59718b7d77f0593e5807f02dec30e97262b97d975ef7d903329453aea7ff", "prompt_hash": "b3a42449127ae9d3b8775822eef31996dc80b9cb3ddfd01f184916e6a51a94f2", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 879, "doc": {"id": "55478486079423907508a06be13ca536", "question": "Where outside of a city would a squirrel live?", "question_concept": "squirrel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["roof", "inside home", "forest", "yard", "park"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where outside of a city would a squirrel live?\nA. roof\nB. inside home\nC. forest\nD. yard\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where outside of a city would a squirrel live?\nA. roof\nB. inside home\nC. forest\nD. yard\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where outside of a city would a squirrel live?\nA. roof\nB. inside home\nC. forest\nD. yard\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where outside of a city would a squirrel live?\nA. roof\nB. inside home\nC. forest\nD. yard\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where outside of a city would a squirrel live?\nA. roof\nB. inside home\nC. forest\nD. yard\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7687621116638184", "False"]], [["-8.768762588500977", "False"]], [["-1.018762230873108", "True"]], [["-7.268762111663818", "False"]], [["-6.268762111663818", "False"]]], "filtered_resps": [["-2.7687621116638184", "False"], ["-8.768762588500977", "False"], ["-1.018762230873108", "True"], ["-7.268762111663818", "False"], ["-6.268762111663818", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "150f6bbd1bb57fa199b39eaf0fe64311f421b8b27ef8ea7494dc3d157ae499e5", "prompt_hash": "4646e9a07ee57e92bbf51189e3fdb9468e13365927699b6c1929c1984bbff1be", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 880, "doc": {"id": "4fa0d61ec82eb1e238d8938d5f43f392", "question": "You should watch out for snakes if floating down what African body of water?", "question_concept": "snake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wet grass", "western texas", "high grass", "amazon river", "tree"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: You should watch out for snakes if floating down what African body of water?\nA. wet grass\nB. western texas\nC. high grass\nD. amazon river\nE. tree\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: You should watch out for snakes if floating down what African body of water?\nA. wet grass\nB. western texas\nC. high grass\nD. amazon river\nE. tree\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: You should watch out for snakes if floating down what African body of water?\nA. wet grass\nB. western texas\nC. high grass\nD. amazon river\nE. tree\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: You should watch out for snakes if floating down what African body of water?\nA. wet grass\nB. western texas\nC. high grass\nD. amazon river\nE. tree\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: You should watch out for snakes if floating down what African body of water?\nA. wet grass\nB. western texas\nC. high grass\nD. amazon river\nE. tree\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7032666206359863", "False"]], [["-4.703266620635986", "False"]], [["-1.9532665014266968", "True"]], [["-2.2032666206359863", "False"]], [["-4.203266620635986", "False"]]], "filtered_resps": [["-2.7032666206359863", "False"], ["-4.703266620635986", "False"], ["-1.9532665014266968", "True"], ["-2.2032666206359863", "False"], ["-4.203266620635986", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bf9041ddd640b14b3da6ab8a13b955f5b0cd95d37f8ea0fc3ac06efe1b24b454", "prompt_hash": "94e1484045298d05187265e6db4c611afa01f54547d9e19fd1cd34aa5882ba9f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 881, "doc": {"id": "b4f79ca5f3595248ee25292ab60ad105", "question": "At the end of the day as he began to eat he paused and thanked her, it wasn't often she would what?", "question_concept": "eat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cook dinner", "did chores", "make food", "stretch out", "get food"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: At the end of the day as he began to eat he paused and thanked her, it wasn't often she would what?\nA. cook dinner\nB. did chores\nC. make food\nD. stretch out\nE. get food\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: At the end of the day as he began to eat he paused and thanked her, it wasn't often she would what?\nA. cook dinner\nB. did chores\nC. make food\nD. stretch out\nE. get food\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: At the end of the day as he began to eat he paused and thanked her, it wasn't often she would what?\nA. cook dinner\nB. did chores\nC. make food\nD. stretch out\nE. get food\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: At the end of the day as he began to eat he paused and thanked her, it wasn't often she would what?\nA. cook dinner\nB. did chores\nC. make food\nD. stretch out\nE. get food\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: At the end of the day as he began to eat he paused and thanked her, it wasn't often she would what?\nA. cook dinner\nB. did chores\nC. make food\nD. stretch out\nE. get food\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.9853514432907104", "False"]], [["-5.2353515625", "False"]], [["-1.7353514432907104", "False"]], [["-5.9853515625", "False"]], [["-4.7353515625", "False"]]], "filtered_resps": [["-1.9853514432907104", "False"], ["-5.2353515625", "False"], ["-1.7353514432907104", "False"], ["-5.9853515625", "False"], ["-4.7353515625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4bece295e895775d0e36e4e673babff9683c10d110ee93809db7e92f99bf39c9", "prompt_hash": "bbb4c2e1b52a50c12b373efad025ff9eac4cb41a34f4bea079c751175be3d85e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 882, "doc": {"id": "c39131d979c9205c11d0e109e18188e4", "question": "To what do trees roots cling?", "question_concept": "trees", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["yard", "orchard", "museum", "countryside", "surface of earth"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: To what do trees roots cling?\nA. yard\nB. orchard\nC. museum\nD. countryside\nE. surface of earth\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: To what do trees roots cling?\nA. yard\nB. orchard\nC. museum\nD. countryside\nE. surface of earth\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: To what do trees roots cling?\nA. yard\nB. orchard\nC. museum\nD. countryside\nE. surface of earth\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: To what do trees roots cling?\nA. yard\nB. orchard\nC. museum\nD. countryside\nE. surface of earth\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: To what do trees roots cling?\nA. yard\nB. orchard\nC. museum\nD. countryside\nE. surface of earth\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.717951774597168", "False"]], [["-6.467951774597168", "False"]], [["-7.467951774597168", "False"]], [["-8.717951774597168", "False"]], [["-1.4679515361785889", "False"]]], "filtered_resps": [["-5.717951774597168", "False"], ["-6.467951774597168", "False"], ["-7.467951774597168", "False"], ["-8.717951774597168", "False"], ["-1.4679515361785889", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2f25c577aa3a7c42e312c828d17705c99f8a9528e6510b4d60342589309ae2d9", "prompt_hash": "524962b3f37a2981eecf468bcc09ccc2803e4a01765b7b6d9998046037a8375e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 883, "doc": {"id": "bd773d64f4e22db2358c6e00cbdf2d83", "question": "What probably has a lot of dust in the back?", "question_concept": "dust", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["closet", "door", "corner", "shelf", "library"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What probably has a lot of dust in the back?\nA. closet\nB. door\nC. corner\nD. shelf\nE. library\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What probably has a lot of dust in the back?\nA. closet\nB. door\nC. corner\nD. shelf\nE. library\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What probably has a lot of dust in the back?\nA. closet\nB. door\nC. corner\nD. shelf\nE. library\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What probably has a lot of dust in the back?\nA. closet\nB. door\nC. corner\nD. shelf\nE. library\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What probably has a lot of dust in the back?\nA. closet\nB. door\nC. corner\nD. shelf\nE. library\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6833334565162659", "True"]], [["-5.433333396911621", "False"]], [["-5.183333396911621", "False"]], [["-4.433333396911621", "False"]], [["-4.683333396911621", "False"]]], "filtered_resps": [["-0.6833334565162659", "True"], ["-5.433333396911621", "False"], ["-5.183333396911621", "False"], ["-4.433333396911621", "False"], ["-4.683333396911621", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "aff578ce64f691d053c6cd2852a47296483c20193187f60dc201452779916b22", "prompt_hash": "0ac91f8f3878d1eb6eb121a8adab757b87902f3c01e8c39b9fdd19cc0e853b2c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 884, "doc": {"id": "2b416120e2fbd84b44b5dcd4eb42ed5c", "question": "At the new comic store he found himself making friends, it was nice to meet people with what?", "question_concept": "making friends", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["smiling", "smile", "open mind", "common interests", "laughter"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: At the new comic store he found himself making friends, it was nice to meet people with what?\nA. smiling\nB. smile\nC. open mind\nD. common interests\nE. laughter\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: At the new comic store he found himself making friends, it was nice to meet people with what?\nA. smiling\nB. smile\nC. open mind\nD. common interests\nE. laughter\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: At the new comic store he found himself making friends, it was nice to meet people with what?\nA. smiling\nB. smile\nC. open mind\nD. common interests\nE. laughter\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: At the new comic store he found himself making friends, it was nice to meet people with what?\nA. smiling\nB. smile\nC. open mind\nD. common interests\nE. laughter\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: At the new comic store he found himself making friends, it was nice to meet people with what?\nA. smiling\nB. smile\nC. open mind\nD. common interests\nE. laughter\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.241708755493164", "False"]], [["-6.241708755493164", "False"]], [["-4.241708755493164", "False"]], [["-1.991708517074585", "False"]], [["-9.991708755493164", "False"]]], "filtered_resps": [["-5.241708755493164", "False"], ["-6.241708755493164", "False"], ["-4.241708755493164", "False"], ["-1.991708517074585", "False"], ["-9.991708755493164", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b80279732937096b1059119ecfdf9d617c2d8313027eaee1a8832dca73dfb86b", "prompt_hash": "2f1060ce75692d4b0eb1004e1bb840323a470ba96a038678d16dffdb6dfc2335", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 885, "doc": {"id": "cef855ec07c66a731741026c2839b0d3", "question": "The student explained he had a clue what neuroepithelium was and got really nervous, he then lost his balance because a what issue?", "question_concept": "neuroepithelium", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tastebud", "retina", "inner ear", "nasal cavity", "autistic"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The student explained he had a clue what neuroepithelium was and got really nervous, he then lost his balance because a what issue?\nA. tastebud\nB. retina\nC. inner ear\nD. nasal cavity\nE. autistic\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The student explained he had a clue what neuroepithelium was and got really nervous, he then lost his balance because a what issue?\nA. tastebud\nB. retina\nC. inner ear\nD. nasal cavity\nE. autistic\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The student explained he had a clue what neuroepithelium was and got really nervous, he then lost his balance because a what issue?\nA. tastebud\nB. retina\nC. inner ear\nD. nasal cavity\nE. autistic\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The student explained he had a clue what neuroepithelium was and got really nervous, he then lost his balance because a what issue?\nA. tastebud\nB. retina\nC. inner ear\nD. nasal cavity\nE. autistic\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The student explained he had a clue what neuroepithelium was and got really nervous, he then lost his balance because a what issue?\nA. tastebud\nB. retina\nC. inner ear\nD. nasal cavity\nE. autistic\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.396295547485352", "False"]], [["-5.146295547485352", "False"]], [["-0.6462957262992859", "True"]], [["-8.146295547485352", "False"]], [["-5.146295547485352", "False"]]], "filtered_resps": [["-4.396295547485352", "False"], ["-5.146295547485352", "False"], ["-0.6462957262992859", "True"], ["-8.146295547485352", "False"], ["-5.146295547485352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a9bf5eb7a3bf11260210730dd458fe640b64009dc2cd3b29708a51a6fce6a3ff", "prompt_hash": "cac96cd0ab203294f3855de861ead277e7c40523f3d378ab1ce0acafcccc83c3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 886, "doc": {"id": "0bbb82c1dc4bfd3b0e0c409a0afd248b", "question": "What could people do that involves talking?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["confession", "state park", "sing", "carnival", "opera"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What could people do that involves talking?\nA. confession\nB. state park\nC. sing\nD. carnival\nE. opera\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could people do that involves talking?\nA. confession\nB. state park\nC. sing\nD. carnival\nE. opera\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could people do that involves talking?\nA. confession\nB. state park\nC. sing\nD. carnival\nE. opera\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could people do that involves talking?\nA. confession\nB. state park\nC. sing\nD. carnival\nE. opera\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could people do that involves talking?\nA. confession\nB. state park\nC. sing\nD. carnival\nE. opera\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.344660997390747", "True"]], [["-6.094660758972168", "False"]], [["-4.094660758972168", "False"]], [["-7.844660758972168", "False"]], [["-3.344660997390747", "False"]]], "filtered_resps": [["-1.344660997390747", "True"], ["-6.094660758972168", "False"], ["-4.094660758972168", "False"], ["-7.844660758972168", "False"], ["-3.344660997390747", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "defd134d61cb1e579e2b0a0ed86a1ddf1b84489ac9123b57b81dd93835e9fac3", "prompt_hash": "2f939074561381dd4186a4ecdb749fe6cab01c170cc1c0baaf27b320f9faf236", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 887, "doc": {"id": "67beae081a9b5ef56988f205f80cf129", "question": "If you're a child answering questions and an adult is asking them that adult is doing what?", "question_concept": "answering questions", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["discussion", "explaning", "teaching", "confusion", "correct"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If you're a child answering questions and an adult is asking them that adult is doing what?\nA. discussion\nB. explaning\nC. teaching\nD. confusion\nE. correct\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you're a child answering questions and an adult is asking them that adult is doing what?\nA. discussion\nB. explaning\nC. teaching\nD. confusion\nE. correct\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you're a child answering questions and an adult is asking them that adult is doing what?\nA. discussion\nB. explaning\nC. teaching\nD. confusion\nE. correct\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you're a child answering questions and an adult is asking them that adult is doing what?\nA. discussion\nB. explaning\nC. teaching\nD. confusion\nE. correct\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you're a child answering questions and an adult is asking them that adult is doing what?\nA. discussion\nB. explaning\nC. teaching\nD. confusion\nE. correct\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5454113483428955", "False"]], [["-4.545411109924316", "False"]], [["-1.0454113483428955", "True"]], [["-6.295411109924316", "False"]], [["-6.545411109924316", "False"]]], "filtered_resps": [["-3.5454113483428955", "False"], ["-4.545411109924316", "False"], ["-1.0454113483428955", "True"], ["-6.295411109924316", "False"], ["-6.545411109924316", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "09abc9358d5d8258d3c8735ff8ed3aa8f8acb9ecf80fef6b2bed5d992bac30b0", "prompt_hash": "f6679c3a306ea805cbe357dfedc8bdb9bfa2fced35e7839fc67f16779d16444b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 888, "doc": {"id": "3b4dcfcab4726496bdbe9535cc669082", "question": "He has lactose intolerant, but was eating dinner made of cheese, what followed for him?", "question_concept": "eating dinner", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["digestive", "feel better", "sleepiness", "indigestion", "illness"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He has lactose intolerant, but was eating dinner made of cheese, what followed for him?\nA. digestive\nB. feel better\nC. sleepiness\nD. indigestion\nE. illness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He has lactose intolerant, but was eating dinner made of cheese, what followed for him?\nA. digestive\nB. feel better\nC. sleepiness\nD. indigestion\nE. illness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He has lactose intolerant, but was eating dinner made of cheese, what followed for him?\nA. digestive\nB. feel better\nC. sleepiness\nD. indigestion\nE. illness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He has lactose intolerant, but was eating dinner made of cheese, what followed for him?\nA. digestive\nB. feel better\nC. sleepiness\nD. indigestion\nE. illness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He has lactose intolerant, but was eating dinner made of cheese, what followed for him?\nA. digestive\nB. feel better\nC. sleepiness\nD. indigestion\nE. illness\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.930698871612549", "False"]], [["-6.430698871612549", "False"]], [["-7.180698871612549", "False"]], [["-2.180698871612549", "False"]], [["-1.4306988716125488", "True"]]], "filtered_resps": [["-3.930698871612549", "False"], ["-6.430698871612549", "False"], ["-7.180698871612549", "False"], ["-2.180698871612549", "False"], ["-1.4306988716125488", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b9a252d57b45c61445dd50bdc0d6259ca30be5b25145dab9bffca19fd24aeccd", "prompt_hash": "b1eda82feadf825e68896809a46a35a92ad87a2cb245c81c0a13e0c14ea2fb80", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 889, "doc": {"id": "eebddf5f35d85e9fe2ecbd9b56c1db60", "question": "The teacher played on the upright piano, she was explaining the song to all the students in the what?", "question_concept": "upright piano", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["music room", "bathroom", "house", "living room", "music store"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The teacher played on the upright piano, she was explaining the song to all the students in the what?\nA. music room\nB. bathroom\nC. house\nD. living room\nE. music store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The teacher played on the upright piano, she was explaining the song to all the students in the what?\nA. music room\nB. bathroom\nC. house\nD. living room\nE. music store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The teacher played on the upright piano, she was explaining the song to all the students in the what?\nA. music room\nB. bathroom\nC. house\nD. living room\nE. music store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The teacher played on the upright piano, she was explaining the song to all the students in the what?\nA. music room\nB. bathroom\nC. house\nD. living room\nE. music store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The teacher played on the upright piano, she was explaining the song to all the students in the what?\nA. music room\nB. bathroom\nC. house\nD. living room\nE. music store\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3405548334121704", "True"]], [["-6.090554714202881", "False"]], [["-6.090554714202881", "False"]], [["-3.840554714202881", "False"]], [["-6.590554714202881", "False"]]], "filtered_resps": [["-1.3405548334121704", "True"], ["-6.090554714202881", "False"], ["-6.090554714202881", "False"], ["-3.840554714202881", "False"], ["-6.590554714202881", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c538a2ae8f1735cdbc5ddd4d60177251176925bf04d41e40f7b7f6fa44a935e3", "prompt_hash": "07bae8b248c98b6d19171d775608b8cb3d6a09fa13ff57295c4d3ba41dcc1951", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 890, "doc": {"id": "5393ba1ce298bd1ac4744c07d7373a9c", "question": "When you get an F, you fail. If you get A's you are?", "question_concept": "fail", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["passed", "completing", "passed", "passing", "succeeding"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When you get an F, you fail. If you get A's you are?\nA. passed\nB. completing\nC. passed\nD. passing\nE. succeeding\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you get an F, you fail. If you get A's you are?\nA. passed\nB. completing\nC. passed\nD. passing\nE. succeeding\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you get an F, you fail. If you get A's you are?\nA. passed\nB. completing\nC. passed\nD. passing\nE. succeeding\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you get an F, you fail. If you get A's you are?\nA. passed\nB. completing\nC. passed\nD. passing\nE. succeeding\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you get an F, you fail. If you get A's you are?\nA. passed\nB. completing\nC. passed\nD. passing\nE. succeeding\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6100058555603027", "False"]], [["-6.110005855560303", "False"]], [["-6.860005855560303", "False"]], [["-4.110005855560303", "False"]], [["-1.1100058555603027", "True"]]], "filtered_resps": [["-3.6100058555603027", "False"], ["-6.110005855560303", "False"], ["-6.860005855560303", "False"], ["-4.110005855560303", "False"], ["-1.1100058555603027", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5201a272388ab3f4dd0f6727db882a9a1079df44d51093facd0725c709812d7b", "prompt_hash": "08aadf89ecbece7d96010b1dc484d6eb7e3f628b15aad9050e234ef0a933f7f8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 891, "doc": {"id": "fde48d43e27cefed6ed9c52514e0bb6d", "question": "What is the main purpose of having a bath?", "question_concept": "having bath", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cleanness", "wetness", "exfoliation", "use water", "hygiene"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is the main purpose of having a bath?\nA. cleanness\nB. wetness\nC. exfoliation\nD. use water\nE. hygiene\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the main purpose of having a bath?\nA. cleanness\nB. wetness\nC. exfoliation\nD. use water\nE. hygiene\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the main purpose of having a bath?\nA. cleanness\nB. wetness\nC. exfoliation\nD. use water\nE. hygiene\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the main purpose of having a bath?\nA. cleanness\nB. wetness\nC. exfoliation\nD. use water\nE. hygiene\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the main purpose of having a bath?\nA. cleanness\nB. wetness\nC. exfoliation\nD. use water\nE. hygiene\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.132147789001465", "False"]], [["-4.882147789001465", "False"]], [["-8.882147789001465", "False"]], [["-8.632147789001465", "False"]], [["-2.132148027420044", "False"]]], "filtered_resps": [["-5.132147789001465", "False"], ["-4.882147789001465", "False"], ["-8.882147789001465", "False"], ["-8.632147789001465", "False"], ["-2.132148027420044", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cd3b39c2fa4a23e3468592ea9d6f6663b33b33378eb31e4931a0c0593a824679", "prompt_hash": "5e2a5d26cf97537cb4b2da11708340c4a39bd08bf47b5779f7ea32a0bf9e75b8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 892, "doc": {"id": "da83d85e28778c082d9a63f5b890b26d", "question": "The ball was hit over a boundary and struck an audience member.  What kind of game were they playing?", "question_concept": "boundary", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sporting event", "sporting", "basketball", "society", "ranch country"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The ball was hit over a boundary and struck an audience member.  What kind of game were they playing?\nA. sporting event\nB. sporting\nC. basketball\nD. society\nE. ranch country\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The ball was hit over a boundary and struck an audience member.  What kind of game were they playing?\nA. sporting event\nB. sporting\nC. basketball\nD. society\nE. ranch country\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The ball was hit over a boundary and struck an audience member.  What kind of game were they playing?\nA. sporting event\nB. sporting\nC. basketball\nD. society\nE. ranch country\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The ball was hit over a boundary and struck an audience member.  What kind of game were they playing?\nA. sporting event\nB. sporting\nC. basketball\nD. society\nE. ranch country\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The ball was hit over a boundary and struck an audience member.  What kind of game were they playing?\nA. sporting event\nB. sporting\nC. basketball\nD. society\nE. ranch country\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7782206535339355", "False"]], [["-4.5282206535339355", "False"]], [["-3.7782206535339355", "False"]], [["-8.028221130371094", "False"]], [["-10.778221130371094", "False"]]], "filtered_resps": [["-1.7782206535339355", "False"], ["-4.5282206535339355", "False"], ["-3.7782206535339355", "False"], ["-8.028221130371094", "False"], ["-10.778221130371094", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d0dc4c5e3d980811f7024dd6b2a8dbfcea55781a9dc46e3e6f811b352fc6a82c", "prompt_hash": "506149273ce29d0ccaffc60dbf9fc3c16f3ab611ca939fe337dd62f830a3d852", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 893, "doc": {"id": "cfa980561efe82e7ae7080d4f081b463", "question": "What is someone operating a vehicle likely to be accused of after becoming inebriated?", "question_concept": "becoming inebriated", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["punish", "arrest", "automobile accidents", "drunk driving", "talking nonsense"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is someone operating a vehicle likely to be accused of after becoming inebriated?\nA. punish\nB. arrest\nC. automobile accidents\nD. drunk driving\nE. talking nonsense\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is someone operating a vehicle likely to be accused of after becoming inebriated?\nA. punish\nB. arrest\nC. automobile accidents\nD. drunk driving\nE. talking nonsense\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is someone operating a vehicle likely to be accused of after becoming inebriated?\nA. punish\nB. arrest\nC. automobile accidents\nD. drunk driving\nE. talking nonsense\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is someone operating a vehicle likely to be accused of after becoming inebriated?\nA. punish\nB. arrest\nC. automobile accidents\nD. drunk driving\nE. talking nonsense\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is someone operating a vehicle likely to be accused of after becoming inebriated?\nA. punish\nB. arrest\nC. automobile accidents\nD. drunk driving\nE. talking nonsense\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.088674068450928", "False"]], [["-4.588674068450928", "False"]], [["-6.588674068450928", "False"]], [["-0.5886742472648621", "True"]], [["-10.088674545288086", "False"]]], "filtered_resps": [["-4.088674068450928", "False"], ["-4.588674068450928", "False"], ["-6.588674068450928", "False"], ["-0.5886742472648621", "True"], ["-10.088674545288086", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7b993c5e8ccd081eaa6c1f07fb8fc8556eab351d0d877ef2d985ac2da8060bc3", "prompt_hash": "bfb7854401a33db444e13ddad78bee020d9eb97d7c39eba72929062f449750cc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 894, "doc": {"id": "384b89e789e0f4b4796120394fb6303b", "question": "Where would you get jewelry if you do not have any?", "question_concept": "jewelry", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["vault", "suitcase", "neighbour's house", "department store", "safe deposit box"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you get jewelry if you do not have any?\nA. vault\nB. suitcase\nC. neighbour's house\nD. department store\nE. safe deposit box\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you get jewelry if you do not have any?\nA. vault\nB. suitcase\nC. neighbour's house\nD. department store\nE. safe deposit box\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you get jewelry if you do not have any?\nA. vault\nB. suitcase\nC. neighbour's house\nD. department store\nE. safe deposit box\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you get jewelry if you do not have any?\nA. vault\nB. suitcase\nC. neighbour's house\nD. department store\nE. safe deposit box\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you get jewelry if you do not have any?\nA. vault\nB. suitcase\nC. neighbour's house\nD. department store\nE. safe deposit box\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.269484996795654", "False"]], [["-4.519484996795654", "False"]], [["-5.269484996795654", "False"]], [["-1.2694848775863647", "True"]], [["-7.769484996795654", "False"]]], "filtered_resps": [["-4.269484996795654", "False"], ["-4.519484996795654", "False"], ["-5.269484996795654", "False"], ["-1.2694848775863647", "True"], ["-7.769484996795654", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0e45e01dc61d0b78b22c8de59877485f882ae56240b8a6c6af42f39cbcab0fef", "prompt_hash": "46912518f5cec0cb6e28e01e68cd3c8db371860957709e3778020fba83b3058e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 895, "doc": {"id": "0d66d33a17e41eaa3278ca7b3930c5ea", "question": "What is a philosopher waiting for to eventually gain through his studies?", "question_concept": "waiting for", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["job", "boredom", "anxiety", "impatience", "wisdom"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is a philosopher waiting for to eventually gain through his studies?\nA. job\nB. boredom\nC. anxiety\nD. impatience\nE. wisdom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a philosopher waiting for to eventually gain through his studies?\nA. job\nB. boredom\nC. anxiety\nD. impatience\nE. wisdom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a philosopher waiting for to eventually gain through his studies?\nA. job\nB. boredom\nC. anxiety\nD. impatience\nE. wisdom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a philosopher waiting for to eventually gain through his studies?\nA. job\nB. boredom\nC. anxiety\nD. impatience\nE. wisdom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a philosopher waiting for to eventually gain through his studies?\nA. job\nB. boredom\nC. anxiety\nD. impatience\nE. wisdom\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.263542890548706", "False"]], [["-6.763543128967285", "False"]], [["-9.263543128967285", "False"]], [["-7.513543128967285", "False"]], [["-1.263542890548706", "True"]]], "filtered_resps": [["-2.263542890548706", "False"], ["-6.763543128967285", "False"], ["-9.263543128967285", "False"], ["-7.513543128967285", "False"], ["-1.263542890548706", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "66cc675299eb00f6e679348ce607f6e27d010f7c1f706299fbc71b3164f54a40", "prompt_hash": "f8164471b6f1b1b36c43cf0cb06059a6134469d4920d2f92559eb7e5a02b8d2c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 896, "doc": {"id": "732183ead4206e51ed4df18b9c9f14fe", "question": "What do young boys do on the ice in the winter?", "question_concept": "winter", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ski", "play hockey", "summer", "knit", "warm"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What do young boys do on the ice in the winter?\nA. ski\nB. play hockey\nC. summer\nD. knit\nE. warm\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do young boys do on the ice in the winter?\nA. ski\nB. play hockey\nC. summer\nD. knit\nE. warm\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do young boys do on the ice in the winter?\nA. ski\nB. play hockey\nC. summer\nD. knit\nE. warm\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do young boys do on the ice in the winter?\nA. ski\nB. play hockey\nC. summer\nD. knit\nE. warm\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do young boys do on the ice in the winter?\nA. ski\nB. play hockey\nC. summer\nD. knit\nE. warm\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.676506042480469", "False"]], [["-0.9265058040618896", "True"]], [["-8.426506042480469", "False"]], [["-8.926506042480469", "False"]], [["-11.176506042480469", "False"]]], "filtered_resps": [["-6.676506042480469", "False"], ["-0.9265058040618896", "True"], ["-8.426506042480469", "False"], ["-8.926506042480469", "False"], ["-11.176506042480469", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5c2d3123d2ab8f9b4bdcc8c5e20bce749a1e2f78a412210935fcf3392be0f843", "prompt_hash": "006889fb0eb692b60559189d407b4b04032053b9e835bf5f0380f40dc2a1cac3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 897, "doc": {"id": "2632ff6c9b781d3aa74e8dd36b990871", "question": "She loved spending money at the thrift store on knickknacks, this resulted in a lot of what on every shelf in her house?", "question_concept": "spending money", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["poverty", "clutter", "getting", "satisfaction", "more happiness"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: She loved spending money at the thrift store on knickknacks, this resulted in a lot of what on every shelf in her house?\nA. poverty\nB. clutter\nC. getting\nD. satisfaction\nE. more happiness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: She loved spending money at the thrift store on knickknacks, this resulted in a lot of what on every shelf in her house?\nA. poverty\nB. clutter\nC. getting\nD. satisfaction\nE. more happiness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: She loved spending money at the thrift store on knickknacks, this resulted in a lot of what on every shelf in her house?\nA. poverty\nB. clutter\nC. getting\nD. satisfaction\nE. more happiness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: She loved spending money at the thrift store on knickknacks, this resulted in a lot of what on every shelf in her house?\nA. poverty\nB. clutter\nC. getting\nD. satisfaction\nE. more happiness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: She loved spending money at the thrift store on knickknacks, this resulted in a lot of what on every shelf in her house?\nA. poverty\nB. clutter\nC. getting\nD. satisfaction\nE. more happiness\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.40674352645874", "False"]], [["-0.656743586063385", "True"]], [["-5.90674352645874", "False"]], [["-7.90674352645874", "False"]], [["-9.656744003295898", "False"]]], "filtered_resps": [["-6.40674352645874", "False"], ["-0.656743586063385", "True"], ["-5.90674352645874", "False"], ["-7.90674352645874", "False"], ["-9.656744003295898", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7b964374535d79d8182b45cecab1ae1619cc2cf1ad90e7223000590bf076455a", "prompt_hash": "9acf445f75a86f7cfaf0e5ed66fef00a13626c4cadd79c6b823e6bc869b202b0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 898, "doc": {"id": "63db79b940f36f0333377f85c19eacb2", "question": "I listened to lecture intensely, what is my goal?", "question_concept": "listen", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["gain confidence", "concentrate", "get attention", "pay attention", "stop talking"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: I listened to lecture intensely, what is my goal?\nA. gain confidence\nB. concentrate\nC. get attention\nD. pay attention\nE. stop talking\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I listened to lecture intensely, what is my goal?\nA. gain confidence\nB. concentrate\nC. get attention\nD. pay attention\nE. stop talking\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I listened to lecture intensely, what is my goal?\nA. gain confidence\nB. concentrate\nC. get attention\nD. pay attention\nE. stop talking\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I listened to lecture intensely, what is my goal?\nA. gain confidence\nB. concentrate\nC. get attention\nD. pay attention\nE. stop talking\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I listened to lecture intensely, what is my goal?\nA. gain confidence\nB. concentrate\nC. get attention\nD. pay attention\nE. stop talking\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.398730754852295", "False"]], [["-2.898730754852295", "False"]], [["-6.648730754852295", "False"]], [["-1.3987306356430054", "True"]], [["-7.398730754852295", "False"]]], "filtered_resps": [["-5.398730754852295", "False"], ["-2.898730754852295", "False"], ["-6.648730754852295", "False"], ["-1.3987306356430054", "True"], ["-7.398730754852295", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2d91276e5175e33f70b77b2f1165e9a1b5bf2cf01a63a270577c5d50675a546b", "prompt_hash": "bbd1d43bc1230147c35088497ca93d5fda5b5ff86b2d0d38ed8846215a2960c6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 899, "doc": {"id": "1520a8fd3116e7b856947c5e308d7ce5", "question": "If a person is using a computer to talk to their granddaughter, what might the computer cause for them?", "question_concept": "using computer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["program created", "stress", "happiness", "ocean", "headache"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If a person is using a computer to talk to their granddaughter, what might the computer cause for them?\nA. program created\nB. stress\nC. happiness\nD. ocean\nE. headache\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a person is using a computer to talk to their granddaughter, what might the computer cause for them?\nA. program created\nB. stress\nC. happiness\nD. ocean\nE. headache\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a person is using a computer to talk to their granddaughter, what might the computer cause for them?\nA. program created\nB. stress\nC. happiness\nD. ocean\nE. headache\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a person is using a computer to talk to their granddaughter, what might the computer cause for them?\nA. program created\nB. stress\nC. happiness\nD. ocean\nE. headache\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a person is using a computer to talk to their granddaughter, what might the computer cause for them?\nA. program created\nB. stress\nC. happiness\nD. ocean\nE. headache\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6280264854431152", "False"]], [["-3.1280264854431152", "False"]], [["-1.8780264854431152", "False"]], [["-6.628026485443115", "False"]], [["-6.128026485443115", "False"]]], "filtered_resps": [["-3.6280264854431152", "False"], ["-3.1280264854431152", "False"], ["-1.8780264854431152", "False"], ["-6.628026485443115", "False"], ["-6.128026485443115", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4e4e7aaa9bf635223aac01d549b520e5e3e7b88b240c8c9563572699f5142bcd", "prompt_hash": "8da03b44389de8a1194e42cf7ef26481309d41963f3d5c302356c4284a1b19af", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 900, "doc": {"id": "bd780fea2d4dd262583446e64c0f314d", "question": "Joe was there to meet a large number of people.  As he filed though the entrance hall, he saw many strangers who came from far away.  What sort of building is he probably in?", "question_concept": "entrance hall", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["person", "box", "convention center", "public building", "large building"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Joe was there to meet a large number of people.  As he filed though the entrance hall, he saw many strangers who came from far away.  What sort of building is he probably in?\nA. person\nB. box\nC. convention center\nD. public building\nE. large building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe was there to meet a large number of people.  As he filed though the entrance hall, he saw many strangers who came from far away.  What sort of building is he probably in?\nA. person\nB. box\nC. convention center\nD. public building\nE. large building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe was there to meet a large number of people.  As he filed though the entrance hall, he saw many strangers who came from far away.  What sort of building is he probably in?\nA. person\nB. box\nC. convention center\nD. public building\nE. large building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe was there to meet a large number of people.  As he filed though the entrance hall, he saw many strangers who came from far away.  What sort of building is he probably in?\nA. person\nB. box\nC. convention center\nD. public building\nE. large building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe was there to meet a large number of people.  As he filed though the entrance hall, he saw many strangers who came from far away.  What sort of building is he probably in?\nA. person\nB. box\nC. convention center\nD. public building\nE. large building\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.606990337371826", "False"]], [["-7.856990337371826", "False"]], [["-2.106990337371826", "False"]], [["-8.106990814208984", "False"]], [["-9.606990814208984", "False"]]], "filtered_resps": [["-4.606990337371826", "False"], ["-7.856990337371826", "False"], ["-2.106990337371826", "False"], ["-8.106990814208984", "False"], ["-9.606990814208984", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2be55d4e210e078b17d6727053df420f6dd37d70b0687435decef39e7942178f", "prompt_hash": "53ab1ee40b663522124ce8faacc1f6b1fc9ae9c3f6fff71a56ebc3e71731b17d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 901, "doc": {"id": "99e0b2ddf88ebed98b977043b7c2331b", "question": "John wanted scatter his wife's remains in a lake in the wilderness.  He had to delay before of where he lived.  Where did he live?", "question_concept": "lake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mountains", "dead body", "pay debts", "state park", "new york"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: John wanted scatter his wife's remains in a lake in the wilderness.  He had to delay before of where he lived.  Where did he live?\nA. mountains\nB. dead body\nC. pay debts\nD. state park\nE. new york\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John wanted scatter his wife's remains in a lake in the wilderness.  He had to delay before of where he lived.  Where did he live?\nA. mountains\nB. dead body\nC. pay debts\nD. state park\nE. new york\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John wanted scatter his wife's remains in a lake in the wilderness.  He had to delay before of where he lived.  Where did he live?\nA. mountains\nB. dead body\nC. pay debts\nD. state park\nE. new york\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John wanted scatter his wife's remains in a lake in the wilderness.  He had to delay before of where he lived.  Where did he live?\nA. mountains\nB. dead body\nC. pay debts\nD. state park\nE. new york\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John wanted scatter his wife's remains in a lake in the wilderness.  He had to delay before of where he lived.  Where did he live?\nA. mountains\nB. dead body\nC. pay debts\nD. state park\nE. new york\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.254049301147461", "False"]], [["-5.754049301147461", "False"]], [["-6.254049301147461", "False"]], [["-2.504049301147461", "False"]], [["-3.254049301147461", "False"]]], "filtered_resps": [["-2.254049301147461", "False"], ["-5.754049301147461", "False"], ["-6.254049301147461", "False"], ["-2.504049301147461", "False"], ["-3.254049301147461", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "93a2dc1bc6727ae986fea39ad7823c00c21bd09569551fd565abf7ee150d5a5b", "prompt_hash": "2c4b813af42c37c7717d1c7a29a353dcb2845d67e883899dcf644a85d4eb5812", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 902, "doc": {"id": "eb0e0c4eaf19c1e9b4df3b4d3a11be3d", "question": "Many towns and cities have trash cans where on sidewalks?", "question_concept": "trash can", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hospital", "park", "corner", "motel", "office"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Many towns and cities have trash cans where on sidewalks?\nA. hospital\nB. park\nC. corner\nD. motel\nE. office\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Many towns and cities have trash cans where on sidewalks?\nA. hospital\nB. park\nC. corner\nD. motel\nE. office\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Many towns and cities have trash cans where on sidewalks?\nA. hospital\nB. park\nC. corner\nD. motel\nE. office\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Many towns and cities have trash cans where on sidewalks?\nA. hospital\nB. park\nC. corner\nD. motel\nE. office\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Many towns and cities have trash cans where on sidewalks?\nA. hospital\nB. park\nC. corner\nD. motel\nE. office\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.515945911407471", "False"]], [["-3.5159459114074707", "False"]], [["-1.5159460306167603", "False"]], [["-7.265945911407471", "False"]], [["-9.515946388244629", "False"]]], "filtered_resps": [["-5.515945911407471", "False"], ["-3.5159459114074707", "False"], ["-1.5159460306167603", "False"], ["-7.265945911407471", "False"], ["-9.515946388244629", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d9d33ec0512ebc9afb95a26835e31b19cb0cbfd0410f464f48b8f58597d8ba40", "prompt_hash": "a4039df4b87ef5cbddff7f0e8810bb4a3b9b3756fd919913fdff1afa3fc6ca87", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 903, "doc": {"id": "467a3b464b08b3ffc9922e2a726554f6", "question": "The family wanted to adopt for enviro-ethical reasons, what did they abhor?", "question_concept": "adopt", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["orphan", "biological child", "give away", "foster child", "abandon"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The family wanted to adopt for enviro-ethical reasons, what did they abhor?\nA. orphan\nB. biological child\nC. give away\nD. foster child\nE. abandon\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The family wanted to adopt for enviro-ethical reasons, what did they abhor?\nA. orphan\nB. biological child\nC. give away\nD. foster child\nE. abandon\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The family wanted to adopt for enviro-ethical reasons, what did they abhor?\nA. orphan\nB. biological child\nC. give away\nD. foster child\nE. abandon\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The family wanted to adopt for enviro-ethical reasons, what did they abhor?\nA. orphan\nB. biological child\nC. give away\nD. foster child\nE. abandon\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The family wanted to adopt for enviro-ethical reasons, what did they abhor?\nA. orphan\nB. biological child\nC. give away\nD. foster child\nE. abandon\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.24190616607666", "False"]], [["-2.74190616607666", "False"]], [["-5.99190616607666", "False"]], [["-3.49190616607666", "False"]], [["-1.9919062852859497", "False"]]], "filtered_resps": [["-2.24190616607666", "False"], ["-2.74190616607666", "False"], ["-5.99190616607666", "False"], ["-3.49190616607666", "False"], ["-1.9919062852859497", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fa496e5e628fefd4bb4117ea4fc89de2039666743147db03a629062a18b800c6", "prompt_hash": "a3426e70fefe16b78301705d6c5153e64e064df03853ee8524152cf4b03ef73c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 904, "doc": {"id": "dea70fe40fac9ad03bf319bf8a480efa", "question": "What happens when airplane engines cut off and are unable to be restarted in flight?", "question_concept": "airplanes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stall", "start melting", "taxi", "crash", "speed up"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What happens when airplane engines cut off and are unable to be restarted in flight?\nA. stall\nB. start melting\nC. taxi\nD. crash\nE. speed up\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens when airplane engines cut off and are unable to be restarted in flight?\nA. stall\nB. start melting\nC. taxi\nD. crash\nE. speed up\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens when airplane engines cut off and are unable to be restarted in flight?\nA. stall\nB. start melting\nC. taxi\nD. crash\nE. speed up\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens when airplane engines cut off and are unable to be restarted in flight?\nA. stall\nB. start melting\nC. taxi\nD. crash\nE. speed up\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens when airplane engines cut off and are unable to be restarted in flight?\nA. stall\nB. start melting\nC. taxi\nD. crash\nE. speed up\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.559419870376587", "True"]], [["-5.309419631958008", "False"]], [["-5.059419631958008", "False"]], [["-4.059419631958008", "False"]], [["-5.809419631958008", "False"]]], "filtered_resps": [["-1.559419870376587", "True"], ["-5.309419631958008", "False"], ["-5.059419631958008", "False"], ["-4.059419631958008", "False"], ["-5.809419631958008", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f56ea9dda67ac1975a7fe9dbf366c6d629bbe7ec41244ec63fe101903db2cff3", "prompt_hash": "705f87c42576afe48a8de8ed2a0753d36448de0700f2f85a37ff8029fb4a193a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 905, "doc": {"id": "2f1680da0d388a8453150ff3637e4689", "question": "Where would you be concerned about finding a cavity?", "question_concept": "cavity", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["solid object", "molar", "dentist", "unbrushed tooth", "teeth"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you be concerned about finding a cavity?\nA. solid object\nB. molar\nC. dentist\nD. unbrushed tooth\nE. teeth\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you be concerned about finding a cavity?\nA. solid object\nB. molar\nC. dentist\nD. unbrushed tooth\nE. teeth\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you be concerned about finding a cavity?\nA. solid object\nB. molar\nC. dentist\nD. unbrushed tooth\nE. teeth\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you be concerned about finding a cavity?\nA. solid object\nB. molar\nC. dentist\nD. unbrushed tooth\nE. teeth\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you be concerned about finding a cavity?\nA. solid object\nB. molar\nC. dentist\nD. unbrushed tooth\nE. teeth\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3359038829803467", "False"]], [["-2.0859038829803467", "False"]], [["-5.335904121398926", "False"]], [["-1.8359038829803467", "True"]], [["-2.3359038829803467", "False"]]], "filtered_resps": [["-2.3359038829803467", "False"], ["-2.0859038829803467", "False"], ["-5.335904121398926", "False"], ["-1.8359038829803467", "True"], ["-2.3359038829803467", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8c0db043c3ac48bf7535896aa9f37d5aac9737ec843f9a338fe0dd905c1ae2b7", "prompt_hash": "0ec87e90ed9887ff2123a45fae5705374bcce4596f9ecfd9dfe0e9d266d9d7c0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 906, "doc": {"id": "8369adc4b4710d00f917d80a75d844d7", "question": "Human beings learn about current events from what print item?", "question_concept": "human beings", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["question authority", "melt", "read newspapers", "act", "dictionary"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Human beings learn about current events from what print item?\nA. question authority\nB. melt\nC. read newspapers\nD. act\nE. dictionary\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Human beings learn about current events from what print item?\nA. question authority\nB. melt\nC. read newspapers\nD. act\nE. dictionary\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Human beings learn about current events from what print item?\nA. question authority\nB. melt\nC. read newspapers\nD. act\nE. dictionary\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Human beings learn about current events from what print item?\nA. question authority\nB. melt\nC. read newspapers\nD. act\nE. dictionary\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Human beings learn about current events from what print item?\nA. question authority\nB. melt\nC. read newspapers\nD. act\nE. dictionary\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.2840495109558105", "False"]], [["-7.5340495109558105", "False"]], [["-1.784049391746521", "False"]], [["-8.534049034118652", "False"]], [["-8.034049034118652", "False"]]], "filtered_resps": [["-6.2840495109558105", "False"], ["-7.5340495109558105", "False"], ["-1.784049391746521", "False"], ["-8.534049034118652", "False"], ["-8.034049034118652", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "37a366c8840699707ecb55df99b1add3f618e6bc3ea50c018e9e53c3664ed578", "prompt_hash": "a830767aa178cf9c15e74e4ade0c16659fe646d81441bdcc88b5d3b0ad6614b6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 907, "doc": {"id": "20a3bb788cf408d9a3e25e610fe60905", "question": "In what kind of environment does an anemone live?", "question_concept": "anemone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["nursery", "south pacific", "desert", "sea water", "atlantic ocean"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: In what kind of environment does an anemone live?\nA. nursery\nB. south pacific\nC. desert\nD. sea water\nE. atlantic ocean\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: In what kind of environment does an anemone live?\nA. nursery\nB. south pacific\nC. desert\nD. sea water\nE. atlantic ocean\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: In what kind of environment does an anemone live?\nA. nursery\nB. south pacific\nC. desert\nD. sea water\nE. atlantic ocean\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: In what kind of environment does an anemone live?\nA. nursery\nB. south pacific\nC. desert\nD. sea water\nE. atlantic ocean\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: In what kind of environment does an anemone live?\nA. nursery\nB. south pacific\nC. desert\nD. sea water\nE. atlantic ocean\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.767415523529053", "False"]], [["-5.517415523529053", "False"]], [["-7.017415523529053", "False"]], [["-1.2674154043197632", "True"]], [["-9.767415046691895", "False"]]], "filtered_resps": [["-4.767415523529053", "False"], ["-5.517415523529053", "False"], ["-7.017415523529053", "False"], ["-1.2674154043197632", "True"], ["-9.767415046691895", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b00ccdb623e380e26fdd47758fef0f89627f7b2c04eabc86a71444a98dcd757d", "prompt_hash": "e94561f7d193c48baa6c7a2c608c7ec3bb75a0efc78e6998405247ab1f3665cc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 908, "doc": {"id": "36c1f50eec01c287b8ef6ffe69fe0528", "question": "He wanted lodging in the actual what, so that he was already where he needed to be?", "question_concept": "lodgings", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["a yurt", "resort area", "big city", "michigan", "going on vacation"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: He wanted lodging in the actual what, so that he was already where he needed to be?\nA. a yurt\nB. resort area\nC. big city\nD. michigan\nE. going on vacation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He wanted lodging in the actual what, so that he was already where he needed to be?\nA. a yurt\nB. resort area\nC. big city\nD. michigan\nE. going on vacation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He wanted lodging in the actual what, so that he was already where he needed to be?\nA. a yurt\nB. resort area\nC. big city\nD. michigan\nE. going on vacation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He wanted lodging in the actual what, so that he was already where he needed to be?\nA. a yurt\nB. resort area\nC. big city\nD. michigan\nE. going on vacation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He wanted lodging in the actual what, so that he was already where he needed to be?\nA. a yurt\nB. resort area\nC. big city\nD. michigan\nE. going on vacation\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5571318864822388", "False"]], [["-3.057131767272949", "False"]], [["-3.557131767272949", "False"]], [["-4.807131767272949", "False"]], [["-4.807131767272949", "False"]]], "filtered_resps": [["-1.5571318864822388", "False"], ["-3.057131767272949", "False"], ["-3.557131767272949", "False"], ["-4.807131767272949", "False"], ["-4.807131767272949", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a66f594a84bf22ce03031b5d8a5031ff1982ade22270e6c44fd674e315796b51", "prompt_hash": "d47ae8bf5512a34079fe37d8d69137edef3baadc715a0b95cd8fce296dc7f494", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 909, "doc": {"id": "5f4825137a27f369fe859e85dfe1793f", "question": "If I am suffering from boredom, and I want to see something beautiful, what should I do?", "question_concept": "boredom", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["see art", "see ghost", "watch film", "grocery shop", "do crossword puzzle"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If I am suffering from boredom, and I want to see something beautiful, what should I do?\nA. see art\nB. see ghost\nC. watch film\nD. grocery shop\nE. do crossword puzzle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I am suffering from boredom, and I want to see something beautiful, what should I do?\nA. see art\nB. see ghost\nC. watch film\nD. grocery shop\nE. do crossword puzzle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I am suffering from boredom, and I want to see something beautiful, what should I do?\nA. see art\nB. see ghost\nC. watch film\nD. grocery shop\nE. do crossword puzzle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I am suffering from boredom, and I want to see something beautiful, what should I do?\nA. see art\nB. see ghost\nC. watch film\nD. grocery shop\nE. do crossword puzzle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I am suffering from boredom, and I want to see something beautiful, what should I do?\nA. see art\nB. see ghost\nC. watch film\nD. grocery shop\nE. do crossword puzzle\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9483501315116882", "True"]], [["-5.448349952697754", "False"]], [["-4.698349952697754", "False"]], [["-8.198349952697754", "False"]], [["-9.198349952697754", "False"]]], "filtered_resps": [["-0.9483501315116882", "True"], ["-5.448349952697754", "False"], ["-4.698349952697754", "False"], ["-8.198349952697754", "False"], ["-9.198349952697754", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2c9ea4c07dea6c20069ae07528b28851eb4fa74561ada2e5845ce76f4f123946", "prompt_hash": "4da856a8f9ccc3c6d43fdfa1c60060e4a5dad4eb539c94253ed1c838633eaf24", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 910, "doc": {"id": "b3dc6d6a5e2f9d7da8eb72816c80b3f8_1", "question": "The goal was to hit the target, but a projectile ball can't hit anything if it isn't in what?", "question_concept": "projectile ball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["motion", "ocean", "flintlock", "arcade", "tennis court"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The goal was to hit the target, but a projectile ball can't hit anything if it isn't in what?\nA. motion\nB. ocean\nC. flintlock\nD. arcade\nE. tennis court\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The goal was to hit the target, but a projectile ball can't hit anything if it isn't in what?\nA. motion\nB. ocean\nC. flintlock\nD. arcade\nE. tennis court\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The goal was to hit the target, but a projectile ball can't hit anything if it isn't in what?\nA. motion\nB. ocean\nC. flintlock\nD. arcade\nE. tennis court\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The goal was to hit the target, but a projectile ball can't hit anything if it isn't in what?\nA. motion\nB. ocean\nC. flintlock\nD. arcade\nE. tennis court\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The goal was to hit the target, but a projectile ball can't hit anything if it isn't in what?\nA. motion\nB. ocean\nC. flintlock\nD. arcade\nE. tennis court\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0126677751541138", "True"]], [["-5.512667655944824", "False"]], [["-5.512667655944824", "False"]], [["-7.512667655944824", "False"]], [["-4.012667655944824", "False"]]], "filtered_resps": [["-1.0126677751541138", "True"], ["-5.512667655944824", "False"], ["-5.512667655944824", "False"], ["-7.512667655944824", "False"], ["-4.012667655944824", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "346cf882e6bb99731fce9e4331b5b84cab96055658269498bfb14f64ce7490c0", "prompt_hash": "adb37911909144fd61e5604efbc126233cc774e7aa35b953791bfd33f9259d5e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 911, "doc": {"id": "63bb6128026ce24209583d0eea75fc27", "question": "Where is a good place to set a cup of coffee while relaxing?", "question_concept": "cup of coffee", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["coffee shop", "kitchen", "hand", "table", "office"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a good place to set a cup of coffee while relaxing?\nA. coffee shop\nB. kitchen\nC. hand\nD. table\nE. office\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a good place to set a cup of coffee while relaxing?\nA. coffee shop\nB. kitchen\nC. hand\nD. table\nE. office\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a good place to set a cup of coffee while relaxing?\nA. coffee shop\nB. kitchen\nC. hand\nD. table\nE. office\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a good place to set a cup of coffee while relaxing?\nA. coffee shop\nB. kitchen\nC. hand\nD. table\nE. office\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a good place to set a cup of coffee while relaxing?\nA. coffee shop\nB. kitchen\nC. hand\nD. table\nE. office\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.644314765930176", "False"]], [["-4.144314765930176", "False"]], [["-5.894314765930176", "False"]], [["-1.3943146467208862", "False"]], [["-9.144314765930176", "False"]]], "filtered_resps": [["-2.644314765930176", "False"], ["-4.144314765930176", "False"], ["-5.894314765930176", "False"], ["-1.3943146467208862", "False"], ["-9.144314765930176", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f73f48b1631fd82c1ce602064f46ca308b47090200c322a91cd187f4761106b3", "prompt_hash": "0f721c7c15d5d5ad0971cd74c578504dc0d3e3ceae77f95f9727f13329acf5f6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 912, "doc": {"id": "e8a9142d2402f818273dd62cf5a7b559_1", "question": "If a fried egg was runny and there was no toast to sop it up, after the meal there'd be a messy what?", "question_concept": "egg", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["henhouse", "garden", "plate", "supermarket", "bird's nest"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If a fried egg was runny and there was no toast to sop it up, after the meal there'd be a messy what?\nA. henhouse\nB. garden\nC. plate\nD. supermarket\nE. bird's nest\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a fried egg was runny and there was no toast to sop it up, after the meal there'd be a messy what?\nA. henhouse\nB. garden\nC. plate\nD. supermarket\nE. bird's nest\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a fried egg was runny and there was no toast to sop it up, after the meal there'd be a messy what?\nA. henhouse\nB. garden\nC. plate\nD. supermarket\nE. bird's nest\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a fried egg was runny and there was no toast to sop it up, after the meal there'd be a messy what?\nA. henhouse\nB. garden\nC. plate\nD. supermarket\nE. bird's nest\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a fried egg was runny and there was no toast to sop it up, after the meal there'd be a messy what?\nA. henhouse\nB. garden\nC. plate\nD. supermarket\nE. bird's nest\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3277173042297363", "False"]], [["-7.327717304229736", "False"]], [["-1.3277174234390259", "True"]], [["-8.577717781066895", "False"]], [["-3.8277173042297363", "False"]]], "filtered_resps": [["-2.3277173042297363", "False"], ["-7.327717304229736", "False"], ["-1.3277174234390259", "True"], ["-8.577717781066895", "False"], ["-3.8277173042297363", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fac58aa93ac95c24368f20fdef2bdbcf7b5dc81d1d9a648f95deaa5458972936", "prompt_hash": "d2f428b1daa22b179422249429add6ccdae791ea4d3bdac7893bf63775d17c71", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 913, "doc": {"id": "ead9c9744aee08678759158efe005175", "question": "If I want to behave with proper aplomb, what manners should I avoid?", "question_concept": "proper", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["inappropriate", "incomplete", "impolite", "none", "incorrect"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If I want to behave with proper aplomb, what manners should I avoid?\nA. inappropriate\nB. incomplete\nC. impolite\nD. none\nE. incorrect\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I want to behave with proper aplomb, what manners should I avoid?\nA. inappropriate\nB. incomplete\nC. impolite\nD. none\nE. incorrect\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I want to behave with proper aplomb, what manners should I avoid?\nA. inappropriate\nB. incomplete\nC. impolite\nD. none\nE. incorrect\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I want to behave with proper aplomb, what manners should I avoid?\nA. inappropriate\nB. incomplete\nC. impolite\nD. none\nE. incorrect\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I want to behave with proper aplomb, what manners should I avoid?\nA. inappropriate\nB. incomplete\nC. impolite\nD. none\nE. incorrect\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.003920555114746", "False"]], [["-6.503920555114746", "False"]], [["-2.753920555114746", "False"]], [["-5.753920555114746", "False"]], [["-6.253920555114746", "False"]]], "filtered_resps": [["-3.003920555114746", "False"], ["-6.503920555114746", "False"], ["-2.753920555114746", "False"], ["-5.753920555114746", "False"], ["-6.253920555114746", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9ccc4378b400c5551825e607d67afeccba4ab62fef20385cb4cb5778e2c5c3f6", "prompt_hash": "1e4b0221f2a16b8c972c90f0b5ba9066e1df636f80ac6be9502678d87d3d757a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 914, "doc": {"id": "ab8bf60f76bc6119459271140ccae781", "question": "Before lifting weights he liked to warm up on the squash court, he really enjoyed the facilities of the what?", "question_concept": "squash court", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["swimming pool", "rich person's house", "country club", "fitness center", "park"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Before lifting weights he liked to warm up on the squash court, he really enjoyed the facilities of the what?\nA. swimming pool\nB. rich person's house\nC. country club\nD. fitness center\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Before lifting weights he liked to warm up on the squash court, he really enjoyed the facilities of the what?\nA. swimming pool\nB. rich person's house\nC. country club\nD. fitness center\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Before lifting weights he liked to warm up on the squash court, he really enjoyed the facilities of the what?\nA. swimming pool\nB. rich person's house\nC. country club\nD. fitness center\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Before lifting weights he liked to warm up on the squash court, he really enjoyed the facilities of the what?\nA. swimming pool\nB. rich person's house\nC. country club\nD. fitness center\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Before lifting weights he liked to warm up on the squash court, he really enjoyed the facilities of the what?\nA. swimming pool\nB. rich person's house\nC. country club\nD. fitness center\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.152540683746338", "False"]], [["-5.902540683746338", "False"]], [["-2.902540683746338", "False"]], [["-1.652540683746338", "False"]], [["-9.65254020690918", "False"]]], "filtered_resps": [["-4.152540683746338", "False"], ["-5.902540683746338", "False"], ["-2.902540683746338", "False"], ["-1.652540683746338", "False"], ["-9.65254020690918", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d0a2ddcb68b86a2559343507009f1e6075236d74868a3b291999f9e24ecc595a", "prompt_hash": "0eda81080b35a645cf6ff92d93fc628a9ed75e5b7714b33eb92337f88d0f64bd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 915, "doc": {"id": "3c6e2d95a63316b31986e8c7979582c9", "question": "What will happen to animals after eating food?", "question_concept": "animals", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bite", "digestion", "feel pleasure", "pass water", "listen to each other"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What will happen to animals after eating food?\nA. bite\nB. digestion\nC. feel pleasure\nD. pass water\nE. listen to each other\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What will happen to animals after eating food?\nA. bite\nB. digestion\nC. feel pleasure\nD. pass water\nE. listen to each other\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What will happen to animals after eating food?\nA. bite\nB. digestion\nC. feel pleasure\nD. pass water\nE. listen to each other\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What will happen to animals after eating food?\nA. bite\nB. digestion\nC. feel pleasure\nD. pass water\nE. listen to each other\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What will happen to animals after eating food?\nA. bite\nB. digestion\nC. feel pleasure\nD. pass water\nE. listen to each other\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.133298397064209", "False"]], [["-0.8832985162734985", "True"]], [["-7.633298397064209", "False"]], [["-6.383298397064209", "False"]], [["-6.883298397064209", "False"]]], "filtered_resps": [["-6.133298397064209", "False"], ["-0.8832985162734985", "True"], ["-7.633298397064209", "False"], ["-6.383298397064209", "False"], ["-6.883298397064209", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0db96281a5c5feb7a88b72c3ff072a05a5698cbb11d8b929f059af53abb5b548", "prompt_hash": "e0a33f5350529242c071baa8043369446fc4494804346eb3a74545145029949a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 916, "doc": {"id": "5c171b9837af49211891ce40e4a10204", "question": "If I wanted to grow plants, where could I put a lot of dirt?", "question_concept": "dirt", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["corner", "street", "closet", "garden", "bathtub"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If I wanted to grow plants, where could I put a lot of dirt?\nA. corner\nB. street\nC. closet\nD. garden\nE. bathtub\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I wanted to grow plants, where could I put a lot of dirt?\nA. corner\nB. street\nC. closet\nD. garden\nE. bathtub\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I wanted to grow plants, where could I put a lot of dirt?\nA. corner\nB. street\nC. closet\nD. garden\nE. bathtub\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I wanted to grow plants, where could I put a lot of dirt?\nA. corner\nB. street\nC. closet\nD. garden\nE. bathtub\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I wanted to grow plants, where could I put a lot of dirt?\nA. corner\nB. street\nC. closet\nD. garden\nE. bathtub\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.076810598373413", "False"]], [["-7.326810836791992", "False"]], [["-7.576810836791992", "False"]], [["-1.576810598373413", "False"]], [["-9.576810836791992", "False"]]], "filtered_resps": [["-3.076810598373413", "False"], ["-7.326810836791992", "False"], ["-7.576810836791992", "False"], ["-1.576810598373413", "False"], ["-9.576810836791992", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "69f5b70412f6bc2b8303d3d8ce6b471b010a7020b199fcc348cb2af40ec0dca1", "prompt_hash": "84b99c3f2fa3f02d07139fe138dd054d0838bfd5cfce4fad8245ec21c703626c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 917, "doc": {"id": "56d0fc282a144565f2c852415c6fa92c", "question": "What does a person often feel about someone judging them guilty?", "question_concept": "judging", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["controversy", "responsibility", "resentment", "judge feelings", "hurt feelings"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What does a person often feel about someone judging them guilty?\nA. controversy\nB. responsibility\nC. resentment\nD. judge feelings\nE. hurt feelings\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a person often feel about someone judging them guilty?\nA. controversy\nB. responsibility\nC. resentment\nD. judge feelings\nE. hurt feelings\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a person often feel about someone judging them guilty?\nA. controversy\nB. responsibility\nC. resentment\nD. judge feelings\nE. hurt feelings\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a person often feel about someone judging them guilty?\nA. controversy\nB. responsibility\nC. resentment\nD. judge feelings\nE. hurt feelings\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a person often feel about someone judging them guilty?\nA. controversy\nB. responsibility\nC. resentment\nD. judge feelings\nE. hurt feelings\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8199501037597656", "False"]], [["-6.069950103759766", "False"]], [["-2.5699501037597656", "False"]], [["-6.819950103759766", "False"]], [["-1.819949984550476", "True"]]], "filtered_resps": [["-2.8199501037597656", "False"], ["-6.069950103759766", "False"], ["-2.5699501037597656", "False"], ["-6.819950103759766", "False"], ["-1.819949984550476", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "47921a1a115afa039476c0c06a162454fc356327d7aeee69aa4c8824a9df3ec1", "prompt_hash": "4ad9e4621edac21e4e2fb2a80e63c7642f358b6b130776212e855a1808f9f448", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 918, "doc": {"id": "5b8a3081c3235d62bc77e2d15f3ad454", "question": "A town between two mountains is located in a what?", "question_concept": "town", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["valley", "hospital", "state", "train station", "michigan"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A town between two mountains is located in a what?\nA. valley\nB. hospital\nC. state\nD. train station\nE. michigan\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A town between two mountains is located in a what?\nA. valley\nB. hospital\nC. state\nD. train station\nE. michigan\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A town between two mountains is located in a what?\nA. valley\nB. hospital\nC. state\nD. train station\nE. michigan\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A town between two mountains is located in a what?\nA. valley\nB. hospital\nC. state\nD. train station\nE. michigan\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A town between two mountains is located in a what?\nA. valley\nB. hospital\nC. state\nD. train station\nE. michigan\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.4028172194957733", "True"]], [["-5.652817249298096", "False"]], [["-6.652817249298096", "False"]], [["-6.902817249298096", "False"]], [["-6.652817249298096", "False"]]], "filtered_resps": [["-0.4028172194957733", "True"], ["-5.652817249298096", "False"], ["-6.652817249298096", "False"], ["-6.902817249298096", "False"], ["-6.652817249298096", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "54304e1f59f988b27a8e4ecd5c6c121523d1eb3ea5dd3d86ff93edcde34b7157", "prompt_hash": "3fb31b32731ad456087cd922118557876448946d6c915188335b261dd04fe9db", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 919, "doc": {"id": "e43c4eaa04243ddee30f29171718eb92", "question": "James need to use a toilet but there were no public ones in sight.  Eventually he broke down and did something very expensive so that he could get a toilet.  Where might he have gone?", "question_concept": "toilet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["motel room", "apartment", "bathroom", "games", "house"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James need to use a toilet but there were no public ones in sight.  Eventually he broke down and did something very expensive so that he could get a toilet.  Where might he have gone?\nA. motel room\nB. apartment\nC. bathroom\nD. games\nE. house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James need to use a toilet but there were no public ones in sight.  Eventually he broke down and did something very expensive so that he could get a toilet.  Where might he have gone?\nA. motel room\nB. apartment\nC. bathroom\nD. games\nE. house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James need to use a toilet but there were no public ones in sight.  Eventually he broke down and did something very expensive so that he could get a toilet.  Where might he have gone?\nA. motel room\nB. apartment\nC. bathroom\nD. games\nE. house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James need to use a toilet but there were no public ones in sight.  Eventually he broke down and did something very expensive so that he could get a toilet.  Where might he have gone?\nA. motel room\nB. apartment\nC. bathroom\nD. games\nE. house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James need to use a toilet but there were no public ones in sight.  Eventually he broke down and did something very expensive so that he could get a toilet.  Where might he have gone?\nA. motel room\nB. apartment\nC. bathroom\nD. games\nE. house\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8882224559783936", "False"]], [["-6.388222694396973", "False"]], [["-6.638222694396973", "False"]], [["-9.138222694396973", "False"]], [["-7.138222694396973", "False"]]], "filtered_resps": [["-1.8882224559783936", "False"], ["-6.388222694396973", "False"], ["-6.638222694396973", "False"], ["-9.138222694396973", "False"], ["-7.138222694396973", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ab4a5c6335be8e518f961f89da023a9b4095888fd211c57a4962f19849c4dde0", "prompt_hash": "db0c1f71da313e990e02afe91f5c7cb0a6fe6fb243ed12698f295d9fb0d1ab37", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 920, "doc": {"id": "84a736d4b702a6869d8fa8523aee6f1b", "question": "Why did the heavy metal band need electricity at the stadium?", "question_concept": "electricity", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["concert", "bedroom", "make person sick", "building", "church"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Why did the heavy metal band need electricity at the stadium?\nA. concert\nB. bedroom\nC. make person sick\nD. building\nE. church\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why did the heavy metal band need electricity at the stadium?\nA. concert\nB. bedroom\nC. make person sick\nD. building\nE. church\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why did the heavy metal band need electricity at the stadium?\nA. concert\nB. bedroom\nC. make person sick\nD. building\nE. church\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why did the heavy metal band need electricity at the stadium?\nA. concert\nB. bedroom\nC. make person sick\nD. building\nE. church\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why did the heavy metal band need electricity at the stadium?\nA. concert\nB. bedroom\nC. make person sick\nD. building\nE. church\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8806664943695068", "True"]], [["-7.380666732788086", "False"]], [["-8.630666732788086", "False"]], [["-4.880666732788086", "False"]], [["-8.380666732788086", "False"]]], "filtered_resps": [["-0.8806664943695068", "True"], ["-7.380666732788086", "False"], ["-8.630666732788086", "False"], ["-4.880666732788086", "False"], ["-8.380666732788086", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "efc50bff852387fa02bd0791a22cec4a728130edfbbac51d61a0fb046c53beda", "prompt_hash": "aee20f3033b60f496e964ef216c401301c17c425ea88cc8e4642063a43ca6192", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 921, "doc": {"id": "72611791cdcb040f2d699827fb9cebc4", "question": "What is a person looking for when completing puzzles or riddles?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["avoid pain", "compliments", "intellectual challenge", "passing grade", "attention"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is a person looking for when completing puzzles or riddles?\nA. avoid pain\nB. compliments\nC. intellectual challenge\nD. passing grade\nE. attention\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a person looking for when completing puzzles or riddles?\nA. avoid pain\nB. compliments\nC. intellectual challenge\nD. passing grade\nE. attention\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a person looking for when completing puzzles or riddles?\nA. avoid pain\nB. compliments\nC. intellectual challenge\nD. passing grade\nE. attention\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a person looking for when completing puzzles or riddles?\nA. avoid pain\nB. compliments\nC. intellectual challenge\nD. passing grade\nE. attention\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a person looking for when completing puzzles or riddles?\nA. avoid pain\nB. compliments\nC. intellectual challenge\nD. passing grade\nE. attention\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.661466121673584", "False"]], [["-7.911466121673584", "False"]], [["-1.411466121673584", "True"]], [["-7.161466121673584", "False"]], [["-8.911466598510742", "False"]]], "filtered_resps": [["-2.661466121673584", "False"], ["-7.911466121673584", "False"], ["-1.411466121673584", "True"], ["-7.161466121673584", "False"], ["-8.911466598510742", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4c404744525bdcca86ef7f8049dab883063e26bd3ff84f8576d58df0c1fdf989", "prompt_hash": "fb0a764420f7f8dd57f7d32985d61d3de441ed71ae0949ffce9769a5a1d076fb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 922, "doc": {"id": "4477fb61fde4bb8695c241dfc366b554", "question": "If someone was making breakfast, they'd probably put two slices of bread in the what?", "question_concept": "bread", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["plastic bag", "pantry", "supermarket", "toaster", "prison"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If someone was making breakfast, they'd probably put two slices of bread in the what?\nA. plastic bag\nB. pantry\nC. supermarket\nD. toaster\nE. prison\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If someone was making breakfast, they'd probably put two slices of bread in the what?\nA. plastic bag\nB. pantry\nC. supermarket\nD. toaster\nE. prison\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If someone was making breakfast, they'd probably put two slices of bread in the what?\nA. plastic bag\nB. pantry\nC. supermarket\nD. toaster\nE. prison\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If someone was making breakfast, they'd probably put two slices of bread in the what?\nA. plastic bag\nB. pantry\nC. supermarket\nD. toaster\nE. prison\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If someone was making breakfast, they'd probably put two slices of bread in the what?\nA. plastic bag\nB. pantry\nC. supermarket\nD. toaster\nE. prison\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.99084997177124", "False"]], [["-6.99084997177124", "False"]], [["-10.240850448608398", "False"]], [["-1.2408499717712402", "False"]], [["-9.990850448608398", "False"]]], "filtered_resps": [["-4.99084997177124", "False"], ["-6.99084997177124", "False"], ["-10.240850448608398", "False"], ["-1.2408499717712402", "False"], ["-9.990850448608398", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "379a9f4cf5d69544f8dac185789362b7903e93b00e97d0effedeb797d16c52e3", "prompt_hash": "bbd827b1cd4b2f867b9aeb866a5384408b0660945142a74fade97edf09c9e569", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 923, "doc": {"id": "ce246bc94a54431b9c0530e71d2456b5", "question": "His house was a mess, he began doing housework to get what?", "question_concept": "doing housework", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boredom", "nice home", "michigan", "feeling satisfied", "house clean"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: His house was a mess, he began doing housework to get what?\nA. boredom\nB. nice home\nC. michigan\nD. feeling satisfied\nE. house clean\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: His house was a mess, he began doing housework to get what?\nA. boredom\nB. nice home\nC. michigan\nD. feeling satisfied\nE. house clean\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: His house was a mess, he began doing housework to get what?\nA. boredom\nB. nice home\nC. michigan\nD. feeling satisfied\nE. house clean\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: His house was a mess, he began doing housework to get what?\nA. boredom\nB. nice home\nC. michigan\nD. feeling satisfied\nE. house clean\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: His house was a mess, he began doing housework to get what?\nA. boredom\nB. nice home\nC. michigan\nD. feeling satisfied\nE. house clean\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.756505489349365", "False"]], [["-2.0065054893493652", "False"]], [["-6.506505489349365", "False"]], [["-2.2565054893493652", "False"]], [["-6.506505489349365", "False"]]], "filtered_resps": [["-4.756505489349365", "False"], ["-2.0065054893493652", "False"], ["-6.506505489349365", "False"], ["-2.2565054893493652", "False"], ["-6.506505489349365", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4c3ea4ea78e95de42674892a6f89b1bbb7f040fe7d2155fc099c223e219d6ea4", "prompt_hash": "607b0f3cbf8ab05ad55f544e30024c7f62773325ec466de23a59bba3310571e6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 924, "doc": {"id": "2eef2d255fe629414f4d24ade8590102", "question": "Where would a corpse be covered by a blanket?", "question_concept": "blanket", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bath store", "bedroom", "hospital", "flower garden", "michigan"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where would a corpse be covered by a blanket?\nA. bath store\nB. bedroom\nC. hospital\nD. flower garden\nE. michigan\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would a corpse be covered by a blanket?\nA. bath store\nB. bedroom\nC. hospital\nD. flower garden\nE. michigan\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would a corpse be covered by a blanket?\nA. bath store\nB. bedroom\nC. hospital\nD. flower garden\nE. michigan\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would a corpse be covered by a blanket?\nA. bath store\nB. bedroom\nC. hospital\nD. flower garden\nE. michigan\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would a corpse be covered by a blanket?\nA. bath store\nB. bedroom\nC. hospital\nD. flower garden\nE. michigan\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3423219919204712", "True"]], [["-1.5923219919204712", "False"]], [["-2.8423218727111816", "False"]], [["-7.592321872711182", "False"]], [["-9.84232234954834", "False"]]], "filtered_resps": [["-1.3423219919204712", "True"], ["-1.5923219919204712", "False"], ["-2.8423218727111816", "False"], ["-7.592321872711182", "False"], ["-9.84232234954834", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d546f75c8cfb26534318f62dd55144876c012119a4dda11c09c1f6e7649a588b", "prompt_hash": "b23db0011c87a09b0e10d272bf990fb3a7da3c6ce30b18bfd711808b1ce3a900", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 925, "doc": {"id": "2f85d53721ccc8b3fa4cfc184186d124", "question": "The man  tried to break the glass in order to make his escape in time, but he could not.  The person in the cat, trying to kill him, did what?", "question_concept": "break", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["accelerate", "putting together", "working", "construct", "train"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The man  tried to break the glass in order to make his escape in time, but he could not.  The person in the cat, trying to kill him, did what?\nA. accelerate\nB. putting together\nC. working\nD. construct\nE. train\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man  tried to break the glass in order to make his escape in time, but he could not.  The person in the cat, trying to kill him, did what?\nA. accelerate\nB. putting together\nC. working\nD. construct\nE. train\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man  tried to break the glass in order to make his escape in time, but he could not.  The person in the cat, trying to kill him, did what?\nA. accelerate\nB. putting together\nC. working\nD. construct\nE. train\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man  tried to break the glass in order to make his escape in time, but he could not.  The person in the cat, trying to kill him, did what?\nA. accelerate\nB. putting together\nC. working\nD. construct\nE. train\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man  tried to break the glass in order to make his escape in time, but he could not.  The person in the cat, trying to kill him, did what?\nA. accelerate\nB. putting together\nC. working\nD. construct\nE. train\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.672806739807129", "True"]], [["-3.172806739807129", "False"]], [["-3.922806739807129", "False"]], [["-4.922806739807129", "False"]], [["-4.422806739807129", "False"]]], "filtered_resps": [["-1.672806739807129", "True"], ["-3.172806739807129", "False"], ["-3.922806739807129", "False"], ["-4.922806739807129", "False"], ["-4.422806739807129", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "00e5d123609b642c54bc673a27ebabc5f76b38ffb39106f9cfb2e520937edd2f", "prompt_hash": "49fb84076229577f8247c3d66d447599453aa0cb65d0b27a90806c74336f7057", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 926, "doc": {"id": "2192c5c2145a6e03755ad89a02e64055", "question": "The trucker plopped on the bench with a sense of relief, where did he arrive?", "question_concept": "bench", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bordello", "rest area", "garden", "bus stop", "state park"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The trucker plopped on the bench with a sense of relief, where did he arrive?\nA. bordello\nB. rest area\nC. garden\nD. bus stop\nE. state park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The trucker plopped on the bench with a sense of relief, where did he arrive?\nA. bordello\nB. rest area\nC. garden\nD. bus stop\nE. state park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The trucker plopped on the bench with a sense of relief, where did he arrive?\nA. bordello\nB. rest area\nC. garden\nD. bus stop\nE. state park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The trucker plopped on the bench with a sense of relief, where did he arrive?\nA. bordello\nB. rest area\nC. garden\nD. bus stop\nE. state park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The trucker plopped on the bench with a sense of relief, where did he arrive?\nA. bordello\nB. rest area\nC. garden\nD. bus stop\nE. state park\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1399664878845215", "False"]], [["-1.139966607093811", "True"]], [["-8.88996696472168", "False"]], [["-8.38996696472168", "False"]], [["-10.63996696472168", "False"]]], "filtered_resps": [["-3.1399664878845215", "False"], ["-1.139966607093811", "True"], ["-8.88996696472168", "False"], ["-8.38996696472168", "False"], ["-10.63996696472168", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3d35d187044aef3c12ae65ad28957ee43c90cbffc66080bab236ae65468d8d2c", "prompt_hash": "64dff8063653f5d3d94b3d18d0bce9d0f1e2ab8d2f156189976d2c3482e8a43d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 927, "doc": {"id": "bea07406aaadeef50110883b6932d86a", "question": "What is part of a republic like the USA?", "question_concept": "republic", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["state", "democratic", "kingdom", "democracy", "dictatorship"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is part of a republic like the USA?\nA. state\nB. democratic\nC. kingdom\nD. democracy\nE. dictatorship\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is part of a republic like the USA?\nA. state\nB. democratic\nC. kingdom\nD. democracy\nE. dictatorship\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is part of a republic like the USA?\nA. state\nB. democratic\nC. kingdom\nD. democracy\nE. dictatorship\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is part of a republic like the USA?\nA. state\nB. democratic\nC. kingdom\nD. democracy\nE. dictatorship\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is part of a republic like the USA?\nA. state\nB. democratic\nC. kingdom\nD. democracy\nE. dictatorship\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9423161149024963", "True"]], [["-3.6923160552978516", "False"]], [["-7.942316055297852", "False"]], [["-3.9423160552978516", "False"]], [["-9.942316055297852", "False"]]], "filtered_resps": [["-0.9423161149024963", "True"], ["-3.6923160552978516", "False"], ["-7.942316055297852", "False"], ["-3.9423160552978516", "False"], ["-9.942316055297852", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c0ccaf0b0a4d8cba759680c8d8a9fc17b1f5eeb45b45fe62127d2b5087436663", "prompt_hash": "2425178cfd432ccb7f1f767d48430a8d8fc0bbbb324385bf6504cd36d7bf0cdc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 928, "doc": {"id": "7a58e7e7bf76658751e850f790922aba", "question": "Where do you keep extra clothing on a hike?", "question_concept": "clothing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["person", "hamper", "closet", "upstairs", "backpack"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you keep extra clothing on a hike?\nA. person\nB. hamper\nC. closet\nD. upstairs\nE. backpack\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you keep extra clothing on a hike?\nA. person\nB. hamper\nC. closet\nD. upstairs\nE. backpack\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you keep extra clothing on a hike?\nA. person\nB. hamper\nC. closet\nD. upstairs\nE. backpack\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you keep extra clothing on a hike?\nA. person\nB. hamper\nC. closet\nD. upstairs\nE. backpack\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you keep extra clothing on a hike?\nA. person\nB. hamper\nC. closet\nD. upstairs\nE. backpack\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.135457992553711", "False"]], [["-5.635457992553711", "False"]], [["-9.135457992553711", "False"]], [["-7.885457992553711", "False"]], [["-0.6354579925537109", "True"]]], "filtered_resps": [["-4.135457992553711", "False"], ["-5.635457992553711", "False"], ["-9.135457992553711", "False"], ["-7.885457992553711", "False"], ["-0.6354579925537109", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1f556f512f7447ffb4eee59bee0c07f3e67d1ef91ce10f5b50b13c5ee9e57209", "prompt_hash": "e9d93f646f3a3bdf02c91781bfede57610296a94e4972d6493dc3c68086e1626", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 929, "doc": {"id": "76b2c6d254f9127b4fd66d90e1a330e7", "question": "What could an apple tree do?", "question_concept": "apple tree", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["new hampshire", "bloom", "washington state", "sunshine", "spontaneously combust"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What could an apple tree do?\nA. new hampshire\nB. bloom\nC. washington state\nD. sunshine\nE. spontaneously combust\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could an apple tree do?\nA. new hampshire\nB. bloom\nC. washington state\nD. sunshine\nE. spontaneously combust\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could an apple tree do?\nA. new hampshire\nB. bloom\nC. washington state\nD. sunshine\nE. spontaneously combust\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could an apple tree do?\nA. new hampshire\nB. bloom\nC. washington state\nD. sunshine\nE. spontaneously combust\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could an apple tree do?\nA. new hampshire\nB. bloom\nC. washington state\nD. sunshine\nE. spontaneously combust\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.326545238494873", "False"]], [["-0.8265451192855835", "True"]], [["-8.076544761657715", "False"]], [["-10.076544761657715", "False"]], [["-9.576544761657715", "False"]]], "filtered_resps": [["-5.326545238494873", "False"], ["-0.8265451192855835", "True"], ["-8.076544761657715", "False"], ["-10.076544761657715", "False"], ["-9.576544761657715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bcc5fd8b27cb0e6fe4f69c28e9c184155708cebfcda63e6189d8fa0e487f1c67", "prompt_hash": "680ae6546aeef78ea17c058ee8c0bc78cec490aa6b23acc23d628ead98b5b52e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 930, "doc": {"id": "cdd3d074031fbd3efeb4f9408abef04e", "question": "What very cold area in the east can a crab be found?", "question_concept": "crab", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fish market", "shallow waters", "atlantic ocean", "fresh water", "shore line"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What very cold area in the east can a crab be found?\nA. fish market\nB. shallow waters\nC. atlantic ocean\nD. fresh water\nE. shore line\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What very cold area in the east can a crab be found?\nA. fish market\nB. shallow waters\nC. atlantic ocean\nD. fresh water\nE. shore line\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What very cold area in the east can a crab be found?\nA. fish market\nB. shallow waters\nC. atlantic ocean\nD. fresh water\nE. shore line\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What very cold area in the east can a crab be found?\nA. fish market\nB. shallow waters\nC. atlantic ocean\nD. fresh water\nE. shore line\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What very cold area in the east can a crab be found?\nA. fish market\nB. shallow waters\nC. atlantic ocean\nD. fresh water\nE. shore line\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.291214942932129", "False"]], [["-5.291214942932129", "False"]], [["-1.0412148237228394", "True"]], [["-5.041214942932129", "False"]], [["-3.541214942932129", "False"]]], "filtered_resps": [["-3.291214942932129", "False"], ["-5.291214942932129", "False"], ["-1.0412148237228394", "True"], ["-5.041214942932129", "False"], ["-3.541214942932129", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8f42f75770950547033b447ee61f7cab4f32d4430393b17631560ddfab45b77c", "prompt_hash": "94b6b60bc8dd2a4015bc6b3f9491f22d87a45f2ebadd1be046dd61c76c5d7b9a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 931, "doc": {"id": "359aed918343d228e67cef329b693904", "question": "The chef wanted to perfect his craft, what did he do?", "question_concept": "chef", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["thin potatos", "prepare food", "study french cooking", "drink", "cook dinner"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The chef wanted to perfect his craft, what did he do?\nA. thin potatos\nB. prepare food\nC. study french cooking\nD. drink\nE. cook dinner\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The chef wanted to perfect his craft, what did he do?\nA. thin potatos\nB. prepare food\nC. study french cooking\nD. drink\nE. cook dinner\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The chef wanted to perfect his craft, what did he do?\nA. thin potatos\nB. prepare food\nC. study french cooking\nD. drink\nE. cook dinner\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The chef wanted to perfect his craft, what did he do?\nA. thin potatos\nB. prepare food\nC. study french cooking\nD. drink\nE. cook dinner\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The chef wanted to perfect his craft, what did he do?\nA. thin potatos\nB. prepare food\nC. study french cooking\nD. drink\nE. cook dinner\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.853763580322266", "False"]], [["-1.6037638187408447", "False"]], [["-1.1037638187408447", "True"]], [["-6.353763580322266", "False"]], [["-6.853763580322266", "False"]]], "filtered_resps": [["-4.853763580322266", "False"], ["-1.6037638187408447", "False"], ["-1.1037638187408447", "True"], ["-6.353763580322266", "False"], ["-6.853763580322266", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b73aac44af7b8383fa6565e54ce51a0aa7ce835d37639002e8855a50f92a5eda", "prompt_hash": "24a7deae39d8f57d08df5adb98f552dad8aea945f5a5303466495c2a741e6940", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 932, "doc": {"id": "cf02cca40a47c2deefd8b2e5a5ff2f70", "question": "She wanted a kitten and puppy so why did she only get the puppy?", "question_concept": "puppy", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["one choice for pet", "cute", "kennel", "soft", "waxy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: She wanted a kitten and puppy so why did she only get the puppy?\nA. one choice for pet\nB. cute\nC. kennel\nD. soft\nE. waxy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: She wanted a kitten and puppy so why did she only get the puppy?\nA. one choice for pet\nB. cute\nC. kennel\nD. soft\nE. waxy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: She wanted a kitten and puppy so why did she only get the puppy?\nA. one choice for pet\nB. cute\nC. kennel\nD. soft\nE. waxy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: She wanted a kitten and puppy so why did she only get the puppy?\nA. one choice for pet\nB. cute\nC. kennel\nD. soft\nE. waxy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: She wanted a kitten and puppy so why did she only get the puppy?\nA. one choice for pet\nB. cute\nC. kennel\nD. soft\nE. waxy\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.103360891342163", "True"]], [["-3.603360891342163", "False"]], [["-6.353361129760742", "False"]], [["-7.603361129760742", "False"]], [["-8.353361129760742", "False"]]], "filtered_resps": [["-1.103360891342163", "True"], ["-3.603360891342163", "False"], ["-6.353361129760742", "False"], ["-7.603361129760742", "False"], ["-8.353361129760742", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d4f430e35213a74455c238051a674deccb0ff679d39c1f2fc67d75d9108b7cb5", "prompt_hash": "bab642e9240a9bc9d8eadf463af61a8bd05aa2af87f8c4226742e4e1cd829d64", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 933, "doc": {"id": "ac1abecdbbd7bcde6592ca645c2ecb1e", "question": "There was no shade for Jenny.  She was forced to lie there exposed to what?", "question_concept": "shade", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["full sunlight", "bright sunshine", "sunny place", "eat cake", "direct sunlight"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: There was no shade for Jenny.  She was forced to lie there exposed to what?\nA. full sunlight\nB. bright sunshine\nC. sunny place\nD. eat cake\nE. direct sunlight\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: There was no shade for Jenny.  She was forced to lie there exposed to what?\nA. full sunlight\nB. bright sunshine\nC. sunny place\nD. eat cake\nE. direct sunlight\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: There was no shade for Jenny.  She was forced to lie there exposed to what?\nA. full sunlight\nB. bright sunshine\nC. sunny place\nD. eat cake\nE. direct sunlight\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: There was no shade for Jenny.  She was forced to lie there exposed to what?\nA. full sunlight\nB. bright sunshine\nC. sunny place\nD. eat cake\nE. direct sunlight\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: There was no shade for Jenny.  She was forced to lie there exposed to what?\nA. full sunlight\nB. bright sunshine\nC. sunny place\nD. eat cake\nE. direct sunlight\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.0854732990264893", "False"]], [["-3.0854732990264893", "False"]], [["-5.83547306060791", "False"]], [["-7.58547306060791", "False"]], [["-1.0854732990264893", "True"]]], "filtered_resps": [["-2.0854732990264893", "False"], ["-3.0854732990264893", "False"], ["-5.83547306060791", "False"], ["-7.58547306060791", "False"], ["-1.0854732990264893", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8b20f038a48bcacf943474adbd4857bb87d49a8e19153793feb41fdd72c868c7", "prompt_hash": "ec5378b364ad9469f92d51481c63049c48b18efcd2fd655a97b22133dffeb9f4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 934, "doc": {"id": "2adbb4fc0d5249dc411dda433f378591", "question": "What could happen to you after you are cleaning house for a long time?", "question_concept": "cleaning house", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["neatness", "tiredness", "order", "exhaustion", "sneezing"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What could happen to you after you are cleaning house for a long time?\nA. neatness\nB. tiredness\nC. order\nD. exhaustion\nE. sneezing\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could happen to you after you are cleaning house for a long time?\nA. neatness\nB. tiredness\nC. order\nD. exhaustion\nE. sneezing\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could happen to you after you are cleaning house for a long time?\nA. neatness\nB. tiredness\nC. order\nD. exhaustion\nE. sneezing\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could happen to you after you are cleaning house for a long time?\nA. neatness\nB. tiredness\nC. order\nD. exhaustion\nE. sneezing\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could happen to you after you are cleaning house for a long time?\nA. neatness\nB. tiredness\nC. order\nD. exhaustion\nE. sneezing\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.116123199462891", "False"]], [["-1.616123080253601", "True"]], [["-2.3661231994628906", "False"]], [["-1.866123080253601", "False"]], [["-8.36612319946289", "False"]]], "filtered_resps": [["-5.116123199462891", "False"], ["-1.616123080253601", "True"], ["-2.3661231994628906", "False"], ["-1.866123080253601", "False"], ["-8.36612319946289", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4456f3d26a5c8f85e9eaa0141d51a603a66e87c92465230d0f172c5aa5ce8e3c", "prompt_hash": "d13b475ac952632f689a9983f3a03979c47fee7d1fac1e99d84bd24e4608a1d3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 935, "doc": {"id": "5a1c8a9dbbb60e523cc1ba14a370729c", "question": "What is someone doing when scheduling when to go to party?", "question_concept": "going to party", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rumpspringa", "meeting new people", "having fun", "meet new people", "plan"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is someone doing when scheduling when to go to party?\nA. rumpspringa\nB. meeting new people\nC. having fun\nD. meet new people\nE. plan\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is someone doing when scheduling when to go to party?\nA. rumpspringa\nB. meeting new people\nC. having fun\nD. meet new people\nE. plan\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is someone doing when scheduling when to go to party?\nA. rumpspringa\nB. meeting new people\nC. having fun\nD. meet new people\nE. plan\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is someone doing when scheduling when to go to party?\nA. rumpspringa\nB. meeting new people\nC. having fun\nD. meet new people\nE. plan\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is someone doing when scheduling when to go to party?\nA. rumpspringa\nB. meeting new people\nC. having fun\nD. meet new people\nE. plan\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.496081829071045", "False"]], [["-4.996081829071045", "False"]], [["-6.746081829071045", "False"]], [["-7.496081829071045", "False"]], [["-1.2460819482803345", "True"]]], "filtered_resps": [["-4.496081829071045", "False"], ["-4.996081829071045", "False"], ["-6.746081829071045", "False"], ["-7.496081829071045", "False"], ["-1.2460819482803345", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "943b6b7b6c0bd3581ce40ad9d2f71d39c6e4b602e2d2b7ce2a68c077ad1e904f", "prompt_hash": "d7345200d46bc576293c44b19fb77a9773a91a4b906cc357a129799fcd5352c3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 936, "doc": {"id": "3665b329f93f7c84edeabe394140f8d2", "question": "What kind of path do comets tend to have?", "question_concept": "comets", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ice", "set orbits", "universe", "space", "solid nucleus"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of path do comets tend to have?\nA. ice\nB. set orbits\nC. universe\nD. space\nE. solid nucleus\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of path do comets tend to have?\nA. ice\nB. set orbits\nC. universe\nD. space\nE. solid nucleus\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of path do comets tend to have?\nA. ice\nB. set orbits\nC. universe\nD. space\nE. solid nucleus\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of path do comets tend to have?\nA. ice\nB. set orbits\nC. universe\nD. space\nE. solid nucleus\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of path do comets tend to have?\nA. ice\nB. set orbits\nC. universe\nD. space\nE. solid nucleus\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.769715309143066", "False"]], [["-1.519715428352356", "False"]], [["-8.269715309143066", "False"]], [["-8.769715309143066", "False"]], [["-7.019715309143066", "False"]]], "filtered_resps": [["-5.769715309143066", "False"], ["-1.519715428352356", "False"], ["-8.269715309143066", "False"], ["-8.769715309143066", "False"], ["-7.019715309143066", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3e2cd526456f9b0f21eb7e154bfe92162879459c8c737476324bc05fefefebcd", "prompt_hash": "96f3143bc30d5abf808067b648c526792c2041ab5f74fa0ac1199ea60cc6ecbb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 937, "doc": {"id": "dbcedaa6a6f1f68bc8f2bf7aef23294e", "question": "What do people feel after having sex that requires them to shower?", "question_concept": "sex", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bedroom", "pleasant", "obesity", "painful", "dirty"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do people feel after having sex that requires them to shower?\nA. bedroom\nB. pleasant\nC. obesity\nD. painful\nE. dirty\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people feel after having sex that requires them to shower?\nA. bedroom\nB. pleasant\nC. obesity\nD. painful\nE. dirty\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people feel after having sex that requires them to shower?\nA. bedroom\nB. pleasant\nC. obesity\nD. painful\nE. dirty\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people feel after having sex that requires them to shower?\nA. bedroom\nB. pleasant\nC. obesity\nD. painful\nE. dirty\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people feel after having sex that requires them to shower?\nA. bedroom\nB. pleasant\nC. obesity\nD. painful\nE. dirty\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7430777549743652", "False"]], [["-1.4930777549743652", "True"]], [["-6.993077754974365", "False"]], [["-6.743077754974365", "False"]], [["-2.4930777549743652", "False"]]], "filtered_resps": [["-3.7430777549743652", "False"], ["-1.4930777549743652", "True"], ["-6.993077754974365", "False"], ["-6.743077754974365", "False"], ["-2.4930777549743652", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b5f5c8b1accd80d64b6fac9bb1cf1c13c4e79be95c39a030f0988b9db266c09d", "prompt_hash": "27685faa24e3f811d49e5b2c0c6f4f6455d7d2d59a37fa1893e2e2803ce9e835", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 938, "doc": {"id": "ba3a2b9ff289c106051163f840a6f5ba", "question": "The vet found malignant tumors on the animals, what is their likely fate?", "question_concept": "animals", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["euthanasia", "pass water", "die of cancer", "feel pain", "feel pleasure"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The vet found malignant tumors on the animals, what is their likely fate?\nA. euthanasia\nB. pass water\nC. die of cancer\nD. feel pain\nE. feel pleasure\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The vet found malignant tumors on the animals, what is their likely fate?\nA. euthanasia\nB. pass water\nC. die of cancer\nD. feel pain\nE. feel pleasure\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The vet found malignant tumors on the animals, what is their likely fate?\nA. euthanasia\nB. pass water\nC. die of cancer\nD. feel pain\nE. feel pleasure\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The vet found malignant tumors on the animals, what is their likely fate?\nA. euthanasia\nB. pass water\nC. die of cancer\nD. feel pain\nE. feel pleasure\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The vet found malignant tumors on the animals, what is their likely fate?\nA. euthanasia\nB. pass water\nC. die of cancer\nD. feel pain\nE. feel pleasure\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5063042640686035", "True"]], [["-6.5063042640686035", "False"]], [["-1.7563042640686035", "False"]], [["-6.2563042640686035", "False"]], [["-6.7563042640686035", "False"]]], "filtered_resps": [["-1.5063042640686035", "True"], ["-6.5063042640686035", "False"], ["-1.7563042640686035", "False"], ["-6.2563042640686035", "False"], ["-6.7563042640686035", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "10146abcd7166d3687562286a033128b54999ad5109aa435861a0be413dc0cbc", "prompt_hash": "78fb88ffad0a5feba8cfde0ff30f2cfb8da60f6cdc2267650e797d55ddf6e745", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 939, "doc": {"id": "13fc28f53423a9b3a656c9431df1b3b5", "question": "What is the thing that is agitated in your head when kissing?", "question_concept": "kissing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sexual stimulation", "herpes", "headache", "catch cold", "happiness"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is the thing that is agitated in your head when kissing?\nA. sexual stimulation\nB. herpes\nC. headache\nD. catch cold\nE. happiness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the thing that is agitated in your head when kissing?\nA. sexual stimulation\nB. herpes\nC. headache\nD. catch cold\nE. happiness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the thing that is agitated in your head when kissing?\nA. sexual stimulation\nB. herpes\nC. headache\nD. catch cold\nE. happiness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the thing that is agitated in your head when kissing?\nA. sexual stimulation\nB. herpes\nC. headache\nD. catch cold\nE. happiness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the thing that is agitated in your head when kissing?\nA. sexual stimulation\nB. herpes\nC. headache\nD. catch cold\nE. happiness\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.343031644821167", "True"]], [["-4.593031883239746", "False"]], [["-4.593031883239746", "False"]], [["-6.343031883239746", "False"]], [["-4.093031883239746", "False"]]], "filtered_resps": [["-1.343031644821167", "True"], ["-4.593031883239746", "False"], ["-4.593031883239746", "False"], ["-6.343031883239746", "False"], ["-4.093031883239746", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "237bcc65d9922cbb95c27c2e5eedb3bf896dc53f8922c0af9a624a9155c37fad", "prompt_hash": "f6fe2a02cf89890f7f6ef0e263c97f1566e77ab29d09d43363bc61d0f6c880a3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 940, "doc": {"id": "3f4b48708d08f8bf7bec796531023f9c", "question": "Billy was reading the newspaper as he commuted to work, but once he got to his destination he balled it up and put it somewhere. Where did it put it?", "question_concept": "newspaper", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["trash", "floor", "subway", "ground", "lawn"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Billy was reading the newspaper as he commuted to work, but once he got to his destination he balled it up and put it somewhere. Where did it put it?\nA. trash\nB. floor\nC. subway\nD. ground\nE. lawn\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Billy was reading the newspaper as he commuted to work, but once he got to his destination he balled it up and put it somewhere. Where did it put it?\nA. trash\nB. floor\nC. subway\nD. ground\nE. lawn\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Billy was reading the newspaper as he commuted to work, but once he got to his destination he balled it up and put it somewhere. Where did it put it?\nA. trash\nB. floor\nC. subway\nD. ground\nE. lawn\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Billy was reading the newspaper as he commuted to work, but once he got to his destination he balled it up and put it somewhere. Where did it put it?\nA. trash\nB. floor\nC. subway\nD. ground\nE. lawn\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Billy was reading the newspaper as he commuted to work, but once he got to his destination he balled it up and put it somewhere. Where did it put it?\nA. trash\nB. floor\nC. subway\nD. ground\nE. lawn\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8707284927368164", "True"]], [["-3.1207284927368164", "False"]], [["-4.120728492736816", "False"]], [["-3.3707284927368164", "False"]], [["-5.370728492736816", "False"]]], "filtered_resps": [["-1.8707284927368164", "True"], ["-3.1207284927368164", "False"], ["-4.120728492736816", "False"], ["-3.3707284927368164", "False"], ["-5.370728492736816", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "554e386ed2ae673b3cd887e1e0348a1b7593bfa25936341620924f7d025fb3f5", "prompt_hash": "27ec6f88a40a86bad64bed7f1642ee98dbc09cb0b6709b6962d1ab53bc216382", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 941, "doc": {"id": "c61790eb63ff6652b878ca051493c07d", "question": "Where do you keep a pail in your house?", "question_concept": "pail", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["garage", "pool", "utility room", "hardware store", "wishing well"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you keep a pail in your house?\nA. garage\nB. pool\nC. utility room\nD. hardware store\nE. wishing well\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you keep a pail in your house?\nA. garage\nB. pool\nC. utility room\nD. hardware store\nE. wishing well\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you keep a pail in your house?\nA. garage\nB. pool\nC. utility room\nD. hardware store\nE. wishing well\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you keep a pail in your house?\nA. garage\nB. pool\nC. utility room\nD. hardware store\nE. wishing well\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you keep a pail in your house?\nA. garage\nB. pool\nC. utility room\nD. hardware store\nE. wishing well\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2527484893798828", "True"]], [["-6.002748489379883", "False"]], [["-1.7527484893798828", "False"]], [["-7.502748489379883", "False"]], [["-8.002748489379883", "False"]]], "filtered_resps": [["-1.2527484893798828", "True"], ["-6.002748489379883", "False"], ["-1.7527484893798828", "False"], ["-7.502748489379883", "False"], ["-8.002748489379883", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bc08a9eb4cabf29cdc6caaf96f93c89b8ae2b87899f68c28f006039d86af588c", "prompt_hash": "d33afe533e88d5f3e32c4bd30975fb23677280267a5eafa36e5afcdb49817cdb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 942, "doc": {"id": "e5ebbe0ea4097bb197ac525b49108362", "question": "what is printed with ink and distributed daily?", "question_concept": "ink", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fountain pen", "squid", "newspaper", "book", "printer"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: what is printed with ink and distributed daily?\nA. fountain pen\nB. squid\nC. newspaper\nD. book\nE. printer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: what is printed with ink and distributed daily?\nA. fountain pen\nB. squid\nC. newspaper\nD. book\nE. printer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: what is printed with ink and distributed daily?\nA. fountain pen\nB. squid\nC. newspaper\nD. book\nE. printer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: what is printed with ink and distributed daily?\nA. fountain pen\nB. squid\nC. newspaper\nD. book\nE. printer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: what is printed with ink and distributed daily?\nA. fountain pen\nB. squid\nC. newspaper\nD. book\nE. printer\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8216564655303955", "False"]], [["-7.071656703948975", "False"]], [["-1.3216564655303955", "False"]], [["-7.071656703948975", "False"]], [["-6.321656703948975", "False"]]], "filtered_resps": [["-3.8216564655303955", "False"], ["-7.071656703948975", "False"], ["-1.3216564655303955", "False"], ["-7.071656703948975", "False"], ["-6.321656703948975", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "aec1f02cfff47452414e8121f2b6b1838f9c4a4e88386abb8e2ed61e4666a1aa", "prompt_hash": "72b4bc25fabe1f5bb0040e9dc0845c126a65310ad8dc9e7c95856db4d2dd67f1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 943, "doc": {"id": "029e36d8f65982b142c319064dc5e32f", "question": "What are people likely to do when an unexpected decent outcome occurs?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kill each other", "thank god", "experience pain", "hatred", "talk to each other"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What are people likely to do when an unexpected decent outcome occurs?\nA. kill each other\nB. thank god\nC. experience pain\nD. hatred\nE. talk to each other\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are people likely to do when an unexpected decent outcome occurs?\nA. kill each other\nB. thank god\nC. experience pain\nD. hatred\nE. talk to each other\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are people likely to do when an unexpected decent outcome occurs?\nA. kill each other\nB. thank god\nC. experience pain\nD. hatred\nE. talk to each other\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are people likely to do when an unexpected decent outcome occurs?\nA. kill each other\nB. thank god\nC. experience pain\nD. hatred\nE. talk to each other\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are people likely to do when an unexpected decent outcome occurs?\nA. kill each other\nB. thank god\nC. experience pain\nD. hatred\nE. talk to each other\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.0255351066589355", "False"]], [["-2.0255351066589355", "False"]], [["-6.5255351066589355", "False"]], [["-7.2755351066589355", "False"]], [["-2.7755351066589355", "False"]]], "filtered_resps": [["-5.0255351066589355", "False"], ["-2.0255351066589355", "False"], ["-6.5255351066589355", "False"], ["-7.2755351066589355", "False"], ["-2.7755351066589355", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b42a8c1adc7d6f0e7fb2d0a02e5fab08768fa840570be50c5c1c26cf4ab33043", "prompt_hash": "b0e70f4d1d7095f9c6b153ace72b80ef7fac3b36d6918c9006d0a2c063c0d0ab", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 944, "doc": {"id": "3d1a67f87b34303f97549ba83e5521c2", "question": "The terrace had Kanji written on it, indicating that it was made where?", "question_concept": "terrace", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["japan", "rice paddy", "garden", "michigan", "italy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The terrace had Kanji written on it, indicating that it was made where?\nA. japan\nB. rice paddy\nC. garden\nD. michigan\nE. italy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The terrace had Kanji written on it, indicating that it was made where?\nA. japan\nB. rice paddy\nC. garden\nD. michigan\nE. italy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The terrace had Kanji written on it, indicating that it was made where?\nA. japan\nB. rice paddy\nC. garden\nD. michigan\nE. italy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The terrace had Kanji written on it, indicating that it was made where?\nA. japan\nB. rice paddy\nC. garden\nD. michigan\nE. italy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The terrace had Kanji written on it, indicating that it was made where?\nA. japan\nB. rice paddy\nC. garden\nD. michigan\nE. italy\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5281659364700317", "True"]], [["-2.028165817260742", "False"]], [["-5.028165817260742", "False"]], [["-7.528165817260742", "False"]], [["-10.778165817260742", "False"]]], "filtered_resps": [["-1.5281659364700317", "True"], ["-2.028165817260742", "False"], ["-5.028165817260742", "False"], ["-7.528165817260742", "False"], ["-10.778165817260742", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "99d2dc156e267e40cf38014bf0203bad57d0a5be7a865fa57b6ca9bfe5ffa312", "prompt_hash": "74bb6f913595d7ecb30baece531a08c7bbc88b9eeee56e3f3d84fac45123ce65", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 945, "doc": {"id": "e050bce7048da1b3743a54153e91694e", "question": "The company sent off many purchases, they used recycled cardboard as their what?", "question_concept": "cardboard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["packaging materials", "recycle bin", "box factory", "warehouse", "bowler hats"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The company sent off many purchases, they used recycled cardboard as their what?\nA. packaging materials\nB. recycle bin\nC. box factory\nD. warehouse\nE. bowler hats\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The company sent off many purchases, they used recycled cardboard as their what?\nA. packaging materials\nB. recycle bin\nC. box factory\nD. warehouse\nE. bowler hats\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The company sent off many purchases, they used recycled cardboard as their what?\nA. packaging materials\nB. recycle bin\nC. box factory\nD. warehouse\nE. bowler hats\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The company sent off many purchases, they used recycled cardboard as their what?\nA. packaging materials\nB. recycle bin\nC. box factory\nD. warehouse\nE. bowler hats\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The company sent off many purchases, they used recycled cardboard as their what?\nA. packaging materials\nB. recycle bin\nC. box factory\nD. warehouse\nE. bowler hats\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5974209308624268", "True"]], [["-7.347420692443848", "False"]], [["-7.847420692443848", "False"]], [["-9.347420692443848", "False"]], [["-9.597420692443848", "False"]]], "filtered_resps": [["-0.5974209308624268", "True"], ["-7.347420692443848", "False"], ["-7.847420692443848", "False"], ["-9.347420692443848", "False"], ["-9.597420692443848", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ff39c2ae0020f3dd913bf39f12210ae087bab3205ae5c02545abc8ea62971b16", "prompt_hash": "52c5c33dad46d63e5548c33760c44cdd0f3f6fec686199d5952cec2a2f89a459", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 946, "doc": {"id": "8233ccb60dd0c0ff3b7ca5d73e5681f2", "question": "Why might a person be known as a liar?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have no home", "false information", "hungry", "made fun of", "brain tumor"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Why might a person be known as a liar?\nA. have no home\nB. false information\nC. hungry\nD. made fun of\nE. brain tumor\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why might a person be known as a liar?\nA. have no home\nB. false information\nC. hungry\nD. made fun of\nE. brain tumor\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why might a person be known as a liar?\nA. have no home\nB. false information\nC. hungry\nD. made fun of\nE. brain tumor\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why might a person be known as a liar?\nA. have no home\nB. false information\nC. hungry\nD. made fun of\nE. brain tumor\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why might a person be known as a liar?\nA. have no home\nB. false information\nC. hungry\nD. made fun of\nE. brain tumor\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6865744590759277", "False"]], [["-1.4365744590759277", "False"]], [["-9.436574935913086", "False"]], [["-9.436574935913086", "False"]], [["-10.186574935913086", "False"]]], "filtered_resps": [["-1.6865744590759277", "False"], ["-1.4365744590759277", "False"], ["-9.436574935913086", "False"], ["-9.436574935913086", "False"], ["-10.186574935913086", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cdb6e5cdadc8b6099d04a2e11ef9bf4d31ccbd979c3d9f8995ad6ee589844c30", "prompt_hash": "db03a88ae208591d4a498934275000f2926dde885a5fab44bdba70296a6145f2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 947, "doc": {"id": "eb4b2cd0f2a69686e5a82250c5806b84", "question": "The child was politely waiting for dessert, he was eventually rewarded for his what?", "question_concept": "waiting for", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["timing", "expenditure of time", "getting bored", "anger", "patience"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The child was politely waiting for dessert, he was eventually rewarded for his what?\nA. timing\nB. expenditure of time\nC. getting bored\nD. anger\nE. patience\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The child was politely waiting for dessert, he was eventually rewarded for his what?\nA. timing\nB. expenditure of time\nC. getting bored\nD. anger\nE. patience\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The child was politely waiting for dessert, he was eventually rewarded for his what?\nA. timing\nB. expenditure of time\nC. getting bored\nD. anger\nE. patience\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The child was politely waiting for dessert, he was eventually rewarded for his what?\nA. timing\nB. expenditure of time\nC. getting bored\nD. anger\nE. patience\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The child was politely waiting for dessert, he was eventually rewarded for his what?\nA. timing\nB. expenditure of time\nC. getting bored\nD. anger\nE. patience\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9912071228027344", "False"]], [["-5.491207122802734", "False"]], [["-6.991207122802734", "False"]], [["-6.991207122802734", "False"]], [["-1.241207242012024", "True"]]], "filtered_resps": [["-3.9912071228027344", "False"], ["-5.491207122802734", "False"], ["-6.991207122802734", "False"], ["-6.991207122802734", "False"], ["-1.241207242012024", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bb0b2285e5716cecbca50aa9ed626253f97e29b52c11322143f6f41b319317d3", "prompt_hash": "ee5ee52a45ad4b88d8bca4176f060825e707f5d10ba6f4542118d2f74ffeea9b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 948, "doc": {"id": "d0bda97a087904320216e4d0b8a08a8d", "question": "The man was giving assistance to a pan handler in the streets, how did he give assistance?", "question_concept": "giving assistance", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feeling good", "killing", "law suits", "out of pocket", "feel loved"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The man was giving assistance to a pan handler in the streets, how did he give assistance?\nA. feeling good\nB. killing\nC. law suits\nD. out of pocket\nE. feel loved\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man was giving assistance to a pan handler in the streets, how did he give assistance?\nA. feeling good\nB. killing\nC. law suits\nD. out of pocket\nE. feel loved\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man was giving assistance to a pan handler in the streets, how did he give assistance?\nA. feeling good\nB. killing\nC. law suits\nD. out of pocket\nE. feel loved\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man was giving assistance to a pan handler in the streets, how did he give assistance?\nA. feeling good\nB. killing\nC. law suits\nD. out of pocket\nE. feel loved\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man was giving assistance to a pan handler in the streets, how did he give assistance?\nA. feeling good\nB. killing\nC. law suits\nD. out of pocket\nE. feel loved\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3560500144958496", "False"]], [["-7.10605001449585", "False"]], [["-7.10605001449585", "False"]], [["-1.8560500144958496", "False"]], [["-8.606050491333008", "False"]]], "filtered_resps": [["-3.3560500144958496", "False"], ["-7.10605001449585", "False"], ["-7.10605001449585", "False"], ["-1.8560500144958496", "False"], ["-8.606050491333008", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "902a7ac91ae069435eddc51fab76b501611cd2ff65a7bd6b5339970026200c99", "prompt_hash": "e8b9b5dac3bc5ee7b02623eb019c892213be2153ccf9fd702898d121d47ae4e5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 949, "doc": {"id": "e216381e9f0ddd1d248ee25fccca2b1f", "question": "What do you call the caretakers of a child?", "question_concept": "child", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["principal", "birth", "loving couple", "act of sex", "parents"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do you call the caretakers of a child?\nA. principal\nB. birth\nC. loving couple\nD. act of sex\nE. parents\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you call the caretakers of a child?\nA. principal\nB. birth\nC. loving couple\nD. act of sex\nE. parents\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you call the caretakers of a child?\nA. principal\nB. birth\nC. loving couple\nD. act of sex\nE. parents\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you call the caretakers of a child?\nA. principal\nB. birth\nC. loving couple\nD. act of sex\nE. parents\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you call the caretakers of a child?\nA. principal\nB. birth\nC. loving couple\nD. act of sex\nE. parents\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.563117504119873", "False"]], [["-7.313117504119873", "False"]], [["-6.563117504119873", "False"]], [["-8.813117980957031", "False"]], [["-1.063117504119873", "True"]]], "filtered_resps": [["-5.563117504119873", "False"], ["-7.313117504119873", "False"], ["-6.563117504119873", "False"], ["-8.813117980957031", "False"], ["-1.063117504119873", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e7c9a27732995d29fe1f733eaea5d00cbb1617938bf63a987f9ad853091fc1e9", "prompt_hash": "a493e6d7d5eca857b744423fcbb15406d03ca34961053adcaf99e146e5e2f0cc", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 950, "doc": {"id": "b1fba9ad6193c6751ddb3f58f7f39b35", "question": "Where would you run in to a niece you only see every one and a while?", "question_concept": "niece", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["family reunion", "brother's house", "family picture book", "family tree", "party"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you run in to a niece you only see every one and a while?\nA. family reunion\nB. brother's house\nC. family picture book\nD. family tree\nE. party\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you run in to a niece you only see every one and a while?\nA. family reunion\nB. brother's house\nC. family picture book\nD. family tree\nE. party\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you run in to a niece you only see every one and a while?\nA. family reunion\nB. brother's house\nC. family picture book\nD. family tree\nE. party\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you run in to a niece you only see every one and a while?\nA. family reunion\nB. brother's house\nC. family picture book\nD. family tree\nE. party\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you run in to a niece you only see every one and a while?\nA. family reunion\nB. brother's house\nC. family picture book\nD. family tree\nE. party\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.154644250869751", "True"]], [["-1.404644250869751", "False"]], [["-4.654644012451172", "False"]], [["-5.654644012451172", "False"]], [["-4.154644012451172", "False"]]], "filtered_resps": [["-1.154644250869751", "True"], ["-1.404644250869751", "False"], ["-4.654644012451172", "False"], ["-5.654644012451172", "False"], ["-4.154644012451172", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "94d8eb3e91ce6171064b0d4cbcb7d6be4d1975a2cce4289c30401e7ba2ce5d37", "prompt_hash": "910771d469985e13be82742f3f4043b3f30d18596e73d81a5b605f76c44ad231", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 951, "doc": {"id": "3ceae7a18073050bd2c0448abef1f393", "question": "Working on the elaborate task was taxing, it require extreme what?", "question_concept": "working", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["holding", "concentration", "energy", "job", "energh"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Working on the elaborate task was taxing, it require extreme what?\nA. holding\nB. concentration\nC. energy\nD. job\nE. energh\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Working on the elaborate task was taxing, it require extreme what?\nA. holding\nB. concentration\nC. energy\nD. job\nE. energh\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Working on the elaborate task was taxing, it require extreme what?\nA. holding\nB. concentration\nC. energy\nD. job\nE. energh\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Working on the elaborate task was taxing, it require extreme what?\nA. holding\nB. concentration\nC. energy\nD. job\nE. energh\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Working on the elaborate task was taxing, it require extreme what?\nA. holding\nB. concentration\nC. energy\nD. job\nE. energh\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.923297882080078", "False"]], [["-2.173297882080078", "False"]], [["-5.423297882080078", "False"]], [["-8.173297882080078", "False"]], [["-8.923297882080078", "False"]]], "filtered_resps": [["-6.923297882080078", "False"], ["-2.173297882080078", "False"], ["-5.423297882080078", "False"], ["-8.173297882080078", "False"], ["-8.923297882080078", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4b0ad5d3bb953ca65bc6eca619f4dfc8caf5290b95140a46acbd41c79449c835", "prompt_hash": "0a388c60a5b50c6a0ac026a653307cd24b845b80cd63225c895ed098db1c217b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 952, "doc": {"id": "f1182e3a070f5a1be529843aa6e5c20c", "question": "What may you have after awaking after a night of heavy drinking?", "question_concept": "awaking", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get up", "discomfort", "discomfort", "headache", "shock"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What may you have after awaking after a night of heavy drinking?\nA. get up\nB. discomfort\nC. discomfort\nD. headache\nE. shock\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What may you have after awaking after a night of heavy drinking?\nA. get up\nB. discomfort\nC. discomfort\nD. headache\nE. shock\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What may you have after awaking after a night of heavy drinking?\nA. get up\nB. discomfort\nC. discomfort\nD. headache\nE. shock\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What may you have after awaking after a night of heavy drinking?\nA. get up\nB. discomfort\nC. discomfort\nD. headache\nE. shock\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What may you have after awaking after a night of heavy drinking?\nA. get up\nB. discomfort\nC. discomfort\nD. headache\nE. shock\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.468881845474243", "False"]], [["-1.9688818454742432", "False"]], [["-4.968881607055664", "False"]], [["-2.718881845474243", "False"]], [["-4.718881607055664", "False"]]], "filtered_resps": [["-2.468881845474243", "False"], ["-1.9688818454742432", "False"], ["-4.968881607055664", "False"], ["-2.718881845474243", "False"], ["-4.718881607055664", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "42f83e45a7336809cddda9c47be1bb060ed2ffedaefe4a5e3f03f9de5ab2c2df", "prompt_hash": "691b55030a599d0804d1b6abb8f565c4fe5bd67111c2345ed9526afe75ee7df5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 953, "doc": {"id": "5799089c131e26473697afc54d5f6964", "question": "What uses a ribbon to put words on paper?", "question_concept": "ribbon", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wrapping paper", "girl's hair", "bath", "floral arrangement", "typewriter"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What uses a ribbon to put words on paper?\nA. wrapping paper\nB. girl's hair\nC. bath\nD. floral arrangement\nE. typewriter\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What uses a ribbon to put words on paper?\nA. wrapping paper\nB. girl's hair\nC. bath\nD. floral arrangement\nE. typewriter\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What uses a ribbon to put words on paper?\nA. wrapping paper\nB. girl's hair\nC. bath\nD. floral arrangement\nE. typewriter\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What uses a ribbon to put words on paper?\nA. wrapping paper\nB. girl's hair\nC. bath\nD. floral arrangement\nE. typewriter\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What uses a ribbon to put words on paper?\nA. wrapping paper\nB. girl's hair\nC. bath\nD. floral arrangement\nE. typewriter\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8976514339447021", "False"]], [["-7.397651672363281", "False"]], [["-9.147651672363281", "False"]], [["-10.647651672363281", "False"]], [["-1.6476514339447021", "False"]]], "filtered_resps": [["-1.8976514339447021", "False"], ["-7.397651672363281", "False"], ["-9.147651672363281", "False"], ["-10.647651672363281", "False"], ["-1.6476514339447021", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f72f3a8e294b772697816b117db15eee73aea1e14280000838d49eeb149bdd34", "prompt_hash": "4de4256229c5b83b72e77a52176655371ca89a2dce45d52902fb2083775e1a88", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 954, "doc": {"id": "7ce1f99e8185489a7113e6d18c71abb0", "question": "Where are sheep likely to live?", "question_concept": "sheep", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["school", "meadow", "lamb", "farm", "fairgrounds"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where are sheep likely to live?\nA. school\nB. meadow\nC. lamb\nD. farm\nE. fairgrounds\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where are sheep likely to live?\nA. school\nB. meadow\nC. lamb\nD. farm\nE. fairgrounds\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where are sheep likely to live?\nA. school\nB. meadow\nC. lamb\nD. farm\nE. fairgrounds\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where are sheep likely to live?\nA. school\nB. meadow\nC. lamb\nD. farm\nE. fairgrounds\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where are sheep likely to live?\nA. school\nB. meadow\nC. lamb\nD. farm\nE. fairgrounds\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.382285118103027", "False"]], [["-2.1322848796844482", "False"]], [["-7.382285118103027", "False"]], [["-1.6322848796844482", "True"]], [["-7.632285118103027", "False"]]], "filtered_resps": [["-5.382285118103027", "False"], ["-2.1322848796844482", "False"], ["-7.382285118103027", "False"], ["-1.6322848796844482", "True"], ["-7.632285118103027", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "39411a130cb49829275d53767e94ef0b73049196b696ad30a07db9a57580951b", "prompt_hash": "2ddf1bdfa39241bec41f2be466af0f425bd7c2488320a30759feb6e26538e85f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 955, "doc": {"id": "69425fb4cd2dc034e9ff223d2d5676ec", "question": "If I was watching TV on the couch and the air was stuffy, I might turn the fan on to make the what more comfortable?", "question_concept": "fan", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["hockey game", "living room", "bathroom", "football stadium", "hot room"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If I was watching TV on the couch and the air was stuffy, I might turn the fan on to make the what more comfortable?\nA. hockey game\nB. living room\nC. bathroom\nD. football stadium\nE. hot room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I was watching TV on the couch and the air was stuffy, I might turn the fan on to make the what more comfortable?\nA. hockey game\nB. living room\nC. bathroom\nD. football stadium\nE. hot room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I was watching TV on the couch and the air was stuffy, I might turn the fan on to make the what more comfortable?\nA. hockey game\nB. living room\nC. bathroom\nD. football stadium\nE. hot room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I was watching TV on the couch and the air was stuffy, I might turn the fan on to make the what more comfortable?\nA. hockey game\nB. living room\nC. bathroom\nD. football stadium\nE. hot room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I was watching TV on the couch and the air was stuffy, I might turn the fan on to make the what more comfortable?\nA. hockey game\nB. living room\nC. bathroom\nD. football stadium\nE. hot room\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.281379222869873", "False"]], [["-1.2813793420791626", "True"]], [["-7.531379222869873", "False"]], [["-9.531379699707031", "False"]], [["-5.531379222869873", "False"]]], "filtered_resps": [["-5.281379222869873", "False"], ["-1.2813793420791626", "True"], ["-7.531379222869873", "False"], ["-9.531379699707031", "False"], ["-5.531379222869873", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a3e5c9198baa5c5f50d3a9247f37f627777c599da494fe14c62e2f13405e584d", "prompt_hash": "5975aaf2045a276caa244e78df89185b51555aa253996161bce4eb26c84bd3ee", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 956, "doc": {"id": "f75b22d5b88ac56ae7df030c1ebeded5", "question": "While walking the student needed to store his writing insturment away, where did he put it?", "question_concept": "writing instrument", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desk drawer", "cabinet", "purse", "classroom", "pocket"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: While walking the student needed to store his writing insturment away, where did he put it?\nA. desk drawer\nB. cabinet\nC. purse\nD. classroom\nE. pocket\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: While walking the student needed to store his writing insturment away, where did he put it?\nA. desk drawer\nB. cabinet\nC. purse\nD. classroom\nE. pocket\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: While walking the student needed to store his writing insturment away, where did he put it?\nA. desk drawer\nB. cabinet\nC. purse\nD. classroom\nE. pocket\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: While walking the student needed to store his writing insturment away, where did he put it?\nA. desk drawer\nB. cabinet\nC. purse\nD. classroom\nE. pocket\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: While walking the student needed to store his writing insturment away, where did he put it?\nA. desk drawer\nB. cabinet\nC. purse\nD. classroom\nE. pocket\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.269218921661377", "False"]], [["-4.019218921661377", "False"]], [["-4.269218921661377", "False"]], [["-5.519218921661377", "False"]], [["-3.519218921661377", "False"]]], "filtered_resps": [["-2.269218921661377", "False"], ["-4.019218921661377", "False"], ["-4.269218921661377", "False"], ["-5.519218921661377", "False"], ["-3.519218921661377", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "04e669c6057d47ef8eafd082bbfad1b0463831162a58ff3d28307940c6b12354", "prompt_hash": "c01bf392d6f4378fccdf1ce887c26ad8aafe242aa4f9c86527a199f3d5f95ed4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 957, "doc": {"id": "4eb3e69c0d42a2287692d2b9d2cb5979", "question": "Who watches a play in an auditorium?", "question_concept": "auditorium", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["building", "crowd", "city", "group", "high school"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Who watches a play in an auditorium?\nA. building\nB. crowd\nC. city\nD. group\nE. high school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Who watches a play in an auditorium?\nA. building\nB. crowd\nC. city\nD. group\nE. high school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Who watches a play in an auditorium?\nA. building\nB. crowd\nC. city\nD. group\nE. high school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Who watches a play in an auditorium?\nA. building\nB. crowd\nC. city\nD. group\nE. high school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Who watches a play in an auditorium?\nA. building\nB. crowd\nC. city\nD. group\nE. high school\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.767792224884033", "False"]], [["-2.767792224884033", "False"]], [["-8.767792701721191", "False"]], [["-4.017792224884033", "False"]], [["-6.517792224884033", "False"]]], "filtered_resps": [["-2.767792224884033", "False"], ["-2.767792224884033", "False"], ["-8.767792701721191", "False"], ["-4.017792224884033", "False"], ["-6.517792224884033", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "19b7ee934312606d3884f3d0201e64e362a942e3a2ef158322e7c24a04f5415e", "prompt_hash": "7f63891ec93e710516dd92a1930f7ddd5bf5c4055827957740f2e11f46786a24", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 958, "doc": {"id": "7d937233b4a9043da0b976dbd42d141b", "question": "What is a possible outcome for committing murder?", "question_concept": "committing murder", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["problems", "incarceration", "trial", "imprisonment", "prosecution"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is a possible outcome for committing murder?\nA. problems\nB. incarceration\nC. trial\nD. imprisonment\nE. prosecution\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a possible outcome for committing murder?\nA. problems\nB. incarceration\nC. trial\nD. imprisonment\nE. prosecution\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a possible outcome for committing murder?\nA. problems\nB. incarceration\nC. trial\nD. imprisonment\nE. prosecution\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a possible outcome for committing murder?\nA. problems\nB. incarceration\nC. trial\nD. imprisonment\nE. prosecution\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a possible outcome for committing murder?\nA. problems\nB. incarceration\nC. trial\nD. imprisonment\nE. prosecution\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.778693675994873", "False"]], [["-1.0286937952041626", "True"]], [["-5.528693675994873", "False"]], [["-2.778693675994873", "False"]], [["-7.278693675994873", "False"]]], "filtered_resps": [["-2.778693675994873", "False"], ["-1.0286937952041626", "True"], ["-5.528693675994873", "False"], ["-2.778693675994873", "False"], ["-7.278693675994873", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a204fadfecbdf340fa510dfa5b7d37cd11790831b0067411a91304605046b13a", "prompt_hash": "e64298d724015433a715c1e8cf57673ea2893b9c526bb62527eccfe177b909bd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 959, "doc": {"id": "6bd176cc91a2a2088807ec446c008856", "question": "where is a good place to obtain new soap?", "question_concept": "soap", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["supermarket", "washing", "cabinet", "own home", "sink"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: where is a good place to obtain new soap?\nA. supermarket\nB. washing\nC. cabinet\nD. own home\nE. sink\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: where is a good place to obtain new soap?\nA. supermarket\nB. washing\nC. cabinet\nD. own home\nE. sink\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: where is a good place to obtain new soap?\nA. supermarket\nB. washing\nC. cabinet\nD. own home\nE. sink\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: where is a good place to obtain new soap?\nA. supermarket\nB. washing\nC. cabinet\nD. own home\nE. sink\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: where is a good place to obtain new soap?\nA. supermarket\nB. washing\nC. cabinet\nD. own home\nE. sink\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7358013987541199", "True"]], [["-6.7358012199401855", "False"]], [["-9.235801696777344", "False"]], [["-9.235801696777344", "False"]], [["-10.235801696777344", "False"]]], "filtered_resps": [["-0.7358013987541199", "True"], ["-6.7358012199401855", "False"], ["-9.235801696777344", "False"], ["-9.235801696777344", "False"], ["-10.235801696777344", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "09fe7ee7382541c7afc7e4f102c3013eb29e22d994656b0353bb81dfafeca5d6", "prompt_hash": "562946f38172ab50f88230c8f91e78b8b858334558582efb34a8beb3ac1d1a5b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 960, "doc": {"id": "c3890d43b84635d9e61c007ca2521d5b", "question": "What do people do for food?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["talk to each other", "complete job", "wear hats", "kill animals", "believe in god"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What do people do for food?\nA. talk to each other\nB. complete job\nC. wear hats\nD. kill animals\nE. believe in god\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people do for food?\nA. talk to each other\nB. complete job\nC. wear hats\nD. kill animals\nE. believe in god\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people do for food?\nA. talk to each other\nB. complete job\nC. wear hats\nD. kill animals\nE. believe in god\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people do for food?\nA. talk to each other\nB. complete job\nC. wear hats\nD. kill animals\nE. believe in god\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people do for food?\nA. talk to each other\nB. complete job\nC. wear hats\nD. kill animals\nE. believe in god\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.161764144897461", "False"]], [["-1.1617640256881714", "True"]], [["-6.411764144897461", "False"]], [["-4.911764144897461", "False"]], [["-6.411764144897461", "False"]]], "filtered_resps": [["-4.161764144897461", "False"], ["-1.1617640256881714", "True"], ["-6.411764144897461", "False"], ["-4.911764144897461", "False"], ["-6.411764144897461", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "83b3c69d5c503d8a1f754e4a61ec47231cb4c8e3f8070ee38c69ffc0283974cd", "prompt_hash": "c9418597c2777b92b3a14ef0a207e7f51e64552fea99adff8d18e76de9c6a102", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 961, "doc": {"id": "6195ed74cf445cb5d991e1076a080dde", "question": "There was many a bottle to choose from behind the cashier where?", "question_concept": "bottle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["grocery store", "diaper bag", "gas station", "liquor store", "medicine cabinet"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: There was many a bottle to choose from behind the cashier where?\nA. grocery store\nB. diaper bag\nC. gas station\nD. liquor store\nE. medicine cabinet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: There was many a bottle to choose from behind the cashier where?\nA. grocery store\nB. diaper bag\nC. gas station\nD. liquor store\nE. medicine cabinet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: There was many a bottle to choose from behind the cashier where?\nA. grocery store\nB. diaper bag\nC. gas station\nD. liquor store\nE. medicine cabinet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: There was many a bottle to choose from behind the cashier where?\nA. grocery store\nB. diaper bag\nC. gas station\nD. liquor store\nE. medicine cabinet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: There was many a bottle to choose from behind the cashier where?\nA. grocery store\nB. diaper bag\nC. gas station\nD. liquor store\nE. medicine cabinet\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4587186574935913", "True"]], [["-5.208718776702881", "False"]], [["-4.708718776702881", "False"]], [["-1.9587186574935913", "False"]], [["-8.708718299865723", "False"]]], "filtered_resps": [["-1.4587186574935913", "True"], ["-5.208718776702881", "False"], ["-4.708718776702881", "False"], ["-1.9587186574935913", "False"], ["-8.708718299865723", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1c8b4bcf6da48d72948024cf7f0be9f65920d95f18825f69eab2a1258ab56211", "prompt_hash": "fd6998c345b3f667d2f6224d0292409ae397014c5ed39d6f3ae7473b71e0be82", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 962, "doc": {"id": "37644422df4bcd28b3f54bbf3fc2c0f8", "question": "They had to know where to go, they got on the national highway after consulting the what?", "question_concept": "national highway", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["canada", "atlas", "united states", "major cities", "book"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: They had to know where to go, they got on the national highway after consulting the what?\nA. canada\nB. atlas\nC. united states\nD. major cities\nE. book\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They had to know where to go, they got on the national highway after consulting the what?\nA. canada\nB. atlas\nC. united states\nD. major cities\nE. book\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They had to know where to go, they got on the national highway after consulting the what?\nA. canada\nB. atlas\nC. united states\nD. major cities\nE. book\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They had to know where to go, they got on the national highway after consulting the what?\nA. canada\nB. atlas\nC. united states\nD. major cities\nE. book\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They had to know where to go, they got on the national highway after consulting the what?\nA. canada\nB. atlas\nC. united states\nD. major cities\nE. book\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.904936790466309", "False"]], [["-1.9049365520477295", "False"]], [["-10.404936790466309", "False"]], [["-10.654936790466309", "False"]], [["-10.404936790466309", "False"]]], "filtered_resps": [["-4.904936790466309", "False"], ["-1.9049365520477295", "False"], ["-10.404936790466309", "False"], ["-10.654936790466309", "False"], ["-10.404936790466309", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2403b4a8be2750bac1999434ce2da7f6a6bb0618906c94bd7430510e05c39762", "prompt_hash": "addf85a3b1ec6948322b7bc9bcdf1de7e4970f3b12548ecb9e23f77a7b16a0b1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 963, "doc": {"id": "23d97480fe45bace231503f8fc367a5b", "question": "What do professors primarily do?", "question_concept": "professors", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["master physics", "state facts", "wear wrinkled tweed jackets", "school students", "teach courses"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do professors primarily do?\nA. master physics\nB. state facts\nC. wear wrinkled tweed jackets\nD. school students\nE. teach courses\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do professors primarily do?\nA. master physics\nB. state facts\nC. wear wrinkled tweed jackets\nD. school students\nE. teach courses\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do professors primarily do?\nA. master physics\nB. state facts\nC. wear wrinkled tweed jackets\nD. school students\nE. teach courses\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do professors primarily do?\nA. master physics\nB. state facts\nC. wear wrinkled tweed jackets\nD. school students\nE. teach courses\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do professors primarily do?\nA. master physics\nB. state facts\nC. wear wrinkled tweed jackets\nD. school students\nE. teach courses\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.095623016357422", "False"]], [["-4.845623016357422", "False"]], [["-6.595623016357422", "False"]], [["-5.595623016357422", "False"]], [["-0.845623254776001", "True"]]], "filtered_resps": [["-5.095623016357422", "False"], ["-4.845623016357422", "False"], ["-6.595623016357422", "False"], ["-5.595623016357422", "False"], ["-0.845623254776001", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ccc40fed42dabb2e46111aa46a3a4b9355240f6711a2bb7f18c1c9ab94faa791", "prompt_hash": "20b962a09528c13163e6d4a05625c80c88b9fac30a839316e2d4ae9310af6c59", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 964, "doc": {"id": "15556e26feaa5a8a29c9f30896e535d4", "question": "Where do you throw a ball at pins?", "question_concept": "ball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bowling alley", "football stadium", "soccer field", "sporting event", "sporting goods store"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you throw a ball at pins?\nA. bowling alley\nB. football stadium\nC. soccer field\nD. sporting event\nE. sporting goods store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you throw a ball at pins?\nA. bowling alley\nB. football stadium\nC. soccer field\nD. sporting event\nE. sporting goods store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you throw a ball at pins?\nA. bowling alley\nB. football stadium\nC. soccer field\nD. sporting event\nE. sporting goods store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you throw a ball at pins?\nA. bowling alley\nB. football stadium\nC. soccer field\nD. sporting event\nE. sporting goods store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you throw a ball at pins?\nA. bowling alley\nB. football stadium\nC. soccer field\nD. sporting event\nE. sporting goods store\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.3829050660133362", "True"]], [["-4.632905006408691", "False"]], [["-8.382905006408691", "False"]], [["-9.382905006408691", "False"]], [["-11.382905006408691", "False"]]], "filtered_resps": [["-0.3829050660133362", "True"], ["-4.632905006408691", "False"], ["-8.382905006408691", "False"], ["-9.382905006408691", "False"], ["-11.382905006408691", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e3f5b6e5b4c7d5154496a5e8f6ba6f1b5341a34f9fafac996e9acf42a80215f6", "prompt_hash": "05cbcfd894d3978775f9bb502859ddb5543d3b65ea2b38de02351b84080fcdfd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 965, "doc": {"id": "6be05d227f4f6fe727218fc8be9df340", "question": "What might you need to do cleaning?", "question_concept": "cleaning", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sing a song", "neatness", "allergies", "healthy living", "using water"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What might you need to do cleaning?\nA. sing a song\nB. neatness\nC. allergies\nD. healthy living\nE. using water\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might you need to do cleaning?\nA. sing a song\nB. neatness\nC. allergies\nD. healthy living\nE. using water\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might you need to do cleaning?\nA. sing a song\nB. neatness\nC. allergies\nD. healthy living\nE. using water\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might you need to do cleaning?\nA. sing a song\nB. neatness\nC. allergies\nD. healthy living\nE. using water\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might you need to do cleaning?\nA. sing a song\nB. neatness\nC. allergies\nD. healthy living\nE. using water\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.38688325881958", "False"]], [["-2.38688325881958", "False"]], [["-2.13688325881958", "False"]], [["-4.38688325881958", "False"]], [["-1.3868833780288696", "True"]]], "filtered_resps": [["-4.38688325881958", "False"], ["-2.38688325881958", "False"], ["-2.13688325881958", "False"], ["-4.38688325881958", "False"], ["-1.3868833780288696", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1c2380e2719425ace2184af30f9886e59d069bcb88fe0df1abc8e2a5be405acf", "prompt_hash": "e9780065bcbe7dae05a29b9d7bc280d19182dc2d5022f8ffc4a212dccd977f05", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 966, "doc": {"id": "3f3ba1d9a3bfe63df11247a968eaddce", "question": "If i were to spit a lot without noticing i may have extra what?", "question_concept": "spitting", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["phlegm", "saliva nd mouth", "disease", "germs", "spittle"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If i were to spit a lot without noticing i may have extra what?\nA. phlegm\nB. saliva nd mouth\nC. disease\nD. germs\nE. spittle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If i were to spit a lot without noticing i may have extra what?\nA. phlegm\nB. saliva nd mouth\nC. disease\nD. germs\nE. spittle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If i were to spit a lot without noticing i may have extra what?\nA. phlegm\nB. saliva nd mouth\nC. disease\nD. germs\nE. spittle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If i were to spit a lot without noticing i may have extra what?\nA. phlegm\nB. saliva nd mouth\nC. disease\nD. germs\nE. spittle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If i were to spit a lot without noticing i may have extra what?\nA. phlegm\nB. saliva nd mouth\nC. disease\nD. germs\nE. spittle\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8597469329833984", "False"]], [["-3.8597469329833984", "False"]], [["-4.109746932983398", "False"]], [["-3.6097469329833984", "False"]], [["-5.359746932983398", "False"]]], "filtered_resps": [["-3.8597469329833984", "False"], ["-3.8597469329833984", "False"], ["-4.109746932983398", "False"], ["-3.6097469329833984", "False"], ["-5.359746932983398", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "693c0e7775464dd7782bc06e47fae48282a9c22a169f052403c5a0e9c5a99626", "prompt_hash": "34b51b9c1e0633a581db71246ff7ac343c68840c4ba833649949d4190336c538", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 967, "doc": {"id": "ca9a3ccfb140aa66816f96ac983b6d9f_1", "question": "If student got a list of supplies from class like paper and pencils, their parent would have to go where?", "question_concept": "pencils", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["classroom", "parking garage", "store", "backpack", "cabinet"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If student got a list of supplies from class like paper and pencils, their parent would have to go where?\nA. classroom\nB. parking garage\nC. store\nD. backpack\nE. cabinet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If student got a list of supplies from class like paper and pencils, their parent would have to go where?\nA. classroom\nB. parking garage\nC. store\nD. backpack\nE. cabinet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If student got a list of supplies from class like paper and pencils, their parent would have to go where?\nA. classroom\nB. parking garage\nC. store\nD. backpack\nE. cabinet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If student got a list of supplies from class like paper and pencils, their parent would have to go where?\nA. classroom\nB. parking garage\nC. store\nD. backpack\nE. cabinet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If student got a list of supplies from class like paper and pencils, their parent would have to go where?\nA. classroom\nB. parking garage\nC. store\nD. backpack\nE. cabinet\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1620616912841797", "False"]], [["-5.66206169128418", "False"]], [["-1.4120615720748901", "True"]], [["-6.91206169128418", "False"]], [["-7.16206169128418", "False"]]], "filtered_resps": [["-2.1620616912841797", "False"], ["-5.66206169128418", "False"], ["-1.4120615720748901", "True"], ["-6.91206169128418", "False"], ["-7.16206169128418", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a37a60a330f3c381311ddc3f1fa39897343cf8dc6178549bbe445821229604b8", "prompt_hash": "bed3437ba83fa3cb58ef5baae8c376b18026e9fa76da6b5ff0fbc4a4a94cff89", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 968, "doc": {"id": "487cabfcd776d89748ee7e7bb681ad59", "question": "Why do young people swallow semen ?", "question_concept": "swallow semen", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["you're into", "prostitute", "you're curious", "curiosity", "heterosexual woman in love"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Why do young people swallow semen ?\nA. you're into\nB. prostitute\nC. you're curious\nD. curiosity\nE. heterosexual woman in love\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why do young people swallow semen ?\nA. you're into\nB. prostitute\nC. you're curious\nD. curiosity\nE. heterosexual woman in love\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why do young people swallow semen ?\nA. you're into\nB. prostitute\nC. you're curious\nD. curiosity\nE. heterosexual woman in love\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why do young people swallow semen ?\nA. you're into\nB. prostitute\nC. you're curious\nD. curiosity\nE. heterosexual woman in love\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why do young people swallow semen ?\nA. you're into\nB. prostitute\nC. you're curious\nD. curiosity\nE. heterosexual woman in love\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.172189235687256", "False"]], [["-6.422189235687256", "False"]], [["-3.672189235687256", "False"]], [["-2.922189235687256", "False"]], [["-2.422189235687256", "False"]]], "filtered_resps": [["-4.172189235687256", "False"], ["-6.422189235687256", "False"], ["-3.672189235687256", "False"], ["-2.922189235687256", "False"], ["-2.422189235687256", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "be2c89787bceb3144b39ec2ae54afdd75c917472a14c16eaf827b832cb0a17f3", "prompt_hash": "9031121e919f2464e5bfa8541cdfb9a6888cae1a382b01dc1abf407dde0f0b2b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 969, "doc": {"id": "6915dfdefe3b1cd5fd8886c8bb84929a", "question": "Sally was standing in queue.  The line was very, very slow.  What was she feeling?", "question_concept": "standing in queue", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["frustration", "delays", "being annoyed", "moving forward", "progress"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Sally was standing in queue.  The line was very, very slow.  What was she feeling?\nA. frustration\nB. delays\nC. being annoyed\nD. moving forward\nE. progress\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sally was standing in queue.  The line was very, very slow.  What was she feeling?\nA. frustration\nB. delays\nC. being annoyed\nD. moving forward\nE. progress\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sally was standing in queue.  The line was very, very slow.  What was she feeling?\nA. frustration\nB. delays\nC. being annoyed\nD. moving forward\nE. progress\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sally was standing in queue.  The line was very, very slow.  What was she feeling?\nA. frustration\nB. delays\nC. being annoyed\nD. moving forward\nE. progress\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sally was standing in queue.  The line was very, very slow.  What was she feeling?\nA. frustration\nB. delays\nC. being annoyed\nD. moving forward\nE. progress\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.372544288635254", "True"]], [["-5.622544288635254", "False"]], [["-3.622544288635254", "False"]], [["-7.872544288635254", "False"]], [["-11.372544288635254", "False"]]], "filtered_resps": [["-1.372544288635254", "True"], ["-5.622544288635254", "False"], ["-3.622544288635254", "False"], ["-7.872544288635254", "False"], ["-11.372544288635254", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "77f29d46fc9d97647924f679c7fb70c27166f2369e47b65511ff2e722e7142a7", "prompt_hash": "180c56fc2e6248dbd9e01a80d467a65d2aa6d1e38b4acd9368e3bae73fe423d6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 970, "doc": {"id": "ec224c1dbfb569cce7ec317fe987ae68", "question": "What is the animal trying to accomplish?", "question_concept": "animal", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sand trap", "live long", "leave home", "feel pain", "eating"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is the animal trying to accomplish?\nA. sand trap\nB. live long\nC. leave home\nD. feel pain\nE. eating\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the animal trying to accomplish?\nA. sand trap\nB. live long\nC. leave home\nD. feel pain\nE. eating\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the animal trying to accomplish?\nA. sand trap\nB. live long\nC. leave home\nD. feel pain\nE. eating\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the animal trying to accomplish?\nA. sand trap\nB. live long\nC. leave home\nD. feel pain\nE. eating\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the animal trying to accomplish?\nA. sand trap\nB. live long\nC. leave home\nD. feel pain\nE. eating\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6696276664733887", "False"]], [["-2.4196276664733887", "False"]], [["-2.9196276664733887", "False"]], [["-3.6696276664733887", "False"]], [["-2.6696276664733887", "False"]]], "filtered_resps": [["-3.6696276664733887", "False"], ["-2.4196276664733887", "False"], ["-2.9196276664733887", "False"], ["-3.6696276664733887", "False"], ["-2.6696276664733887", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bfc058c73fa6694af070394df068c0f87745cfe0024aa59645d148d251ae9a01", "prompt_hash": "99b97e41b9108a867ff7e0ea92a17ace5ca24751b387edc8eac9dce734ce17c0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 971, "doc": {"id": "0cba8ddda21e29c8c53482e131d741cd", "question": "James and Holly went dancing together. As they danced, he  pressed himself against her what?", "question_concept": "dancing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["euphoria", "moving body", "rhythmic movement", "happiness", "fatigue"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: James and Holly went dancing together. As they danced, he  pressed himself against her what?\nA. euphoria\nB. moving body\nC. rhythmic movement\nD. happiness\nE. fatigue\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James and Holly went dancing together. As they danced, he  pressed himself against her what?\nA. euphoria\nB. moving body\nC. rhythmic movement\nD. happiness\nE. fatigue\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James and Holly went dancing together. As they danced, he  pressed himself against her what?\nA. euphoria\nB. moving body\nC. rhythmic movement\nD. happiness\nE. fatigue\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James and Holly went dancing together. As they danced, he  pressed himself against her what?\nA. euphoria\nB. moving body\nC. rhythmic movement\nD. happiness\nE. fatigue\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James and Holly went dancing together. As they danced, he  pressed himself against her what?\nA. euphoria\nB. moving body\nC. rhythmic movement\nD. happiness\nE. fatigue\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.7325639724731445", "False"]], [["-1.732564091682434", "False"]], [["-4.4825639724731445", "False"]], [["-7.4825639724731445", "False"]], [["-8.232563972473145", "False"]]], "filtered_resps": [["-5.7325639724731445", "False"], ["-1.732564091682434", "False"], ["-4.4825639724731445", "False"], ["-7.4825639724731445", "False"], ["-8.232563972473145", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "112edce621cf93de13ee1d0aa97d26c38d0783522d9a3063d42d51d6f5638e9b", "prompt_hash": "1b51ec36225bc7869728cbade48cfb2cb657316b3cc4f769800bb358e6299fc6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 972, "doc": {"id": "e65559cd9f5d96b577caeb78d9033502", "question": "If a house has a subscription, what likely shows up in the driveway every morning?", "question_concept": "house", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["subdivision", "newspaper", "street", "laundry mat", "surface of earth"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If a house has a subscription, what likely shows up in the driveway every morning?\nA. subdivision\nB. newspaper\nC. street\nD. laundry mat\nE. surface of earth\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a house has a subscription, what likely shows up in the driveway every morning?\nA. subdivision\nB. newspaper\nC. street\nD. laundry mat\nE. surface of earth\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a house has a subscription, what likely shows up in the driveway every morning?\nA. subdivision\nB. newspaper\nC. street\nD. laundry mat\nE. surface of earth\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a house has a subscription, what likely shows up in the driveway every morning?\nA. subdivision\nB. newspaper\nC. street\nD. laundry mat\nE. surface of earth\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a house has a subscription, what likely shows up in the driveway every morning?\nA. subdivision\nB. newspaper\nC. street\nD. laundry mat\nE. surface of earth\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.109431505203247", "False"]], [["-1.109431505203247", "True"]], [["-6.609431266784668", "False"]], [["-5.609431266784668", "False"]], [["-8.609431266784668", "False"]]], "filtered_resps": [["-3.109431505203247", "False"], ["-1.109431505203247", "True"], ["-6.609431266784668", "False"], ["-5.609431266784668", "False"], ["-8.609431266784668", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7c4c46459595f56315765e856ef09090e51070339be37d39719e680cd992373f", "prompt_hash": "672e90719efb8be16e491272a36538aa3292b6e3716191ad6ebc0f5e2889d35d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 973, "doc": {"id": "b8937a30f25093910c040f4e63e1d352", "question": "What does a person do when they feel dirty?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feel lucky", "cross street", "wash themselves", "eat", "wonder what happened"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What does a person do when they feel dirty?\nA. feel lucky\nB. cross street\nC. wash themselves\nD. eat\nE. wonder what happened\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a person do when they feel dirty?\nA. feel lucky\nB. cross street\nC. wash themselves\nD. eat\nE. wonder what happened\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a person do when they feel dirty?\nA. feel lucky\nB. cross street\nC. wash themselves\nD. eat\nE. wonder what happened\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a person do when they feel dirty?\nA. feel lucky\nB. cross street\nC. wash themselves\nD. eat\nE. wonder what happened\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a person do when they feel dirty?\nA. feel lucky\nB. cross street\nC. wash themselves\nD. eat\nE. wonder what happened\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.767563819885254", "False"]], [["-8.267563819885254", "False"]], [["-1.017563819885254", "True"]], [["-7.517563819885254", "False"]], [["-9.017563819885254", "False"]]], "filtered_resps": [["-3.767563819885254", "False"], ["-8.267563819885254", "False"], ["-1.017563819885254", "True"], ["-7.517563819885254", "False"], ["-9.017563819885254", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "be006dc09cd0ad103532377d123fea867d5ad535edddd276aa0cec0ec8d454d7", "prompt_hash": "901b15e75cab1d38690c2809afaf50e66782281f80b3a03577a5bd312f640d14", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 974, "doc": {"id": "aabe8eb218468fc63b6c9aa6d428c951", "question": "After the weight cut he was worried about his energy levels, but this was part of participating in a what?", "question_concept": "energy", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["work", "wrestle", "play sports", "matter", "sleep"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: After the weight cut he was worried about his energy levels, but this was part of participating in a what?\nA. work\nB. wrestle\nC. play sports\nD. matter\nE. sleep\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: After the weight cut he was worried about his energy levels, but this was part of participating in a what?\nA. work\nB. wrestle\nC. play sports\nD. matter\nE. sleep\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: After the weight cut he was worried about his energy levels, but this was part of participating in a what?\nA. work\nB. wrestle\nC. play sports\nD. matter\nE. sleep\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: After the weight cut he was worried about his energy levels, but this was part of participating in a what?\nA. work\nB. wrestle\nC. play sports\nD. matter\nE. sleep\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: After the weight cut he was worried about his energy levels, but this was part of participating in a what?\nA. work\nB. wrestle\nC. play sports\nD. matter\nE. sleep\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.056936264038086", "False"]], [["-1.806936502456665", "False"]], [["-5.306936264038086", "False"]], [["-5.306936264038086", "False"]], [["-7.056936264038086", "False"]]], "filtered_resps": [["-5.056936264038086", "False"], ["-1.806936502456665", "False"], ["-5.306936264038086", "False"], ["-5.306936264038086", "False"], ["-7.056936264038086", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fe9149db8ac0b6e668148f8baf147545722062f3c5838861a2016a3a4eee0511", "prompt_hash": "a39582fec01fa41cf2e28709e14c94c169eafed6c04591a0f12ba668c9fa5a05", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 975, "doc": {"id": "43ba9669564217f2f909f33acbedaf95", "question": "what does a person do to stay healthy?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fever", "eat every day", "excited", "headache", "expressive"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: what does a person do to stay healthy?\nA. fever\nB. eat every day\nC. excited\nD. headache\nE. expressive\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: what does a person do to stay healthy?\nA. fever\nB. eat every day\nC. excited\nD. headache\nE. expressive\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: what does a person do to stay healthy?\nA. fever\nB. eat every day\nC. excited\nD. headache\nE. expressive\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: what does a person do to stay healthy?\nA. fever\nB. eat every day\nC. excited\nD. headache\nE. expressive\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: what does a person do to stay healthy?\nA. fever\nB. eat every day\nC. excited\nD. headache\nE. expressive\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.572618007659912", "False"]], [["-1.072618007659912", "True"]], [["-8.07261848449707", "False"]], [["-8.82261848449707", "False"]], [["-5.322618007659912", "False"]]], "filtered_resps": [["-3.572618007659912", "False"], ["-1.072618007659912", "True"], ["-8.07261848449707", "False"], ["-8.82261848449707", "False"], ["-5.322618007659912", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5a73369fbe4af1edb66053bc2d17373d7554e576985e80e3dcd730282c02e80d", "prompt_hash": "079e6cc8752b13092c7e999e5210a2f809912bbb93713f2acc35eee7703758bd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 976, "doc": {"id": "2b9b625c788584b8d41f1a74d740e126", "question": "Who is the guard here for?", "question_concept": "guard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["man post", "attack", "intimidation", "prisoner", "unprotected"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Who is the guard here for?\nA. man post\nB. attack\nC. intimidation\nD. prisoner\nE. unprotected\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Who is the guard here for?\nA. man post\nB. attack\nC. intimidation\nD. prisoner\nE. unprotected\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Who is the guard here for?\nA. man post\nB. attack\nC. intimidation\nD. prisoner\nE. unprotected\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Who is the guard here for?\nA. man post\nB. attack\nC. intimidation\nD. prisoner\nE. unprotected\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Who is the guard here for?\nA. man post\nB. attack\nC. intimidation\nD. prisoner\nE. unprotected\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.660581111907959", "False"]], [["-6.160581111907959", "False"]], [["-6.660581111907959", "False"]], [["-2.160581111907959", "False"]], [["-7.660581111907959", "False"]]], "filtered_resps": [["-2.660581111907959", "False"], ["-6.160581111907959", "False"], ["-6.660581111907959", "False"], ["-2.160581111907959", "False"], ["-7.660581111907959", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cf5c93a74dea3f0aca9abbfe272fefd165b9aded6c2fe7f1982ad7a87ed758e0", "prompt_hash": "495f22187307cd07db2ae66ce57be871206b653ff5dbd83fe941f0ca7f3e04eb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 977, "doc": {"id": "eb6807290df71b040e2c7bcc5d11fdea", "question": "If a person stutters when he experiences anxiety or excitement, he'll have difficult doing what?", "question_concept": "excitement", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["express information", "dance", "library", "go somewhere", "study"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If a person stutters when he experiences anxiety or excitement, he'll have difficult doing what?\nA. express information\nB. dance\nC. library\nD. go somewhere\nE. study\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a person stutters when he experiences anxiety or excitement, he'll have difficult doing what?\nA. express information\nB. dance\nC. library\nD. go somewhere\nE. study\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a person stutters when he experiences anxiety or excitement, he'll have difficult doing what?\nA. express information\nB. dance\nC. library\nD. go somewhere\nE. study\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a person stutters when he experiences anxiety or excitement, he'll have difficult doing what?\nA. express information\nB. dance\nC. library\nD. go somewhere\nE. study\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a person stutters when he experiences anxiety or excitement, he'll have difficult doing what?\nA. express information\nB. dance\nC. library\nD. go somewhere\nE. study\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0124493837356567", "True"]], [["-5.762449264526367", "False"]], [["-7.512449264526367", "False"]], [["-6.012449264526367", "False"]], [["-9.262449264526367", "False"]]], "filtered_resps": [["-1.0124493837356567", "True"], ["-5.762449264526367", "False"], ["-7.512449264526367", "False"], ["-6.012449264526367", "False"], ["-9.262449264526367", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "92e6ef6ba60d4a142c10e0030c32d9ce20f65a0a66c9dc598b5f56fcfcb245f1", "prompt_hash": "7139ec3e124c9457ceeaa91087d4fd17f8ff44383b9e832cf97e85b716af38ee", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 978, "doc": {"id": "f06852fb4bb2764dc208a991d037f211", "question": "Where can you keep letter opener when it likely to be needed soon?", "question_concept": "letter opener", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["office supply store", "stationery store", "dek", "martyr's chest", "refrigerator"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you keep letter opener when it likely to be needed soon?\nA. office supply store\nB. stationery store\nC. dek\nD. martyr's chest\nE. refrigerator\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you keep letter opener when it likely to be needed soon?\nA. office supply store\nB. stationery store\nC. dek\nD. martyr's chest\nE. refrigerator\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you keep letter opener when it likely to be needed soon?\nA. office supply store\nB. stationery store\nC. dek\nD. martyr's chest\nE. refrigerator\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you keep letter opener when it likely to be needed soon?\nA. office supply store\nB. stationery store\nC. dek\nD. martyr's chest\nE. refrigerator\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you keep letter opener when it likely to be needed soon?\nA. office supply store\nB. stationery store\nC. dek\nD. martyr's chest\nE. refrigerator\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.659180998802185", "True"]], [["-5.409181118011475", "False"]], [["-2.4091811180114746", "False"]], [["-7.159181118011475", "False"]], [["-4.659181118011475", "False"]]], "filtered_resps": [["-1.659180998802185", "True"], ["-5.409181118011475", "False"], ["-2.4091811180114746", "False"], ["-7.159181118011475", "False"], ["-4.659181118011475", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c55df18d8cb5523c0f91df6d05bd60ed3fefbefba6063f5ade3d87d2e78c546f", "prompt_hash": "ab6f58ca0312a79b28e868a5d3c4d0ea4a2826f0ea6f4e1fcb4e6946b88afbfc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 979, "doc": {"id": "5efadabaf61b5174916e3ab659bcd283", "question": "Danny found that the carpet did not ,match the drapes, which was disappointing, because this place was expensive.  But it was the only place in town that wasn't booked solid for the week and he needed it while he was in town, so he couldn't complain.   Where might this place be?", "question_concept": "carpet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["brothel", "restaurant", "building", "bowling alley", "at hotel"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Danny found that the carpet did not ,match the drapes, which was disappointing, because this place was expensive.  But it was the only place in town that wasn't booked solid for the week and he needed it while he was in town, so he couldn't complain.   Where might this place be?\nA. brothel\nB. restaurant\nC. building\nD. bowling alley\nE. at hotel\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Danny found that the carpet did not ,match the drapes, which was disappointing, because this place was expensive.  But it was the only place in town that wasn't booked solid for the week and he needed it while he was in town, so he couldn't complain.   Where might this place be?\nA. brothel\nB. restaurant\nC. building\nD. bowling alley\nE. at hotel\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Danny found that the carpet did not ,match the drapes, which was disappointing, because this place was expensive.  But it was the only place in town that wasn't booked solid for the week and he needed it while he was in town, so he couldn't complain.   Where might this place be?\nA. brothel\nB. restaurant\nC. building\nD. bowling alley\nE. at hotel\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Danny found that the carpet did not ,match the drapes, which was disappointing, because this place was expensive.  But it was the only place in town that wasn't booked solid for the week and he needed it while he was in town, so he couldn't complain.   Where might this place be?\nA. brothel\nB. restaurant\nC. building\nD. bowling alley\nE. at hotel\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Danny found that the carpet did not ,match the drapes, which was disappointing, because this place was expensive.  But it was the only place in town that wasn't booked solid for the week and he needed it while he was in town, so he couldn't complain.   Where might this place be?\nA. brothel\nB. restaurant\nC. building\nD. bowling alley\nE. at hotel\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.60568904876709", "False"]], [["-5.10568904876709", "False"]], [["-4.85568904876709", "False"]], [["-6.10568904876709", "False"]], [["-1.6056891679763794", "False"]]], "filtered_resps": [["-3.60568904876709", "False"], ["-5.10568904876709", "False"], ["-4.85568904876709", "False"], ["-6.10568904876709", "False"], ["-1.6056891679763794", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e482d78ec613d1cdc3c4067ecd63e68e0ac89d4c774c0bf4709f73742de2dbaf", "prompt_hash": "46b6f7fcad2431ff186bd64b3d2e6de42f5742254afa3492807a402f00c5e6b7", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 980, "doc": {"id": "e9d4c747018ff81b8c0aefb5abc3c539", "question": "What do people need to do to change their lives?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["face problems", "better themselves", "pay bills", "become disillusioned", "eat chicken"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do people need to do to change their lives?\nA. face problems\nB. better themselves\nC. pay bills\nD. become disillusioned\nE. eat chicken\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people need to do to change their lives?\nA. face problems\nB. better themselves\nC. pay bills\nD. become disillusioned\nE. eat chicken\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people need to do to change their lives?\nA. face problems\nB. better themselves\nC. pay bills\nD. become disillusioned\nE. eat chicken\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people need to do to change their lives?\nA. face problems\nB. better themselves\nC. pay bills\nD. become disillusioned\nE. eat chicken\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people need to do to change their lives?\nA. face problems\nB. better themselves\nC. pay bills\nD. become disillusioned\nE. eat chicken\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.199094772338867", "False"]], [["-1.199094533920288", "True"]], [["-7.699094772338867", "False"]], [["-7.699094772338867", "False"]], [["-6.199094772338867", "False"]]], "filtered_resps": [["-5.199094772338867", "False"], ["-1.199094533920288", "True"], ["-7.699094772338867", "False"], ["-7.699094772338867", "False"], ["-6.199094772338867", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6d7cb9c2f59e106189a7af6c502aa396c01cb8df09d9b5897e2a8af0a3d9aa32", "prompt_hash": "49ae91f2f4943e70d2e238f78e7b6d5cfccbd07291337a4ee97a30ef0092a396", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 981, "doc": {"id": "30a8cfd186f1aae5acd425a52d058863", "question": "Humans need shelter to survive.  They usually find shelter where?", "question_concept": "human", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["underpass", "homes", "workplace", "school", "space shuttle"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Humans need shelter to survive.  They usually find shelter where?\nA. underpass\nB. homes\nC. workplace\nD. school\nE. space shuttle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Humans need shelter to survive.  They usually find shelter where?\nA. underpass\nB. homes\nC. workplace\nD. school\nE. space shuttle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Humans need shelter to survive.  They usually find shelter where?\nA. underpass\nB. homes\nC. workplace\nD. school\nE. space shuttle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Humans need shelter to survive.  They usually find shelter where?\nA. underpass\nB. homes\nC. workplace\nD. school\nE. space shuttle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Humans need shelter to survive.  They usually find shelter where?\nA. underpass\nB. homes\nC. workplace\nD. school\nE. space shuttle\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.2730913162231445", "False"]], [["-1.523091197013855", "False"]], [["-8.773091316223145", "False"]], [["-8.523091316223145", "False"]], [["-8.773091316223145", "False"]]], "filtered_resps": [["-6.2730913162231445", "False"], ["-1.523091197013855", "False"], ["-8.773091316223145", "False"], ["-8.523091316223145", "False"], ["-8.773091316223145", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "49f510113df9494ff14e8bf1ce358bc1244564c93f0bdbc380462b1cd1e3c7c5", "prompt_hash": "bf4c71d0605451df37b1be6d45ec5e643582a6d92b1eb716aa6aa5040ed6bc14", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 982, "doc": {"id": "9e7805871c8a276300a89fe910a90949", "question": "Someone who had a very bad flight might be given a trip in this to make up for it?", "question_concept": "bad", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["first class", "propitious", "reputable", "one", "sufficient"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Someone who had a very bad flight might be given a trip in this to make up for it?\nA. first class\nB. propitious\nC. reputable\nD. one\nE. sufficient\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Someone who had a very bad flight might be given a trip in this to make up for it?\nA. first class\nB. propitious\nC. reputable\nD. one\nE. sufficient\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Someone who had a very bad flight might be given a trip in this to make up for it?\nA. first class\nB. propitious\nC. reputable\nD. one\nE. sufficient\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Someone who had a very bad flight might be given a trip in this to make up for it?\nA. first class\nB. propitious\nC. reputable\nD. one\nE. sufficient\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Someone who had a very bad flight might be given a trip in this to make up for it?\nA. first class\nB. propitious\nC. reputable\nD. one\nE. sufficient\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1858928203582764", "True"]], [["-5.9358930587768555", "False"]], [["-7.4358930587768555", "False"]], [["-6.9358930587768555", "False"]], [["-7.4358930587768555", "False"]]], "filtered_resps": [["-1.1858928203582764", "True"], ["-5.9358930587768555", "False"], ["-7.4358930587768555", "False"], ["-6.9358930587768555", "False"], ["-7.4358930587768555", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7ff31e59df8f0bd254f0e4434114141573836889bc50a452f0f8bc30a1c800f3", "prompt_hash": "b7c262db6f68e4999b2f116ae511f841b7be77132d2be7ee07713101e7b164d3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 983, "doc": {"id": "047c2d8c65d297b39aa42821c1ca76a9", "question": "Nature can be good and bad for the person who walks, what are some things?", "question_concept": "hike", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["seeing bear", "see beautiful views", "get wet", "getting lost", "murdered by a landshark"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Nature can be good and bad for the person who walks, what are some things?\nA. seeing bear\nB. see beautiful views\nC. get wet\nD. getting lost\nE. murdered by a landshark\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Nature can be good and bad for the person who walks, what are some things?\nA. seeing bear\nB. see beautiful views\nC. get wet\nD. getting lost\nE. murdered by a landshark\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Nature can be good and bad for the person who walks, what are some things?\nA. seeing bear\nB. see beautiful views\nC. get wet\nD. getting lost\nE. murdered by a landshark\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Nature can be good and bad for the person who walks, what are some things?\nA. seeing bear\nB. see beautiful views\nC. get wet\nD. getting lost\nE. murdered by a landshark\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Nature can be good and bad for the person who walks, what are some things?\nA. seeing bear\nB. see beautiful views\nC. get wet\nD. getting lost\nE. murdered by a landshark\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9928083419799805", "False"]], [["-1.742808222770691", "False"]], [["-4.7428083419799805", "False"]], [["-6.2428083419799805", "False"]], [["-5.7428083419799805", "False"]]], "filtered_resps": [["-2.9928083419799805", "False"], ["-1.742808222770691", "False"], ["-4.7428083419799805", "False"], ["-6.2428083419799805", "False"], ["-5.7428083419799805", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "df1697c21d1adad7e3f6f2d4fc4e3b7681dd253e778f8b7f849f05cb19f562c5", "prompt_hash": "0bfab52db7a638d4a0a75e20befc7ffb253e10c2e85c760c4141746c4a7bee1c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 984, "doc": {"id": "0bed77da54b6c54facd0ee6614aad72e", "question": "Jim decided to lose weight.  He thought that exercise is the best way to lose weight because you can't get rid of what?", "question_concept": "exercise", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["need for food", "fitness", "sweating", "fastfood", "thirst"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Jim decided to lose weight.  He thought that exercise is the best way to lose weight because you can't get rid of what?\nA. need for food\nB. fitness\nC. sweating\nD. fastfood\nE. thirst\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Jim decided to lose weight.  He thought that exercise is the best way to lose weight because you can't get rid of what?\nA. need for food\nB. fitness\nC. sweating\nD. fastfood\nE. thirst\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Jim decided to lose weight.  He thought that exercise is the best way to lose weight because you can't get rid of what?\nA. need for food\nB. fitness\nC. sweating\nD. fastfood\nE. thirst\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Jim decided to lose weight.  He thought that exercise is the best way to lose weight because you can't get rid of what?\nA. need for food\nB. fitness\nC. sweating\nD. fastfood\nE. thirst\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Jim decided to lose weight.  He thought that exercise is the best way to lose weight because you can't get rid of what?\nA. need for food\nB. fitness\nC. sweating\nD. fastfood\nE. thirst\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.280570149421692", "True"]], [["-5.030570030212402", "False"]], [["-5.780570030212402", "False"]], [["-5.530570030212402", "False"]], [["-4.030570030212402", "False"]]], "filtered_resps": [["-1.280570149421692", "True"], ["-5.030570030212402", "False"], ["-5.780570030212402", "False"], ["-5.530570030212402", "False"], ["-4.030570030212402", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "68a8aab342de5bdf14ba172dcb5f70b4d1eef73f3ae49cd13cabd3df056fc0dd", "prompt_hash": "c1189834dc857894d964e930f38d0c3d71efff83e08850388cc3806642ffd6c8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 985, "doc": {"id": "32e2adee67aace0a98c830fb39463015", "question": "Nature creates more beautiful structures than those that are what?", "question_concept": "nature", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["artificial", "indoors", "city", "man made", "eat cake"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Nature creates more beautiful structures than those that are what?\nA. artificial\nB. indoors\nC. city\nD. man made\nE. eat cake\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Nature creates more beautiful structures than those that are what?\nA. artificial\nB. indoors\nC. city\nD. man made\nE. eat cake\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Nature creates more beautiful structures than those that are what?\nA. artificial\nB. indoors\nC. city\nD. man made\nE. eat cake\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Nature creates more beautiful structures than those that are what?\nA. artificial\nB. indoors\nC. city\nD. man made\nE. eat cake\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Nature creates more beautiful structures than those that are what?\nA. artificial\nB. indoors\nC. city\nD. man made\nE. eat cake\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7420388460159302", "True"]], [["-6.492038726806641", "False"]], [["-6.242038726806641", "False"]], [["-2.2420387268066406", "False"]], [["-6.742038726806641", "False"]]], "filtered_resps": [["-1.7420388460159302", "True"], ["-6.492038726806641", "False"], ["-6.242038726806641", "False"], ["-2.2420387268066406", "False"], ["-6.742038726806641", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "38edb3851feaef6e7d2613f0b545ac4df93d1a90fa9f5cef121534d8667d21d4", "prompt_hash": "c5c08d7a864a42941da959fea997c329e66620b7d9cffb3903c23c7a17100581", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 986, "doc": {"id": "8272f08792b873885f93d4c148e307e5", "question": "The water in clouds turn in to what when it gets cold?", "question_concept": "water", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["typhoon", "snowflake", "laddle", "teardrops", "sink"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The water in clouds turn in to what when it gets cold?\nA. typhoon\nB. snowflake\nC. laddle\nD. teardrops\nE. sink\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The water in clouds turn in to what when it gets cold?\nA. typhoon\nB. snowflake\nC. laddle\nD. teardrops\nE. sink\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The water in clouds turn in to what when it gets cold?\nA. typhoon\nB. snowflake\nC. laddle\nD. teardrops\nE. sink\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The water in clouds turn in to what when it gets cold?\nA. typhoon\nB. snowflake\nC. laddle\nD. teardrops\nE. sink\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The water in clouds turn in to what when it gets cold?\nA. typhoon\nB. snowflake\nC. laddle\nD. teardrops\nE. sink\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.2323198318481445", "False"]], [["-1.9823198318481445", "False"]], [["-4.9823198318481445", "False"]], [["-4.9823198318481445", "False"]], [["-6.7323198318481445", "False"]]], "filtered_resps": [["-6.2323198318481445", "False"], ["-1.9823198318481445", "False"], ["-4.9823198318481445", "False"], ["-4.9823198318481445", "False"], ["-6.7323198318481445", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "13a62887c1be6aba6d3bfe145386bb95f723ce7bff16fa86604fafbb2f3be957", "prompt_hash": "5def9b55c3017196cd0eee342e961089b2c45911fc3ff125a0eabf5d9545b8e4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 987, "doc": {"id": "bc05bc6b4df7a3d25a361515fe8912ad", "question": "What southern U.S. state is know for having many swamps?", "question_concept": "swamp", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wetlands", "new york", "michigan", "louisiana", "river delta"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What southern U.S. state is know for having many swamps?\nA. wetlands\nB. new york\nC. michigan\nD. louisiana\nE. river delta\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What southern U.S. state is know for having many swamps?\nA. wetlands\nB. new york\nC. michigan\nD. louisiana\nE. river delta\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What southern U.S. state is know for having many swamps?\nA. wetlands\nB. new york\nC. michigan\nD. louisiana\nE. river delta\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What southern U.S. state is know for having many swamps?\nA. wetlands\nB. new york\nC. michigan\nD. louisiana\nE. river delta\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What southern U.S. state is know for having many swamps?\nA. wetlands\nB. new york\nC. michigan\nD. louisiana\nE. river delta\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.000767707824707", "False"]], [["-9.250767707824707", "False"]], [["-8.750767707824707", "False"]], [["-2.000767707824707", "False"]], [["-10.750767707824707", "False"]]], "filtered_resps": [["-6.000767707824707", "False"], ["-9.250767707824707", "False"], ["-8.750767707824707", "False"], ["-2.000767707824707", "False"], ["-10.750767707824707", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3c9397d434d032740dc87f371d7cab6624abbe78bd8a07689c49f76d40a6febf", "prompt_hash": "73c5353713e79512fa8f624b7619165667cfde53d7763925b91524153c84412c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 988, "doc": {"id": "b893a6e7a2b172bd71f03c9dbee4f960", "question": "When going to sleep what happens to your body?", "question_concept": "going to sleep", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["snoring", "latency", "dreams", "relaxation", "dreaming"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When going to sleep what happens to your body?\nA. snoring\nB. latency\nC. dreams\nD. relaxation\nE. dreaming\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When going to sleep what happens to your body?\nA. snoring\nB. latency\nC. dreams\nD. relaxation\nE. dreaming\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When going to sleep what happens to your body?\nA. snoring\nB. latency\nC. dreams\nD. relaxation\nE. dreaming\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When going to sleep what happens to your body?\nA. snoring\nB. latency\nC. dreams\nD. relaxation\nE. dreaming\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When going to sleep what happens to your body?\nA. snoring\nB. latency\nC. dreams\nD. relaxation\nE. dreaming\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.735905647277832", "False"]], [["-3.235905647277832", "False"]], [["-6.485905647277832", "False"]], [["-2.235905647277832", "False"]], [["-6.485905647277832", "False"]]], "filtered_resps": [["-4.735905647277832", "False"], ["-3.235905647277832", "False"], ["-6.485905647277832", "False"], ["-2.235905647277832", "False"], ["-6.485905647277832", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7be8c52b95e8e504e0d19fe8d99dcd482f3aa2756e674081764d1a20140ad8ca", "prompt_hash": "e78fe609b2a758cca00e983fb6edbad877a05d7702d0a41672d825c3fd932db7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 989, "doc": {"id": "cf8e30dd6956d03e3f0f0397112a8696", "question": "Where is a monkey likely to enjoy being?", "question_concept": "monkey", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["banana tree", "sailor suit", "theatre", "mulberry bush", "research laboratory"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a monkey likely to enjoy being?\nA. banana tree\nB. sailor suit\nC. theatre\nD. mulberry bush\nE. research laboratory\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a monkey likely to enjoy being?\nA. banana tree\nB. sailor suit\nC. theatre\nD. mulberry bush\nE. research laboratory\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a monkey likely to enjoy being?\nA. banana tree\nB. sailor suit\nC. theatre\nD. mulberry bush\nE. research laboratory\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a monkey likely to enjoy being?\nA. banana tree\nB. sailor suit\nC. theatre\nD. mulberry bush\nE. research laboratory\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a monkey likely to enjoy being?\nA. banana tree\nB. sailor suit\nC. theatre\nD. mulberry bush\nE. research laboratory\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0616647005081177", "True"]], [["-6.561664581298828", "False"]], [["-5.811664581298828", "False"]], [["-5.811664581298828", "False"]], [["-6.561664581298828", "False"]]], "filtered_resps": [["-1.0616647005081177", "True"], ["-6.561664581298828", "False"], ["-5.811664581298828", "False"], ["-5.811664581298828", "False"], ["-6.561664581298828", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "36d139524cdac14fa4cc08dc08f0a368373cf36d1d5fb165e32465f4048c0edc", "prompt_hash": "f6b764ea3eb7c6460688f93cb896ac3116925695a44dde47db097d9a11070d5d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 990, "doc": {"id": "159d50e325b59c6d29ec371500e173b4", "question": "What is a form of anaerobic exercising?", "question_concept": "exercising", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shortness of breath", "lift weights", "error", "fall down", "run"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is a form of anaerobic exercising?\nA. shortness of breath\nB. lift weights\nC. error\nD. fall down\nE. run\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a form of anaerobic exercising?\nA. shortness of breath\nB. lift weights\nC. error\nD. fall down\nE. run\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a form of anaerobic exercising?\nA. shortness of breath\nB. lift weights\nC. error\nD. fall down\nE. run\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a form of anaerobic exercising?\nA. shortness of breath\nB. lift weights\nC. error\nD. fall down\nE. run\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a form of anaerobic exercising?\nA. shortness of breath\nB. lift weights\nC. error\nD. fall down\nE. run\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7562596797943115", "False"]], [["-1.0062596797943115", "True"]], [["-7.756259918212891", "False"]], [["-9.50625991821289", "False"]], [["-5.256259918212891", "False"]]], "filtered_resps": [["-2.7562596797943115", "False"], ["-1.0062596797943115", "True"], ["-7.756259918212891", "False"], ["-9.50625991821289", "False"], ["-5.256259918212891", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ef0de78306c3f7279072b104885e0c4958e31f293043dbeb1e2b4c060a710361", "prompt_hash": "3cb002f1efd100abd60d095ec98f98401a10d358e55787a9743742d75dab432b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 991, "doc": {"id": "17eafc807b198236faf06a66f4c05313", "question": "The earth is one planet in what?", "question_concept": "earth", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tree", "orbit", "solar system", "fotograph", "dreams"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The earth is one planet in what?\nA. tree\nB. orbit\nC. solar system\nD. fotograph\nE. dreams\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The earth is one planet in what?\nA. tree\nB. orbit\nC. solar system\nD. fotograph\nE. dreams\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The earth is one planet in what?\nA. tree\nB. orbit\nC. solar system\nD. fotograph\nE. dreams\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The earth is one planet in what?\nA. tree\nB. orbit\nC. solar system\nD. fotograph\nE. dreams\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The earth is one planet in what?\nA. tree\nB. orbit\nC. solar system\nD. fotograph\nE. dreams\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.696974754333496", "False"]], [["-6.696974754333496", "False"]], [["-1.4469746351242065", "False"]], [["-9.696974754333496", "False"]], [["-7.696974754333496", "False"]]], "filtered_resps": [["-6.696974754333496", "False"], ["-6.696974754333496", "False"], ["-1.4469746351242065", "False"], ["-9.696974754333496", "False"], ["-7.696974754333496", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7734a60a8f842e24498976c810463f796566a5b6c0144648d106121a48f23bf8", "prompt_hash": "12fda50d7e1b28447800a1f854d03b213ddd74ac2b5d5381e12e43a21a3ca839", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 992, "doc": {"id": "24eebfa678112100803da16dde148b2d", "question": "Where would you put a container can after you buy it?", "question_concept": "container can", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pantry", "store", "gas", "liquid", "garage"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put a container can after you buy it?\nA. pantry\nB. store\nC. gas\nD. liquid\nE. garage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put a container can after you buy it?\nA. pantry\nB. store\nC. gas\nD. liquid\nE. garage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put a container can after you buy it?\nA. pantry\nB. store\nC. gas\nD. liquid\nE. garage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put a container can after you buy it?\nA. pantry\nB. store\nC. gas\nD. liquid\nE. garage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put a container can after you buy it?\nA. pantry\nB. store\nC. gas\nD. liquid\nE. garage\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7117190957069397", "True"]], [["-5.211719036102295", "False"]], [["-6.711719036102295", "False"]], [["-7.711719036102295", "False"]], [["-4.461719036102295", "False"]]], "filtered_resps": [["-0.7117190957069397", "True"], ["-5.211719036102295", "False"], ["-6.711719036102295", "False"], ["-7.711719036102295", "False"], ["-4.461719036102295", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "74dc3a351960db4bad190701e22804d6d60cb61f44863454c881d016d946316b", "prompt_hash": "763d2ee4d1aabcb3ba9e67caf3a4e8c3a56e5c2ad7300abd64acd0651ca060c1", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 993, "doc": {"id": "ec882fc3a9bfaeae2a26fe31c2ef2c07", "question": "Where did you meet your best friend since Kindergarten?", "question_concept": "friends", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["friend's house", "school", "fraternity house", "internet cafe", "airplane"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where did you meet your best friend since Kindergarten?\nA. friend's house\nB. school\nC. fraternity house\nD. internet cafe\nE. airplane\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where did you meet your best friend since Kindergarten?\nA. friend's house\nB. school\nC. fraternity house\nD. internet cafe\nE. airplane\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where did you meet your best friend since Kindergarten?\nA. friend's house\nB. school\nC. fraternity house\nD. internet cafe\nE. airplane\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where did you meet your best friend since Kindergarten?\nA. friend's house\nB. school\nC. fraternity house\nD. internet cafe\nE. airplane\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where did you meet your best friend since Kindergarten?\nA. friend's house\nB. school\nC. fraternity house\nD. internet cafe\nE. airplane\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.218867301940918", "False"]], [["-0.7188673615455627", "True"]], [["-6.718867301940918", "False"]], [["-7.218867301940918", "False"]], [["-9.468867301940918", "False"]]], "filtered_resps": [["-4.218867301940918", "False"], ["-0.7188673615455627", "True"], ["-6.718867301940918", "False"], ["-7.218867301940918", "False"], ["-9.468867301940918", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "27b52f038ab2c5737635b01c2f8f60ee828f501052926a055e4723221379a6b0", "prompt_hash": "ea79297d713ffa80890a1f1d8e00e82dd231444048baac836bcad74e167334b7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 994, "doc": {"id": "0a006d16d9042e0c170935e5fbf7f9af", "question": "James was below the balloon.  He watched it rise.  What direction did he look in?", "question_concept": "below", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["upstairs", "aloft", "diagonal", "upstream", "upwards"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: James was below the balloon.  He watched it rise.  What direction did he look in?\nA. upstairs\nB. aloft\nC. diagonal\nD. upstream\nE. upwards\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James was below the balloon.  He watched it rise.  What direction did he look in?\nA. upstairs\nB. aloft\nC. diagonal\nD. upstream\nE. upwards\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James was below the balloon.  He watched it rise.  What direction did he look in?\nA. upstairs\nB. aloft\nC. diagonal\nD. upstream\nE. upwards\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James was below the balloon.  He watched it rise.  What direction did he look in?\nA. upstairs\nB. aloft\nC. diagonal\nD. upstream\nE. upwards\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James was below the balloon.  He watched it rise.  What direction did he look in?\nA. upstairs\nB. aloft\nC. diagonal\nD. upstream\nE. upwards\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.682342529296875", "False"]], [["-2.682342529296875", "False"]], [["-8.182342529296875", "False"]], [["-7.682342529296875", "False"]], [["-1.9323424100875854", "False"]]], "filtered_resps": [["-4.682342529296875", "False"], ["-2.682342529296875", "False"], ["-8.182342529296875", "False"], ["-7.682342529296875", "False"], ["-1.9323424100875854", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "434d0397d5828aa5784131a6118bae532c0afc15cb6f7246a24b1388cb2c07dd", "prompt_hash": "18b6ae797f9a18d4ecbde15bebf40a171a8153909b0ceb38bfc2468c553b0f20", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 995, "doc": {"id": "d33a81660058e570a18fb2eafa284a78", "question": "John and Tim like playing. It makes them what?", "question_concept": "playing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feeling happy", "learning", "injury", "burn", "get hungry"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: John and Tim like playing. It makes them what?\nA. feeling happy\nB. learning\nC. injury\nD. burn\nE. get hungry\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John and Tim like playing. It makes them what?\nA. feeling happy\nB. learning\nC. injury\nD. burn\nE. get hungry\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John and Tim like playing. It makes them what?\nA. feeling happy\nB. learning\nC. injury\nD. burn\nE. get hungry\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John and Tim like playing. It makes them what?\nA. feeling happy\nB. learning\nC. injury\nD. burn\nE. get hungry\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John and Tim like playing. It makes them what?\nA. feeling happy\nB. learning\nC. injury\nD. burn\nE. get hungry\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8635347485542297", "True"]], [["-3.863534688949585", "False"]], [["-5.613534927368164", "False"]], [["-7.863534927368164", "False"]], [["-9.363534927368164", "False"]]], "filtered_resps": [["-0.8635347485542297", "True"], ["-3.863534688949585", "False"], ["-5.613534927368164", "False"], ["-7.863534927368164", "False"], ["-9.363534927368164", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a83e8309feceee7ee367a6683f14bee3a867202560fb373585775774dc2ed612", "prompt_hash": "e976e619f58cb4787e65151eb81ab97171d457b193c4a55192c9173d7db9c0c5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 996, "doc": {"id": "1e09c3136a743b862e783700b7667028", "question": "What could happen if someone is seeing new presents at a birthday party?", "question_concept": "seeing new", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["envy", "jealousy", "education", "fear", "excitement"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What could happen if someone is seeing new presents at a birthday party?\nA. envy\nB. jealousy\nC. education\nD. fear\nE. excitement\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could happen if someone is seeing new presents at a birthday party?\nA. envy\nB. jealousy\nC. education\nD. fear\nE. excitement\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could happen if someone is seeing new presents at a birthday party?\nA. envy\nB. jealousy\nC. education\nD. fear\nE. excitement\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could happen if someone is seeing new presents at a birthday party?\nA. envy\nB. jealousy\nC. education\nD. fear\nE. excitement\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could happen if someone is seeing new presents at a birthday party?\nA. envy\nB. jealousy\nC. education\nD. fear\nE. excitement\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.392688035964966", "False"]], [["-2.392688035964966", "False"]], [["-6.392687797546387", "False"]], [["-7.142687797546387", "False"]], [["-1.3926880359649658", "True"]]], "filtered_resps": [["-2.392688035964966", "False"], ["-2.392688035964966", "False"], ["-6.392687797546387", "False"], ["-7.142687797546387", "False"], ["-1.3926880359649658", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6f8cb4f3663a28798fa6598f01454026bf2ec09590a690770de06ce1c90bb48c", "prompt_hash": "7259f0080359b81bccd902d09ef522bea4d218da0c594a7711d849085e5e70ed", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 997, "doc": {"id": "5e851c47682bdf79ec7c139ecf124c9a", "question": "Joe's cat smelled something delicious and jumped into this, causing him to panic and fear for its life. Where might it have jumped?", "question_concept": "cat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["meat loaf", "bedroom", "microwave", "living room", "floor"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Joe's cat smelled something delicious and jumped into this, causing him to panic and fear for its life. Where might it have jumped?\nA. meat loaf\nB. bedroom\nC. microwave\nD. living room\nE. floor\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Joe's cat smelled something delicious and jumped into this, causing him to panic and fear for its life. Where might it have jumped?\nA. meat loaf\nB. bedroom\nC. microwave\nD. living room\nE. floor\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Joe's cat smelled something delicious and jumped into this, causing him to panic and fear for its life. Where might it have jumped?\nA. meat loaf\nB. bedroom\nC. microwave\nD. living room\nE. floor\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Joe's cat smelled something delicious and jumped into this, causing him to panic and fear for its life. Where might it have jumped?\nA. meat loaf\nB. bedroom\nC. microwave\nD. living room\nE. floor\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Joe's cat smelled something delicious and jumped into this, causing him to panic and fear for its life. Where might it have jumped?\nA. meat loaf\nB. bedroom\nC. microwave\nD. living room\nE. floor\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.84779691696167", "False"]], [["-4.09779691696167", "False"]], [["-1.3477970361709595", "True"]], [["-6.34779691696167", "False"]], [["-6.84779691696167", "False"]]], "filtered_resps": [["-2.84779691696167", "False"], ["-4.09779691696167", "False"], ["-1.3477970361709595", "True"], ["-6.34779691696167", "False"], ["-6.84779691696167", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "af24c9e195254b530aebf1995b36980bee6c2d4b7e31306ba539e02f81e18e2a", "prompt_hash": "4c6a331c16daa53aed8d00aa5ad3b229cd828309fa49257cc888eadc12c3d29a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 998, "doc": {"id": "b148f18fb8b5a504b67078ef6ac29717", "question": "Why would a person put flowers in a room with dirty gym socks?", "question_concept": "flowers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["continue to grow", "plant themselves", "many colors", "smell good", "make pretty"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Why would a person put flowers in a room with dirty gym socks?\nA. continue to grow\nB. plant themselves\nC. many colors\nD. smell good\nE. make pretty\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would a person put flowers in a room with dirty gym socks?\nA. continue to grow\nB. plant themselves\nC. many colors\nD. smell good\nE. make pretty\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would a person put flowers in a room with dirty gym socks?\nA. continue to grow\nB. plant themselves\nC. many colors\nD. smell good\nE. make pretty\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would a person put flowers in a room with dirty gym socks?\nA. continue to grow\nB. plant themselves\nC. many colors\nD. smell good\nE. make pretty\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would a person put flowers in a room with dirty gym socks?\nA. continue to grow\nB. plant themselves\nC. many colors\nD. smell good\nE. make pretty\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1337804794311523", "False"]], [["-5.633780479431152", "False"]], [["-5.383780479431152", "False"]], [["-1.8837804794311523", "True"]], [["-4.633780479431152", "False"]]], "filtered_resps": [["-2.1337804794311523", "False"], ["-5.633780479431152", "False"], ["-5.383780479431152", "False"], ["-1.8837804794311523", "True"], ["-4.633780479431152", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a3c6a8a76816a8c1b2edc77cbd1b11935bad54ca9c36888ec6075404dad77dbb", "prompt_hash": "bf4c943343d09a2a63bc450b5d4af036dc1c582da8292cd34f95a45e9f0b0ed6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 999, "doc": {"id": "b6bbe013995fdb5def3d504319af0791", "question": "The table wasn't level.  some parts were higher and some were lower with no rhyme or reason.   It was very what?", "question_concept": "level", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["electrical circuit", "build evenly", "uneven", "unbalanced", "tilted"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The table wasn't level.  some parts were higher and some were lower with no rhyme or reason.   It was very what?\nA. electrical circuit\nB. build evenly\nC. uneven\nD. unbalanced\nE. tilted\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The table wasn't level.  some parts were higher and some were lower with no rhyme or reason.   It was very what?\nA. electrical circuit\nB. build evenly\nC. uneven\nD. unbalanced\nE. tilted\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The table wasn't level.  some parts were higher and some were lower with no rhyme or reason.   It was very what?\nA. electrical circuit\nB. build evenly\nC. uneven\nD. unbalanced\nE. tilted\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The table wasn't level.  some parts were higher and some were lower with no rhyme or reason.   It was very what?\nA. electrical circuit\nB. build evenly\nC. uneven\nD. unbalanced\nE. tilted\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The table wasn't level.  some parts were higher and some were lower with no rhyme or reason.   It was very what?\nA. electrical circuit\nB. build evenly\nC. uneven\nD. unbalanced\nE. tilted\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.333744049072266", "False"]], [["-5.583744049072266", "False"]], [["-1.0837441682815552", "True"]], [["-6.583744049072266", "False"]], [["-7.833744049072266", "False"]]], "filtered_resps": [["-5.333744049072266", "False"], ["-5.583744049072266", "False"], ["-1.0837441682815552", "True"], ["-6.583744049072266", "False"], ["-7.833744049072266", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "863c0410f8906f8e90c8f9a68d4d5af5f0aa465d6f5787e5870feb358246c42b", "prompt_hash": "20a84e633c7b5b5daccb170e6801dbe47ea4467cada1dacff424095d4c59358d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1000, "doc": {"id": "0c2fa15a02d0b6ca6707e98fac7589e4", "question": "The person signed up for home insurance, what is he seeking?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["financial security", "live well", "good relationship", "compliments", "discounted furniture"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The person signed up for home insurance, what is he seeking?\nA. financial security\nB. live well\nC. good relationship\nD. compliments\nE. discounted furniture\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The person signed up for home insurance, what is he seeking?\nA. financial security\nB. live well\nC. good relationship\nD. compliments\nE. discounted furniture\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The person signed up for home insurance, what is he seeking?\nA. financial security\nB. live well\nC. good relationship\nD. compliments\nE. discounted furniture\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The person signed up for home insurance, what is he seeking?\nA. financial security\nB. live well\nC. good relationship\nD. compliments\nE. discounted furniture\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The person signed up for home insurance, what is he seeking?\nA. financial security\nB. live well\nC. good relationship\nD. compliments\nE. discounted furniture\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6702290177345276", "True"]], [["-7.670228958129883", "False"]], [["-9.670228958129883", "False"]], [["-9.170228958129883", "False"]], [["-10.920228958129883", "False"]]], "filtered_resps": [["-0.6702290177345276", "True"], ["-7.670228958129883", "False"], ["-9.670228958129883", "False"], ["-9.170228958129883", "False"], ["-10.920228958129883", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5fbdf5c582370c2d971fc48a960c1c67bd78951ace1655de3f483a06792a8fca", "prompt_hash": "b137017f1b206919db5c7127467d09cad9d6e5bd53f45b7410cac1c88fea5fd0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1001, "doc": {"id": "a656e74a943f9e2698a25bbcfb4e96db", "question": "James know that committing murder was wrong, but he thought that he could get away with it.  He was really troubled  and fearful because of what?", "question_concept": "committing murder", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["happiness", "problems", "prosecution", "distress", "misery"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: James know that committing murder was wrong, but he thought that he could get away with it.  He was really troubled  and fearful because of what?\nA. happiness\nB. problems\nC. prosecution\nD. distress\nE. misery\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James know that committing murder was wrong, but he thought that he could get away with it.  He was really troubled  and fearful because of what?\nA. happiness\nB. problems\nC. prosecution\nD. distress\nE. misery\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James know that committing murder was wrong, but he thought that he could get away with it.  He was really troubled  and fearful because of what?\nA. happiness\nB. problems\nC. prosecution\nD. distress\nE. misery\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James know that committing murder was wrong, but he thought that he could get away with it.  He was really troubled  and fearful because of what?\nA. happiness\nB. problems\nC. prosecution\nD. distress\nE. misery\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James know that committing murder was wrong, but he thought that he could get away with it.  He was really troubled  and fearful because of what?\nA. happiness\nB. problems\nC. prosecution\nD. distress\nE. misery\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.519159317016602", "False"]], [["-4.769159317016602", "False"]], [["-2.5191590785980225", "False"]], [["-1.5191590785980225", "True"]], [["-8.269159317016602", "False"]]], "filtered_resps": [["-5.519159317016602", "False"], ["-4.769159317016602", "False"], ["-2.5191590785980225", "False"], ["-1.5191590785980225", "True"], ["-8.269159317016602", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3463e69ea404396944ada041d6505d26cf04de21b9934fa164cfc35a196c595e", "prompt_hash": "563be98779b4608683e933a4e6c9ef62e2f96203e93eb8b673e87bf2bd4e1ab6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1002, "doc": {"id": "8086f022f2d4a4888ae1f8c7e4541ab9", "question": "How can someone die from eating hamburger?", "question_concept": "eating hamburger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["gas", "getting full", "mad cow disease", "death", "feel full"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: How can someone die from eating hamburger?\nA. gas\nB. getting full\nC. mad cow disease\nD. death\nE. feel full\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How can someone die from eating hamburger?\nA. gas\nB. getting full\nC. mad cow disease\nD. death\nE. feel full\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How can someone die from eating hamburger?\nA. gas\nB. getting full\nC. mad cow disease\nD. death\nE. feel full\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How can someone die from eating hamburger?\nA. gas\nB. getting full\nC. mad cow disease\nD. death\nE. feel full\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How can someone die from eating hamburger?\nA. gas\nB. getting full\nC. mad cow disease\nD. death\nE. feel full\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.693299293518066", "False"]], [["-7.693299293518066", "False"]], [["-0.4432995319366455", "True"]], [["-5.943299293518066", "False"]], [["-3.6932995319366455", "False"]]], "filtered_resps": [["-4.693299293518066", "False"], ["-7.693299293518066", "False"], ["-0.4432995319366455", "True"], ["-5.943299293518066", "False"], ["-3.6932995319366455", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "de73cd49458c406cd9f94688c4acd70d92e53180637fa6374ee44396b9b0a019", "prompt_hash": "4fd14cb4864f1e749e020cfd6c86172b10409ffbcd91d261f000b65777197760", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1003, "doc": {"id": "5655a3002dd9a6b7dabede1dd26a5893", "question": "Where would using a boat not require navigation skills?", "question_concept": "boat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["water", "ocean", "garage", "harbor", "river"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would using a boat not require navigation skills?\nA. water\nB. ocean\nC. garage\nD. harbor\nE. river\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would using a boat not require navigation skills?\nA. water\nB. ocean\nC. garage\nD. harbor\nE. river\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would using a boat not require navigation skills?\nA. water\nB. ocean\nC. garage\nD. harbor\nE. river\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would using a boat not require navigation skills?\nA. water\nB. ocean\nC. garage\nD. harbor\nE. river\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would using a boat not require navigation skills?\nA. water\nB. ocean\nC. garage\nD. harbor\nE. river\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8553221225738525", "False"]], [["-6.105321884155273", "False"]], [["-0.6053220629692078", "True"]], [["-8.105321884155273", "False"]], [["-8.355321884155273", "False"]]], "filtered_resps": [["-3.8553221225738525", "False"], ["-6.105321884155273", "False"], ["-0.6053220629692078", "True"], ["-8.105321884155273", "False"], ["-8.355321884155273", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "505b9f7f1c0048b563840e6e6c106e59ad9fbab7fe4f456c8c442380267ea069", "prompt_hash": "2a2f2760d65d8e9c0f8b68cf9a803da1f7e12dff32ac76449f4bc30433665203", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1004, "doc": {"id": "17d9bfaee1efac51b1ca240125bc5977", "question": "What does a self assured person often do?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["acknowledgment", "focused", "know what time", "feel important", "trust himself"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What does a self assured person often do?\nA. acknowledgment\nB. focused\nC. know what time\nD. feel important\nE. trust himself\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a self assured person often do?\nA. acknowledgment\nB. focused\nC. know what time\nD. feel important\nE. trust himself\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a self assured person often do?\nA. acknowledgment\nB. focused\nC. know what time\nD. feel important\nE. trust himself\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a self assured person often do?\nA. acknowledgment\nB. focused\nC. know what time\nD. feel important\nE. trust himself\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a self assured person often do?\nA. acknowledgment\nB. focused\nC. know what time\nD. feel important\nE. trust himself\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2287726402282715", "True"]], [["-3.4787726402282715", "False"]], [["-6.2287726402282715", "False"]], [["-5.7287726402282715", "False"]], [["-1.9787726402282715", "False"]]], "filtered_resps": [["-1.2287726402282715", "True"], ["-3.4787726402282715", "False"], ["-6.2287726402282715", "False"], ["-5.7287726402282715", "False"], ["-1.9787726402282715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ab45145bb28a0e3db0dd9a90eab84329cdb67f188a4e78a0111a445a52a401c4", "prompt_hash": "dc7e28eef1bb6f344c18a28d043972aaa8c9bf5b4a9fdeec12b58acc720ae729", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1005, "doc": {"id": "801431167b8bff06b9870abe9721536b", "question": "He was very outgoing, for him making friends was no personal what?", "question_concept": "making friends", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["scary", "having friends", "good feeling", "conflict", "friendship"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He was very outgoing, for him making friends was no personal what?\nA. scary\nB. having friends\nC. good feeling\nD. conflict\nE. friendship\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was very outgoing, for him making friends was no personal what?\nA. scary\nB. having friends\nC. good feeling\nD. conflict\nE. friendship\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was very outgoing, for him making friends was no personal what?\nA. scary\nB. having friends\nC. good feeling\nD. conflict\nE. friendship\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was very outgoing, for him making friends was no personal what?\nA. scary\nB. having friends\nC. good feeling\nD. conflict\nE. friendship\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was very outgoing, for him making friends was no personal what?\nA. scary\nB. having friends\nC. good feeling\nD. conflict\nE. friendship\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.3234477043151855", "False"]], [["-3.3234477043151855", "False"]], [["-2.5734477043151855", "False"]], [["-3.8234477043151855", "False"]], [["-3.8234477043151855", "False"]]], "filtered_resps": [["-3.3234477043151855", "False"], ["-3.3234477043151855", "False"], ["-2.5734477043151855", "False"], ["-3.8234477043151855", "False"], ["-3.8234477043151855", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "21fc7613fa94eb99f3397bc0f31e2a186d786c318302afab8561fdea81ba25ac", "prompt_hash": "81412cbd460d9952d3a48ff755203b609d015e7806ed280a8cb200ade24017fa", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1006, "doc": {"id": "85ebdd4f1a3c2ac900eee8e75e48ccaa", "question": "What do you feel when giving assistance to the needy?", "question_concept": "giving assistance", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["reward", "boredom", "pleasure", "happiness", "satisfaction"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What do you feel when giving assistance to the needy?\nA. reward\nB. boredom\nC. pleasure\nD. happiness\nE. satisfaction\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you feel when giving assistance to the needy?\nA. reward\nB. boredom\nC. pleasure\nD. happiness\nE. satisfaction\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you feel when giving assistance to the needy?\nA. reward\nB. boredom\nC. pleasure\nD. happiness\nE. satisfaction\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you feel when giving assistance to the needy?\nA. reward\nB. boredom\nC. pleasure\nD. happiness\nE. satisfaction\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you feel when giving assistance to the needy?\nA. reward\nB. boredom\nC. pleasure\nD. happiness\nE. satisfaction\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8613805770874023", "False"]], [["-6.111380577087402", "False"]], [["-3.6113805770874023", "False"]], [["-3.6113805770874023", "False"]], [["-1.861380696296692", "True"]]], "filtered_resps": [["-2.8613805770874023", "False"], ["-6.111380577087402", "False"], ["-3.6113805770874023", "False"], ["-3.6113805770874023", "False"], ["-1.861380696296692", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "898fec2bbc82351fd4efa8ee72d1c0415b2d889c8cdd8835d967e0940734136f", "prompt_hash": "73a1a150742eb20f3652455f56f9173515ba144890b3cf5d37f68c7a514a0090", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1007, "doc": {"id": "db1eb157671109bbb9113b0f71a6b957", "question": "Paul wants carrots and doesn't need to drive anywhere. He gets them from where?", "question_concept": "carrots", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["refrigerator", "store", "farmer's market", "supermarket", "dryer"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Paul wants carrots and doesn't need to drive anywhere. He gets them from where?\nA. refrigerator\nB. store\nC. farmer's market\nD. supermarket\nE. dryer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Paul wants carrots and doesn't need to drive anywhere. He gets them from where?\nA. refrigerator\nB. store\nC. farmer's market\nD. supermarket\nE. dryer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Paul wants carrots and doesn't need to drive anywhere. He gets them from where?\nA. refrigerator\nB. store\nC. farmer's market\nD. supermarket\nE. dryer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Paul wants carrots and doesn't need to drive anywhere. He gets them from where?\nA. refrigerator\nB. store\nC. farmer's market\nD. supermarket\nE. dryer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Paul wants carrots and doesn't need to drive anywhere. He gets them from where?\nA. refrigerator\nB. store\nC. farmer's market\nD. supermarket\nE. dryer\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.022280693054199", "False"]], [["-3.772280693054199", "False"]], [["-2.022280693054199", "False"]], [["-4.522280693054199", "False"]], [["-6.522280693054199", "False"]]], "filtered_resps": [["-4.022280693054199", "False"], ["-3.772280693054199", "False"], ["-2.022280693054199", "False"], ["-4.522280693054199", "False"], ["-6.522280693054199", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "36a40a9e98640395c2e945d1192219d918820e92332747bdec1589293b29d0c0", "prompt_hash": "1d4b11eb6a2eaaf33903b1a091f3adea3282646d73e6bd35eb838cebef44d236", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 1008, "doc": {"id": "c02a3c2d4f726b9e1be99533a24a6ab4", "question": "He was a sloppy eater, so where did he leave a mess?", "question_concept": "mess", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sailboat", "desk", "closet", "table", "apartment"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He was a sloppy eater, so where did he leave a mess?\nA. sailboat\nB. desk\nC. closet\nD. table\nE. apartment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He was a sloppy eater, so where did he leave a mess?\nA. sailboat\nB. desk\nC. closet\nD. table\nE. apartment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He was a sloppy eater, so where did he leave a mess?\nA. sailboat\nB. desk\nC. closet\nD. table\nE. apartment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He was a sloppy eater, so where did he leave a mess?\nA. sailboat\nB. desk\nC. closet\nD. table\nE. apartment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He was a sloppy eater, so where did he leave a mess?\nA. sailboat\nB. desk\nC. closet\nD. table\nE. apartment\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.4193010330200195", "False"]], [["-3.6693010330200195", "False"]], [["-8.91930103302002", "False"]], [["-2.1693010330200195", "False"]], [["-6.9193010330200195", "False"]]], "filtered_resps": [["-3.4193010330200195", "False"], ["-3.6693010330200195", "False"], ["-8.91930103302002", "False"], ["-2.1693010330200195", "False"], ["-6.9193010330200195", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "36353384432e593572aa6996e31b9d7f74a3371382ca137163b4ac18f740ac70", "prompt_hash": "a4e4bf088f219503a807078b687359b1c7465631e923527cddfe29ca50b1faf8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1009, "doc": {"id": "3ed6391c539e6daa5b5fdb1b6d5d8ace", "question": "What does every person want?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["life partner", "larger house", "second chances", "money", "headache"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What does every person want?\nA. life partner\nB. larger house\nC. second chances\nD. money\nE. headache\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does every person want?\nA. life partner\nB. larger house\nC. second chances\nD. money\nE. headache\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does every person want?\nA. life partner\nB. larger house\nC. second chances\nD. money\nE. headache\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does every person want?\nA. life partner\nB. larger house\nC. second chances\nD. money\nE. headache\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does every person want?\nA. life partner\nB. larger house\nC. second chances\nD. money\nE. headache\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8883090019226074", "False"]], [["-5.638309001922607", "False"]], [["-2.3883090019226074", "False"]], [["-4.888309001922607", "False"]], [["-3.6383090019226074", "False"]]], "filtered_resps": [["-3.8883090019226074", "False"], ["-5.638309001922607", "False"], ["-2.3883090019226074", "False"], ["-4.888309001922607", "False"], ["-3.6383090019226074", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2bfc0fbd10ad5f0a2fe12ce01bf4898ca58b268955e92aff3e28f36e243b66fc", "prompt_hash": "8e9b31c2282d335387dffd8d12119344efb91b6f55ddac0b6fa6f47f7b6cb3da", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 1010, "doc": {"id": "1db19a32a3edbff9981976dc9ec800ce", "question": "If a small flying animal picks up a string, where are they taking it?", "question_concept": "string", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bird's nest", "park", "guitar", "kite", "quark"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: If a small flying animal picks up a string, where are they taking it?\nA. bird's nest\nB. park\nC. guitar\nD. kite\nE. quark\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a small flying animal picks up a string, where are they taking it?\nA. bird's nest\nB. park\nC. guitar\nD. kite\nE. quark\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a small flying animal picks up a string, where are they taking it?\nA. bird's nest\nB. park\nC. guitar\nD. kite\nE. quark\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a small flying animal picks up a string, where are they taking it?\nA. bird's nest\nB. park\nC. guitar\nD. kite\nE. quark\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a small flying animal picks up a string, where are they taking it?\nA. bird's nest\nB. park\nC. guitar\nD. kite\nE. quark\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0125560760498047", "True"]], [["-4.762556076049805", "False"]], [["-5.262556076049805", "False"]], [["-4.262556076049805", "False"]], [["-7.012556076049805", "False"]]], "filtered_resps": [["-1.0125560760498047", "True"], ["-4.762556076049805", "False"], ["-5.262556076049805", "False"], ["-4.262556076049805", "False"], ["-7.012556076049805", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d2bd3a86c4fb1568d67fd4937b4a5c777e0c951c2750c80df722c8c1da1e3fae", "prompt_hash": "b4f44eb7c03ac61beb1d6b4538758c2d183b9a8eb1c9dfac062dd171a6bd8c5b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1011, "doc": {"id": "1e5a138b4c7d456c37abf4990b402bbe", "question": "He had no issue committing perjury, he had a what that he would get away with it?", "question_concept": "committing perjury", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["confidence", "go to jail", "telling lies", "lying", "manual"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: He had no issue committing perjury, he had a what that he would get away with it?\nA. confidence\nB. go to jail\nC. telling lies\nD. lying\nE. manual\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He had no issue committing perjury, he had a what that he would get away with it?\nA. confidence\nB. go to jail\nC. telling lies\nD. lying\nE. manual\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He had no issue committing perjury, he had a what that he would get away with it?\nA. confidence\nB. go to jail\nC. telling lies\nD. lying\nE. manual\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He had no issue committing perjury, he had a what that he would get away with it?\nA. confidence\nB. go to jail\nC. telling lies\nD. lying\nE. manual\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He had no issue committing perjury, he had a what that he would get away with it?\nA. confidence\nB. go to jail\nC. telling lies\nD. lying\nE. manual\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5597051382064819", "True"]], [["-6.0597052574157715", "False"]], [["-6.0597052574157715", "False"]], [["-6.0597052574157715", "False"]], [["-9.309704780578613", "False"]]], "filtered_resps": [["-0.5597051382064819", "True"], ["-6.0597052574157715", "False"], ["-6.0597052574157715", "False"], ["-6.0597052574157715", "False"], ["-9.309704780578613", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bfdbf1b020114846da2eaab1481617b969215545b9c94cf5f8879338a9c92bb7", "prompt_hash": "14bb9fe82862dce4e46245bfc6cd8c342fcc72f54e97b316e40fc1b0c8090e09", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1012, "doc": {"id": "9402864beae075392d2ee6c10115fc21", "question": "What could go to a tennis court?", "question_concept": "tennis court", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["desert", "college campus", "recreational center", "athletic club", "park"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What could go to a tennis court?\nA. desert\nB. college campus\nC. recreational center\nD. athletic club\nE. park\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could go to a tennis court?\nA. desert\nB. college campus\nC. recreational center\nD. athletic club\nE. park\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could go to a tennis court?\nA. desert\nB. college campus\nC. recreational center\nD. athletic club\nE. park\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could go to a tennis court?\nA. desert\nB. college campus\nC. recreational center\nD. athletic club\nE. park\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could go to a tennis court?\nA. desert\nB. college campus\nC. recreational center\nD. athletic club\nE. park\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.623135566711426", "False"]], [["-3.623135566711426", "False"]], [["-0.8731356263160706", "True"]], [["-2.873135566711426", "False"]], [["-3.873135566711426", "False"]]], "filtered_resps": [["-2.623135566711426", "False"], ["-3.623135566711426", "False"], ["-0.8731356263160706", "True"], ["-2.873135566711426", "False"], ["-3.873135566711426", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f0d44ee49463dba54b9619343e70276194d1ec21bdb9863750a1c5e98395e13c", "prompt_hash": "db377565ed54b68a2572fe055894719f06b65bfdc9c01d72a65246e64aced9d7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1013, "doc": {"id": "25136807f7b2e78b115698daa1677b4a", "question": "What could you use to fill a cup and then drink from it?", "question_concept": "cup", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sand box", "kitchen cabinet", "waterfall", "water fountain", "table"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What could you use to fill a cup and then drink from it?\nA. sand box\nB. kitchen cabinet\nC. waterfall\nD. water fountain\nE. table\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could you use to fill a cup and then drink from it?\nA. sand box\nB. kitchen cabinet\nC. waterfall\nD. water fountain\nE. table\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could you use to fill a cup and then drink from it?\nA. sand box\nB. kitchen cabinet\nC. waterfall\nD. water fountain\nE. table\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could you use to fill a cup and then drink from it?\nA. sand box\nB. kitchen cabinet\nC. waterfall\nD. water fountain\nE. table\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could you use to fill a cup and then drink from it?\nA. sand box\nB. kitchen cabinet\nC. waterfall\nD. water fountain\nE. table\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.070690631866455", "False"]], [["-5.320690631866455", "False"]], [["-4.070690631866455", "False"]], [["-1.320690631866455", "True"]], [["-5.070690631866455", "False"]]], "filtered_resps": [["-3.070690631866455", "False"], ["-5.320690631866455", "False"], ["-4.070690631866455", "False"], ["-1.320690631866455", "True"], ["-5.070690631866455", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "363d9c3864e32c09eb9e1ff0fc9cc840eac63e4538fdc9d4a85736220c9e7f42", "prompt_hash": "42a03bb3c30b70bcdcdefa542de96c9ff3cd0c8d3ff0cd01b0d79a6bb0cabc5e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1014, "doc": {"id": "bc10bf2bfae26a2226823d42956f6cf0", "question": "The two played video games all night in the living room, he enjoyed visiting where?", "question_concept": "living room", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["formal seating", "friend's house", "movies", "home", "apartment"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The two played video games all night in the living room, he enjoyed visiting where?\nA. formal seating\nB. friend's house\nC. movies\nD. home\nE. apartment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The two played video games all night in the living room, he enjoyed visiting where?\nA. formal seating\nB. friend's house\nC. movies\nD. home\nE. apartment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The two played video games all night in the living room, he enjoyed visiting where?\nA. formal seating\nB. friend's house\nC. movies\nD. home\nE. apartment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The two played video games all night in the living room, he enjoyed visiting where?\nA. formal seating\nB. friend's house\nC. movies\nD. home\nE. apartment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The two played video games all night in the living room, he enjoyed visiting where?\nA. formal seating\nB. friend's house\nC. movies\nD. home\nE. apartment\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.785120010375977", "False"]], [["-2.5351202487945557", "False"]], [["-7.035120010375977", "False"]], [["-2.0351202487945557", "False"]], [["-4.285120010375977", "False"]]], "filtered_resps": [["-4.785120010375977", "False"], ["-2.5351202487945557", "False"], ["-7.035120010375977", "False"], ["-2.0351202487945557", "False"], ["-4.285120010375977", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9431386a11a153dd09020d4e3939f17a682faa0b830d79be27301da343869091", "prompt_hash": "1ef39ef8b1352b9a1ddee06f5d41818add392207967ec9bdda935e33671c02a8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 1015, "doc": {"id": "5a6559db6bae37e3a8af7350be212219", "question": "The weasel ran up away from danger, somebody joked only our first president could get him down from the what?", "question_concept": "weasel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["washington dc", "ladder", "natural history museum", "cherry tree", "chicken coop"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The weasel ran up away from danger, somebody joked only our first president could get him down from the what?\nA. washington dc\nB. ladder\nC. natural history museum\nD. cherry tree\nE. chicken coop\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The weasel ran up away from danger, somebody joked only our first president could get him down from the what?\nA. washington dc\nB. ladder\nC. natural history museum\nD. cherry tree\nE. chicken coop\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The weasel ran up away from danger, somebody joked only our first president could get him down from the what?\nA. washington dc\nB. ladder\nC. natural history museum\nD. cherry tree\nE. chicken coop\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The weasel ran up away from danger, somebody joked only our first president could get him down from the what?\nA. washington dc\nB. ladder\nC. natural history museum\nD. cherry tree\nE. chicken coop\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The weasel ran up away from danger, somebody joked only our first president could get him down from the what?\nA. washington dc\nB. ladder\nC. natural history museum\nD. cherry tree\nE. chicken coop\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.863189220428467", "False"]], [["-4.363189220428467", "False"]], [["-5.863189220428467", "False"]], [["-2.613189220428467", "False"]], [["-4.613189220428467", "False"]]], "filtered_resps": [["-4.863189220428467", "False"], ["-4.363189220428467", "False"], ["-5.863189220428467", "False"], ["-2.613189220428467", "False"], ["-4.613189220428467", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "76e0ea66ed5e0582db31c998ecbf7c931058bd0244e602b4acf3680382479f84", "prompt_hash": "f6dedd736e1d8b821aaf66bc6e762133718a26c50abe76d416f154555587d5a2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1016, "doc": {"id": "7ae17f5aecacf18c94a47cc48deb6c36", "question": "If you were looking for a blowfish, you wouldn't look on dry land, you'd look in a what?", "question_concept": "blowfish", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fish market", "jungle", "sea water", "body of water", "soup"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If you were looking for a blowfish, you wouldn't look on dry land, you'd look in a what?\nA. fish market\nB. jungle\nC. sea water\nD. body of water\nE. soup\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you were looking for a blowfish, you wouldn't look on dry land, you'd look in a what?\nA. fish market\nB. jungle\nC. sea water\nD. body of water\nE. soup\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you were looking for a blowfish, you wouldn't look on dry land, you'd look in a what?\nA. fish market\nB. jungle\nC. sea water\nD. body of water\nE. soup\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you were looking for a blowfish, you wouldn't look on dry land, you'd look in a what?\nA. fish market\nB. jungle\nC. sea water\nD. body of water\nE. soup\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you were looking for a blowfish, you wouldn't look on dry land, you'd look in a what?\nA. fish market\nB. jungle\nC. sea water\nD. body of water\nE. soup\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9113340377807617", "False"]], [["-2.4113340377807617", "False"]], [["-2.4113340377807617", "False"]], [["-1.6613340377807617", "True"]], [["-8.411334037780762", "False"]]], "filtered_resps": [["-3.9113340377807617", "False"], ["-2.4113340377807617", "False"], ["-2.4113340377807617", "False"], ["-1.6613340377807617", "True"], ["-8.411334037780762", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7f81c1431443685e4577f6198509fa845307287117a6787fe615cca2d803909d", "prompt_hash": "c663c0bda5ceb73f553eec614a6a0bc1d0b53ee308401ef17c5009621e77a60a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1017, "doc": {"id": "5d809e0ee19badc66071653630ea7c51", "question": "George checked the rotor of the Apache, which wasn't powered by internal combustion, but by what?", "question_concept": "rotor", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["jet engine", "helicopter", "electric motor", "rotator", "electrical circuit"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: George checked the rotor of the Apache, which wasn't powered by internal combustion, but by what?\nA. jet engine\nB. helicopter\nC. electric motor\nD. rotator\nE. electrical circuit\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: George checked the rotor of the Apache, which wasn't powered by internal combustion, but by what?\nA. jet engine\nB. helicopter\nC. electric motor\nD. rotator\nE. electrical circuit\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: George checked the rotor of the Apache, which wasn't powered by internal combustion, but by what?\nA. jet engine\nB. helicopter\nC. electric motor\nD. rotator\nE. electrical circuit\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: George checked the rotor of the Apache, which wasn't powered by internal combustion, but by what?\nA. jet engine\nB. helicopter\nC. electric motor\nD. rotator\nE. electrical circuit\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: George checked the rotor of the Apache, which wasn't powered by internal combustion, but by what?\nA. jet engine\nB. helicopter\nC. electric motor\nD. rotator\nE. electrical circuit\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7452611923217773", "False"]], [["-4.995261192321777", "False"]], [["-1.7452611923217773", "False"]], [["-7.745261192321777", "False"]], [["-7.745261192321777", "False"]]], "filtered_resps": [["-3.7452611923217773", "False"], ["-4.995261192321777", "False"], ["-1.7452611923217773", "False"], ["-7.745261192321777", "False"], ["-7.745261192321777", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "66aabd6a28061702cd5ea55fd8d194cf98e13a5daa15d2775deabe05910a1a66", "prompt_hash": "8bbf626ce47dbdb244a8c82e2b83c8f3004fa6290dfe7ad8ff49b4a5da57bfc2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 1018, "doc": {"id": "ad0943fc37034cd2b7e485021f8b1b8c", "question": "The poker dealer spread the flop of cards across the what?", "question_concept": "cards", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["players", "play games", "casino", "table", "toy store"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The poker dealer spread the flop of cards across the what?\nA. players\nB. play games\nC. casino\nD. table\nE. toy store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The poker dealer spread the flop of cards across the what?\nA. players\nB. play games\nC. casino\nD. table\nE. toy store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The poker dealer spread the flop of cards across the what?\nA. players\nB. play games\nC. casino\nD. table\nE. toy store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The poker dealer spread the flop of cards across the what?\nA. players\nB. play games\nC. casino\nD. table\nE. toy store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The poker dealer spread the flop of cards across the what?\nA. players\nB. play games\nC. casino\nD. table\nE. toy store\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1421871185302734", "False"]], [["-7.142187118530273", "False"]], [["-7.142187118530273", "False"]], [["-1.3921869993209839", "False"]], [["-9.892187118530273", "False"]]], "filtered_resps": [["-3.1421871185302734", "False"], ["-7.142187118530273", "False"], ["-7.142187118530273", "False"], ["-1.3921869993209839", "False"], ["-9.892187118530273", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "90afdbac330026037d5e9f638019466ba23ef372b1ae372d16b4cf96dccc9b23", "prompt_hash": "d66cd2a9273740319227cf3c47583715a1a17b9779c024c8794d2ab32a470533", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1019, "doc": {"id": "c2a8c6814ed3e207771cfc23b3b42cf1", "question": "Where is a salt shaker most often kept?", "question_concept": "saltshaker", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cruet", "table setting", "kitchen cupboard", "cabinet", "store"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a salt shaker most often kept?\nA. cruet\nB. table setting\nC. kitchen cupboard\nD. cabinet\nE. store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a salt shaker most often kept?\nA. cruet\nB. table setting\nC. kitchen cupboard\nD. cabinet\nE. store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a salt shaker most often kept?\nA. cruet\nB. table setting\nC. kitchen cupboard\nD. cabinet\nE. store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a salt shaker most often kept?\nA. cruet\nB. table setting\nC. kitchen cupboard\nD. cabinet\nE. store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a salt shaker most often kept?\nA. cruet\nB. table setting\nC. kitchen cupboard\nD. cabinet\nE. store\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8012893199920654", "True"]], [["-2.0512893199920654", "False"]], [["-4.5512895584106445", "False"]], [["-6.5512895584106445", "False"]], [["-9.551289558410645", "False"]]], "filtered_resps": [["-0.8012893199920654", "True"], ["-2.0512893199920654", "False"], ["-4.5512895584106445", "False"], ["-6.5512895584106445", "False"], ["-9.551289558410645", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3ea1efc82a8d4b4802ced7d6f1d95cf03c5fc1f8e7c0e396c03d3eb909bb3e10", "prompt_hash": "e811fa1683beb356be12eee7519e345b5fd1a115bd5541bd1e6d6f7ccb666665", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 1020, "doc": {"id": "0b52cc905fff0ca69a45e6353d10e401", "question": "Where would you put a dollar if you want to go to a store and buy something with it?", "question_concept": "dollar", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cash drawer", "teh bank", "safety deposit box", "pocket", "piggy bank"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you put a dollar if you want to go to a store and buy something with it?\nA. cash drawer\nB. teh bank\nC. safety deposit box\nD. pocket\nE. piggy bank\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you put a dollar if you want to go to a store and buy something with it?\nA. cash drawer\nB. teh bank\nC. safety deposit box\nD. pocket\nE. piggy bank\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you put a dollar if you want to go to a store and buy something with it?\nA. cash drawer\nB. teh bank\nC. safety deposit box\nD. pocket\nE. piggy bank\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you put a dollar if you want to go to a store and buy something with it?\nA. cash drawer\nB. teh bank\nC. safety deposit box\nD. pocket\nE. piggy bank\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you put a dollar if you want to go to a store and buy something with it?\nA. cash drawer\nB. teh bank\nC. safety deposit box\nD. pocket\nE. piggy bank\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0194926261901855", "False"]], [["-4.0194926261901855", "False"]], [["-7.5194926261901855", "False"]], [["-2.0194926261901855", "False"]], [["-5.2694926261901855", "False"]]], "filtered_resps": [["-3.0194926261901855", "False"], ["-4.0194926261901855", "False"], ["-7.5194926261901855", "False"], ["-2.0194926261901855", "False"], ["-5.2694926261901855", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d1bab56b7360310fbdea211e36cb7389bd375e703b55e32cfeefe1486c362a80", "prompt_hash": "16531dcb2d976a299d985f2866cf683b233e8457afd4de60aba0b24849e12b0c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1021, "doc": {"id": "30d0c2006613eec41ae814d76c17a798", "question": "What room is likely to have a sideboard on the counter?", "question_concept": "sideboard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["home", "serve food buffet", "dining room", "living room", "kitchen"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What room is likely to have a sideboard on the counter?\nA. home\nB. serve food buffet\nC. dining room\nD. living room\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What room is likely to have a sideboard on the counter?\nA. home\nB. serve food buffet\nC. dining room\nD. living room\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What room is likely to have a sideboard on the counter?\nA. home\nB. serve food buffet\nC. dining room\nD. living room\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What room is likely to have a sideboard on the counter?\nA. home\nB. serve food buffet\nC. dining room\nD. living room\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What room is likely to have a sideboard on the counter?\nA. home\nB. serve food buffet\nC. dining room\nD. living room\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7782279253005981", "True"]], [["-3.0282278060913086", "False"]], [["-1.7782279253005981", "True"]], [["-6.778227806091309", "False"]], [["-4.528227806091309", "False"]]], "filtered_resps": [["-1.7782279253005981", "True"], ["-3.0282278060913086", "False"], ["-1.7782279253005981", "True"], ["-6.778227806091309", "False"], ["-4.528227806091309", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "78186cab75330a72dff81f2273ac165bb8a1acd3ce49fe6440a28a1f90e28dbc", "prompt_hash": "64779ba40889b3200cd6ac1b5a58f5680d808b3b1de554fa5a0cc93897447864", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1022, "doc": {"id": "f7a6d0d816d14210f3af5dabe21bf804", "question": "What is unlikely to get bugs on its windshield due to bugs' inability to reach it when it is moving?", "question_concept": "windshield", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["airplane", "scooter", "motorboat", "car", "motor vehicle"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is unlikely to get bugs on its windshield due to bugs' inability to reach it when it is moving?\nA. airplane\nB. scooter\nC. motorboat\nD. car\nE. motor vehicle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is unlikely to get bugs on its windshield due to bugs' inability to reach it when it is moving?\nA. airplane\nB. scooter\nC. motorboat\nD. car\nE. motor vehicle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is unlikely to get bugs on its windshield due to bugs' inability to reach it when it is moving?\nA. airplane\nB. scooter\nC. motorboat\nD. car\nE. motor vehicle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is unlikely to get bugs on its windshield due to bugs' inability to reach it when it is moving?\nA. airplane\nB. scooter\nC. motorboat\nD. car\nE. motor vehicle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is unlikely to get bugs on its windshield due to bugs' inability to reach it when it is moving?\nA. airplane\nB. scooter\nC. motorboat\nD. car\nE. motor vehicle\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0769257545471191", "True"]], [["-2.076925754547119", "False"]], [["-2.826925754547119", "False"]], [["-5.076925754547119", "False"]], [["-4.326925754547119", "False"]]], "filtered_resps": [["-1.0769257545471191", "True"], ["-2.076925754547119", "False"], ["-2.826925754547119", "False"], ["-5.076925754547119", "False"], ["-4.326925754547119", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "415ac6ad21f2180130ca5c39ca709a6a631a604b63fbf75ab792d4394cf9a98f", "prompt_hash": "6c7da578f0723d243238e2190587a924a7e8953ae517268c50ab5b84948da4bd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1023, "doc": {"id": "c306ab28498b67c53decb9dde1d78bd5", "question": "What mall store sells jeans for a decent price?", "question_concept": "jeans", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["clothing store", "bedroom", "thrift store", "apartment", "gap"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What mall store sells jeans for a decent price?\nA. clothing store\nB. bedroom\nC. thrift store\nD. apartment\nE. gap\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What mall store sells jeans for a decent price?\nA. clothing store\nB. bedroom\nC. thrift store\nD. apartment\nE. gap\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What mall store sells jeans for a decent price?\nA. clothing store\nB. bedroom\nC. thrift store\nD. apartment\nE. gap\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What mall store sells jeans for a decent price?\nA. clothing store\nB. bedroom\nC. thrift store\nD. apartment\nE. gap\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What mall store sells jeans for a decent price?\nA. clothing store\nB. bedroom\nC. thrift store\nD. apartment\nE. gap\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.178495407104492", "False"]], [["-6.428495407104492", "False"]], [["-6.178495407104492", "False"]], [["-9.428495407104492", "False"]], [["-1.1784954071044922", "True"]]], "filtered_resps": [["-2.178495407104492", "False"], ["-6.428495407104492", "False"], ["-6.178495407104492", "False"], ["-9.428495407104492", "False"], ["-1.1784954071044922", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6b33281c25c8022823c62715763e8d641d890887a6c75fee9d8fb11a29ae0dba", "prompt_hash": "bee7e8b2cc17ce9708347c0ba4ea09297cacbe368cb6b0fd3f706cbb90daaa61", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1024, "doc": {"id": "637c710ec9582fd9b9e8eaa3f3fe83bb", "question": "Where can a bath towel be borrowed?", "question_concept": "towel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cupboard", "at hotel", "swimming pool", "clothes line", "backpack"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where can a bath towel be borrowed?\nA. cupboard\nB. at hotel\nC. swimming pool\nD. clothes line\nE. backpack\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can a bath towel be borrowed?\nA. cupboard\nB. at hotel\nC. swimming pool\nD. clothes line\nE. backpack\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can a bath towel be borrowed?\nA. cupboard\nB. at hotel\nC. swimming pool\nD. clothes line\nE. backpack\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can a bath towel be borrowed?\nA. cupboard\nB. at hotel\nC. swimming pool\nD. clothes line\nE. backpack\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can a bath towel be borrowed?\nA. cupboard\nB. at hotel\nC. swimming pool\nD. clothes line\nE. backpack\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1631267070770264", "False"]], [["-1.1631267070770264", "False"]], [["-8.163126945495605", "False"]], [["-9.913126945495605", "False"]], [["-10.163126945495605", "False"]]], "filtered_resps": [["-2.1631267070770264", "False"], ["-1.1631267070770264", "False"], ["-8.163126945495605", "False"], ["-9.913126945495605", "False"], ["-10.163126945495605", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e574c26276a2f9b1f60df7892217a5963ea1f917a2dd60170d082b90ac29b785", "prompt_hash": "76194dd211664890150b98e5f1b45a9d8c37da884b68bd6192e66be589395d15", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1025, "doc": {"id": "9ae52783d8fdb5cc2e8caa01542c3341", "question": "Why do people stop caring about their problems?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["no problems", "better themselves", "face problems", "learn from each other", "become disillusioned"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Why do people stop caring about their problems?\nA. no problems\nB. better themselves\nC. face problems\nD. learn from each other\nE. become disillusioned\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why do people stop caring about their problems?\nA. no problems\nB. better themselves\nC. face problems\nD. learn from each other\nE. become disillusioned\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why do people stop caring about their problems?\nA. no problems\nB. better themselves\nC. face problems\nD. learn from each other\nE. become disillusioned\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why do people stop caring about their problems?\nA. no problems\nB. better themselves\nC. face problems\nD. learn from each other\nE. become disillusioned\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why do people stop caring about their problems?\nA. no problems\nB. better themselves\nC. face problems\nD. learn from each other\nE. become disillusioned\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.25147008895874", "False"]], [["-4.25147008895874", "False"]], [["-6.75147008895874", "False"]], [["-6.75147008895874", "False"]], [["-1.2514702081680298", "True"]]], "filtered_resps": [["-4.25147008895874", "False"], ["-4.25147008895874", "False"], ["-6.75147008895874", "False"], ["-6.75147008895874", "False"], ["-1.2514702081680298", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0c52c0e945e681c44a0eea778ed43ad24fb1b7c197850cbfa763f0d694552d1a", "prompt_hash": "07dd77c475438a162ffcff0441bc63091a377fcdd91063bbe6485a0e747c38e0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1026, "doc": {"id": "4f23829b96b38b5633ecc3325281726d", "question": "John rode on the plain until it reached the ocean and couldn't go any farther. What might he have bee on?", "question_concept": "plain", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mountain", "fancy", "sandplain", "cliff", "gorge"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: John rode on the plain until it reached the ocean and couldn't go any farther. What might he have bee on?\nA. mountain\nB. fancy\nC. sandplain\nD. cliff\nE. gorge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John rode on the plain until it reached the ocean and couldn't go any farther. What might he have bee on?\nA. mountain\nB. fancy\nC. sandplain\nD. cliff\nE. gorge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John rode on the plain until it reached the ocean and couldn't go any farther. What might he have bee on?\nA. mountain\nB. fancy\nC. sandplain\nD. cliff\nE. gorge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John rode on the plain until it reached the ocean and couldn't go any farther. What might he have bee on?\nA. mountain\nB. fancy\nC. sandplain\nD. cliff\nE. gorge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John rode on the plain until it reached the ocean and couldn't go any farther. What might he have bee on?\nA. mountain\nB. fancy\nC. sandplain\nD. cliff\nE. gorge\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1605100631713867", "False"]], [["-5.160510063171387", "False"]], [["-1.9105099439620972", "True"]], [["-2.6605100631713867", "False"]], [["-7.160510063171387", "False"]]], "filtered_resps": [["-2.1605100631713867", "False"], ["-5.160510063171387", "False"], ["-1.9105099439620972", "True"], ["-2.6605100631713867", "False"], ["-7.160510063171387", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bc45d1b29a8199fa7d21bbb2fe0f6af4d8cf30b8e8174f4153308db09b6af966", "prompt_hash": "757e7453f187d76c1f541799eb9108a433c0379ff5759211bd7dcff8e68ab156", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1027, "doc": {"id": "3fcdc0b03e3c8b10692d642676931f4b", "question": "They were never going to be big actors, but they all had passion for the local what?", "question_concept": "actors", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["theater", "opera", "show", "television", "blockbuster feature"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: They were never going to be big actors, but they all had passion for the local what?\nA. theater\nB. opera\nC. show\nD. television\nE. blockbuster feature\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They were never going to be big actors, but they all had passion for the local what?\nA. theater\nB. opera\nC. show\nD. television\nE. blockbuster feature\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They were never going to be big actors, but they all had passion for the local what?\nA. theater\nB. opera\nC. show\nD. television\nE. blockbuster feature\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They were never going to be big actors, but they all had passion for the local what?\nA. theater\nB. opera\nC. show\nD. television\nE. blockbuster feature\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They were never going to be big actors, but they all had passion for the local what?\nA. theater\nB. opera\nC. show\nD. television\nE. blockbuster feature\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8703992962837219", "True"]], [["-5.870399475097656", "False"]], [["-7.370399475097656", "False"]], [["-6.870399475097656", "False"]], [["-9.120399475097656", "False"]]], "filtered_resps": [["-0.8703992962837219", "True"], ["-5.870399475097656", "False"], ["-7.370399475097656", "False"], ["-6.870399475097656", "False"], ["-9.120399475097656", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5325df61365e1a13e609de42cfe60f7dc42f180b60aeddc90072bb1d0095704b", "prompt_hash": "3e676b93a3c60d25cbe0ea84914974459e46f7b59c31c0f4867acffdc924b17d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1028, "doc": {"id": "ddd606743cf71679438a85280f64593a", "question": "Where would you use a folding chair but not store one?", "question_concept": "folding chair", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["beach", "city hall", "closet", "garage", "school"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you use a folding chair but not store one?\nA. beach\nB. city hall\nC. closet\nD. garage\nE. school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you use a folding chair but not store one?\nA. beach\nB. city hall\nC. closet\nD. garage\nE. school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you use a folding chair but not store one?\nA. beach\nB. city hall\nC. closet\nD. garage\nE. school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you use a folding chair but not store one?\nA. beach\nB. city hall\nC. closet\nD. garage\nE. school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you use a folding chair but not store one?\nA. beach\nB. city hall\nC. closet\nD. garage\nE. school\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3671711683273315", "True"]], [["-3.867171287536621", "False"]], [["-6.867171287536621", "False"]], [["-7.617171287536621", "False"]], [["-3.367171287536621", "False"]]], "filtered_resps": [["-1.3671711683273315", "True"], ["-3.867171287536621", "False"], ["-6.867171287536621", "False"], ["-7.617171287536621", "False"], ["-3.367171287536621", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8e438aead55035748bb8c0e6087499970d0d0528dd13fe8acefa46a24f2ae2f7", "prompt_hash": "13770c600fa057977f8defbf69d6cc5295dd8d4906fc7a66673011c1a166217f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1029, "doc": {"id": "420641003ba20b966887dfac684efb17", "question": "If you spend a long time shopping in uncomfortable shoes, you might develop what?", "question_concept": "shopping", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tiredness", "calluses", "bankruptcy", "standing in line", "sleepyness"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you spend a long time shopping in uncomfortable shoes, you might develop what?\nA. tiredness\nB. calluses\nC. bankruptcy\nD. standing in line\nE. sleepyness\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you spend a long time shopping in uncomfortable shoes, you might develop what?\nA. tiredness\nB. calluses\nC. bankruptcy\nD. standing in line\nE. sleepyness\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you spend a long time shopping in uncomfortable shoes, you might develop what?\nA. tiredness\nB. calluses\nC. bankruptcy\nD. standing in line\nE. sleepyness\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you spend a long time shopping in uncomfortable shoes, you might develop what?\nA. tiredness\nB. calluses\nC. bankruptcy\nD. standing in line\nE. sleepyness\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you spend a long time shopping in uncomfortable shoes, you might develop what?\nA. tiredness\nB. calluses\nC. bankruptcy\nD. standing in line\nE. sleepyness\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.553021430969238", "False"]], [["-0.8030215501785278", "True"]], [["-7.553021430969238", "False"]], [["-7.803021430969238", "False"]], [["-9.053021430969238", "False"]]], "filtered_resps": [["-5.553021430969238", "False"], ["-0.8030215501785278", "True"], ["-7.553021430969238", "False"], ["-7.803021430969238", "False"], ["-9.053021430969238", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "09694bd125225a0614922eb124011cd8513d7eb7d093c754286f2b54888ca583", "prompt_hash": "fe8fc2e90a802cf6616388902c5896ec5ee5d551b1f9dba187a249fe74760776", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1030, "doc": {"id": "064c3074a682893d49c3c5b4f1e89984", "question": "What does impeachment mean for the president?", "question_concept": "president", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["vote", "election", "trouble", "board room", "corporation"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What does impeachment mean for the president?\nA. vote\nB. election\nC. trouble\nD. board room\nE. corporation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does impeachment mean for the president?\nA. vote\nB. election\nC. trouble\nD. board room\nE. corporation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does impeachment mean for the president?\nA. vote\nB. election\nC. trouble\nD. board room\nE. corporation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does impeachment mean for the president?\nA. vote\nB. election\nC. trouble\nD. board room\nE. corporation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does impeachment mean for the president?\nA. vote\nB. election\nC. trouble\nD. board room\nE. corporation\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.459850311279297", "False"]], [["-6.959850311279297", "False"]], [["-1.7098500728607178", "False"]], [["-6.459850311279297", "False"]], [["-6.709850311279297", "False"]]], "filtered_resps": [["-4.459850311279297", "False"], ["-6.959850311279297", "False"], ["-1.7098500728607178", "False"], ["-6.459850311279297", "False"], ["-6.709850311279297", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eaea8951a1077bce2c7e5dfd44911bad57cf8f7462d7a40c5bc252b01872b3a2", "prompt_hash": "88571625d30820e1a4715be37ebf69cf25318f25191cfc77d6475af069edf967", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1031, "doc": {"id": "c640116ca6905d5256edadb616b3f76e", "question": "Noble citizen of the Roman empire believed those born with lower status were what to them?", "question_concept": "noble", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["loser", "ignoble", "peasant", "inferior", "plebeian"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Noble citizen of the Roman empire believed those born with lower status were what to them?\nA. loser\nB. ignoble\nC. peasant\nD. inferior\nE. plebeian\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Noble citizen of the Roman empire believed those born with lower status were what to them?\nA. loser\nB. ignoble\nC. peasant\nD. inferior\nE. plebeian\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Noble citizen of the Roman empire believed those born with lower status were what to them?\nA. loser\nB. ignoble\nC. peasant\nD. inferior\nE. plebeian\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Noble citizen of the Roman empire believed those born with lower status were what to them?\nA. loser\nB. ignoble\nC. peasant\nD. inferior\nE. plebeian\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Noble citizen of the Roman empire believed those born with lower status were what to them?\nA. loser\nB. ignoble\nC. peasant\nD. inferior\nE. plebeian\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.140171051025391", "False"]], [["-3.1401708126068115", "False"]], [["-6.140171051025391", "False"]], [["-1.3901708126068115", "True"]], [["-4.140171051025391", "False"]]], "filtered_resps": [["-4.140171051025391", "False"], ["-3.1401708126068115", "False"], ["-6.140171051025391", "False"], ["-1.3901708126068115", "True"], ["-4.140171051025391", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e0c358ce7da09b63dccdcd7c3a6ab51d2fd11ca59c0c5635c5a500a98eadb0ee", "prompt_hash": "a4dfd2583291464b93df68fb5443f1e959a9f280113c43d36b5ae8af35a5a26e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1032, "doc": {"id": "35ad89c198d5d6311a71c993bb7b6cba", "question": "Spraining an ankle while playing baseball will cause what?", "question_concept": "playing baseball", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["strikes", "eating", "injury", "sore muscles", "pain"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Spraining an ankle while playing baseball will cause what?\nA. strikes\nB. eating\nC. injury\nD. sore muscles\nE. pain\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Spraining an ankle while playing baseball will cause what?\nA. strikes\nB. eating\nC. injury\nD. sore muscles\nE. pain\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Spraining an ankle while playing baseball will cause what?\nA. strikes\nB. eating\nC. injury\nD. sore muscles\nE. pain\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Spraining an ankle while playing baseball will cause what?\nA. strikes\nB. eating\nC. injury\nD. sore muscles\nE. pain\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Spraining an ankle while playing baseball will cause what?\nA. strikes\nB. eating\nC. injury\nD. sore muscles\nE. pain\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3189172744750977", "False"]], [["-5.068917274475098", "False"]], [["-1.8189172744750977", "False"]], [["-7.318917274475098", "False"]], [["-4.818917274475098", "False"]]], "filtered_resps": [["-2.3189172744750977", "False"], ["-5.068917274475098", "False"], ["-1.8189172744750977", "False"], ["-7.318917274475098", "False"], ["-4.818917274475098", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "055db395ae5b0ad789cd7df75212f99e7f9a1cf9ba25e9c9921e923f337b12bf", "prompt_hash": "52b27673a12993f454a67ff25448d1f297089c28f8ace81b23ec5d1398fc9e0e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1033, "doc": {"id": "916bbd27545446ca5d83d07c10d013ea", "question": "John was traveling to a new city and took time to check out a business.  He noticed that its carpet was stained with sauces and ketchup. What type of business might that be?", "question_concept": "carpet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bedroom", "chair", "bowling alley", "at hotel", "restaurant"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: John was traveling to a new city and took time to check out a business.  He noticed that its carpet was stained with sauces and ketchup. What type of business might that be?\nA. bedroom\nB. chair\nC. bowling alley\nD. at hotel\nE. restaurant\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John was traveling to a new city and took time to check out a business.  He noticed that its carpet was stained with sauces and ketchup. What type of business might that be?\nA. bedroom\nB. chair\nC. bowling alley\nD. at hotel\nE. restaurant\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John was traveling to a new city and took time to check out a business.  He noticed that its carpet was stained with sauces and ketchup. What type of business might that be?\nA. bedroom\nB. chair\nC. bowling alley\nD. at hotel\nE. restaurant\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John was traveling to a new city and took time to check out a business.  He noticed that its carpet was stained with sauces and ketchup. What type of business might that be?\nA. bedroom\nB. chair\nC. bowling alley\nD. at hotel\nE. restaurant\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John was traveling to a new city and took time to check out a business.  He noticed that its carpet was stained with sauces and ketchup. What type of business might that be?\nA. bedroom\nB. chair\nC. bowling alley\nD. at hotel\nE. restaurant\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6025633811950684", "False"]], [["-5.102563381195068", "False"]], [["-3.6025633811950684", "False"]], [["-6.102563381195068", "False"]], [["-1.352563500404358", "True"]]], "filtered_resps": [["-3.6025633811950684", "False"], ["-5.102563381195068", "False"], ["-3.6025633811950684", "False"], ["-6.102563381195068", "False"], ["-1.352563500404358", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f6921414dd22039b734c5a57bfc2b5ffa10da617597d0b175ae3cab9beeba6b6", "prompt_hash": "42f667021c956cee4f8b69256b69a1f020048b2db83a94c2d025480c5cb98089", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1034, "doc": {"id": "e40fd2c17fe2cde4bd4af540d35fd518", "question": "If you have a condo in a Wisconsin city known for beer, where are you?", "question_concept": "condo", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["city", "electrical circuit", "residential area", "suburbia", "milwaukee"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If you have a condo in a Wisconsin city known for beer, where are you?\nA. city\nB. electrical circuit\nC. residential area\nD. suburbia\nE. milwaukee\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you have a condo in a Wisconsin city known for beer, where are you?\nA. city\nB. electrical circuit\nC. residential area\nD. suburbia\nE. milwaukee\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you have a condo in a Wisconsin city known for beer, where are you?\nA. city\nB. electrical circuit\nC. residential area\nD. suburbia\nE. milwaukee\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you have a condo in a Wisconsin city known for beer, where are you?\nA. city\nB. electrical circuit\nC. residential area\nD. suburbia\nE. milwaukee\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you have a condo in a Wisconsin city known for beer, where are you?\nA. city\nB. electrical circuit\nC. residential area\nD. suburbia\nE. milwaukee\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.319211006164551", "False"]], [["-6.819211006164551", "False"]], [["-6.819211006164551", "False"]], [["-9.31921100616455", "False"]], [["-0.8192112445831299", "True"]]], "filtered_resps": [["-5.319211006164551", "False"], ["-6.819211006164551", "False"], ["-6.819211006164551", "False"], ["-9.31921100616455", "False"], ["-0.8192112445831299", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e18495b250ec4e441864777c93a70fb6f5e3c7824a1c7c2e1d3d56ba105205f7", "prompt_hash": "8ae99c5e473a3fd4e1b5213a7afe34e75b10a2d79bf117bfdab7fadf5e8091d3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1035, "doc": {"id": "98a04457025f18c2287d5c610ff8000d", "question": "Where is hard to read note likely to be?", "question_concept": "note", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fridge", "sheet music", "desk", "bed", "medical chart"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where is hard to read note likely to be?\nA. fridge\nB. sheet music\nC. desk\nD. bed\nE. medical chart\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is hard to read note likely to be?\nA. fridge\nB. sheet music\nC. desk\nD. bed\nE. medical chart\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is hard to read note likely to be?\nA. fridge\nB. sheet music\nC. desk\nD. bed\nE. medical chart\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is hard to read note likely to be?\nA. fridge\nB. sheet music\nC. desk\nD. bed\nE. medical chart\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is hard to read note likely to be?\nA. fridge\nB. sheet music\nC. desk\nD. bed\nE. medical chart\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.417799949645996", "False"]], [["-2.417799949645996", "False"]], [["-4.917799949645996", "False"]], [["-6.667799949645996", "False"]], [["-1.4178000688552856", "True"]]], "filtered_resps": [["-2.417799949645996", "False"], ["-2.417799949645996", "False"], ["-4.917799949645996", "False"], ["-6.667799949645996", "False"], ["-1.4178000688552856", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f30de8fbbc9e421a8e65382ba63d9b5f2cb4e7a70972c83db8c28d6c15bb3cf4", "prompt_hash": "ca5da37117352b171659ebfc8bbeaf56b34a5df02a101a9e7a2d4555989e2fa7", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1036, "doc": {"id": "f656a475f07d3adba9d1486eda8e834a", "question": "How does someone go about buying beer?", "question_concept": "buying beer", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have no money", "pants", "relaxation", "lose money", "spend money"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: How does someone go about buying beer?\nA. have no money\nB. pants\nC. relaxation\nD. lose money\nE. spend money\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How does someone go about buying beer?\nA. have no money\nB. pants\nC. relaxation\nD. lose money\nE. spend money\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How does someone go about buying beer?\nA. have no money\nB. pants\nC. relaxation\nD. lose money\nE. spend money\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How does someone go about buying beer?\nA. have no money\nB. pants\nC. relaxation\nD. lose money\nE. spend money\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How does someone go about buying beer?\nA. have no money\nB. pants\nC. relaxation\nD. lose money\nE. spend money\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.095698833465576", "False"]], [["-5.095698833465576", "False"]], [["-7.345698833465576", "False"]], [["-8.595698356628418", "False"]], [["-0.8456986546516418", "True"]]], "filtered_resps": [["-5.095698833465576", "False"], ["-5.095698833465576", "False"], ["-7.345698833465576", "False"], ["-8.595698356628418", "False"], ["-0.8456986546516418", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0b1332a0f2cb9b2b76d757a6d3c10ab8455637a6fe9fc629f9c103fe88940371", "prompt_hash": "6b955dcfe1989853642415660b77020021a5e19af4d37bb8a57b55a8930e17dc", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1037, "doc": {"id": "c865b3547c2a2e3c3916d7be6ab25752", "question": "If there is gum on your shoe where did it likely come from?", "question_concept": "gum", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["shelf", "movies", "sidewalk", "water fountain", "table"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If there is gum on your shoe where did it likely come from?\nA. shelf\nB. movies\nC. sidewalk\nD. water fountain\nE. table\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If there is gum on your shoe where did it likely come from?\nA. shelf\nB. movies\nC. sidewalk\nD. water fountain\nE. table\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If there is gum on your shoe where did it likely come from?\nA. shelf\nB. movies\nC. sidewalk\nD. water fountain\nE. table\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If there is gum on your shoe where did it likely come from?\nA. shelf\nB. movies\nC. sidewalk\nD. water fountain\nE. table\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If there is gum on your shoe where did it likely come from?\nA. shelf\nB. movies\nC. sidewalk\nD. water fountain\nE. table\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.719653129577637", "False"]], [["-6.219653129577637", "False"]], [["-1.2196530103683472", "True"]], [["-5.719653129577637", "False"]], [["-7.219653129577637", "False"]]], "filtered_resps": [["-4.719653129577637", "False"], ["-6.219653129577637", "False"], ["-1.2196530103683472", "True"], ["-5.719653129577637", "False"], ["-7.219653129577637", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "12fec9646bfd5e64852f8324f3b05ff4cfc4a0bd52791481b3db77bb1f733fe0", "prompt_hash": "cfcb50928c5f3d5b2a1ac0da983456e6b79dcbf7efdd063761ad8a4391b57c0b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1038, "doc": {"id": "abd30bab9b96f902fead5378d4f4a1e4", "question": "If a person isn't able to pay their bills what must they do?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["know everything", "acknowledgment", "make more money", "throw a party", "spare time"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If a person isn't able to pay their bills what must they do?\nA. know everything\nB. acknowledgment\nC. make more money\nD. throw a party\nE. spare time\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a person isn't able to pay their bills what must they do?\nA. know everything\nB. acknowledgment\nC. make more money\nD. throw a party\nE. spare time\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a person isn't able to pay their bills what must they do?\nA. know everything\nB. acknowledgment\nC. make more money\nD. throw a party\nE. spare time\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a person isn't able to pay their bills what must they do?\nA. know everything\nB. acknowledgment\nC. make more money\nD. throw a party\nE. spare time\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a person isn't able to pay their bills what must they do?\nA. know everything\nB. acknowledgment\nC. make more money\nD. throw a party\nE. spare time\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.627338409423828", "False"]], [["-2.627338409423828", "False"]], [["-1.8773382902145386", "True"]], [["-7.627338409423828", "False"]], [["-8.127338409423828", "False"]]], "filtered_resps": [["-3.627338409423828", "False"], ["-2.627338409423828", "False"], ["-1.8773382902145386", "True"], ["-7.627338409423828", "False"], ["-8.127338409423828", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "843a1642bbe7ddec42751a8330ee3b8d0f9c0d061dadf90734054acc5a59f6e8", "prompt_hash": "79f80dc69d2b92a16afce779ca161e997189557b2eb500d118778a560588f314", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1039, "doc": {"id": "a4b44a986e7f9045432e20ea75611df4", "question": "What is main benefit to exercising?", "question_concept": "exercising", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["losing weight", "healthy", "get in shape", "weight loss", "sweat"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is main benefit to exercising?\nA. losing weight\nB. healthy\nC. get in shape\nD. weight loss\nE. sweat\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is main benefit to exercising?\nA. losing weight\nB. healthy\nC. get in shape\nD. weight loss\nE. sweat\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is main benefit to exercising?\nA. losing weight\nB. healthy\nC. get in shape\nD. weight loss\nE. sweat\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is main benefit to exercising?\nA. losing weight\nB. healthy\nC. get in shape\nD. weight loss\nE. sweat\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is main benefit to exercising?\nA. losing weight\nB. healthy\nC. get in shape\nD. weight loss\nE. sweat\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9616050720214844", "False"]], [["-2.4616050720214844", "False"]], [["-4.461605072021484", "False"]], [["-4.711605072021484", "False"]], [["-5.211605072021484", "False"]]], "filtered_resps": [["-2.9616050720214844", "False"], ["-2.4616050720214844", "False"], ["-4.461605072021484", "False"], ["-4.711605072021484", "False"], ["-5.211605072021484", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e96f37110e7be20154f476c9793af0b02d8413ee7dceeef931a62bc3902f1ebe", "prompt_hash": "ed20a3481655c032abbdb8003db6f93c403cd48e05636ecba941112fcaf32755", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1040, "doc": {"id": "1f492f556fae64f72ce36b6caa242dd0", "question": "Steve thought that it was possible, but he agreed that it was what?", "question_concept": "possible", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["no go", "unable", "unlikely", "impossibility", "cant do"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Steve thought that it was possible, but he agreed that it was what?\nA. no go\nB. unable\nC. unlikely\nD. impossibility\nE. cant do\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Steve thought that it was possible, but he agreed that it was what?\nA. no go\nB. unable\nC. unlikely\nD. impossibility\nE. cant do\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Steve thought that it was possible, but he agreed that it was what?\nA. no go\nB. unable\nC. unlikely\nD. impossibility\nE. cant do\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Steve thought that it was possible, but he agreed that it was what?\nA. no go\nB. unable\nC. unlikely\nD. impossibility\nE. cant do\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Steve thought that it was possible, but he agreed that it was what?\nA. no go\nB. unable\nC. unlikely\nD. impossibility\nE. cant do\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.513885498046875", "False"]], [["-3.263885498046875", "False"]], [["-2.513885498046875", "False"]], [["-3.763885498046875", "False"]], [["-5.763885498046875", "False"]]], "filtered_resps": [["-3.513885498046875", "False"], ["-3.263885498046875", "False"], ["-2.513885498046875", "False"], ["-3.763885498046875", "False"], ["-5.763885498046875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b1030039fd684eaedddc760fdbccdbff53f04456d24d8ba2bf82df02974ce5aa", "prompt_hash": "174736b6b68e639640ce0c14928cf634fa007bc11816f4faa1ed11f9bab78fb8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1041, "doc": {"id": "d0c67c7ae6f2361fe237110455127866", "question": "What region of a west coast U.S. city would you find a Japanese restaurant?", "question_concept": "japanese restaurant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["california", "tokio", "downtown", "narnia", "large town"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What region of a west coast U.S. city would you find a Japanese restaurant?\nA. california\nB. tokio\nC. downtown\nD. narnia\nE. large town\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What region of a west coast U.S. city would you find a Japanese restaurant?\nA. california\nB. tokio\nC. downtown\nD. narnia\nE. large town\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What region of a west coast U.S. city would you find a Japanese restaurant?\nA. california\nB. tokio\nC. downtown\nD. narnia\nE. large town\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What region of a west coast U.S. city would you find a Japanese restaurant?\nA. california\nB. tokio\nC. downtown\nD. narnia\nE. large town\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What region of a west coast U.S. city would you find a Japanese restaurant?\nA. california\nB. tokio\nC. downtown\nD. narnia\nE. large town\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7878570556640625", "True"]], [["-5.7878570556640625", "False"]], [["-2.5378570556640625", "False"]], [["-6.7878570556640625", "False"]], [["-7.7878570556640625", "False"]]], "filtered_resps": [["-0.7878570556640625", "True"], ["-5.7878570556640625", "False"], ["-2.5378570556640625", "False"], ["-6.7878570556640625", "False"], ["-7.7878570556640625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9319a86dd7c0818d17e1611b18ec9128aec98b8e76433373332ee9aaea2a9eeb", "prompt_hash": "79e0e01e67c87ed1cd2fdc5bcda45e91481d24fdf73e6ac23c8398fb4b98a75c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1042, "doc": {"id": "7bb279e38a1c9eb47a0c7af979a131a2", "question": "What is a tactic used to interfere with learning about science?", "question_concept": "learning about science", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["repetition", "sense of wonder", "accidents", "intimidation", "increased knowledge"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is a tactic used to interfere with learning about science?\nA. repetition\nB. sense of wonder\nC. accidents\nD. intimidation\nE. increased knowledge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a tactic used to interfere with learning about science?\nA. repetition\nB. sense of wonder\nC. accidents\nD. intimidation\nE. increased knowledge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a tactic used to interfere with learning about science?\nA. repetition\nB. sense of wonder\nC. accidents\nD. intimidation\nE. increased knowledge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a tactic used to interfere with learning about science?\nA. repetition\nB. sense of wonder\nC. accidents\nD. intimidation\nE. increased knowledge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a tactic used to interfere with learning about science?\nA. repetition\nB. sense of wonder\nC. accidents\nD. intimidation\nE. increased knowledge\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.415673017501831", "False"]], [["-7.41567325592041", "False"]], [["-6.41567325592041", "False"]], [["-1.165673017501831", "True"]], [["-6.66567325592041", "False"]]], "filtered_resps": [["-3.415673017501831", "False"], ["-7.41567325592041", "False"], ["-6.41567325592041", "False"], ["-1.165673017501831", "True"], ["-6.66567325592041", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3a0011acc91e5b4d62f4675364240ab801d1cbca6001f5e33096b2e0939b98f1", "prompt_hash": "9f8426cd04cef0891805d5055e693df08de0f0af0619e0f49e3e8286cf523d86", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1043, "doc": {"id": "3095078e4771053d9d5fa8d4f5f3dc38", "question": "What do people usually feel when falling in love?", "question_concept": "falling in love", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["getting married", "pain", "happiness", "getting married", "suffering"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What do people usually feel when falling in love?\nA. getting married\nB. pain\nC. happiness\nD. getting married\nE. suffering\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do people usually feel when falling in love?\nA. getting married\nB. pain\nC. happiness\nD. getting married\nE. suffering\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do people usually feel when falling in love?\nA. getting married\nB. pain\nC. happiness\nD. getting married\nE. suffering\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do people usually feel when falling in love?\nA. getting married\nB. pain\nC. happiness\nD. getting married\nE. suffering\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do people usually feel when falling in love?\nA. getting married\nB. pain\nC. happiness\nD. getting married\nE. suffering\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.983750820159912", "False"]], [["-5.233750820159912", "False"]], [["-1.4837509393692017", "True"]], [["-7.483750820159912", "False"]], [["-8.48375129699707", "False"]]], "filtered_resps": [["-4.983750820159912", "False"], ["-5.233750820159912", "False"], ["-1.4837509393692017", "True"], ["-7.483750820159912", "False"], ["-8.48375129699707", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "388f0063d42edb4e6ce86aaab67315e826b913ebe32a025958fa94b8699c9db1", "prompt_hash": "366dba843d6b0571fc516f5d36ea5ef25e69da8a37fbbd4b50b8f794ab64da85", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1044, "doc": {"id": "b23edb651e623e5d1e03e8ed3937e8fc", "question": "The tiger was stuck in what animal prison where he got lazy and fat?", "question_concept": "tiger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["jungle", "zoo", "kill", "india", "eat cake"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The tiger was stuck in what animal prison where he got lazy and fat?\nA. jungle\nB. zoo\nC. kill\nD. india\nE. eat cake\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The tiger was stuck in what animal prison where he got lazy and fat?\nA. jungle\nB. zoo\nC. kill\nD. india\nE. eat cake\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The tiger was stuck in what animal prison where he got lazy and fat?\nA. jungle\nB. zoo\nC. kill\nD. india\nE. eat cake\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The tiger was stuck in what animal prison where he got lazy and fat?\nA. jungle\nB. zoo\nC. kill\nD. india\nE. eat cake\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The tiger was stuck in what animal prison where he got lazy and fat?\nA. jungle\nB. zoo\nC. kill\nD. india\nE. eat cake\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.453824996948242", "False"]], [["-1.9538249969482422", "False"]], [["-8.703824996948242", "False"]], [["-10.203824996948242", "False"]], [["-9.703824996948242", "False"]]], "filtered_resps": [["-3.453824996948242", "False"], ["-1.9538249969482422", "False"], ["-8.703824996948242", "False"], ["-10.203824996948242", "False"], ["-9.703824996948242", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f9d3a1a53d17228143241d04bd0dee3d12387c7af82cd30bbc3e4855a944e0cb", "prompt_hash": "85f8f04450cf272a6031539acdf9bd35206f79007c68c6938a640866580e8016", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1045, "doc": {"id": "acf6b667e9353b1743b7c4f60a6a9017", "question": "What do parents tell a child to do on the weekend?", "question_concept": "child", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["study", "begin school", "go out to play", "row boat", "clean room"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What do parents tell a child to do on the weekend?\nA. study\nB. begin school\nC. go out to play\nD. row boat\nE. clean room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do parents tell a child to do on the weekend?\nA. study\nB. begin school\nC. go out to play\nD. row boat\nE. clean room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do parents tell a child to do on the weekend?\nA. study\nB. begin school\nC. go out to play\nD. row boat\nE. clean room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do parents tell a child to do on the weekend?\nA. study\nB. begin school\nC. go out to play\nD. row boat\nE. clean room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do parents tell a child to do on the weekend?\nA. study\nB. begin school\nC. go out to play\nD. row boat\nE. clean room\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.1669535636901855", "False"]], [["-6.6669535636901855", "False"]], [["-1.166953444480896", "True"]], [["-5.9169535636901855", "False"]], [["-9.166953086853027", "False"]]], "filtered_resps": [["-5.1669535636901855", "False"], ["-6.6669535636901855", "False"], ["-1.166953444480896", "True"], ["-5.9169535636901855", "False"], ["-9.166953086853027", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1fb0af3dfd261f6635c0a95d133a5326111c656d024ae11a3793a179ef8e936b", "prompt_hash": "8b7c778e045c23aff631a2ac7db57360e1031970c4424cc65503a11980f465bc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1046, "doc": {"id": "15b090801256085ad465e74af47cbee9", "question": "Why are dogs often known as man's best friend?", "question_concept": "dogs", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["aggressive", "friendly", "very loyal", "found outside", "very smart"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Why are dogs often known as man's best friend?\nA. aggressive\nB. friendly\nC. very loyal\nD. found outside\nE. very smart\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why are dogs often known as man's best friend?\nA. aggressive\nB. friendly\nC. very loyal\nD. found outside\nE. very smart\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why are dogs often known as man's best friend?\nA. aggressive\nB. friendly\nC. very loyal\nD. found outside\nE. very smart\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why are dogs often known as man's best friend?\nA. aggressive\nB. friendly\nC. very loyal\nD. found outside\nE. very smart\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why are dogs often known as man's best friend?\nA. aggressive\nB. friendly\nC. very loyal\nD. found outside\nE. very smart\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.154417514801025", "False"]], [["-5.154417514801025", "False"]], [["-0.6544176936149597", "True"]], [["-3.1544177532196045", "False"]], [["-9.404417991638184", "False"]]], "filtered_resps": [["-6.154417514801025", "False"], ["-5.154417514801025", "False"], ["-0.6544176936149597", "True"], ["-3.1544177532196045", "False"], ["-9.404417991638184", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d19b1fc7996640c3b373261604945e827804640285a42b1d0f21308399b2f35b", "prompt_hash": "421064308640866c1a4a89381a39b58a620fc188e92c4bee4b5beabbc592a456", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1047, "doc": {"id": "790b3f583e9bc9424c771691ecc70c20", "question": "Where can you buy a two wheel transportation machine?", "question_concept": "wheel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boat", "michigan", "train station", "bicycle shop", "trunk of car"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you buy a two wheel transportation machine?\nA. boat\nB. michigan\nC. train station\nD. bicycle shop\nE. trunk of car\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you buy a two wheel transportation machine?\nA. boat\nB. michigan\nC. train station\nD. bicycle shop\nE. trunk of car\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you buy a two wheel transportation machine?\nA. boat\nB. michigan\nC. train station\nD. bicycle shop\nE. trunk of car\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you buy a two wheel transportation machine?\nA. boat\nB. michigan\nC. train station\nD. bicycle shop\nE. trunk of car\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you buy a two wheel transportation machine?\nA. boat\nB. michigan\nC. train station\nD. bicycle shop\nE. trunk of car\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9198808670043945", "False"]], [["-3.9198808670043945", "False"]], [["-7.4198808670043945", "False"]], [["-1.169880747795105", "False"]], [["-9.669880867004395", "False"]]], "filtered_resps": [["-2.9198808670043945", "False"], ["-3.9198808670043945", "False"], ["-7.4198808670043945", "False"], ["-1.169880747795105", "False"], ["-9.669880867004395", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a94e890c6b16435f0c4cf1690c2040a9daf642529751569a7faaa9234d19970b", "prompt_hash": "7f2f9c2f39358bb4ea8bc2da2c2b589333c377cce819adad5a5a37d378a2b100", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1048, "doc": {"id": "22b8219d43a38a1130e0a35ece152337", "question": "Where might an alien use a vacuum?", "question_concept": "vacuum", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["space", "closet", "kitchen", "orbit", "container"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where might an alien use a vacuum?\nA. space\nB. closet\nC. kitchen\nD. orbit\nE. container\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where might an alien use a vacuum?\nA. space\nB. closet\nC. kitchen\nD. orbit\nE. container\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where might an alien use a vacuum?\nA. space\nB. closet\nC. kitchen\nD. orbit\nE. container\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where might an alien use a vacuum?\nA. space\nB. closet\nC. kitchen\nD. orbit\nE. container\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where might an alien use a vacuum?\nA. space\nB. closet\nC. kitchen\nD. orbit\nE. container\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0635285377502441", "True"]], [["-3.813528537750244", "False"]], [["-5.563528537750244", "False"]], [["-5.813528537750244", "False"]], [["-5.563528537750244", "False"]]], "filtered_resps": [["-1.0635285377502441", "True"], ["-3.813528537750244", "False"], ["-5.563528537750244", "False"], ["-5.813528537750244", "False"], ["-5.563528537750244", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dfbed2c54d0e41303ca70ed41bd599f671a25aad80852a7ce320cd8a1aca3420", "prompt_hash": "4ff785666f54e4c6c81855ff4d76f98706761f9231020f8085222969fbf557e8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1049, "doc": {"id": "5d4233146435ab0ca211e8ac9bfce76f", "question": "Where do you buy condoms?", "question_concept": "condoms", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["supermarket", "sock drawer", "cd store", "medicine chest", "bedroom"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where do you buy condoms?\nA. supermarket\nB. sock drawer\nC. cd store\nD. medicine chest\nE. bedroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do you buy condoms?\nA. supermarket\nB. sock drawer\nC. cd store\nD. medicine chest\nE. bedroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do you buy condoms?\nA. supermarket\nB. sock drawer\nC. cd store\nD. medicine chest\nE. bedroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do you buy condoms?\nA. supermarket\nB. sock drawer\nC. cd store\nD. medicine chest\nE. bedroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do you buy condoms?\nA. supermarket\nB. sock drawer\nC. cd store\nD. medicine chest\nE. bedroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5549966096878052", "True"]], [["-5.804996490478516", "False"]], [["-5.804996490478516", "False"]], [["-7.054996490478516", "False"]], [["-9.054996490478516", "False"]]], "filtered_resps": [["-0.5549966096878052", "True"], ["-5.804996490478516", "False"], ["-5.804996490478516", "False"], ["-7.054996490478516", "False"], ["-9.054996490478516", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e62affef6664ba79e0e73b80984f70503ab159d422a511e0afa2b6310f29d06a", "prompt_hash": "f7745141739c3d532a07d53910fc647eda3b9a562770fa98cf6cb513302257f1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1050, "doc": {"id": "be737cd4db844574ef594442ce6c9453", "question": "What animal is known for being a follower?", "question_concept": "sheep", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["goat", "expensive", "lion", "wolf", "meadow"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What animal is known for being a follower?\nA. goat\nB. expensive\nC. lion\nD. wolf\nE. meadow\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What animal is known for being a follower?\nA. goat\nB. expensive\nC. lion\nD. wolf\nE. meadow\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What animal is known for being a follower?\nA. goat\nB. expensive\nC. lion\nD. wolf\nE. meadow\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What animal is known for being a follower?\nA. goat\nB. expensive\nC. lion\nD. wolf\nE. meadow\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What animal is known for being a follower?\nA. goat\nB. expensive\nC. lion\nD. wolf\nE. meadow\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6724416017532349", "False"]], [["-5.922441482543945", "False"]], [["-4.672441482543945", "False"]], [["-1.4224416017532349", "True"]], [["-9.922441482543945", "False"]]], "filtered_resps": [["-1.6724416017532349", "False"], ["-5.922441482543945", "False"], ["-4.672441482543945", "False"], ["-1.4224416017532349", "True"], ["-9.922441482543945", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dc052163aec6bc99fb1dd28bb5634de0e1da815420550d20ab89a938d018a600", "prompt_hash": "06a16c0a3482f67f188f43e2371ed5391828d098325e65def9adb916861fe459", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 1051, "doc": {"id": "550164b7cf4e03153484136f10122c70", "question": "The soldier was told to get to the rendezvous point, for there he was suppose to what?", "question_concept": "soldier", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fight enemy", "go to war", "fight for freedom", "wait for orders", "follow instructions"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The soldier was told to get to the rendezvous point, for there he was suppose to what?\nA. fight enemy\nB. go to war\nC. fight for freedom\nD. wait for orders\nE. follow instructions\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The soldier was told to get to the rendezvous point, for there he was suppose to what?\nA. fight enemy\nB. go to war\nC. fight for freedom\nD. wait for orders\nE. follow instructions\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The soldier was told to get to the rendezvous point, for there he was suppose to what?\nA. fight enemy\nB. go to war\nC. fight for freedom\nD. wait for orders\nE. follow instructions\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The soldier was told to get to the rendezvous point, for there he was suppose to what?\nA. fight enemy\nB. go to war\nC. fight for freedom\nD. wait for orders\nE. follow instructions\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The soldier was told to get to the rendezvous point, for there he was suppose to what?\nA. fight enemy\nB. go to war\nC. fight for freedom\nD. wait for orders\nE. follow instructions\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.094893455505371", "False"]], [["-6.094893455505371", "False"]], [["-6.094893455505371", "False"]], [["-1.344893217086792", "True"]], [["-4.094893455505371", "False"]]], "filtered_resps": [["-4.094893455505371", "False"], ["-6.094893455505371", "False"], ["-6.094893455505371", "False"], ["-1.344893217086792", "True"], ["-4.094893455505371", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cc05659f086e3b9b6f6e1445a0a5b3a6787b9b0536fc082e76d44f3644f422a6", "prompt_hash": "882df9a29f3c3acdff10ab2b71688187b6e3e45a0a417f57aea297ff79cfeb89", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1052, "doc": {"id": "a617eb4d27edea93e7fd630ce00c8219", "question": "If you want to kill someone you can do what to them with a gun?", "question_concept": "kill", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sip through", "damnation", "shoot", "commit crime", "eat breakfast"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If you want to kill someone you can do what to them with a gun?\nA. sip through\nB. damnation\nC. shoot\nD. commit crime\nE. eat breakfast\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you want to kill someone you can do what to them with a gun?\nA. sip through\nB. damnation\nC. shoot\nD. commit crime\nE. eat breakfast\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you want to kill someone you can do what to them with a gun?\nA. sip through\nB. damnation\nC. shoot\nD. commit crime\nE. eat breakfast\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you want to kill someone you can do what to them with a gun?\nA. sip through\nB. damnation\nC. shoot\nD. commit crime\nE. eat breakfast\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you want to kill someone you can do what to them with a gun?\nA. sip through\nB. damnation\nC. shoot\nD. commit crime\nE. eat breakfast\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.82249641418457", "False"]], [["-6.07249641418457", "False"]], [["-0.8224962949752808", "True"]], [["-3.5724964141845703", "False"]], [["-6.57249641418457", "False"]]], "filtered_resps": [["-5.82249641418457", "False"], ["-6.07249641418457", "False"], ["-0.8224962949752808", "True"], ["-3.5724964141845703", "False"], ["-6.57249641418457", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9ee8ca27eef7eb18838365764761b92b807cc652f5f8a256fcd2bad17083cebd", "prompt_hash": "f50557f9531816747cdfd9ff7fc2a5ae3d7f3055922e5d468f8c7bb569e99891", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1053, "doc": {"id": "bd47827418d5b8d7fb3502a398644435", "question": "The hostess greeted the employees to the program, she then led them to their what?", "question_concept": "hostess", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["group people", "welcome guests", "occupations", "work room", "seat customer"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The hostess greeted the employees to the program, she then led them to their what?\nA. group people\nB. welcome guests\nC. occupations\nD. work room\nE. seat customer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The hostess greeted the employees to the program, she then led them to their what?\nA. group people\nB. welcome guests\nC. occupations\nD. work room\nE. seat customer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The hostess greeted the employees to the program, she then led them to their what?\nA. group people\nB. welcome guests\nC. occupations\nD. work room\nE. seat customer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The hostess greeted the employees to the program, she then led them to their what?\nA. group people\nB. welcome guests\nC. occupations\nD. work room\nE. seat customer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The hostess greeted the employees to the program, she then led them to their what?\nA. group people\nB. welcome guests\nC. occupations\nD. work room\nE. seat customer\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.90971302986145", "False"]], [["-5.409712791442871", "False"]], [["-7.159712791442871", "False"]], [["-1.1597130298614502", "True"]], [["-8.409712791442871", "False"]]], "filtered_resps": [["-3.90971302986145", "False"], ["-5.409712791442871", "False"], ["-7.159712791442871", "False"], ["-1.1597130298614502", "True"], ["-8.409712791442871", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "30366b16bd219d141bcbca6d6bf34ddc84428516d47ea4201a346aef4f663aec", "prompt_hash": "9f214e2701110561f948f6543cee0d9ac7114c3914677471cb300317ca44bf71", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1054, "doc": {"id": "31487ab8b1e8f12e252590cc58bd19c2", "question": "Where is a likely place to store unused soap?", "question_concept": "soap", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cabinet", "supermarket", "jail", "butt", "own home"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a likely place to store unused soap?\nA. cabinet\nB. supermarket\nC. jail\nD. butt\nE. own home\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a likely place to store unused soap?\nA. cabinet\nB. supermarket\nC. jail\nD. butt\nE. own home\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a likely place to store unused soap?\nA. cabinet\nB. supermarket\nC. jail\nD. butt\nE. own home\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a likely place to store unused soap?\nA. cabinet\nB. supermarket\nC. jail\nD. butt\nE. own home\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a likely place to store unused soap?\nA. cabinet\nB. supermarket\nC. jail\nD. butt\nE. own home\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.45554637908935547", "True"]], [["-6.7055463790893555", "False"]], [["-6.4555463790893555", "False"]], [["-7.4555463790893555", "False"]], [["-3.9555463790893555", "False"]]], "filtered_resps": [["-0.45554637908935547", "True"], ["-6.7055463790893555", "False"], ["-6.4555463790893555", "False"], ["-7.4555463790893555", "False"], ["-3.9555463790893555", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1a328a7090460515a11a3f73a9ec882619399bce51adb8d11a734785e30a1f50", "prompt_hash": "c3e977a168b270e867209ae8a7d142a66bcb38e4b5a5ac124847ec65776420a2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1055, "doc": {"id": "ce2fd94212243f843b3f357046051f57", "question": "Loss of someone you love can cause what kind of feeling in your heart?", "question_concept": "love", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["painful", "happy", "blind", "contagious", "bring joy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Loss of someone you love can cause what kind of feeling in your heart?\nA. painful\nB. happy\nC. blind\nD. contagious\nE. bring joy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Loss of someone you love can cause what kind of feeling in your heart?\nA. painful\nB. happy\nC. blind\nD. contagious\nE. bring joy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Loss of someone you love can cause what kind of feeling in your heart?\nA. painful\nB. happy\nC. blind\nD. contagious\nE. bring joy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Loss of someone you love can cause what kind of feeling in your heart?\nA. painful\nB. happy\nC. blind\nD. contagious\nE. bring joy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Loss of someone you love can cause what kind of feeling in your heart?\nA. painful\nB. happy\nC. blind\nD. contagious\nE. bring joy\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7778351902961731", "True"]], [["-6.027835369110107", "False"]], [["-6.277835369110107", "False"]], [["-7.027835369110107", "False"]], [["-9.27783489227295", "False"]]], "filtered_resps": [["-0.7778351902961731", "True"], ["-6.027835369110107", "False"], ["-6.277835369110107", "False"], ["-7.027835369110107", "False"], ["-9.27783489227295", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "25cdf68bf41dfae43c1e977f2ee066b4c72f7533f836ba16e464ad8d2266febd", "prompt_hash": "05d2be375d8bdeae3ebcd3f3c08f3126a26badac840c6807be661c92eed1965d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1056, "doc": {"id": "f87f40db71a56b5beda3194550202dc9_1", "question": "Where in your home would you keep a ballpoint pen when not in use?", "question_concept": "ballpoint pen", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["backpack", "bank", "desk drawer", "eat cake", "office desk"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Where in your home would you keep a ballpoint pen when not in use?\nA. backpack\nB. bank\nC. desk drawer\nD. eat cake\nE. office desk\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where in your home would you keep a ballpoint pen when not in use?\nA. backpack\nB. bank\nC. desk drawer\nD. eat cake\nE. office desk\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where in your home would you keep a ballpoint pen when not in use?\nA. backpack\nB. bank\nC. desk drawer\nD. eat cake\nE. office desk\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where in your home would you keep a ballpoint pen when not in use?\nA. backpack\nB. bank\nC. desk drawer\nD. eat cake\nE. office desk\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where in your home would you keep a ballpoint pen when not in use?\nA. backpack\nB. bank\nC. desk drawer\nD. eat cake\nE. office desk\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.71622633934021", "False"]], [["-4.966226100921631", "False"]], [["-0.9662262797355652", "True"]], [["-8.466226577758789", "False"]], [["-3.71622633934021", "False"]]], "filtered_resps": [["-2.71622633934021", "False"], ["-4.966226100921631", "False"], ["-0.9662262797355652", "True"], ["-8.466226577758789", "False"], ["-3.71622633934021", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cfb8e1aad0052f0e49683c8d046f12ed6b90ab2737b90bc3d648a410063409cc", "prompt_hash": "5534b2b03feceb116e00d0c50c20c716720a9deb8fecdb9a084eb9ed58e55450", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1057, "doc": {"id": "0b25bbd9e9aa976655e1975e31331709", "question": "James was someone who was caught in his own delusions.  To him, the truth didn't do what what?", "question_concept": "truth", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["work to advantage", "matter to", "help", "free mind", "further knowledge"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: James was someone who was caught in his own delusions.  To him, the truth didn't do what what?\nA. work to advantage\nB. matter to\nC. help\nD. free mind\nE. further knowledge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James was someone who was caught in his own delusions.  To him, the truth didn't do what what?\nA. work to advantage\nB. matter to\nC. help\nD. free mind\nE. further knowledge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James was someone who was caught in his own delusions.  To him, the truth didn't do what what?\nA. work to advantage\nB. matter to\nC. help\nD. free mind\nE. further knowledge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James was someone who was caught in his own delusions.  To him, the truth didn't do what what?\nA. work to advantage\nB. matter to\nC. help\nD. free mind\nE. further knowledge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James was someone who was caught in his own delusions.  To him, the truth didn't do what what?\nA. work to advantage\nB. matter to\nC. help\nD. free mind\nE. further knowledge\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7617006301879883", "False"]], [["-1.5117006301879883", "False"]], [["-2.2617006301879883", "False"]], [["-4.011700630187988", "False"]], [["-8.761700630187988", "False"]]], "filtered_resps": [["-3.7617006301879883", "False"], ["-1.5117006301879883", "False"], ["-2.2617006301879883", "False"], ["-4.011700630187988", "False"], ["-8.761700630187988", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "da150ec30015a87b41b90f090fb189b84770550db608257d9171ccc568633e9a", "prompt_hash": "9124e2742cbb7d140242ea1315c2464e960614f84664e8fe0f22d3e7083807c2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1058, "doc": {"id": "925232b4c9bba945a38ac7ef0f15f8d0", "question": "He wanted to live somewhere were every yard was uniform in size and landscaping, where should he look for a house?", "question_concept": "yard", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["city", "three feet", "subdivision", "parking garage", "michigan"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: He wanted to live somewhere were every yard was uniform in size and landscaping, where should he look for a house?\nA. city\nB. three feet\nC. subdivision\nD. parking garage\nE. michigan\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He wanted to live somewhere were every yard was uniform in size and landscaping, where should he look for a house?\nA. city\nB. three feet\nC. subdivision\nD. parking garage\nE. michigan\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He wanted to live somewhere were every yard was uniform in size and landscaping, where should he look for a house?\nA. city\nB. three feet\nC. subdivision\nD. parking garage\nE. michigan\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He wanted to live somewhere were every yard was uniform in size and landscaping, where should he look for a house?\nA. city\nB. three feet\nC. subdivision\nD. parking garage\nE. michigan\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He wanted to live somewhere were every yard was uniform in size and landscaping, where should he look for a house?\nA. city\nB. three feet\nC. subdivision\nD. parking garage\nE. michigan\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1528396606445312", "False"]], [["-9.402839660644531", "False"]], [["-1.1528395414352417", "True"]], [["-8.902839660644531", "False"]], [["-10.652839660644531", "False"]]], "filtered_resps": [["-2.1528396606445312", "False"], ["-9.402839660644531", "False"], ["-1.1528395414352417", "True"], ["-8.902839660644531", "False"], ["-10.652839660644531", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5ce8ebcfbe68ac57ace1b8f55ff6cf71813405949aa206c14a062baeae5de451", "prompt_hash": "dd739525ad8ff2c0010a6b6cbba8ea26f5958f5398da497073e5548263cf408c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1059, "doc": {"id": "3338109fcafaaa370c8900a53e1b3ed8", "question": "The flasks was used to distill elements, where was is being used?", "question_concept": "flask", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["laboratory", "chemistry lab", "coat pocket", "after hours speakeasy", "bordello"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The flasks was used to distill elements, where was is being used?\nA. laboratory\nB. chemistry lab\nC. coat pocket\nD. after hours speakeasy\nE. bordello\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The flasks was used to distill elements, where was is being used?\nA. laboratory\nB. chemistry lab\nC. coat pocket\nD. after hours speakeasy\nE. bordello\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The flasks was used to distill elements, where was is being used?\nA. laboratory\nB. chemistry lab\nC. coat pocket\nD. after hours speakeasy\nE. bordello\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The flasks was used to distill elements, where was is being used?\nA. laboratory\nB. chemistry lab\nC. coat pocket\nD. after hours speakeasy\nE. bordello\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The flasks was used to distill elements, where was is being used?\nA. laboratory\nB. chemistry lab\nC. coat pocket\nD. after hours speakeasy\nE. bordello\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.471972942352295", "False"]], [["-1.7219730615615845", "False"]], [["-7.721972942352295", "False"]], [["-6.721972942352295", "False"]], [["-10.471973419189453", "False"]]], "filtered_resps": [["-2.471972942352295", "False"], ["-1.7219730615615845", "False"], ["-7.721972942352295", "False"], ["-6.721972942352295", "False"], ["-10.471973419189453", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7acc48f7a20c674c857c34c7d9b02997bf32a91452146916673c6a1ebb3771a5", "prompt_hash": "3809f35ed99d8b61177fc724441363cbea2382eed74e159ad739d4e52e851197", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1060, "doc": {"id": "e172a93c72d305ee8262a8deb00d9fc3", "question": "What was the man encouraged to do after he expressed his anger violently?", "question_concept": "anger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cool off", "punch", "illustrate point", "fight", "release energy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What was the man encouraged to do after he expressed his anger violently?\nA. cool off\nB. punch\nC. illustrate point\nD. fight\nE. release energy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What was the man encouraged to do after he expressed his anger violently?\nA. cool off\nB. punch\nC. illustrate point\nD. fight\nE. release energy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What was the man encouraged to do after he expressed his anger violently?\nA. cool off\nB. punch\nC. illustrate point\nD. fight\nE. release energy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What was the man encouraged to do after he expressed his anger violently?\nA. cool off\nB. punch\nC. illustrate point\nD. fight\nE. release energy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What was the man encouraged to do after he expressed his anger violently?\nA. cool off\nB. punch\nC. illustrate point\nD. fight\nE. release energy\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7435908913612366", "True"]], [["-6.493590831756592", "False"]], [["-6.493590831756592", "False"]], [["-8.99359130859375", "False"]], [["-8.49359130859375", "False"]]], "filtered_resps": [["-0.7435908913612366", "True"], ["-6.493590831756592", "False"], ["-6.493590831756592", "False"], ["-8.99359130859375", "False"], ["-8.49359130859375", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "dd1e3a092802783b29b9fad6f458b115a2f029caf13483d24983a1e0e7be2adf", "prompt_hash": "5cb0cdb6d8b4c239ee710ccbd75aa91bba1b052863e0ba0b3d9dd955320a36d8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1061, "doc": {"id": "f1c2e37abf17d9e4ad16eb40f966c79f", "question": "Where can a student learn to play a triangle?", "question_concept": "triangle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["math class", "math book", "in pythagorus' band", "orchestra", "music class"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where can a student learn to play a triangle?\nA. math class\nB. math book\nC. in pythagorus' band\nD. orchestra\nE. music class\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can a student learn to play a triangle?\nA. math class\nB. math book\nC. in pythagorus' band\nD. orchestra\nE. music class\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can a student learn to play a triangle?\nA. math class\nB. math book\nC. in pythagorus' band\nD. orchestra\nE. music class\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can a student learn to play a triangle?\nA. math class\nB. math book\nC. in pythagorus' band\nD. orchestra\nE. music class\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can a student learn to play a triangle?\nA. math class\nB. math book\nC. in pythagorus' band\nD. orchestra\nE. music class\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.541684150695801", "False"]], [["-6.291684150695801", "False"]], [["-2.291684150695801", "False"]], [["-3.541684150695801", "False"]], [["-1.0416841506958008", "True"]]], "filtered_resps": [["-2.541684150695801", "False"], ["-6.291684150695801", "False"], ["-2.291684150695801", "False"], ["-3.541684150695801", "False"], ["-1.0416841506958008", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eceaa92ae47b5ce21bc21760442f72740562483f0633161b2fce56bd73243b40", "prompt_hash": "4b62b3da925324474a2dad435ecbd1163e5befc9c90234873dd744e245d0590d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1062, "doc": {"id": "d29252ddaf7c7ef491abcce342d7bb98", "question": "What do you need to do to use television if it is already turned on?", "question_concept": "use television", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["get wet", "open eyes", "kill", "plug in", "first turn on power"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What do you need to do to use television if it is already turned on?\nA. get wet\nB. open eyes\nC. kill\nD. plug in\nE. first turn on power\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you need to do to use television if it is already turned on?\nA. get wet\nB. open eyes\nC. kill\nD. plug in\nE. first turn on power\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you need to do to use television if it is already turned on?\nA. get wet\nB. open eyes\nC. kill\nD. plug in\nE. first turn on power\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you need to do to use television if it is already turned on?\nA. get wet\nB. open eyes\nC. kill\nD. plug in\nE. first turn on power\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you need to do to use television if it is already turned on?\nA. get wet\nB. open eyes\nC. kill\nD. plug in\nE. first turn on power\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.830162525177002", "False"]], [["-1.830162525177002", "True"]], [["-6.080162525177002", "False"]], [["-3.080162525177002", "False"]], [["-3.330162525177002", "False"]]], "filtered_resps": [["-4.830162525177002", "False"], ["-1.830162525177002", "True"], ["-6.080162525177002", "False"], ["-3.080162525177002", "False"], ["-3.330162525177002", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1ce0e5e2ee6b1aead0aba2e7fe040283ee01d2eb3527d3e2f40c65ec17d8356b", "prompt_hash": "cd93d7e5f8f467f83b892610b94c2cfd97c5ff7e01f715e34c56ed1b24dce5f4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1063, "doc": {"id": "8c3c6b34bdb650a6517bca3786406c99", "question": "The guys had a regular poker game, rather than going to the movies this what their what?", "question_concept": "playing poker", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["competition", "fun game", "losing money", "fun", "social event"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The guys had a regular poker game, rather than going to the movies this what their what?\nA. competition\nB. fun game\nC. losing money\nD. fun\nE. social event\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The guys had a regular poker game, rather than going to the movies this what their what?\nA. competition\nB. fun game\nC. losing money\nD. fun\nE. social event\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The guys had a regular poker game, rather than going to the movies this what their what?\nA. competition\nB. fun game\nC. losing money\nD. fun\nE. social event\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The guys had a regular poker game, rather than going to the movies this what their what?\nA. competition\nB. fun game\nC. losing money\nD. fun\nE. social event\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The guys had a regular poker game, rather than going to the movies this what their what?\nA. competition\nB. fun game\nC. losing money\nD. fun\nE. social event\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.047905921936035", "False"]], [["-2.797905921936035", "False"]], [["-6.797905921936035", "False"]], [["-2.547905921936035", "False"]], [["-1.7979059219360352", "False"]]], "filtered_resps": [["-4.047905921936035", "False"], ["-2.797905921936035", "False"], ["-6.797905921936035", "False"], ["-2.547905921936035", "False"], ["-1.7979059219360352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "466507db366c685508c234b9028af0fb465c2d01a711a7f1db0e5636d7c7015a", "prompt_hash": "dfe6d106ec9ee2875f15e47023752925d89cb7ac70f7c3cc6476af666ee48aea", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1064, "doc": {"id": "ff1bf2ec835c9df8695ae0cfb5281646", "question": "When you stroke a dogs fur what have you done?", "question_concept": "dog", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["start fighting", "play", "lots of attention", "petted", "bone"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When you stroke a dogs fur what have you done?\nA. start fighting\nB. play\nC. lots of attention\nD. petted\nE. bone\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you stroke a dogs fur what have you done?\nA. start fighting\nB. play\nC. lots of attention\nD. petted\nE. bone\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you stroke a dogs fur what have you done?\nA. start fighting\nB. play\nC. lots of attention\nD. petted\nE. bone\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you stroke a dogs fur what have you done?\nA. start fighting\nB. play\nC. lots of attention\nD. petted\nE. bone\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you stroke a dogs fur what have you done?\nA. start fighting\nB. play\nC. lots of attention\nD. petted\nE. bone\nAnswer:", "arg_1": " E"}}, "resps": [[["-7.104303359985352", "False"]], [["-6.354303359985352", "False"]], [["-8.604303359985352", "False"]], [["-1.6043034791946411", "False"]], [["-9.854303359985352", "False"]]], "filtered_resps": [["-7.104303359985352", "False"], ["-6.354303359985352", "False"], ["-8.604303359985352", "False"], ["-1.6043034791946411", "False"], ["-9.854303359985352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1dd34b8b5bbf271450f9893c225ee8187dfa25c210dd5dd4906936ebc5b5088c", "prompt_hash": "e1645586ae771c1ccdc2978564506245022588b5cc25d8fe7e0b7bd57bd9201a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1065, "doc": {"id": "c7526b682e64f355384631b35cd78fc9", "question": "Dan fell off a bar stool.  He did this because he was what than ever before?", "question_concept": "bar stool", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kitchen", "drunker", "tavern", "restaurant", "shorter"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Dan fell off a bar stool.  He did this because he was what than ever before?\nA. kitchen\nB. drunker\nC. tavern\nD. restaurant\nE. shorter\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Dan fell off a bar stool.  He did this because he was what than ever before?\nA. kitchen\nB. drunker\nC. tavern\nD. restaurant\nE. shorter\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Dan fell off a bar stool.  He did this because he was what than ever before?\nA. kitchen\nB. drunker\nC. tavern\nD. restaurant\nE. shorter\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Dan fell off a bar stool.  He did this because he was what than ever before?\nA. kitchen\nB. drunker\nC. tavern\nD. restaurant\nE. shorter\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Dan fell off a bar stool.  He did this because he was what than ever before?\nA. kitchen\nB. drunker\nC. tavern\nD. restaurant\nE. shorter\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.961150169372559", "False"]], [["-1.2111501693725586", "False"]], [["-8.711150169372559", "False"]], [["-8.961150169372559", "False"]], [["-8.711150169372559", "False"]]], "filtered_resps": [["-5.961150169372559", "False"], ["-1.2111501693725586", "False"], ["-8.711150169372559", "False"], ["-8.961150169372559", "False"], ["-8.711150169372559", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f40fc740f9583d1cbbc047bbd918bf99120ee300d8ef0bc5ea57cc405484342e", "prompt_hash": "5463e730eb946d22d0a75d25fbf86c3f85b9e60fca99d7703baa98119d1653eb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1066, "doc": {"id": "0fba83d3997f048adcc31937221af77e", "question": "The wood was still rough to the touch, what did the woodworker have to do?", "question_concept": "wood", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["petrify", "sanded", "warp", "composted", "clean"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: The wood was still rough to the touch, what did the woodworker have to do?\nA. petrify\nB. sanded\nC. warp\nD. composted\nE. clean\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The wood was still rough to the touch, what did the woodworker have to do?\nA. petrify\nB. sanded\nC. warp\nD. composted\nE. clean\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The wood was still rough to the touch, what did the woodworker have to do?\nA. petrify\nB. sanded\nC. warp\nD. composted\nE. clean\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The wood was still rough to the touch, what did the woodworker have to do?\nA. petrify\nB. sanded\nC. warp\nD. composted\nE. clean\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The wood was still rough to the touch, what did the woodworker have to do?\nA. petrify\nB. sanded\nC. warp\nD. composted\nE. clean\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9320130348205566", "False"]], [["-0.9320131540298462", "True"]], [["-8.432013511657715", "False"]], [["-8.932013511657715", "False"]], [["-7.932013034820557", "False"]]], "filtered_resps": [["-3.9320130348205566", "False"], ["-0.9320131540298462", "True"], ["-8.432013511657715", "False"], ["-8.932013511657715", "False"], ["-7.932013034820557", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2be544b25c4577bde88e49c56e536342718e5bdf0237383f8986b7b97752cf95", "prompt_hash": "27efe66c547cb7e55c4d88e16bb59bc477a15b5a96475f46a6af95a361fc789e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1067, "doc": {"id": "a5456dc611aa93b81d7ab6ed8e160f85", "question": "The chief saw his entire tribe wiped out, he was a leader with a single what?", "question_concept": "chief", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["peon", "indian", "minister", "follower", "employee"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The chief saw his entire tribe wiped out, he was a leader with a single what?\nA. peon\nB. indian\nC. minister\nD. follower\nE. employee\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The chief saw his entire tribe wiped out, he was a leader with a single what?\nA. peon\nB. indian\nC. minister\nD. follower\nE. employee\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The chief saw his entire tribe wiped out, he was a leader with a single what?\nA. peon\nB. indian\nC. minister\nD. follower\nE. employee\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The chief saw his entire tribe wiped out, he was a leader with a single what?\nA. peon\nB. indian\nC. minister\nD. follower\nE. employee\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The chief saw his entire tribe wiped out, he was a leader with a single what?\nA. peon\nB. indian\nC. minister\nD. follower\nE. employee\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.469322681427002", "False"]], [["-1.969322681427002", "False"]], [["-3.719322681427002", "False"]], [["-4.469322681427002", "False"]], [["-6.719322681427002", "False"]]], "filtered_resps": [["-2.469322681427002", "False"], ["-1.969322681427002", "False"], ["-3.719322681427002", "False"], ["-4.469322681427002", "False"], ["-6.719322681427002", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "034e40cf4088e8a6406d7d09a9c8ebcf018e72d84163f3f0abc439e6865e4a6c", "prompt_hash": "b35fe1b75cf6e3b09d2f84a846243b3a03bacb67bbd61991e74d079915005423", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1068, "doc": {"id": "11416df796f63d2f0dddc846b9c139d3", "question": "The flower grew tall to compete for sunlight, what did its neighbor do?", "question_concept": "flower", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["blossom", "park", "open", "cast shadow", "vase"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The flower grew tall to compete for sunlight, what did its neighbor do?\nA. blossom\nB. park\nC. open\nD. cast shadow\nE. vase\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The flower grew tall to compete for sunlight, what did its neighbor do?\nA. blossom\nB. park\nC. open\nD. cast shadow\nE. vase\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The flower grew tall to compete for sunlight, what did its neighbor do?\nA. blossom\nB. park\nC. open\nD. cast shadow\nE. vase\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The flower grew tall to compete for sunlight, what did its neighbor do?\nA. blossom\nB. park\nC. open\nD. cast shadow\nE. vase\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The flower grew tall to compete for sunlight, what did its neighbor do?\nA. blossom\nB. park\nC. open\nD. cast shadow\nE. vase\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7022745609283447", "False"]], [["-6.702274322509766", "False"]], [["-4.202274322509766", "False"]], [["-0.7022745013237", "True"]], [["-9.702274322509766", "False"]]], "filtered_resps": [["-3.7022745609283447", "False"], ["-6.702274322509766", "False"], ["-4.202274322509766", "False"], ["-0.7022745013237", "True"], ["-9.702274322509766", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e1afb6edffa5a3ba83e91474f7ec26bc63fda76ec84dc746f4d1b032f86468de", "prompt_hash": "0eecdd2291a005504fd84c93e72afe1880a42565cdb7425416753e10b7af5333", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1069, "doc": {"id": "c908d7c4633c5e6add9463bdd47cb27e", "question": "If while driving to work another car makes a careless maneuver, what emotion might you feel?", "question_concept": "driving to work", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boredom", "happiness", "transportation cost", "getting there", "road rage"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: If while driving to work another car makes a careless maneuver, what emotion might you feel?\nA. boredom\nB. happiness\nC. transportation cost\nD. getting there\nE. road rage\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If while driving to work another car makes a careless maneuver, what emotion might you feel?\nA. boredom\nB. happiness\nC. transportation cost\nD. getting there\nE. road rage\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If while driving to work another car makes a careless maneuver, what emotion might you feel?\nA. boredom\nB. happiness\nC. transportation cost\nD. getting there\nE. road rage\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If while driving to work another car makes a careless maneuver, what emotion might you feel?\nA. boredom\nB. happiness\nC. transportation cost\nD. getting there\nE. road rage\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If while driving to work another car makes a careless maneuver, what emotion might you feel?\nA. boredom\nB. happiness\nC. transportation cost\nD. getting there\nE. road rage\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.341148853302002", "False"]], [["-7.091148853302002", "False"]], [["-6.591148853302002", "False"]], [["-5.341148853302002", "False"]], [["-0.8411489129066467", "True"]]], "filtered_resps": [["-5.341148853302002", "False"], ["-7.091148853302002", "False"], ["-6.591148853302002", "False"], ["-5.341148853302002", "False"], ["-0.8411489129066467", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "20e495a41f83c6c6909cbff8f14ce99c0b4c2792985a5aff09e6663228a50238", "prompt_hash": "54012183e282870bb6d2fb731f45a6787eba1787898af4c7d403c5efb9bc5d6a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1070, "doc": {"id": "7e522a60756f854c5331125f998bc36b", "question": "What kind of food makes someone sick?", "question_concept": "food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boat", "necessary to live", "edible", "unhealthy", "kitchen"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What kind of food makes someone sick?\nA. boat\nB. necessary to live\nC. edible\nD. unhealthy\nE. kitchen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What kind of food makes someone sick?\nA. boat\nB. necessary to live\nC. edible\nD. unhealthy\nE. kitchen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What kind of food makes someone sick?\nA. boat\nB. necessary to live\nC. edible\nD. unhealthy\nE. kitchen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What kind of food makes someone sick?\nA. boat\nB. necessary to live\nC. edible\nD. unhealthy\nE. kitchen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What kind of food makes someone sick?\nA. boat\nB. necessary to live\nC. edible\nD. unhealthy\nE. kitchen\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.460381507873535", "False"]], [["-5.960381507873535", "False"]], [["-4.460381507873535", "False"]], [["-0.9603813886642456", "True"]], [["-7.210381507873535", "False"]]], "filtered_resps": [["-4.460381507873535", "False"], ["-5.960381507873535", "False"], ["-4.460381507873535", "False"], ["-0.9603813886642456", "True"], ["-7.210381507873535", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6866c01af0a69ac7246829c7a0afe781bfabe3b228813f5f31d90cf28f05f9cd", "prompt_hash": "eed6f0331ebd4cd8a0ee71024ba449244e0c7df86153c7bc95072c6d528edbd7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1071, "doc": {"id": "f4a75bf3f115b826a8097edfd0ff2781", "question": "Where would you find the sharpest parts of a triangle?", "question_concept": "triangle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["three vertices", "point", "3 sides", "three sides", "math book"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find the sharpest parts of a triangle?\nA. three vertices\nB. point\nC. 3 sides\nD. three sides\nE. math book\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find the sharpest parts of a triangle?\nA. three vertices\nB. point\nC. 3 sides\nD. three sides\nE. math book\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find the sharpest parts of a triangle?\nA. three vertices\nB. point\nC. 3 sides\nD. three sides\nE. math book\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find the sharpest parts of a triangle?\nA. three vertices\nB. point\nC. 3 sides\nD. three sides\nE. math book\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find the sharpest parts of a triangle?\nA. three vertices\nB. point\nC. 3 sides\nD. three sides\nE. math book\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.5484673976898193", "True"]], [["-5.048467636108398", "False"]], [["-6.548467636108398", "False"]], [["-7.798467636108398", "False"]], [["-8.048467636108398", "False"]]], "filtered_resps": [["-0.5484673976898193", "True"], ["-5.048467636108398", "False"], ["-6.548467636108398", "False"], ["-7.798467636108398", "False"], ["-8.048467636108398", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "75cd0dbae36b39243af5aee76c24692a9bfbb7232ea61ae797c73bc44364d81c", "prompt_hash": "c0c55fd7a452d329bea7c537782db87f880a6f80d58e30b2268b4cc9cb4ec7db", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1072, "doc": {"id": "02f43014a135cbd39f23b044c99de96e", "question": "How might a automobile get off a freeway?", "question_concept": "automobile", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["exit ramp", "garage", "driveway", "repair shop", "stop light"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: How might a automobile get off a freeway?\nA. exit ramp\nB. garage\nC. driveway\nD. repair shop\nE. stop light\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How might a automobile get off a freeway?\nA. exit ramp\nB. garage\nC. driveway\nD. repair shop\nE. stop light\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How might a automobile get off a freeway?\nA. exit ramp\nB. garage\nC. driveway\nD. repair shop\nE. stop light\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How might a automobile get off a freeway?\nA. exit ramp\nB. garage\nC. driveway\nD. repair shop\nE. stop light\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How might a automobile get off a freeway?\nA. exit ramp\nB. garage\nC. driveway\nD. repair shop\nE. stop light\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7046897411346436", "True"]], [["-7.704689979553223", "False"]], [["-7.204689979553223", "False"]], [["-9.204689979553223", "False"]], [["-8.704689979553223", "False"]]], "filtered_resps": [["-0.7046897411346436", "True"], ["-7.704689979553223", "False"], ["-7.204689979553223", "False"], ["-9.204689979553223", "False"], ["-8.704689979553223", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "30a5cb045752df64f6605085ab82f9bc92557368b93db4f74ecfec7001525370", "prompt_hash": "903f6e22610ad1c59d4ca3d0630ef32dd829b23e6d33dc48257a09d5b46b13fb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1073, "doc": {"id": "8cf478192696744b3427f7c109019af5", "question": "What does going to bed with your spouse for sex lead to?", "question_concept": "going to bed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bad dreams", "a good nights sleep", "rest", "sleepiness", "get pregnant"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What does going to bed with your spouse for sex lead to?\nA. bad dreams\nB. a good nights sleep\nC. rest\nD. sleepiness\nE. get pregnant\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does going to bed with your spouse for sex lead to?\nA. bad dreams\nB. a good nights sleep\nC. rest\nD. sleepiness\nE. get pregnant\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does going to bed with your spouse for sex lead to?\nA. bad dreams\nB. a good nights sleep\nC. rest\nD. sleepiness\nE. get pregnant\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does going to bed with your spouse for sex lead to?\nA. bad dreams\nB. a good nights sleep\nC. rest\nD. sleepiness\nE. get pregnant\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does going to bed with your spouse for sex lead to?\nA. bad dreams\nB. a good nights sleep\nC. rest\nD. sleepiness\nE. get pregnant\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.674046516418457", "False"]], [["-2.924046516418457", "False"]], [["-5.674046516418457", "False"]], [["-5.424046516418457", "False"]], [["-3.674046516418457", "False"]]], "filtered_resps": [["-3.674046516418457", "False"], ["-2.924046516418457", "False"], ["-5.674046516418457", "False"], ["-5.424046516418457", "False"], ["-3.674046516418457", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "08b52902099ec5216e8feba854027601fb3b7eae24e73fdc24770fa426595c12", "prompt_hash": "4dcd90873d63327b7eb2d648e5560f35b6f491ea14e727c4e58d89881dc74696", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1074, "doc": {"id": "4ccd43cdff044bc4c644dadff1ff1e0b", "question": "What would it be if they get a surprising show over and over?", "question_concept": "surprising", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["surprise", "fight", "annoyance", "might scare", "irritated"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What would it be if they get a surprising show over and over?\nA. surprise\nB. fight\nC. annoyance\nD. might scare\nE. irritated\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would it be if they get a surprising show over and over?\nA. surprise\nB. fight\nC. annoyance\nD. might scare\nE. irritated\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would it be if they get a surprising show over and over?\nA. surprise\nB. fight\nC. annoyance\nD. might scare\nE. irritated\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would it be if they get a surprising show over and over?\nA. surprise\nB. fight\nC. annoyance\nD. might scare\nE. irritated\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would it be if they get a surprising show over and over?\nA. surprise\nB. fight\nC. annoyance\nD. might scare\nE. irritated\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.6865098476409912", "False"]], [["-4.93651008605957", "False"]], [["-3.936509847640991", "False"]], [["-3.186509847640991", "False"]], [["-5.68651008605957", "False"]]], "filtered_resps": [["-1.6865098476409912", "False"], ["-4.93651008605957", "False"], ["-3.936509847640991", "False"], ["-3.186509847640991", "False"], ["-5.68651008605957", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e9e6b1c725cea2581438a92764732dc2589d71b0d3bc5e89cf847a08c72bbdc2", "prompt_hash": "ae271651e0609f42fee2a193bc71728bc5d88d4b09ddc71b9028248b84deaadc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1075, "doc": {"id": "7b7941b883328ad39048d4dfb1eb5623", "question": "Sally thought that competing wasn't worth the risk. If she pushed more what might happen?", "question_concept": "competing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pressure", "trying harder", "put harder", "enemies", "death"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Sally thought that competing wasn't worth the risk. If she pushed more what might happen?\nA. pressure\nB. trying harder\nC. put harder\nD. enemies\nE. death\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sally thought that competing wasn't worth the risk. If she pushed more what might happen?\nA. pressure\nB. trying harder\nC. put harder\nD. enemies\nE. death\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sally thought that competing wasn't worth the risk. If she pushed more what might happen?\nA. pressure\nB. trying harder\nC. put harder\nD. enemies\nE. death\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sally thought that competing wasn't worth the risk. If she pushed more what might happen?\nA. pressure\nB. trying harder\nC. put harder\nD. enemies\nE. death\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sally thought that competing wasn't worth the risk. If she pushed more what might happen?\nA. pressure\nB. trying harder\nC. put harder\nD. enemies\nE. death\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.905202865600586", "False"]], [["-2.905202865600586", "False"]], [["-3.405202865600586", "False"]], [["-2.155202865600586", "False"]], [["-3.405202865600586", "False"]]], "filtered_resps": [["-1.905202865600586", "False"], ["-2.905202865600586", "False"], ["-3.405202865600586", "False"], ["-2.155202865600586", "False"], ["-3.405202865600586", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f66390de31458a578f042fd3468174cb6c97404d8b673f31d845d6d438d42e28", "prompt_hash": "e4481d9e8b039312c8980ba95d4fbad5c496d03834709842a1b88a1f549e83ac", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1076, "doc": {"id": "008b7ba0c039f6d0d542c6c90aae173c", "question": "John is sitting in a toilet stall in a bathroom, outside he can hear cars going around in circles.  What is the function of the place he is most likely at?", "question_concept": "bathroom", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eating food", "public place", "race track", "at hotel", "public building"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: John is sitting in a toilet stall in a bathroom, outside he can hear cars going around in circles.  What is the function of the place he is most likely at?\nA. eating food\nB. public place\nC. race track\nD. at hotel\nE. public building\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John is sitting in a toilet stall in a bathroom, outside he can hear cars going around in circles.  What is the function of the place he is most likely at?\nA. eating food\nB. public place\nC. race track\nD. at hotel\nE. public building\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John is sitting in a toilet stall in a bathroom, outside he can hear cars going around in circles.  What is the function of the place he is most likely at?\nA. eating food\nB. public place\nC. race track\nD. at hotel\nE. public building\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John is sitting in a toilet stall in a bathroom, outside he can hear cars going around in circles.  What is the function of the place he is most likely at?\nA. eating food\nB. public place\nC. race track\nD. at hotel\nE. public building\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John is sitting in a toilet stall in a bathroom, outside he can hear cars going around in circles.  What is the function of the place he is most likely at?\nA. eating food\nB. public place\nC. race track\nD. at hotel\nE. public building\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.226169586181641", "False"]], [["-3.4761693477630615", "False"]], [["-4.476169586181641", "False"]], [["-5.976169586181641", "False"]], [["-1.9761693477630615", "False"]]], "filtered_resps": [["-4.226169586181641", "False"], ["-3.4761693477630615", "False"], ["-4.476169586181641", "False"], ["-5.976169586181641", "False"], ["-1.9761693477630615", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7c718ce9c9fda8fd3ccd4ae52b9b0f76b9aed413ad7c68b3aa9378cc91f0d93a", "prompt_hash": "d3793a2e26cb02fb609db467048dae9998d37472366e8fb04a47ed4680f472f9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1077, "doc": {"id": "4c968fa73699a38639ba3ffa1745bc21", "question": "What event might one buy tickets for seats?", "question_concept": "seats", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["park", "show", "auditorium", "movies", "rest area"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What event might one buy tickets for seats?\nA. park\nB. show\nC. auditorium\nD. movies\nE. rest area\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What event might one buy tickets for seats?\nA. park\nB. show\nC. auditorium\nD. movies\nE. rest area\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What event might one buy tickets for seats?\nA. park\nB. show\nC. auditorium\nD. movies\nE. rest area\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What event might one buy tickets for seats?\nA. park\nB. show\nC. auditorium\nD. movies\nE. rest area\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What event might one buy tickets for seats?\nA. park\nB. show\nC. auditorium\nD. movies\nE. rest area\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2365360260009766", "False"]], [["-1.4865360260009766", "True"]], [["-4.736536026000977", "False"]], [["-4.486536026000977", "False"]], [["-8.736536026000977", "False"]]], "filtered_resps": [["-3.2365360260009766", "False"], ["-1.4865360260009766", "True"], ["-4.736536026000977", "False"], ["-4.486536026000977", "False"], ["-8.736536026000977", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "aeedd06d83f08030e0d83dee612ac339ce20a2c0af5e26c91c458499e8a3005d", "prompt_hash": "4e9af47498da1e5e8e312e9adac8330aed1af68b94286e6c792161c16103cabf", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1078, "doc": {"id": "b1d5cdbf8ef7b3954a6a352bd4df5866", "question": "The merchant wanted to open in a high-traffic space, where did he rent space?", "question_concept": "merchant", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mall", "business", "store", "sale", "sell goods"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The merchant wanted to open in a high-traffic space, where did he rent space?\nA. mall\nB. business\nC. store\nD. sale\nE. sell goods\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The merchant wanted to open in a high-traffic space, where did he rent space?\nA. mall\nB. business\nC. store\nD. sale\nE. sell goods\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The merchant wanted to open in a high-traffic space, where did he rent space?\nA. mall\nB. business\nC. store\nD. sale\nE. sell goods\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The merchant wanted to open in a high-traffic space, where did he rent space?\nA. mall\nB. business\nC. store\nD. sale\nE. sell goods\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The merchant wanted to open in a high-traffic space, where did he rent space?\nA. mall\nB. business\nC. store\nD. sale\nE. sell goods\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0000653266906738", "True"]], [["-5.500065326690674", "False"]], [["-2.750065326690674", "False"]], [["-7.750065326690674", "False"]], [["-8.000064849853516", "False"]]], "filtered_resps": [["-1.0000653266906738", "True"], ["-5.500065326690674", "False"], ["-2.750065326690674", "False"], ["-7.750065326690674", "False"], ["-8.000064849853516", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c0c9eb40045613bed08548354ea7b2746c0cd3653bd9f9177cb7ef8ca35f5478", "prompt_hash": "ecbc078299ab92edf6ac724d30e35da9f4936fe99ec713c8359356e85e044df9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1079, "doc": {"id": "c3bc395561113c96ec43afd715da5061", "question": "The newlyweds began copulating their marriage, they wanted many what?", "question_concept": "copulating", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["babies", "odors", "sadness", "rapport", "ejaculation"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: The newlyweds began copulating their marriage, they wanted many what?\nA. babies\nB. odors\nC. sadness\nD. rapport\nE. ejaculation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The newlyweds began copulating their marriage, they wanted many what?\nA. babies\nB. odors\nC. sadness\nD. rapport\nE. ejaculation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The newlyweds began copulating their marriage, they wanted many what?\nA. babies\nB. odors\nC. sadness\nD. rapport\nE. ejaculation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The newlyweds began copulating their marriage, they wanted many what?\nA. babies\nB. odors\nC. sadness\nD. rapport\nE. ejaculation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The newlyweds began copulating their marriage, they wanted many what?\nA. babies\nB. odors\nC. sadness\nD. rapport\nE. ejaculation\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1529121398925781", "True"]], [["-6.402912139892578", "False"]], [["-6.402912139892578", "False"]], [["-3.402912139892578", "False"]], [["-5.902912139892578", "False"]]], "filtered_resps": [["-1.1529121398925781", "True"], ["-6.402912139892578", "False"], ["-6.402912139892578", "False"], ["-3.402912139892578", "False"], ["-5.902912139892578", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "698cf45c23da2231a200cdee9873e9060b9ef1709074b7e9b1c8acac145be58a", "prompt_hash": "d4fa31278c6ce07ef720b4607a0b7d8352341d8ff8d0b46df15025a1fa23594a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1080, "doc": {"id": "d0bd5b5ee7319d1c4727e38d429dd54e", "question": "How does a planet usually move around the sun?", "question_concept": "planet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["writing", "universe", "outer space", "outerspace", "orbit"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: How does a planet usually move around the sun?\nA. writing\nB. universe\nC. outer space\nD. outerspace\nE. orbit\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How does a planet usually move around the sun?\nA. writing\nB. universe\nC. outer space\nD. outerspace\nE. orbit\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How does a planet usually move around the sun?\nA. writing\nB. universe\nC. outer space\nD. outerspace\nE. orbit\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How does a planet usually move around the sun?\nA. writing\nB. universe\nC. outer space\nD. outerspace\nE. orbit\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How does a planet usually move around the sun?\nA. writing\nB. universe\nC. outer space\nD. outerspace\nE. orbit\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7828161716461182", "False"]], [["-6.782815933227539", "False"]], [["-8.032815933227539", "False"]], [["-8.282815933227539", "False"]], [["-1.2828161716461182", "True"]]], "filtered_resps": [["-1.7828161716461182", "False"], ["-6.782815933227539", "False"], ["-8.032815933227539", "False"], ["-8.282815933227539", "False"], ["-1.2828161716461182", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a427e393f6e2e5c232aceebb79265c3053f9bb15f0029741c465711d148bc12f", "prompt_hash": "86fc7142eebeb5ae7064896f535f642891adf3d6178eef06df6df55c310fbeb0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1081, "doc": {"id": "81f5e741d970578867495ceea5a0c848", "question": "When a group of people are talking at work they might be doing what?", "question_concept": "talking", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["having a concert.", "cough", "sharing of ideas", "speak", "sneeze"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: When a group of people are talking at work they might be doing what?\nA. having a concert.\nB. cough\nC. sharing of ideas\nD. speak\nE. sneeze\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When a group of people are talking at work they might be doing what?\nA. having a concert.\nB. cough\nC. sharing of ideas\nD. speak\nE. sneeze\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When a group of people are talking at work they might be doing what?\nA. having a concert.\nB. cough\nC. sharing of ideas\nD. speak\nE. sneeze\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When a group of people are talking at work they might be doing what?\nA. having a concert.\nB. cough\nC. sharing of ideas\nD. speak\nE. sneeze\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When a group of people are talking at work they might be doing what?\nA. having a concert.\nB. cough\nC. sharing of ideas\nD. speak\nE. sneeze\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.323686599731445", "False"]], [["-5.323686599731445", "False"]], [["-1.3236865997314453", "True"]], [["-5.073686599731445", "False"]], [["-10.323686599731445", "False"]]], "filtered_resps": [["-5.323686599731445", "False"], ["-5.323686599731445", "False"], ["-1.3236865997314453", "True"], ["-5.073686599731445", "False"], ["-10.323686599731445", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "701603d79cd85077425c2dd9d6e5bbfa1595ba97757e722f3879884f80052a7a", "prompt_hash": "716cf790ff90e904b928d5a396617db28945c00422b4e42324d8671c51908bfa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1082, "doc": {"id": "6714593a8d1f8ae39930c1f0316e9ffc", "question": "What emotion leads to punching?", "question_concept": "punching", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fists", "hitting", "boxing gloves", "anger", "hands"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What emotion leads to punching?\nA. fists\nB. hitting\nC. boxing gloves\nD. anger\nE. hands\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What emotion leads to punching?\nA. fists\nB. hitting\nC. boxing gloves\nD. anger\nE. hands\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What emotion leads to punching?\nA. fists\nB. hitting\nC. boxing gloves\nD. anger\nE. hands\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What emotion leads to punching?\nA. fists\nB. hitting\nC. boxing gloves\nD. anger\nE. hands\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What emotion leads to punching?\nA. fists\nB. hitting\nC. boxing gloves\nD. anger\nE. hands\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.664117813110352", "False"]], [["-5.914117813110352", "False"]], [["-8.414117813110352", "False"]], [["-1.164117693901062", "True"]], [["-10.914117813110352", "False"]]], "filtered_resps": [["-4.664117813110352", "False"], ["-5.914117813110352", "False"], ["-8.414117813110352", "False"], ["-1.164117693901062", "True"], ["-10.914117813110352", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1b4eebaf1501bc3a6fd5e52ce267c585049baea9a7bc3f746e46194245e70f17", "prompt_hash": "40d639cccd9b333d37203badc4cbe39452b498787b6446e96777d2ff8cf4e347", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1083, "doc": {"id": "75cb55aec7e64f592c01eee5d4578dcd", "question": "They kept doing things the same, she suggested they also try doing things what?", "question_concept": "also", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["differently", "otherwise", "expensive", "only", "mere"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: They kept doing things the same, she suggested they also try doing things what?\nA. differently\nB. otherwise\nC. expensive\nD. only\nE. mere\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They kept doing things the same, she suggested they also try doing things what?\nA. differently\nB. otherwise\nC. expensive\nD. only\nE. mere\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They kept doing things the same, she suggested they also try doing things what?\nA. differently\nB. otherwise\nC. expensive\nD. only\nE. mere\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They kept doing things the same, she suggested they also try doing things what?\nA. differently\nB. otherwise\nC. expensive\nD. only\nE. mere\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They kept doing things the same, she suggested they also try doing things what?\nA. differently\nB. otherwise\nC. expensive\nD. only\nE. mere\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7265127897262573", "True"]], [["-7.226512908935547", "False"]], [["-10.476512908935547", "False"]], [["-10.226512908935547", "False"]], [["-10.476512908935547", "False"]]], "filtered_resps": [["-0.7265127897262573", "True"], ["-7.226512908935547", "False"], ["-10.476512908935547", "False"], ["-10.226512908935547", "False"], ["-10.476512908935547", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6a76e2d68513f3c006e294862e17b29d96c0fb2566367b199056446fb1d4b146", "prompt_hash": "186bcf6fc36e94c0f599c02ad89f127a2d220fef7d56d24881f586427c5e87c3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1084, "doc": {"id": "0b30831fb1862bc62339bdf930cbc447", "question": "Where could you find a shark before it was caught?", "question_concept": "shark", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pool hall", "tomales bay", "marine museum", "business", "desert"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where could you find a shark before it was caught?\nA. pool hall\nB. tomales bay\nC. marine museum\nD. business\nE. desert\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where could you find a shark before it was caught?\nA. pool hall\nB. tomales bay\nC. marine museum\nD. business\nE. desert\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where could you find a shark before it was caught?\nA. pool hall\nB. tomales bay\nC. marine museum\nD. business\nE. desert\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where could you find a shark before it was caught?\nA. pool hall\nB. tomales bay\nC. marine museum\nD. business\nE. desert\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where could you find a shark before it was caught?\nA. pool hall\nB. tomales bay\nC. marine museum\nD. business\nE. desert\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.414743661880493", "False"]], [["-1.1647436618804932", "True"]], [["-5.914743423461914", "False"]], [["-8.164743423461914", "False"]], [["-8.414743423461914", "False"]]], "filtered_resps": [["-3.414743661880493", "False"], ["-1.1647436618804932", "True"], ["-5.914743423461914", "False"], ["-8.164743423461914", "False"], ["-8.414743423461914", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b1f578314f6b1fae63df8aa77b9e0c133df3c11693da96030d8453a6f67e8a63", "prompt_hash": "69223312a878503ee3ca394f4892d610d1eb6099b5aaa35bd9e1b2becef252c7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1085, "doc": {"id": "29c194d032a266a7160bff6f546a4d9d", "question": "Where is one likely to find poker chips?", "question_concept": "chips", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["supermarket", "pantry", "motherboard", "bar", "bar"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is one likely to find poker chips?\nA. supermarket\nB. pantry\nC. motherboard\nD. bar\nE. bar\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is one likely to find poker chips?\nA. supermarket\nB. pantry\nC. motherboard\nD. bar\nE. bar\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is one likely to find poker chips?\nA. supermarket\nB. pantry\nC. motherboard\nD. bar\nE. bar\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is one likely to find poker chips?\nA. supermarket\nB. pantry\nC. motherboard\nD. bar\nE. bar\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is one likely to find poker chips?\nA. supermarket\nB. pantry\nC. motherboard\nD. bar\nE. bar\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.951364994049072", "False"]], [["-7.201364994049072", "False"]], [["-6.451364994049072", "False"]], [["-1.9513651132583618", "False"]], [["-4.451364994049072", "False"]]], "filtered_resps": [["-4.951364994049072", "False"], ["-7.201364994049072", "False"], ["-6.451364994049072", "False"], ["-1.9513651132583618", "False"], ["-4.451364994049072", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9155aea77cfe9d1ad25befe3b0588ade2adf4209d4ca5eed88d335c996d6fc98", "prompt_hash": "29d7787432859ab0a3570c6ded847c041c5b5d8cd1c6c0bf17a4e1b3a48dd4c6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1086, "doc": {"id": "ea33206992fb7ad1c3476e9673bb4a9c", "question": "Dance can be elegant and specific, or you can just have fun and what?", "question_concept": "dance", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["falling down", "trip", "fall down", "move around", "celebrate"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Dance can be elegant and specific, or you can just have fun and what?\nA. falling down\nB. trip\nC. fall down\nD. move around\nE. celebrate\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Dance can be elegant and specific, or you can just have fun and what?\nA. falling down\nB. trip\nC. fall down\nD. move around\nE. celebrate\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Dance can be elegant and specific, or you can just have fun and what?\nA. falling down\nB. trip\nC. fall down\nD. move around\nE. celebrate\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Dance can be elegant and specific, or you can just have fun and what?\nA. falling down\nB. trip\nC. fall down\nD. move around\nE. celebrate\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Dance can be elegant and specific, or you can just have fun and what?\nA. falling down\nB. trip\nC. fall down\nD. move around\nE. celebrate\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.443739414215088", "False"]], [["-4.943739414215088", "False"]], [["-5.943739414215088", "False"]], [["-1.6937392950057983", "False"]], [["-1.6937392950057983", "False"]]], "filtered_resps": [["-4.443739414215088", "False"], ["-4.943739414215088", "False"], ["-5.943739414215088", "False"], ["-1.6937392950057983", "False"], ["-1.6937392950057983", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "432d98cd615994b4bfb85e6f01f74f0c8f4a52ecc463033067131603557f3754", "prompt_hash": "e691d4980e491c159b94ba79150197d43e855df2296acbf3e446b2d8e828d081", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1087, "doc": {"id": "2b7dd91da5dde1560ace2cd82af926de", "question": "Where can one obtain a bass fiddle?", "question_concept": "bass fiddle", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["jazz band", "string quartet", "group band", "nursery rhyme", "music store"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where can one obtain a bass fiddle?\nA. jazz band\nB. string quartet\nC. group band\nD. nursery rhyme\nE. music store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can one obtain a bass fiddle?\nA. jazz band\nB. string quartet\nC. group band\nD. nursery rhyme\nE. music store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can one obtain a bass fiddle?\nA. jazz band\nB. string quartet\nC. group band\nD. nursery rhyme\nE. music store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can one obtain a bass fiddle?\nA. jazz band\nB. string quartet\nC. group band\nD. nursery rhyme\nE. music store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can one obtain a bass fiddle?\nA. jazz band\nB. string quartet\nC. group band\nD. nursery rhyme\nE. music store\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.4340126514434814", "False"]], [["-3.6840126514434814", "False"]], [["-5.684012413024902", "False"]], [["-7.934012413024902", "False"]], [["-1.1840126514434814", "True"]]], "filtered_resps": [["-2.4340126514434814", "False"], ["-3.6840126514434814", "False"], ["-5.684012413024902", "False"], ["-7.934012413024902", "False"], ["-1.1840126514434814", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ea450b890c633abe26e54796c03b7c8da8ccdb05f8ed143f34427db33e01066d", "prompt_hash": "f1d2417e97bc60a80ef58041af1f577ca166c5c5fee4a15df872fdfa349bade5", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1088, "doc": {"id": "eb50f536830ba18ab987c7ff652e2aba", "question": "Why does having a disability sometimes making academic tasks hard for a person?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mentally challenged", "have choice", "lots of space", "hungry", "acknowledgment"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Why does having a disability sometimes making academic tasks hard for a person?\nA. mentally challenged\nB. have choice\nC. lots of space\nD. hungry\nE. acknowledgment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why does having a disability sometimes making academic tasks hard for a person?\nA. mentally challenged\nB. have choice\nC. lots of space\nD. hungry\nE. acknowledgment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why does having a disability sometimes making academic tasks hard for a person?\nA. mentally challenged\nB. have choice\nC. lots of space\nD. hungry\nE. acknowledgment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why does having a disability sometimes making academic tasks hard for a person?\nA. mentally challenged\nB. have choice\nC. lots of space\nD. hungry\nE. acknowledgment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why does having a disability sometimes making academic tasks hard for a person?\nA. mentally challenged\nB. have choice\nC. lots of space\nD. hungry\nE. acknowledgment\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.493430256843567", "True"]], [["-5.493430137634277", "False"]], [["-6.243430137634277", "False"]], [["-4.243430137634277", "False"]], [["-6.493430137634277", "False"]]], "filtered_resps": [["-1.493430256843567", "True"], ["-5.493430137634277", "False"], ["-6.243430137634277", "False"], ["-4.243430137634277", "False"], ["-6.493430137634277", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "874fbf2c239bbfa25dc923b8dc54262877626adc71a31c76ef6c0b65b0532b55", "prompt_hash": "1df27470f228f533f2d05646ff6caa9e5cce5b64535fb1e1fe29db47e2c6297f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1089, "doc": {"id": "6bc3ebcfd04965c25bde71339955746c", "question": "What is the purpose of playing games for children?", "question_concept": "playing games", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["winning", "learning", "losing", "fatigue", "skill"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is the purpose of playing games for children?\nA. winning\nB. learning\nC. losing\nD. fatigue\nE. skill\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the purpose of playing games for children?\nA. winning\nB. learning\nC. losing\nD. fatigue\nE. skill\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the purpose of playing games for children?\nA. winning\nB. learning\nC. losing\nD. fatigue\nE. skill\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the purpose of playing games for children?\nA. winning\nB. learning\nC. losing\nD. fatigue\nE. skill\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the purpose of playing games for children?\nA. winning\nB. learning\nC. losing\nD. fatigue\nE. skill\nAnswer:", "arg_1": " E"}}, "resps": [[["-7.083311080932617", "False"]], [["-1.333310842514038", "True"]], [["-9.083311080932617", "False"]], [["-9.833311080932617", "False"]], [["-8.333311080932617", "False"]]], "filtered_resps": [["-7.083311080932617", "False"], ["-1.333310842514038", "True"], ["-9.083311080932617", "False"], ["-9.833311080932617", "False"], ["-8.333311080932617", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c5a4acf61d92df29f61679332fe701a56e9c725184b1100ec539f8721cc72fd0", "prompt_hash": "144640bb6fc4f61d3d9d8dce4eef1c3927946f0c0e2cabe2c8458aef784c00cd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1090, "doc": {"id": "163898952cb6baf3a6440696e1352e86", "question": "If for some reason you were to start killing people, what would you be likely to receive?", "question_concept": "killing people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feelings of guilt", "prison sentence", "terrible", "encouragement", "die"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If for some reason you were to start killing people, what would you be likely to receive?\nA. feelings of guilt\nB. prison sentence\nC. terrible\nD. encouragement\nE. die\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If for some reason you were to start killing people, what would you be likely to receive?\nA. feelings of guilt\nB. prison sentence\nC. terrible\nD. encouragement\nE. die\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If for some reason you were to start killing people, what would you be likely to receive?\nA. feelings of guilt\nB. prison sentence\nC. terrible\nD. encouragement\nE. die\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If for some reason you were to start killing people, what would you be likely to receive?\nA. feelings of guilt\nB. prison sentence\nC. terrible\nD. encouragement\nE. die\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If for some reason you were to start killing people, what would you be likely to receive?\nA. feelings of guilt\nB. prison sentence\nC. terrible\nD. encouragement\nE. die\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.6992931365966797", "False"]], [["-1.6992931365966797", "False"]], [["-6.44929313659668", "False"]], [["-7.19929313659668", "False"]], [["-6.44929313659668", "False"]]], "filtered_resps": [["-3.6992931365966797", "False"], ["-1.6992931365966797", "False"], ["-6.44929313659668", "False"], ["-7.19929313659668", "False"], ["-6.44929313659668", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ea2b0a0f09e23dbd482284b358ff458f27aedbe8f989348c0299af46f16b16fa", "prompt_hash": "48ceaa7d41794930f86b279625e2c6147269964546586fd984fa6545a7868e0f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1091, "doc": {"id": "aa984e2b487d08889bc0c73bab5ac945", "question": "If someone laughs after surprising them they have a good sense of what?", "question_concept": "surprising", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["joy", "fight", "frightened", "humor", "laughter"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If someone laughs after surprising them they have a good sense of what?\nA. joy\nB. fight\nC. frightened\nD. humor\nE. laughter\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If someone laughs after surprising them they have a good sense of what?\nA. joy\nB. fight\nC. frightened\nD. humor\nE. laughter\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If someone laughs after surprising them they have a good sense of what?\nA. joy\nB. fight\nC. frightened\nD. humor\nE. laughter\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If someone laughs after surprising them they have a good sense of what?\nA. joy\nB. fight\nC. frightened\nD. humor\nE. laughter\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If someone laughs after surprising them they have a good sense of what?\nA. joy\nB. fight\nC. frightened\nD. humor\nE. laughter\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7513508796691895", "False"]], [["-6.0013508796691895", "False"]], [["-6.5013508796691895", "False"]], [["-1.501350998878479", "True"]], [["-6.7513508796691895", "False"]]], "filtered_resps": [["-2.7513508796691895", "False"], ["-6.0013508796691895", "False"], ["-6.5013508796691895", "False"], ["-1.501350998878479", "True"], ["-6.7513508796691895", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "be3e81dc5c8a4aad4e07bab081507acad5f8029b43d14db72de8a8aac108020c", "prompt_hash": "5698760e7a2234f2406fd159fb275f904a01965ad0178b2202c834f50841d22e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1092, "doc": {"id": "d78baca23e0a636a8961e17119047e63", "question": "People played a variety of games in the soccer field.  It was the closest thing they had to what?", "question_concept": "soccer field", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["town", "beach", "park", "near", "outside"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: People played a variety of games in the soccer field.  It was the closest thing they had to what?\nA. town\nB. beach\nC. park\nD. near\nE. outside\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: People played a variety of games in the soccer field.  It was the closest thing they had to what?\nA. town\nB. beach\nC. park\nD. near\nE. outside\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: People played a variety of games in the soccer field.  It was the closest thing they had to what?\nA. town\nB. beach\nC. park\nD. near\nE. outside\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: People played a variety of games in the soccer field.  It was the closest thing they had to what?\nA. town\nB. beach\nC. park\nD. near\nE. outside\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: People played a variety of games in the soccer field.  It was the closest thing they had to what?\nA. town\nB. beach\nC. park\nD. near\nE. outside\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.795137882232666", "False"]], [["-3.295137882232666", "False"]], [["-2.045137882232666", "False"]], [["-6.795137882232666", "False"]], [["-3.795137882232666", "False"]]], "filtered_resps": [["-3.795137882232666", "False"], ["-3.295137882232666", "False"], ["-2.045137882232666", "False"], ["-6.795137882232666", "False"], ["-3.795137882232666", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "947acf434d61ff59564432a3bb8cd61473275bf916046079346e5cb1c27abea2", "prompt_hash": "da18bafbd237b0b8a0d7987bd69f179880b5904543f29b79a601507e4081b2bf", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1093, "doc": {"id": "ac6378b5e8462dc1bde1155d706213d8", "question": "What is likely to have a better school cafeteria?", "question_concept": "school cafeteria", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["high school", "canteen", "polytechnic", "large room", "all kinds of schools"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is likely to have a better school cafeteria?\nA. high school\nB. canteen\nC. polytechnic\nD. large room\nE. all kinds of schools\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is likely to have a better school cafeteria?\nA. high school\nB. canteen\nC. polytechnic\nD. large room\nE. all kinds of schools\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is likely to have a better school cafeteria?\nA. high school\nB. canteen\nC. polytechnic\nD. large room\nE. all kinds of schools\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is likely to have a better school cafeteria?\nA. high school\nB. canteen\nC. polytechnic\nD. large room\nE. all kinds of schools\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is likely to have a better school cafeteria?\nA. high school\nB. canteen\nC. polytechnic\nD. large room\nE. all kinds of schools\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5252717733383179", "False"]], [["-5.025271892547607", "False"]], [["-5.025271892547607", "False"]], [["-7.275271892547607", "False"]], [["-1.2752717733383179", "True"]]], "filtered_resps": [["-1.5252717733383179", "False"], ["-5.025271892547607", "False"], ["-5.025271892547607", "False"], ["-7.275271892547607", "False"], ["-1.2752717733383179", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9a0ffa7a655653b4dee7e0920651968c9b53b6186fcd4d7087a30b99d210b6d7", "prompt_hash": "469f2f7674c9575c2c2f3d6d8a7e30e17ca4bd6b8d2d71c0fc07ee70fb1f7501", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1094, "doc": {"id": "c1aebf059c5102f4e773f7fe4afe13f0", "question": "When someone has little knowledge and is judging someone they are considered what?", "question_concept": "judging", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["objectivity", "knowing yourself", "experience", "ignorance", "introduction"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: When someone has little knowledge and is judging someone they are considered what?\nA. objectivity\nB. knowing yourself\nC. experience\nD. ignorance\nE. introduction\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When someone has little knowledge and is judging someone they are considered what?\nA. objectivity\nB. knowing yourself\nC. experience\nD. ignorance\nE. introduction\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When someone has little knowledge and is judging someone they are considered what?\nA. objectivity\nB. knowing yourself\nC. experience\nD. ignorance\nE. introduction\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When someone has little knowledge and is judging someone they are considered what?\nA. objectivity\nB. knowing yourself\nC. experience\nD. ignorance\nE. introduction\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When someone has little knowledge and is judging someone they are considered what?\nA. objectivity\nB. knowing yourself\nC. experience\nD. ignorance\nE. introduction\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.733670473098755", "False"]], [["-5.483670234680176", "False"]], [["-8.233670234680176", "False"]], [["-1.4836704730987549", "False"]], [["-9.233670234680176", "False"]]], "filtered_resps": [["-3.733670473098755", "False"], ["-5.483670234680176", "False"], ["-8.233670234680176", "False"], ["-1.4836704730987549", "False"], ["-9.233670234680176", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1b4ad8af97ea0763ef004e101efc9636d983ae8c361a5b7642fe37223286b3de", "prompt_hash": "cbcbbb29777183c738c77eb57d8babec867334a4943e529c228b371d91071f23", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1095, "doc": {"id": "1017807310a25d3ea4a4ec305e91cba3", "question": "She wanted to get in shape, but she couldn't stay focused on the hour long what?", "question_concept": "get in shape", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sweating", "excercise", "work out", "video", "swim"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: She wanted to get in shape, but she couldn't stay focused on the hour long what?\nA. sweating\nB. excercise\nC. work out\nD. video\nE. swim\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: She wanted to get in shape, but she couldn't stay focused on the hour long what?\nA. sweating\nB. excercise\nC. work out\nD. video\nE. swim\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: She wanted to get in shape, but she couldn't stay focused on the hour long what?\nA. sweating\nB. excercise\nC. work out\nD. video\nE. swim\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: She wanted to get in shape, but she couldn't stay focused on the hour long what?\nA. sweating\nB. excercise\nC. work out\nD. video\nE. swim\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: She wanted to get in shape, but she couldn't stay focused on the hour long what?\nA. sweating\nB. excercise\nC. work out\nD. video\nE. swim\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.902058124542236", "False"]], [["-4.652058124542236", "False"]], [["-1.6520580053329468", "False"]], [["-6.402058124542236", "False"]], [["-9.402057647705078", "False"]]], "filtered_resps": [["-4.902058124542236", "False"], ["-4.652058124542236", "False"], ["-1.6520580053329468", "False"], ["-6.402058124542236", "False"], ["-9.402057647705078", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e9e21dfd47eacfbd9ee871e56ed701bb4b3737bf14e7d2856f5b0cd598a132d9", "prompt_hash": "e278ff19707a2e057d6b6c62416ef6783e4393525e0de4fcd8645ec00c531784", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1096, "doc": {"id": "7192c9f5c513aac9042bad595ff5af9f", "question": "When you do something and have fun, its something you?", "question_concept": "have fun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["spontaneous", "stop working", "pay for", "do like", "do enjoy"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: When you do something and have fun, its something you?\nA. spontaneous\nB. stop working\nC. pay for\nD. do like\nE. do enjoy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When you do something and have fun, its something you?\nA. spontaneous\nB. stop working\nC. pay for\nD. do like\nE. do enjoy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When you do something and have fun, its something you?\nA. spontaneous\nB. stop working\nC. pay for\nD. do like\nE. do enjoy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When you do something and have fun, its something you?\nA. spontaneous\nB. stop working\nC. pay for\nD. do like\nE. do enjoy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When you do something and have fun, its something you?\nA. spontaneous\nB. stop working\nC. pay for\nD. do like\nE. do enjoy\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.466687202453613", "False"]], [["-4.966687202453613", "False"]], [["-7.716687202453613", "False"]], [["-7.966687202453613", "False"]], [["-1.7166872024536133", "False"]]], "filtered_resps": [["-4.466687202453613", "False"], ["-4.966687202453613", "False"], ["-7.716687202453613", "False"], ["-7.966687202453613", "False"], ["-1.7166872024536133", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ce013913b5ad6935fdd2b213b8d58335106602d4276a228a6521e071e7875fdc", "prompt_hash": "27805dfe1d3644fd1aaca0e064ed3f80de48f67721a463673e162989d024c367", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1097, "doc": {"id": "7c05e8d5a057085455eea243fbd1cd90", "question": "What is a salesman responsible to do at work?", "question_concept": "salesman", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["traveling to chicago", "get fired", "books", "sell products", "service account"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What is a salesman responsible to do at work?\nA. traveling to chicago\nB. get fired\nC. books\nD. sell products\nE. service account\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a salesman responsible to do at work?\nA. traveling to chicago\nB. get fired\nC. books\nD. sell products\nE. service account\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a salesman responsible to do at work?\nA. traveling to chicago\nB. get fired\nC. books\nD. sell products\nE. service account\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a salesman responsible to do at work?\nA. traveling to chicago\nB. get fired\nC. books\nD. sell products\nE. service account\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a salesman responsible to do at work?\nA. traveling to chicago\nB. get fired\nC. books\nD. sell products\nE. service account\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8027408123016357", "False"]], [["-8.302741050720215", "False"]], [["-7.802741050720215", "False"]], [["-1.8027408123016357", "False"]], [["-9.552741050720215", "False"]]], "filtered_resps": [["-1.8027408123016357", "False"], ["-8.302741050720215", "False"], ["-7.802741050720215", "False"], ["-1.8027408123016357", "False"], ["-9.552741050720215", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c91b876fdba1aa0f2fac6fb3fc57af37d1a982bd8306f22088ccd97d689d8173", "prompt_hash": "55e69ad94d39a4b13b14f691ba25503601157ec42f37b2e8c88074e4bd6e0ab8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1098, "doc": {"id": "3cb91a71a6567da870eedf37becc97ef", "question": "How does going jogging generally affect one's self esteem?", "question_concept": "going jogging", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feel better", "feel pride", "sweating", "ocean", "arthritis"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: How does going jogging generally affect one's self esteem?\nA. feel better\nB. feel pride\nC. sweating\nD. ocean\nE. arthritis\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How does going jogging generally affect one's self esteem?\nA. feel better\nB. feel pride\nC. sweating\nD. ocean\nE. arthritis\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How does going jogging generally affect one's self esteem?\nA. feel better\nB. feel pride\nC. sweating\nD. ocean\nE. arthritis\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How does going jogging generally affect one's self esteem?\nA. feel better\nB. feel pride\nC. sweating\nD. ocean\nE. arthritis\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How does going jogging generally affect one's self esteem?\nA. feel better\nB. feel pride\nC. sweating\nD. ocean\nE. arthritis\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3146957159042358", "True"]], [["-4.064695835113525", "False"]], [["-6.564695835113525", "False"]], [["-7.814695835113525", "False"]], [["-10.314695358276367", "False"]]], "filtered_resps": [["-1.3146957159042358", "True"], ["-4.064695835113525", "False"], ["-6.564695835113525", "False"], ["-7.814695835113525", "False"], ["-10.314695358276367", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6b1a162b76b67f3e25c1c53ab71bfc63dce57aeb538759a6c88a00f74ae6f4ab", "prompt_hash": "16172eda9a64f18b18a75768ce22e092f24293d4fd46dc7dea1c07b44fbaffc1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1099, "doc": {"id": "9b4bbf3c4d24ecdb4b27320afb706808", "question": "Where would you find people standing in a line outside?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["bus depot", "end of line", "opera", "neighbor's house", "meeting"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find people standing in a line outside?\nA. bus depot\nB. end of line\nC. opera\nD. neighbor's house\nE. meeting\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find people standing in a line outside?\nA. bus depot\nB. end of line\nC. opera\nD. neighbor's house\nE. meeting\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find people standing in a line outside?\nA. bus depot\nB. end of line\nC. opera\nD. neighbor's house\nE. meeting\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find people standing in a line outside?\nA. bus depot\nB. end of line\nC. opera\nD. neighbor's house\nE. meeting\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find people standing in a line outside?\nA. bus depot\nB. end of line\nC. opera\nD. neighbor's house\nE. meeting\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7056158781051636", "True"]], [["-4.705615997314453", "False"]], [["-5.205615997314453", "False"]], [["-7.205615997314453", "False"]], [["-5.705615997314453", "False"]]], "filtered_resps": [["-0.7056158781051636", "True"], ["-4.705615997314453", "False"], ["-5.205615997314453", "False"], ["-7.205615997314453", "False"], ["-5.705615997314453", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cb98f4076264dae2074ccd5c88274e46c86829523ce8e6f6936c8ca913b76c17", "prompt_hash": "0e432f0922ecb5a29e9a1badd8f775bec8971ff355394399b9033bac4c762bdf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1100, "doc": {"id": "43df3a316880d8bab346c06bd43b94dd", "question": "If you are committing perjury you have done what while under oath?", "question_concept": "committing perjury", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["crime", "disrespect judge", "embarrassment", "lie", "indictment"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: If you are committing perjury you have done what while under oath?\nA. crime\nB. disrespect judge\nC. embarrassment\nD. lie\nE. indictment\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you are committing perjury you have done what while under oath?\nA. crime\nB. disrespect judge\nC. embarrassment\nD. lie\nE. indictment\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you are committing perjury you have done what while under oath?\nA. crime\nB. disrespect judge\nC. embarrassment\nD. lie\nE. indictment\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you are committing perjury you have done what while under oath?\nA. crime\nB. disrespect judge\nC. embarrassment\nD. lie\nE. indictment\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you are committing perjury you have done what while under oath?\nA. crime\nB. disrespect judge\nC. embarrassment\nD. lie\nE. indictment\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.334761619567871", "False"]], [["-6.334761619567871", "False"]], [["-7.584761619567871", "False"]], [["-1.5847615003585815", "True"]], [["-9.834761619567871", "False"]]], "filtered_resps": [["-3.334761619567871", "False"], ["-6.334761619567871", "False"], ["-7.584761619567871", "False"], ["-1.5847615003585815", "True"], ["-9.834761619567871", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c062ae866c3f6968db3c8f2265501be9f4ae192dd91147f00ea673793a17bfc1", "prompt_hash": "7a51fac91fa5cce7579e1c9ab8e7835fd1bac39ac23f67686a71bf144fb3cc94", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1101, "doc": {"id": "858a5eaa587fe0e266722228671a6bd1", "question": "Where can you find the meaning of \"ficus\"?", "question_concept": "ficus", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dictionary", "apartment", "libary", "middle east", "arboretum"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you find the meaning of \"ficus\"?\nA. dictionary\nB. apartment\nC. libary\nD. middle east\nE. arboretum\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you find the meaning of \"ficus\"?\nA. dictionary\nB. apartment\nC. libary\nD. middle east\nE. arboretum\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you find the meaning of \"ficus\"?\nA. dictionary\nB. apartment\nC. libary\nD. middle east\nE. arboretum\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you find the meaning of \"ficus\"?\nA. dictionary\nB. apartment\nC. libary\nD. middle east\nE. arboretum\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you find the meaning of \"ficus\"?\nA. dictionary\nB. apartment\nC. libary\nD. middle east\nE. arboretum\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1943546533584595", "False"]], [["-8.694355010986328", "False"]], [["-7.19435453414917", "False"]], [["-8.444355010986328", "False"]], [["-9.444355010986328", "False"]]], "filtered_resps": [["-1.1943546533584595", "False"], ["-8.694355010986328", "False"], ["-7.19435453414917", "False"], ["-8.444355010986328", "False"], ["-9.444355010986328", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6ec39e85d5fdd31bd618a91bb98bba188470801c6bd1b24bbcf83c9f485195df", "prompt_hash": "85dfdeec267801ca1af5982b7905038ee6b6f4d43cf7f7f36e3944859099c788", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1102, "doc": {"id": "34005ef0caafefc8585c9fcd50e94557", "question": "When are people buying products more?", "question_concept": "buying products", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["debt", "economic boom", "being able to use", "disagreements", "trading"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: When are people buying products more?\nA. debt\nB. economic boom\nC. being able to use\nD. disagreements\nE. trading\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When are people buying products more?\nA. debt\nB. economic boom\nC. being able to use\nD. disagreements\nE. trading\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When are people buying products more?\nA. debt\nB. economic boom\nC. being able to use\nD. disagreements\nE. trading\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When are people buying products more?\nA. debt\nB. economic boom\nC. being able to use\nD. disagreements\nE. trading\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When are people buying products more?\nA. debt\nB. economic boom\nC. being able to use\nD. disagreements\nE. trading\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.165841102600098", "False"]], [["-1.4158413410186768", "True"]], [["-5.665841102600098", "False"]], [["-6.915841102600098", "False"]], [["-7.165841102600098", "False"]]], "filtered_resps": [["-5.165841102600098", "False"], ["-1.4158413410186768", "True"], ["-5.665841102600098", "False"], ["-6.915841102600098", "False"], ["-7.165841102600098", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "07317ddee89e9a3e7576715240a46009e5b3a91e0b14a218b3692a29ebbe36f4", "prompt_hash": "92e121294aa4173a3ab7778056b92518538709a9c051de3b3bcd16ae9b29edf7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1103, "doc": {"id": "f61d83f90b92a8d537989e55ee70542d", "question": "The buildings were intended to not have residential kitchens in them, what were they designed for?", "question_concept": "buildings", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["large city", "small", "eat cake", "university", "town"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The buildings were intended to not have residential kitchens in them, what were they designed for?\nA. large city\nB. small\nC. eat cake\nD. university\nE. town\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The buildings were intended to not have residential kitchens in them, what were they designed for?\nA. large city\nB. small\nC. eat cake\nD. university\nE. town\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The buildings were intended to not have residential kitchens in them, what were they designed for?\nA. large city\nB. small\nC. eat cake\nD. university\nE. town\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The buildings were intended to not have residential kitchens in them, what were they designed for?\nA. large city\nB. small\nC. eat cake\nD. university\nE. town\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The buildings were intended to not have residential kitchens in them, what were they designed for?\nA. large city\nB. small\nC. eat cake\nD. university\nE. town\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.879546880722046", "False"]], [["-2.879546880722046", "False"]], [["-4.379547119140625", "False"]], [["-2.629546880722046", "False"]], [["-4.879547119140625", "False"]]], "filtered_resps": [["-1.879546880722046", "False"], ["-2.879546880722046", "False"], ["-4.379547119140625", "False"], ["-2.629546880722046", "False"], ["-4.879547119140625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0b13b823d7be29e510e98bfb332b70dfbe77fe80fd6433549de59a82c952099f", "prompt_hash": "68da34f5da5e6026b02c182332bac71e36464f7b64ed7c94f719e4e7e20fc3be", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1104, "doc": {"id": "3bf06235a537adc9d85431846595b800", "question": "Animals come in all types, some fly thanks to their lightweight hollow what?", "question_concept": "animals", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["tails", "bones", "eyes", "heads", "bodies"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Animals come in all types, some fly thanks to their lightweight hollow what?\nA. tails\nB. bones\nC. eyes\nD. heads\nE. bodies\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Animals come in all types, some fly thanks to their lightweight hollow what?\nA. tails\nB. bones\nC. eyes\nD. heads\nE. bodies\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Animals come in all types, some fly thanks to their lightweight hollow what?\nA. tails\nB. bones\nC. eyes\nD. heads\nE. bodies\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Animals come in all types, some fly thanks to their lightweight hollow what?\nA. tails\nB. bones\nC. eyes\nD. heads\nE. bodies\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Animals come in all types, some fly thanks to their lightweight hollow what?\nA. tails\nB. bones\nC. eyes\nD. heads\nE. bodies\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.383178234100342", "False"]], [["-1.3831781148910522", "True"]], [["-6.883178234100342", "False"]], [["-8.633177757263184", "False"]], [["-2.633178234100342", "False"]]], "filtered_resps": [["-5.383178234100342", "False"], ["-1.3831781148910522", "True"], ["-6.883178234100342", "False"], ["-8.633177757263184", "False"], ["-2.633178234100342", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "15d1650145cf88e5f982f071de5868adeec172e419015235d73bfe390fb8f16a", "prompt_hash": "4f767b6ebfa58a5d38a323cba2ea27b12ae23d65593490c3c4a144903a152e72", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1105, "doc": {"id": "79ec11d8072ce42779adfe0a19bd5374", "question": "The child felt like it was all pretend, he didn't understand what?", "question_concept": "pretend", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["people believe", "daydreams", "transcendentalism", "laughter", "religion"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The child felt like it was all pretend, he didn't understand what?\nA. people believe\nB. daydreams\nC. transcendentalism\nD. laughter\nE. religion\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The child felt like it was all pretend, he didn't understand what?\nA. people believe\nB. daydreams\nC. transcendentalism\nD. laughter\nE. religion\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The child felt like it was all pretend, he didn't understand what?\nA. people believe\nB. daydreams\nC. transcendentalism\nD. laughter\nE. religion\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The child felt like it was all pretend, he didn't understand what?\nA. people believe\nB. daydreams\nC. transcendentalism\nD. laughter\nE. religion\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The child felt like it was all pretend, he didn't understand what?\nA. people believe\nB. daydreams\nC. transcendentalism\nD. laughter\nE. religion\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9965901374816895", "False"]], [["-2.7465901374816895", "False"]], [["-3.4965901374816895", "False"]], [["-5.7465901374816895", "False"]], [["-1.7465900182724", "False"]]], "filtered_resps": [["-2.9965901374816895", "False"], ["-2.7465901374816895", "False"], ["-3.4965901374816895", "False"], ["-5.7465901374816895", "False"], ["-1.7465900182724", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "17862065444cf785a1107f2861202f75f03ee6933d10dc5b27f5671065fecc9b", "prompt_hash": "2fca90c6f3a4f8dfabbd8028e92954269854fe2700d4cff13e950490df401443", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1106, "doc": {"id": "2982d0eae1bf880f5930341af7665716", "question": "Where is a lake likely to be glacial?", "question_concept": "lake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["michigan", "new york", "new york", "mountains", "countryside"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a lake likely to be glacial?\nA. michigan\nB. new york\nC. new york\nD. mountains\nE. countryside\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a lake likely to be glacial?\nA. michigan\nB. new york\nC. new york\nD. mountains\nE. countryside\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a lake likely to be glacial?\nA. michigan\nB. new york\nC. new york\nD. mountains\nE. countryside\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a lake likely to be glacial?\nA. michigan\nB. new york\nC. new york\nD. mountains\nE. countryside\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a lake likely to be glacial?\nA. michigan\nB. new york\nC. new york\nD. mountains\nE. countryside\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1054601669311523", "False"]], [["-7.605460166931152", "False"]], [["-6.105460166931152", "False"]], [["-1.8554600477218628", "False"]], [["-9.355460166931152", "False"]]], "filtered_resps": [["-3.1054601669311523", "False"], ["-7.605460166931152", "False"], ["-6.105460166931152", "False"], ["-1.8554600477218628", "False"], ["-9.355460166931152", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ea975c7e95b337a9d7a6b809356c771cd3b257588650f66a251f9c2827ae9f0c", "prompt_hash": "55de256ec80498b2ff54ca0b490b3d3b397500f67961cff5c32028762c2bfaea", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1107, "doc": {"id": "ba9132ebf2bc3ad21e6a0631dc4e0a77", "question": "They needed grape juice for their party, they went to buy it and other snacks at the what?", "question_concept": "grape", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["field", "restaurant", "salad", "market", "food store"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: They needed grape juice for their party, they went to buy it and other snacks at the what?\nA. field\nB. restaurant\nC. salad\nD. market\nE. food store\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: They needed grape juice for their party, they went to buy it and other snacks at the what?\nA. field\nB. restaurant\nC. salad\nD. market\nE. food store\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: They needed grape juice for their party, they went to buy it and other snacks at the what?\nA. field\nB. restaurant\nC. salad\nD. market\nE. food store\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: They needed grape juice for their party, they went to buy it and other snacks at the what?\nA. field\nB. restaurant\nC. salad\nD. market\nE. food store\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: They needed grape juice for their party, they went to buy it and other snacks at the what?\nA. field\nB. restaurant\nC. salad\nD. market\nE. food store\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.23670768737793", "False"]], [["-4.48670768737793", "False"]], [["-9.23670768737793", "False"]], [["-1.9867078065872192", "False"]], [["-1.7367078065872192", "True"]]], "filtered_resps": [["-4.23670768737793", "False"], ["-4.48670768737793", "False"], ["-9.23670768737793", "False"], ["-1.9867078065872192", "False"], ["-1.7367078065872192", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ba7dd499047cc9ec6604bcf0b09200155ccb9cc61bb0acf8b6efa10b0edda9f1", "prompt_hash": "c84ef5348e32adf2e29bacaad4fe38d34204b64b27cd3395862674886d432f13", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1108, "doc": {"id": "d06de16a4aaeaef32b398c1213257b4a", "question": "Why do some people get passports and go to different locations?", "question_concept": "people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["believe in god", "smoke marijuana", "desire to travel", "use weapons", "throw away"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Why do some people get passports and go to different locations?\nA. believe in god\nB. smoke marijuana\nC. desire to travel\nD. use weapons\nE. throw away\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why do some people get passports and go to different locations?\nA. believe in god\nB. smoke marijuana\nC. desire to travel\nD. use weapons\nE. throw away\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why do some people get passports and go to different locations?\nA. believe in god\nB. smoke marijuana\nC. desire to travel\nD. use weapons\nE. throw away\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why do some people get passports and go to different locations?\nA. believe in god\nB. smoke marijuana\nC. desire to travel\nD. use weapons\nE. throw away\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why do some people get passports and go to different locations?\nA. believe in god\nB. smoke marijuana\nC. desire to travel\nD. use weapons\nE. throw away\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.878071308135986", "False"]], [["-8.128071784973145", "False"]], [["-0.8780714273452759", "True"]], [["-8.628071784973145", "False"]], [["-8.878071784973145", "False"]]], "filtered_resps": [["-5.878071308135986", "False"], ["-8.128071784973145", "False"], ["-0.8780714273452759", "True"], ["-8.628071784973145", "False"], ["-8.878071784973145", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fdee7c8774874b2c18389c83baef403297544cd2ab77106f08414f054ee10f30", "prompt_hash": "f9625d745167c2675e3b7b6207f387361151f729936c868423890d75a9c17a61", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1109, "doc": {"id": "eee9476bf29498b7d74b043afe316fc6", "question": "Where do apples form on an apple tree?", "question_concept": "apple tree", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["south africa", "sunshine", "new york", "bloom", "trunk"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where do apples form on an apple tree?\nA. south africa\nB. sunshine\nC. new york\nD. bloom\nE. trunk\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where do apples form on an apple tree?\nA. south africa\nB. sunshine\nC. new york\nD. bloom\nE. trunk\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where do apples form on an apple tree?\nA. south africa\nB. sunshine\nC. new york\nD. bloom\nE. trunk\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where do apples form on an apple tree?\nA. south africa\nB. sunshine\nC. new york\nD. bloom\nE. trunk\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where do apples form on an apple tree?\nA. south africa\nB. sunshine\nC. new york\nD. bloom\nE. trunk\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.150120735168457", "False"]], [["-6.150120735168457", "False"]], [["-7.150120735168457", "False"]], [["-1.650120496749878", "False"]], [["-6.900120735168457", "False"]]], "filtered_resps": [["-4.150120735168457", "False"], ["-6.150120735168457", "False"], ["-7.150120735168457", "False"], ["-1.650120496749878", "False"], ["-6.900120735168457", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c8fe5df03147e6896bb2cf7fde588d2f2889d3145bef930c10b0cf1b7b462f21", "prompt_hash": "360743fd0a7f19af627f75faed904d9c64734a40248f56bb5fe4c8aa4bae015b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1110, "doc": {"id": "a85441d6a0e3f871d81a9f19b31360b7", "question": "Where areas are there likely to be many nightclubs?", "question_concept": "nightclub", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["manhattan", "drink and dance", "alcohol", "major city", "downtown area"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where areas are there likely to be many nightclubs?\nA. manhattan\nB. drink and dance\nC. alcohol\nD. major city\nE. downtown area\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where areas are there likely to be many nightclubs?\nA. manhattan\nB. drink and dance\nC. alcohol\nD. major city\nE. downtown area\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where areas are there likely to be many nightclubs?\nA. manhattan\nB. drink and dance\nC. alcohol\nD. major city\nE. downtown area\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where areas are there likely to be many nightclubs?\nA. manhattan\nB. drink and dance\nC. alcohol\nD. major city\nE. downtown area\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where areas are there likely to be many nightclubs?\nA. manhattan\nB. drink and dance\nC. alcohol\nD. major city\nE. downtown area\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.7775627374649048", "False"]], [["-6.027562618255615", "False"]], [["-6.777562618255615", "False"]], [["-2.5275626182556152", "False"]], [["-2.2775626182556152", "False"]]], "filtered_resps": [["-1.7775627374649048", "False"], ["-6.027562618255615", "False"], ["-6.777562618255615", "False"], ["-2.5275626182556152", "False"], ["-2.2775626182556152", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7759cb3337ed2a5d389dfd80dc8c917ac8c203a756cd566bcc0facad2ef3b499", "prompt_hash": "cb500059ee5c5b7221df9b885117ac97b6bceb4f7ec5798a7582431dfc7d7127", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1111, "doc": {"id": "f11a2975898033893d6a38f75d791fdf", "question": "What can machines do that humans cannot?", "question_concept": "machines", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fail to work", "perform work", "answering questions", "see work", "fly"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What can machines do that humans cannot?\nA. fail to work\nB. perform work\nC. answering questions\nD. see work\nE. fly\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can machines do that humans cannot?\nA. fail to work\nB. perform work\nC. answering questions\nD. see work\nE. fly\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can machines do that humans cannot?\nA. fail to work\nB. perform work\nC. answering questions\nD. see work\nE. fly\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can machines do that humans cannot?\nA. fail to work\nB. perform work\nC. answering questions\nD. see work\nE. fly\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can machines do that humans cannot?\nA. fail to work\nB. perform work\nC. answering questions\nD. see work\nE. fly\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.661393642425537", "False"]], [["-1.9113935232162476", "False"]], [["-3.161393642425537", "False"]], [["-5.411393642425537", "False"]], [["-4.161393642425537", "False"]]], "filtered_resps": [["-4.661393642425537", "False"], ["-1.9113935232162476", "False"], ["-3.161393642425537", "False"], ["-5.411393642425537", "False"], ["-4.161393642425537", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1b0224a76241d52a44c1e128068aad0ac5f8a17c4df63afa562d177306470ef0", "prompt_hash": "f770f10cacd2a549d65d78125e028515d0cd99dc3fc07f18e47ad88c43701289", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1112, "doc": {"id": "a2977fd575faba162d04a490dabd1b9b", "question": "What does someone stop doing when being dead?", "question_concept": "dead", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["moving", "working", "breathing", "alive", "deadworks"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What does someone stop doing when being dead?\nA. moving\nB. working\nC. breathing\nD. alive\nE. deadworks\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does someone stop doing when being dead?\nA. moving\nB. working\nC. breathing\nD. alive\nE. deadworks\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does someone stop doing when being dead?\nA. moving\nB. working\nC. breathing\nD. alive\nE. deadworks\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does someone stop doing when being dead?\nA. moving\nB. working\nC. breathing\nD. alive\nE. deadworks\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does someone stop doing when being dead?\nA. moving\nB. working\nC. breathing\nD. alive\nE. deadworks\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.2960896492004395", "False"]], [["-6.2960896492004395", "False"]], [["-2.5460896492004395", "False"]], [["-4.5460896492004395", "False"]], [["-7.7960896492004395", "False"]]], "filtered_resps": [["-3.2960896492004395", "False"], ["-6.2960896492004395", "False"], ["-2.5460896492004395", "False"], ["-4.5460896492004395", "False"], ["-7.7960896492004395", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2ca40b400eeb1ace7321008eac93bb6d0031b7a9c2dd451deff2ff9db7d688fa", "prompt_hash": "6d68554477f15b971aeb55da1d47e9ed302bc10c1bc5de780111c5f6f95005fa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1113, "doc": {"id": "cd39e442204d3edf7acc185fd59c8a44", "question": "The place where my linen closet is really needs repainting a light color as it only has one overhead light.", "question_concept": "linen closet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["house", "home", "pool house", "hallway", "bedroom"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The place where my linen closet is really needs repainting a light color as it only has one overhead light.\nA. house\nB. home\nC. pool house\nD. hallway\nE. bedroom\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The place where my linen closet is really needs repainting a light color as it only has one overhead light.\nA. house\nB. home\nC. pool house\nD. hallway\nE. bedroom\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The place where my linen closet is really needs repainting a light color as it only has one overhead light.\nA. house\nB. home\nC. pool house\nD. hallway\nE. bedroom\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The place where my linen closet is really needs repainting a light color as it only has one overhead light.\nA. house\nB. home\nC. pool house\nD. hallway\nE. bedroom\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The place where my linen closet is really needs repainting a light color as it only has one overhead light.\nA. house\nB. home\nC. pool house\nD. hallway\nE. bedroom\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1149380207061768", "True"]], [["-3.1149380207061768", "False"]], [["-4.114937782287598", "False"]], [["-4.364937782287598", "False"]], [["-5.864937782287598", "False"]]], "filtered_resps": [["-1.1149380207061768", "True"], ["-3.1149380207061768", "False"], ["-4.114937782287598", "False"], ["-4.364937782287598", "False"], ["-5.864937782287598", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0d24a112ca93674d75c01e92a34d947176c9d11bf1c831e038ebcaa5c596850f", "prompt_hash": "9a68b93580f8b0e0c98bf0dc2b6e4cd8e6562b1080f9e90e84262821d0bcaa16", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1114, "doc": {"id": "c77e1039d78cdff197a370fcda0f2b9f", "question": "Punk rock music is an important part of what action sport?", "question_concept": "music", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["skate", "listen", "opera", "opera", "relax"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Punk rock music is an important part of what action sport?\nA. skate\nB. listen\nC. opera\nD. opera\nE. relax\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Punk rock music is an important part of what action sport?\nA. skate\nB. listen\nC. opera\nD. opera\nE. relax\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Punk rock music is an important part of what action sport?\nA. skate\nB. listen\nC. opera\nD. opera\nE. relax\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Punk rock music is an important part of what action sport?\nA. skate\nB. listen\nC. opera\nD. opera\nE. relax\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Punk rock music is an important part of what action sport?\nA. skate\nB. listen\nC. opera\nD. opera\nE. relax\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2973209619522095", "True"]], [["-7.54732084274292", "False"]], [["-8.047321319580078", "False"]], [["-9.047321319580078", "False"]], [["-10.797321319580078", "False"]]], "filtered_resps": [["-1.2973209619522095", "True"], ["-7.54732084274292", "False"], ["-8.047321319580078", "False"], ["-9.047321319580078", "False"], ["-10.797321319580078", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "070d8780b5a9b8e8ab55b0d3c98f748ff779a933ef5dd8d87a82c0b9fea427a4", "prompt_hash": "f9de368ee78c5047b620378f018d975a0d870d8effdab2f99b691098854b35e6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1115, "doc": {"id": "f537f6bb8527724e0b1e1c1051326cd5", "question": "Where might a mouse be found to make it country?", "question_concept": "mouse", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kitchen", "cook", "computer lab", "old barn", "research laboratory"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where might a mouse be found to make it country?\nA. kitchen\nB. cook\nC. computer lab\nD. old barn\nE. research laboratory\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where might a mouse be found to make it country?\nA. kitchen\nB. cook\nC. computer lab\nD. old barn\nE. research laboratory\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where might a mouse be found to make it country?\nA. kitchen\nB. cook\nC. computer lab\nD. old barn\nE. research laboratory\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where might a mouse be found to make it country?\nA. kitchen\nB. cook\nC. computer lab\nD. old barn\nE. research laboratory\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where might a mouse be found to make it country?\nA. kitchen\nB. cook\nC. computer lab\nD. old barn\nE. research laboratory\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.3563026189804077", "True"]], [["-5.856302738189697", "False"]], [["-2.6063027381896973", "False"]], [["-2.3563027381896973", "False"]], [["-4.356302738189697", "False"]]], "filtered_resps": [["-1.3563026189804077", "True"], ["-5.856302738189697", "False"], ["-2.6063027381896973", "False"], ["-2.3563027381896973", "False"], ["-4.356302738189697", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "5d793faf8244cc0a6440ed0757940338a43bc1babdde8ddb74dd94862df1a83f", "prompt_hash": "d206948e1f1a6ecac8e5c2f04efba67a85156b3568f97c751184610e0291bbc5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1116, "doc": {"id": "d3b145911a76fd6fbe9a23ab027be024", "question": "Where is a bird likely to make it's home?", "question_concept": "bird", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["forest", "nest", "roof", "leaves", "sky"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a bird likely to make it's home?\nA. forest\nB. nest\nC. roof\nD. leaves\nE. sky\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a bird likely to make it's home?\nA. forest\nB. nest\nC. roof\nD. leaves\nE. sky\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a bird likely to make it's home?\nA. forest\nB. nest\nC. roof\nD. leaves\nE. sky\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a bird likely to make it's home?\nA. forest\nB. nest\nC. roof\nD. leaves\nE. sky\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a bird likely to make it's home?\nA. forest\nB. nest\nC. roof\nD. leaves\nE. sky\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.0179557800292969", "True"]], [["-1.7679557800292969", "False"]], [["-5.767955780029297", "False"]], [["-5.267955780029297", "False"]], [["-7.267955780029297", "False"]]], "filtered_resps": [["-1.0179557800292969", "True"], ["-1.7679557800292969", "False"], ["-5.767955780029297", "False"], ["-5.267955780029297", "False"], ["-7.267955780029297", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a2962da10feb256ecb9c51bf4566055dfd51ffe983e9dbb7ec77847e13c2b620", "prompt_hash": "703171398552df51660a954c4302ca15fb0c5e2b4a4dc363455d9279700f7c1d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1117, "doc": {"id": "dc2fa76467ff342abdb4cf142f92dddd", "question": "When a person suffers from hunger early in the day what do they do?", "question_concept": "hunger", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat hamburger", "eat breakfast", "open fridge", "buy food", "cook dinner"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: When a person suffers from hunger early in the day what do they do?\nA. eat hamburger\nB. eat breakfast\nC. open fridge\nD. buy food\nE. cook dinner\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When a person suffers from hunger early in the day what do they do?\nA. eat hamburger\nB. eat breakfast\nC. open fridge\nD. buy food\nE. cook dinner\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When a person suffers from hunger early in the day what do they do?\nA. eat hamburger\nB. eat breakfast\nC. open fridge\nD. buy food\nE. cook dinner\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When a person suffers from hunger early in the day what do they do?\nA. eat hamburger\nB. eat breakfast\nC. open fridge\nD. buy food\nE. cook dinner\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When a person suffers from hunger early in the day what do they do?\nA. eat hamburger\nB. eat breakfast\nC. open fridge\nD. buy food\nE. cook dinner\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.921733379364014", "False"]], [["-1.1717334985733032", "True"]], [["-6.921733379364014", "False"]], [["-7.921733379364014", "False"]], [["-7.171733379364014", "False"]]], "filtered_resps": [["-4.921733379364014", "False"], ["-1.1717334985733032", "True"], ["-6.921733379364014", "False"], ["-7.921733379364014", "False"], ["-7.171733379364014", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "df6f03157dcfa00dc03705b8895ce992641797491c8493dfa99d147b5bf2267d", "prompt_hash": "5aeb2a835f84e40c06345b02bcac681f901c9e68245b48e575baad98791aabd4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1118, "doc": {"id": "246249cd7976358051a9811ff9c30736", "question": "How would you express information if you do not have a pen or pencil?", "question_concept": "express information", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["may disagree", "close mouth", "write down", "talk", "eyes"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: How would you express information if you do not have a pen or pencil?\nA. may disagree\nB. close mouth\nC. write down\nD. talk\nE. eyes\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How would you express information if you do not have a pen or pencil?\nA. may disagree\nB. close mouth\nC. write down\nD. talk\nE. eyes\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How would you express information if you do not have a pen or pencil?\nA. may disagree\nB. close mouth\nC. write down\nD. talk\nE. eyes\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How would you express information if you do not have a pen or pencil?\nA. may disagree\nB. close mouth\nC. write down\nD. talk\nE. eyes\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How would you express information if you do not have a pen or pencil?\nA. may disagree\nB. close mouth\nC. write down\nD. talk\nE. eyes\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9949214458465576", "False"]], [["-3.9949214458465576", "False"]], [["-3.2449214458465576", "False"]], [["-1.4949214458465576", "True"]], [["-4.494921684265137", "False"]]], "filtered_resps": [["-3.9949214458465576", "False"], ["-3.9949214458465576", "False"], ["-3.2449214458465576", "False"], ["-1.4949214458465576", "True"], ["-4.494921684265137", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1869c2bb201a3a59d4bda4423331489d12e43abb645f70c3de836c6f236b9af5", "prompt_hash": "7e7af24f2de02f8b805a8818b03a28365a5795f880e7cf786836ab405c3733be", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1119, "doc": {"id": "32be8cbc1b5a967310bcab8b80563481", "question": "What does everyone feel of monsters?", "question_concept": "everyone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["looking for love", "afraid of", "good at", "make pet", "different"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What does everyone feel of monsters?\nA. looking for love\nB. afraid of\nC. good at\nD. make pet\nE. different\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does everyone feel of monsters?\nA. looking for love\nB. afraid of\nC. good at\nD. make pet\nE. different\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does everyone feel of monsters?\nA. looking for love\nB. afraid of\nC. good at\nD. make pet\nE. different\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does everyone feel of monsters?\nA. looking for love\nB. afraid of\nC. good at\nD. make pet\nE. different\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does everyone feel of monsters?\nA. looking for love\nB. afraid of\nC. good at\nD. make pet\nE. different\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.180240631103516", "False"]], [["-1.6802407503128052", "True"]], [["-6.930240631103516", "False"]], [["-5.180240631103516", "False"]], [["-3.6802406311035156", "False"]]], "filtered_resps": [["-5.180240631103516", "False"], ["-1.6802407503128052", "True"], ["-6.930240631103516", "False"], ["-5.180240631103516", "False"], ["-3.6802406311035156", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bbdda22dd4745188b17d1eacdc9664de0046619b9f619a4db729dca9a14e1f9d", "prompt_hash": "f3fe7fddda261ea4bd344d0740760964e8c3b8b265ea7c320f7215b8d164a599", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1120, "doc": {"id": "ad769851a59375865607452d3bf2a45d", "question": "Why does someone want to examine thing closely?", "question_concept": "examine thing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["buy", "learn about", "buy", "complex", "interesting"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Why does someone want to examine thing closely?\nA. buy\nB. learn about\nC. buy\nD. complex\nE. interesting\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why does someone want to examine thing closely?\nA. buy\nB. learn about\nC. buy\nD. complex\nE. interesting\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why does someone want to examine thing closely?\nA. buy\nB. learn about\nC. buy\nD. complex\nE. interesting\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why does someone want to examine thing closely?\nA. buy\nB. learn about\nC. buy\nD. complex\nE. interesting\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why does someone want to examine thing closely?\nA. buy\nB. learn about\nC. buy\nD. complex\nE. interesting\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.446561813354492", "False"]], [["-1.196561574935913", "True"]], [["-7.946561813354492", "False"]], [["-7.196561813354492", "False"]], [["-6.946561813354492", "False"]]], "filtered_resps": [["-5.446561813354492", "False"], ["-1.196561574935913", "True"], ["-7.946561813354492", "False"], ["-7.196561813354492", "False"], ["-6.946561813354492", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1f8956bf21296f2790e9791f97cb0eeb3de7f599ee709b54f3638cc64b7f11e6", "prompt_hash": "c81a07c050f1759759729126f315197e233de2faadebb5178c05b9476f720a0f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1121, "doc": {"id": "5ea6b94d1a911365b06cf776919413e8", "question": "What does \tdrinking alcohol lead to?", "question_concept": "drinking alcohol", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["have fun", "intoxication", "vomiting", "drinking more alcohol", "nausea"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What does \tdrinking alcohol lead to?\nA. have fun\nB. intoxication\nC. vomiting\nD. drinking more alcohol\nE. nausea\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does \tdrinking alcohol lead to?\nA. have fun\nB. intoxication\nC. vomiting\nD. drinking more alcohol\nE. nausea\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does \tdrinking alcohol lead to?\nA. have fun\nB. intoxication\nC. vomiting\nD. drinking more alcohol\nE. nausea\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does \tdrinking alcohol lead to?\nA. have fun\nB. intoxication\nC. vomiting\nD. drinking more alcohol\nE. nausea\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does \tdrinking alcohol lead to?\nA. have fun\nB. intoxication\nC. vomiting\nD. drinking more alcohol\nE. nausea\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.677420139312744", "False"]], [["-0.9274201393127441", "True"]], [["-6.927420139312744", "False"]], [["-4.427420139312744", "False"]], [["-8.177419662475586", "False"]]], "filtered_resps": [["-5.677420139312744", "False"], ["-0.9274201393127441", "True"], ["-6.927420139312744", "False"], ["-4.427420139312744", "False"], ["-8.177419662475586", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "943ed3d07f6f5c9008adf6919cb4ed50814527bfdc9fac835e3be60209f898d7", "prompt_hash": "67e35d8ff5c847bf239d393ec8d0668b11c50a2ee6094f2108ced0336bd99125", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1122, "doc": {"id": "820df15b615d221e38a71fcc44461085", "question": "Where would your hear a bass clarinet along side other wood wind instruments?", "question_concept": "bass clarinet", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["opera house", "school band", "music store", "orchestra", "bathroom stall"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would your hear a bass clarinet along side other wood wind instruments?\nA. opera house\nB. school band\nC. music store\nD. orchestra\nE. bathroom stall\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would your hear a bass clarinet along side other wood wind instruments?\nA. opera house\nB. school band\nC. music store\nD. orchestra\nE. bathroom stall\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would your hear a bass clarinet along side other wood wind instruments?\nA. opera house\nB. school band\nC. music store\nD. orchestra\nE. bathroom stall\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would your hear a bass clarinet along side other wood wind instruments?\nA. opera house\nB. school band\nC. music store\nD. orchestra\nE. bathroom stall\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would your hear a bass clarinet along side other wood wind instruments?\nA. opera house\nB. school band\nC. music store\nD. orchestra\nE. bathroom stall\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.7796616554260254", "False"]], [["-2.5296616554260254", "False"]], [["-7.779661655426025", "False"]], [["-1.7796616554260254", "True"]], [["-9.779661178588867", "False"]]], "filtered_resps": [["-2.7796616554260254", "False"], ["-2.5296616554260254", "False"], ["-7.779661655426025", "False"], ["-1.7796616554260254", "True"], ["-9.779661178588867", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "badd088e58f209281e25391bdc9ad515c82dbf7db626e22ce25e5a05efb3db4b", "prompt_hash": "d12228167be6b37a063927f645f19901f721931d1e53bdf696b92dbc71d96aca", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1123, "doc": {"id": "0a4a00ba435397c4a0496dd2c2426be7", "question": "What is the opposite of a little of something?", "question_concept": "little", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["much", "plenty", "more", "big", "lot of"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is the opposite of a little of something?\nA. much\nB. plenty\nC. more\nD. big\nE. lot of\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the opposite of a little of something?\nA. much\nB. plenty\nC. more\nD. big\nE. lot of\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the opposite of a little of something?\nA. much\nB. plenty\nC. more\nD. big\nE. lot of\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the opposite of a little of something?\nA. much\nB. plenty\nC. more\nD. big\nE. lot of\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the opposite of a little of something?\nA. much\nB. plenty\nC. more\nD. big\nE. lot of\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.645447850227356", "True"]], [["-2.8954477310180664", "False"]], [["-5.395447731018066", "False"]], [["-5.895447731018066", "False"]], [["-7.145447731018066", "False"]]], "filtered_resps": [["-1.645447850227356", "True"], ["-2.8954477310180664", "False"], ["-5.395447731018066", "False"], ["-5.895447731018066", "False"], ["-7.145447731018066", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "139f4fc7fde293351c25c4bbd72b5f4e167849745b7cfcc6705f811465b94eb1", "prompt_hash": "b5e5517c48c3727ba189ca327ff25b0a8bdeb1925f48b43cfef011ddc38bc045", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1124, "doc": {"id": "a7f29f4aebe0e3bcb77038fea71bf28c", "question": "The princess was pure, the evil wizard wished to do what to her?", "question_concept": "pure", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["dirty", "tarnish", "corrupt", "contaminated", "applied"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The princess was pure, the evil wizard wished to do what to her?\nA. dirty\nB. tarnish\nC. corrupt\nD. contaminated\nE. applied\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The princess was pure, the evil wizard wished to do what to her?\nA. dirty\nB. tarnish\nC. corrupt\nD. contaminated\nE. applied\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The princess was pure, the evil wizard wished to do what to her?\nA. dirty\nB. tarnish\nC. corrupt\nD. contaminated\nE. applied\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The princess was pure, the evil wizard wished to do what to her?\nA. dirty\nB. tarnish\nC. corrupt\nD. contaminated\nE. applied\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The princess was pure, the evil wizard wished to do what to her?\nA. dirty\nB. tarnish\nC. corrupt\nD. contaminated\nE. applied\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.88019323348999", "False"]], [["-1.3801933526992798", "True"]], [["-2.3801932334899902", "False"]], [["-7.38019323348999", "False"]], [["-7.13019323348999", "False"]]], "filtered_resps": [["-5.88019323348999", "False"], ["-1.3801933526992798", "True"], ["-2.3801932334899902", "False"], ["-7.38019323348999", "False"], ["-7.13019323348999", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c086b597df443307b295aa56c77e6c03afc657e4fae1d48ebab16d34a73f8b1b", "prompt_hash": "7cec26e5ddf22d55a8f4f0d2d9176972c6339a62bf0890aed799ba67a2c102ee", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1125, "doc": {"id": "ecd32cc0c17d4738a27bba3399f04591", "question": "The piece of paper was worth a lot of money, it was an old Apple Inc what?", "question_concept": "paper", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["notebook", "copy machine", "stock certificate", "ream", "thumb drive"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The piece of paper was worth a lot of money, it was an old Apple Inc what?\nA. notebook\nB. copy machine\nC. stock certificate\nD. ream\nE. thumb drive\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The piece of paper was worth a lot of money, it was an old Apple Inc what?\nA. notebook\nB. copy machine\nC. stock certificate\nD. ream\nE. thumb drive\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The piece of paper was worth a lot of money, it was an old Apple Inc what?\nA. notebook\nB. copy machine\nC. stock certificate\nD. ream\nE. thumb drive\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The piece of paper was worth a lot of money, it was an old Apple Inc what?\nA. notebook\nB. copy machine\nC. stock certificate\nD. ream\nE. thumb drive\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The piece of paper was worth a lot of money, it was an old Apple Inc what?\nA. notebook\nB. copy machine\nC. stock certificate\nD. ream\nE. thumb drive\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.7039260864257812", "False"]], [["-6.203926086425781", "False"]], [["-0.9539260864257812", "True"]], [["-9.203926086425781", "False"]], [["-7.953926086425781", "False"]]], "filtered_resps": [["-3.7039260864257812", "False"], ["-6.203926086425781", "False"], ["-0.9539260864257812", "True"], ["-9.203926086425781", "False"], ["-7.953926086425781", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2abc6841d1758dac0ac90d0cfc4b70ad7af1c9ffbca328928c35d040fc563a5e", "prompt_hash": "33ed485f898a78c782796c031183c421a7c46e8b40249e24a910641faa1a404d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1126, "doc": {"id": "8b2af2d865b7dc500427786c846eacaf", "question": "During the winter hunt he could hear every motion in the woods, this was because of the what of everything?", "question_concept": "motion", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["being still", "silence", "stationary", "stillness", "standing still"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: During the winter hunt he could hear every motion in the woods, this was because of the what of everything?\nA. being still\nB. silence\nC. stationary\nD. stillness\nE. standing still\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: During the winter hunt he could hear every motion in the woods, this was because of the what of everything?\nA. being still\nB. silence\nC. stationary\nD. stillness\nE. standing still\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: During the winter hunt he could hear every motion in the woods, this was because of the what of everything?\nA. being still\nB. silence\nC. stationary\nD. stillness\nE. standing still\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: During the winter hunt he could hear every motion in the woods, this was because of the what of everything?\nA. being still\nB. silence\nC. stationary\nD. stillness\nE. standing still\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: During the winter hunt he could hear every motion in the woods, this was because of the what of everything?\nA. being still\nB. silence\nC. stationary\nD. stillness\nE. standing still\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.954390048980713", "False"]], [["-2.204390287399292", "False"]], [["-7.204390048980713", "False"]], [["-2.454390287399292", "False"]], [["-9.204390525817871", "False"]]], "filtered_resps": [["-4.954390048980713", "False"], ["-2.204390287399292", "False"], ["-7.204390048980713", "False"], ["-2.454390287399292", "False"], ["-9.204390525817871", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ff3d41a08982ffa851fb371af7b2ae99603442f0bb5349e6de3b7c1a9b110932", "prompt_hash": "699a8b2210f050d53ef59303737645b5a0fdc674398bd0f343af1d22e884ab23", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1127, "doc": {"id": "383282aace64dd49138bac2392f8b38e", "question": "If a car-less person want to listen to talk radio in private, where might they listen to it?", "question_concept": "radio", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["trunk", "bedroom", "diner", "space shuttle", "shop"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If a car-less person want to listen to talk radio in private, where might they listen to it?\nA. trunk\nB. bedroom\nC. diner\nD. space shuttle\nE. shop\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If a car-less person want to listen to talk radio in private, where might they listen to it?\nA. trunk\nB. bedroom\nC. diner\nD. space shuttle\nE. shop\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If a car-less person want to listen to talk radio in private, where might they listen to it?\nA. trunk\nB. bedroom\nC. diner\nD. space shuttle\nE. shop\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If a car-less person want to listen to talk radio in private, where might they listen to it?\nA. trunk\nB. bedroom\nC. diner\nD. space shuttle\nE. shop\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If a car-less person want to listen to talk radio in private, where might they listen to it?\nA. trunk\nB. bedroom\nC. diner\nD. space shuttle\nE. shop\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5656681060791016", "False"]], [["-1.5656681060791016", "True"]], [["-3.5656681060791016", "False"]], [["-7.065668106079102", "False"]], [["-8.815668106079102", "False"]]], "filtered_resps": [["-2.5656681060791016", "False"], ["-1.5656681060791016", "True"], ["-3.5656681060791016", "False"], ["-7.065668106079102", "False"], ["-8.815668106079102", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "00cdaa27d74598da41e50f5771609a5c21e2bd22124c24d2f6d733a34a42893f", "prompt_hash": "04210361fa85ca41134c63bb6332c2caa2e520289a4cb4e37a17d67db61abfa8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1128, "doc": {"id": "eaf6838d29bcd4ebf408da2f75aa65c3", "question": "Billy was an astronaut.  When he looked at the world from space, how did it look?", "question_concept": "world", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["diverse", "round", "square", "orange", "complicated"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Billy was an astronaut.  When he looked at the world from space, how did it look?\nA. diverse\nB. round\nC. square\nD. orange\nE. complicated\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Billy was an astronaut.  When he looked at the world from space, how did it look?\nA. diverse\nB. round\nC. square\nD. orange\nE. complicated\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Billy was an astronaut.  When he looked at the world from space, how did it look?\nA. diverse\nB. round\nC. square\nD. orange\nE. complicated\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Billy was an astronaut.  When he looked at the world from space, how did it look?\nA. diverse\nB. round\nC. square\nD. orange\nE. complicated\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Billy was an astronaut.  When he looked at the world from space, how did it look?\nA. diverse\nB. round\nC. square\nD. orange\nE. complicated\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1330482959747314", "False"]], [["-1.3830482959747314", "True"]], [["-7.633048057556152", "False"]], [["-8.383048057556152", "False"]], [["-8.633048057556152", "False"]]], "filtered_resps": [["-3.1330482959747314", "False"], ["-1.3830482959747314", "True"], ["-7.633048057556152", "False"], ["-8.383048057556152", "False"], ["-8.633048057556152", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "40b71367963b78072c6f8e5da16e0b945b5a6736cabc5d9ead5e55a2e3b16f6a", "prompt_hash": "afdb4b1a32c4309f0ed4855765c2fb2e458f36e124b7696290b99f3ec54615e4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1129, "doc": {"id": "7c8bc9c0e56389eef033bca40c88c151", "question": "Where is a good place to have a fireplace in a house?", "question_concept": "fireplace", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["big house", "train", "cabin", "living room", "home"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a good place to have a fireplace in a house?\nA. big house\nB. train\nC. cabin\nD. living room\nE. home\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a good place to have a fireplace in a house?\nA. big house\nB. train\nC. cabin\nD. living room\nE. home\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a good place to have a fireplace in a house?\nA. big house\nB. train\nC. cabin\nD. living room\nE. home\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a good place to have a fireplace in a house?\nA. big house\nB. train\nC. cabin\nD. living room\nE. home\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a good place to have a fireplace in a house?\nA. big house\nB. train\nC. cabin\nD. living room\nE. home\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3288519382476807", "False"]], [["-7.828851699829102", "False"]], [["-5.328851699829102", "False"]], [["-2.5788519382476807", "False"]], [["-6.578851699829102", "False"]]], "filtered_resps": [["-2.3288519382476807", "False"], ["-7.828851699829102", "False"], ["-5.328851699829102", "False"], ["-2.5788519382476807", "False"], ["-6.578851699829102", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fa353af387c6e7ac5a6a4b64feb129c06eeb6615bb9b6ac859da6576b56a8b88", "prompt_hash": "bed3121f66ac7a333b1a6973b1c52cbcde77132ad2d48e83749c42f882c95d99", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1130, "doc": {"id": "ca60a46c9007e4b6213f50bfb5342fdd", "question": "If you own a cat where is the last place you'd want to find it?", "question_concept": "cat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["trouble", "dog's mouth", "backyard", "nature", "home"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you own a cat where is the last place you'd want to find it?\nA. trouble\nB. dog's mouth\nC. backyard\nD. nature\nE. home\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you own a cat where is the last place you'd want to find it?\nA. trouble\nB. dog's mouth\nC. backyard\nD. nature\nE. home\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you own a cat where is the last place you'd want to find it?\nA. trouble\nB. dog's mouth\nC. backyard\nD. nature\nE. home\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you own a cat where is the last place you'd want to find it?\nA. trouble\nB. dog's mouth\nC. backyard\nD. nature\nE. home\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you own a cat where is the last place you'd want to find it?\nA. trouble\nB. dog's mouth\nC. backyard\nD. nature\nE. home\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.4458986520767212", "True"]], [["-1.6958986520767212", "False"]], [["-4.445898532867432", "False"]], [["-5.195898532867432", "False"]], [["-3.1958985328674316", "False"]]], "filtered_resps": [["-1.4458986520767212", "True"], ["-1.6958986520767212", "False"], ["-4.445898532867432", "False"], ["-5.195898532867432", "False"], ["-3.1958985328674316", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "342c6deab7c188940fb639c9f2df19dcfe85311969a102ebf6c36e26eaf7b70d", "prompt_hash": "5664fab2863627c5cd5488ff24fe2f0a5e4ca279f72398fd78f5d6af320e0c7d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 1131, "doc": {"id": "f50209f04d11690d7c8f30e29b35ff02", "question": "Where would you find a kosher deli along side a number of different places to eat?", "question_concept": "kosher deli", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["los angeles", "food court", "new york city", "jewish community", "jewish neighborhoods"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find a kosher deli along side a number of different places to eat?\nA. los angeles\nB. food court\nC. new york city\nD. jewish community\nE. jewish neighborhoods\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find a kosher deli along side a number of different places to eat?\nA. los angeles\nB. food court\nC. new york city\nD. jewish community\nE. jewish neighborhoods\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find a kosher deli along side a number of different places to eat?\nA. los angeles\nB. food court\nC. new york city\nD. jewish community\nE. jewish neighborhoods\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find a kosher deli along side a number of different places to eat?\nA. los angeles\nB. food court\nC. new york city\nD. jewish community\nE. jewish neighborhoods\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find a kosher deli along side a number of different places to eat?\nA. los angeles\nB. food court\nC. new york city\nD. jewish community\nE. jewish neighborhoods\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.8662118911743164", "False"]], [["-3.6162118911743164", "False"]], [["-2.3662118911743164", "False"]], [["-4.616211891174316", "False"]], [["-4.616211891174316", "False"]]], "filtered_resps": [["-2.8662118911743164", "False"], ["-3.6162118911743164", "False"], ["-2.3662118911743164", "False"], ["-4.616211891174316", "False"], ["-4.616211891174316", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "289064107c9f7c23d6688047e44eb3d81a574355d390919e23623ef8c45b980a", "prompt_hash": "b908ab3758f93ecd97b234eebcfef3cccb2437329c9c362160f9d2575bcb22a5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 1132, "doc": {"id": "d725f1c2e150a3221de31612123f3f46", "question": "What do you do when you're going to market?", "question_concept": "going to market", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["buy food", "see other people", "buying vegetables", "buy a fat pig", "traveling"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do you do when you're going to market?\nA. buy food\nB. see other people\nC. buying vegetables\nD. buy a fat pig\nE. traveling\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you do when you're going to market?\nA. buy food\nB. see other people\nC. buying vegetables\nD. buy a fat pig\nE. traveling\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you do when you're going to market?\nA. buy food\nB. see other people\nC. buying vegetables\nD. buy a fat pig\nE. traveling\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you do when you're going to market?\nA. buy food\nB. see other people\nC. buying vegetables\nD. buy a fat pig\nE. traveling\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you do when you're going to market?\nA. buy food\nB. see other people\nC. buying vegetables\nD. buy a fat pig\nE. traveling\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.3116989135742188", "False"]], [["-3.0616989135742188", "False"]], [["-2.3116989135742188", "False"]], [["-5.311698913574219", "False"]], [["-2.8116989135742188", "False"]]], "filtered_resps": [["-2.3116989135742188", "False"], ["-3.0616989135742188", "False"], ["-2.3116989135742188", "False"], ["-5.311698913574219", "False"], ["-2.8116989135742188", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3af50d9b77f8ef200ab4c7da9e15e07564431f14df6f6f6aaf8a4838626f483d", "prompt_hash": "43f83bdeee26424a0e8d4e3da836f69f5995c47235b3843f4c75158608c4d7c0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1133, "doc": {"id": "f7735d721dfdc94621154951d4eaa4cf", "question": "She feared that she had cancer, but upon discovering truth that she hadn't, what was her attitude toward life?", "question_concept": "discovering truth", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["conclusion", "pain", "happiness", "relief", "boring"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: She feared that she had cancer, but upon discovering truth that she hadn't, what was her attitude toward life?\nA. conclusion\nB. pain\nC. happiness\nD. relief\nE. boring\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: She feared that she had cancer, but upon discovering truth that she hadn't, what was her attitude toward life?\nA. conclusion\nB. pain\nC. happiness\nD. relief\nE. boring\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: She feared that she had cancer, but upon discovering truth that she hadn't, what was her attitude toward life?\nA. conclusion\nB. pain\nC. happiness\nD. relief\nE. boring\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: She feared that she had cancer, but upon discovering truth that she hadn't, what was her attitude toward life?\nA. conclusion\nB. pain\nC. happiness\nD. relief\nE. boring\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: She feared that she had cancer, but upon discovering truth that she hadn't, what was her attitude toward life?\nA. conclusion\nB. pain\nC. happiness\nD. relief\nE. boring\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.4608049392700195", "False"]], [["-6.9608049392700195", "False"]], [["-5.7108049392700195", "False"]], [["-1.4608047008514404", "False"]], [["-10.96080493927002", "False"]]], "filtered_resps": [["-5.4608049392700195", "False"], ["-6.9608049392700195", "False"], ["-5.7108049392700195", "False"], ["-1.4608047008514404", "False"], ["-10.96080493927002", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "343b2d7735fcd1f19556d6f35df5470b916b21bbecd919b4e74c4cc244c06322", "prompt_hash": "cafd46c85f7e11611cba8003e23073bef382aa9ace53f0f6b6a41e1dc7ae31a2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1134, "doc": {"id": "eaf980db7e945b1cf6d648fa55ddcb5e", "question": "What is the feeling of one having fun?", "question_concept": "having fun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["smiling", "pleasure", "hurt", "injuries", "laughter"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What is the feeling of one having fun?\nA. smiling\nB. pleasure\nC. hurt\nD. injuries\nE. laughter\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is the feeling of one having fun?\nA. smiling\nB. pleasure\nC. hurt\nD. injuries\nE. laughter\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is the feeling of one having fun?\nA. smiling\nB. pleasure\nC. hurt\nD. injuries\nE. laughter\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is the feeling of one having fun?\nA. smiling\nB. pleasure\nC. hurt\nD. injuries\nE. laughter\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is the feeling of one having fun?\nA. smiling\nB. pleasure\nC. hurt\nD. injuries\nE. laughter\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.500818252563477", "False"]], [["-1.5008180141448975", "True"]], [["-8.000818252563477", "False"]], [["-9.000818252563477", "False"]], [["-6.000818252563477", "False"]]], "filtered_resps": [["-4.500818252563477", "False"], ["-1.5008180141448975", "True"], ["-8.000818252563477", "False"], ["-9.000818252563477", "False"], ["-6.000818252563477", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "040d4a3750a97fa92dcbc52a8ec2ef46f32512dedde794f33b8dc4e348fbd977", "prompt_hash": "9b9ec95baf640a2530d9d67c82574853e6ff2d1408558098c11f5ca913da4024", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1135, "doc": {"id": "8bbfe8cd056d612e9d3190f278bef287", "question": "If I keep getting crumbs under my table, what should I put under it?", "question_concept": "table", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["conference", "neighbor's house", "rug", "net", "card room"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If I keep getting crumbs under my table, what should I put under it?\nA. conference\nB. neighbor's house\nC. rug\nD. net\nE. card room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I keep getting crumbs under my table, what should I put under it?\nA. conference\nB. neighbor's house\nC. rug\nD. net\nE. card room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I keep getting crumbs under my table, what should I put under it?\nA. conference\nB. neighbor's house\nC. rug\nD. net\nE. card room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I keep getting crumbs under my table, what should I put under it?\nA. conference\nB. neighbor's house\nC. rug\nD. net\nE. card room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I keep getting crumbs under my table, what should I put under it?\nA. conference\nB. neighbor's house\nC. rug\nD. net\nE. card room\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.1431832313537598", "False"]], [["-6.39318323135376", "False"]], [["-1.3931832313537598", "True"]], [["-6.14318323135376", "False"]], [["-6.64318323135376", "False"]]], "filtered_resps": [["-3.1431832313537598", "False"], ["-6.39318323135376", "False"], ["-1.3931832313537598", "True"], ["-6.14318323135376", "False"], ["-6.64318323135376", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e4ae2cd7f4b20106fc7c0cdc3ccf41f48088b9eaf3d4cc3e5ef6b36bef940642", "prompt_hash": "8eb190f223672bd98c872c3efeb1cb6dafcccaf52ba5af964298989d7bd072f3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1136, "doc": {"id": "aa7c4c351cf8d59792aa68e3de339db4", "question": "Christians believe you will go to heaven if you're what?", "question_concept": "dying", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["unable to work", "born again", "change of color", "dead", "no longer exist"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Christians believe you will go to heaven if you're what?\nA. unable to work\nB. born again\nC. change of color\nD. dead\nE. no longer exist\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Christians believe you will go to heaven if you're what?\nA. unable to work\nB. born again\nC. change of color\nD. dead\nE. no longer exist\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Christians believe you will go to heaven if you're what?\nA. unable to work\nB. born again\nC. change of color\nD. dead\nE. no longer exist\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Christians believe you will go to heaven if you're what?\nA. unable to work\nB. born again\nC. change of color\nD. dead\nE. no longer exist\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Christians believe you will go to heaven if you're what?\nA. unable to work\nB. born again\nC. change of color\nD. dead\nE. no longer exist\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.341437339782715", "False"]], [["-0.8414374589920044", "True"]], [["-7.841437339782715", "False"]], [["-7.841437339782715", "False"]], [["-9.091437339782715", "False"]]], "filtered_resps": [["-6.341437339782715", "False"], ["-0.8414374589920044", "True"], ["-7.841437339782715", "False"], ["-7.841437339782715", "False"], ["-9.091437339782715", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6be7f960f61f520d63ac4ae049517a8c56594c48f7ed786bf1b1a7a41d60da7c", "prompt_hash": "6864d667f772706dc3b93d00c126fe42f0afc76adcb00ca0d376215cad1217b1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1137, "doc": {"id": "23df3bac9cfcb156f4cfd8a05f21c5e2", "question": "James loved to surf but he wasn't good at it. He would always do what?", "question_concept": "surf", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["wipe out", "enjoy yourself", "start fighting", "get wet", "drown"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James loved to surf but he wasn't good at it. He would always do what?\nA. wipe out\nB. enjoy yourself\nC. start fighting\nD. get wet\nE. drown\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James loved to surf but he wasn't good at it. He would always do what?\nA. wipe out\nB. enjoy yourself\nC. start fighting\nD. get wet\nE. drown\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James loved to surf but he wasn't good at it. He would always do what?\nA. wipe out\nB. enjoy yourself\nC. start fighting\nD. get wet\nE. drown\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James loved to surf but he wasn't good at it. He would always do what?\nA. wipe out\nB. enjoy yourself\nC. start fighting\nD. get wet\nE. drown\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James loved to surf but he wasn't good at it. He would always do what?\nA. wipe out\nB. enjoy yourself\nC. start fighting\nD. get wet\nE. drown\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.2778891324996948", "True"]], [["-2.5278892517089844", "False"]], [["-5.777889251708984", "False"]], [["-5.027889251708984", "False"]], [["-9.277889251708984", "False"]]], "filtered_resps": [["-1.2778891324996948", "True"], ["-2.5278892517089844", "False"], ["-5.777889251708984", "False"], ["-5.027889251708984", "False"], ["-9.277889251708984", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d6d6177bde1687fbb7119e9adab216eaf3a9840f975ae260e1bbc0e5ee3b3b6e", "prompt_hash": "0ab082540c9ef0887d6fc1c9976a462ce8dfadce0309e03e5c3560380243b01c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1138, "doc": {"id": "d21777d771dc6fd08e769d378651817e", "question": "Sarah gave her brother a guy to her home.  While she was gone, he used it to do what?", "question_concept": "key", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["front door", "turn lock", "solution to problem", "install", "open doors"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Sarah gave her brother a guy to her home.  While she was gone, he used it to do what?\nA. front door\nB. turn lock\nC. solution to problem\nD. install\nE. open doors\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sarah gave her brother a guy to her home.  While she was gone, he used it to do what?\nA. front door\nB. turn lock\nC. solution to problem\nD. install\nE. open doors\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sarah gave her brother a guy to her home.  While she was gone, he used it to do what?\nA. front door\nB. turn lock\nC. solution to problem\nD. install\nE. open doors\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sarah gave her brother a guy to her home.  While she was gone, he used it to do what?\nA. front door\nB. turn lock\nC. solution to problem\nD. install\nE. open doors\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sarah gave her brother a guy to her home.  While she was gone, he used it to do what?\nA. front door\nB. turn lock\nC. solution to problem\nD. install\nE. open doors\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.9531068801879883", "False"]], [["-2.4531068801879883", "False"]], [["-4.703106880187988", "False"]], [["-3.9531068801879883", "False"]], [["-2.4531068801879883", "False"]]], "filtered_resps": [["-2.9531068801879883", "False"], ["-2.4531068801879883", "False"], ["-4.703106880187988", "False"], ["-3.9531068801879883", "False"], ["-2.4531068801879883", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "861e8b6f336dffde7e44ad17be52cc2cf8e5c29429770d73e94c12b7d291b2b5", "prompt_hash": "8a3d367b80c341f8d35299fe9f1f622eee2a4c76af3cca3b98e97fd3d5c4c4a4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1139, "doc": {"id": "611a4cc0e288b8a11afa923f48cb2ab4", "question": "When did mammoth's live?", "question_concept": "mammoth", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["boscage", "forest", "prehistory", "prehistoric times", "ancient times"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: When did mammoth's live?\nA. boscage\nB. forest\nC. prehistory\nD. prehistoric times\nE. ancient times\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When did mammoth's live?\nA. boscage\nB. forest\nC. prehistory\nD. prehistoric times\nE. ancient times\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When did mammoth's live?\nA. boscage\nB. forest\nC. prehistory\nD. prehistoric times\nE. ancient times\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When did mammoth's live?\nA. boscage\nB. forest\nC. prehistory\nD. prehistoric times\nE. ancient times\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When did mammoth's live?\nA. boscage\nB. forest\nC. prehistory\nD. prehistoric times\nE. ancient times\nAnswer:", "arg_1": " E"}}, "resps": [[["-7.027312755584717", "False"]], [["-7.027312755584717", "False"]], [["-4.277312755584717", "False"]], [["-1.7773127555847168", "False"]], [["-9.777313232421875", "False"]]], "filtered_resps": [["-7.027312755584717", "False"], ["-7.027312755584717", "False"], ["-4.277312755584717", "False"], ["-1.7773127555847168", "False"], ["-9.777313232421875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a5066963aed49d6f457e02e261d7cc49be78061e6e2428c252dc86716b0ebdbd", "prompt_hash": "2758c6d0a71a109cb7284b25628cd8c65a41ba0db3573b3b0e5838e645d05e62", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1140, "doc": {"id": "8e7941ce31996ca83cc0a68f7313c96d", "question": "After killing people, the murderer went to church after feeling what?", "question_concept": "killing people", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["murder", "remorse", "religious", "retaliation", "anguish"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: After killing people, the murderer went to church after feeling what?\nA. murder\nB. remorse\nC. religious\nD. retaliation\nE. anguish\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: After killing people, the murderer went to church after feeling what?\nA. murder\nB. remorse\nC. religious\nD. retaliation\nE. anguish\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: After killing people, the murderer went to church after feeling what?\nA. murder\nB. remorse\nC. religious\nD. retaliation\nE. anguish\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: After killing people, the murderer went to church after feeling what?\nA. murder\nB. remorse\nC. religious\nD. retaliation\nE. anguish\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: After killing people, the murderer went to church after feeling what?\nA. murder\nB. remorse\nC. religious\nD. retaliation\nE. anguish\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.324044227600098", "False"]], [["-1.3240444660186768", "True"]], [["-7.074044227600098", "False"]], [["-7.574044227600098", "False"]], [["-4.824044227600098", "False"]]], "filtered_resps": [["-4.324044227600098", "False"], ["-1.3240444660186768", "True"], ["-7.074044227600098", "False"], ["-7.574044227600098", "False"], ["-4.824044227600098", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ba1637ebd3325f280157f3d216d6f00f08f08347c763bd14431728602afabfa2", "prompt_hash": "5855a4daee41a0358f5a47fcbccdded0cdc64b783a284c48f8c87814d373b294", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1141, "doc": {"id": "ea02772e27f5bd40eced3b65e8c6427f", "question": "What might result in an unsuccessful suicide attempt?", "question_concept": "committing suicide", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["die", "interruption", "bleed", "hatred", "dying"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What might result in an unsuccessful suicide attempt?\nA. die\nB. interruption\nC. bleed\nD. hatred\nE. dying\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What might result in an unsuccessful suicide attempt?\nA. die\nB. interruption\nC. bleed\nD. hatred\nE. dying\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What might result in an unsuccessful suicide attempt?\nA. die\nB. interruption\nC. bleed\nD. hatred\nE. dying\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What might result in an unsuccessful suicide attempt?\nA. die\nB. interruption\nC. bleed\nD. hatred\nE. dying\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What might result in an unsuccessful suicide attempt?\nA. die\nB. interruption\nC. bleed\nD. hatred\nE. dying\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1055965423583984", "False"]], [["-1.605596661567688", "True"]], [["-6.105596542358398", "False"]], [["-5.605596542358398", "False"]], [["-6.605596542358398", "False"]]], "filtered_resps": [["-2.1055965423583984", "False"], ["-1.605596661567688", "True"], ["-6.105596542358398", "False"], ["-5.605596542358398", "False"], ["-6.605596542358398", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bc9b2f8240b78c69fb06c5926f56408d09fc76f4f0274ca3068fa419cd8399d9", "prompt_hash": "320c30d810704ee6a96c5b35a33bffca2010c5d73bb39a6af5d04ace9f14270a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1142, "doc": {"id": "de54d03e69d9765872f95ff06ed21499", "question": "What can happen if you are buying products that someone else does not want you to buy?", "question_concept": "buying products", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["joy", "disagreements", "agony", "pleasure", "owning"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What can happen if you are buying products that someone else does not want you to buy?\nA. joy\nB. disagreements\nC. agony\nD. pleasure\nE. owning\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What can happen if you are buying products that someone else does not want you to buy?\nA. joy\nB. disagreements\nC. agony\nD. pleasure\nE. owning\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What can happen if you are buying products that someone else does not want you to buy?\nA. joy\nB. disagreements\nC. agony\nD. pleasure\nE. owning\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What can happen if you are buying products that someone else does not want you to buy?\nA. joy\nB. disagreements\nC. agony\nD. pleasure\nE. owning\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What can happen if you are buying products that someone else does not want you to buy?\nA. joy\nB. disagreements\nC. agony\nD. pleasure\nE. owning\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.214033603668213", "False"]], [["-1.4640337228775024", "True"]], [["-2.714033603668213", "False"]], [["-7.214033603668213", "False"]], [["-6.464033603668213", "False"]]], "filtered_resps": [["-6.214033603668213", "False"], ["-1.4640337228775024", "True"], ["-2.714033603668213", "False"], ["-7.214033603668213", "False"], ["-6.464033603668213", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ee1c8d3a3c005bdbb8be7f87e108929fcc22af85e145c71df5f3c0cd2e02154a", "prompt_hash": "7b86e5664fa580dc81dca56660bf942929fd9d6f1d0e44a7d9c9e29a11834904", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1143, "doc": {"id": "b231a732a3fdf0621391e7e385f8d651", "question": "The child was getting many gifts for his birthday, his father reminded him to do what after opening each one?", "question_concept": "getting", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["show appreciation", "asking for", "exchanging", "say thank", "smile"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The child was getting many gifts for his birthday, his father reminded him to do what after opening each one?\nA. show appreciation\nB. asking for\nC. exchanging\nD. say thank\nE. smile\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The child was getting many gifts for his birthday, his father reminded him to do what after opening each one?\nA. show appreciation\nB. asking for\nC. exchanging\nD. say thank\nE. smile\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The child was getting many gifts for his birthday, his father reminded him to do what after opening each one?\nA. show appreciation\nB. asking for\nC. exchanging\nD. say thank\nE. smile\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The child was getting many gifts for his birthday, his father reminded him to do what after opening each one?\nA. show appreciation\nB. asking for\nC. exchanging\nD. say thank\nE. smile\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The child was getting many gifts for his birthday, his father reminded him to do what after opening each one?\nA. show appreciation\nB. asking for\nC. exchanging\nD. say thank\nE. smile\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.5125843286514282", "True"]], [["-5.762584209442139", "False"]], [["-6.512584209442139", "False"]], [["-3.0125842094421387", "False"]], [["-5.762584209442139", "False"]]], "filtered_resps": [["-1.5125843286514282", "True"], ["-5.762584209442139", "False"], ["-6.512584209442139", "False"], ["-3.0125842094421387", "False"], ["-5.762584209442139", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ccb9b18048f40c1ca0b45ed3a5156cab9abdb254b4f07bbe3bb8931d327e4bdf", "prompt_hash": "ba129da9c251c8103ce2bf98833a34120cb52375103d8671708b3fcd4d14e3cd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1144, "doc": {"id": "b9121c3228f961c5ad68958c702cd94b", "question": "Bob stands in the grass surrounded by trees and nature, where is Bob?", "question_concept": "grass", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["rest area", "desert", "state park", "fairgrounds", "soccer game"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Bob stands in the grass surrounded by trees and nature, where is Bob?\nA. rest area\nB. desert\nC. state park\nD. fairgrounds\nE. soccer game\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Bob stands in the grass surrounded by trees and nature, where is Bob?\nA. rest area\nB. desert\nC. state park\nD. fairgrounds\nE. soccer game\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Bob stands in the grass surrounded by trees and nature, where is Bob?\nA. rest area\nB. desert\nC. state park\nD. fairgrounds\nE. soccer game\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Bob stands in the grass surrounded by trees and nature, where is Bob?\nA. rest area\nB. desert\nC. state park\nD. fairgrounds\nE. soccer game\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Bob stands in the grass surrounded by trees and nature, where is Bob?\nA. rest area\nB. desert\nC. state park\nD. fairgrounds\nE. soccer game\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.383516788482666", "False"]], [["-4.883516788482666", "False"]], [["-0.883516788482666", "True"]], [["-6.383516788482666", "False"]], [["-9.133516311645508", "False"]]], "filtered_resps": [["-4.383516788482666", "False"], ["-4.883516788482666", "False"], ["-0.883516788482666", "True"], ["-6.383516788482666", "False"], ["-9.133516311645508", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c96b5dc825ef20adec97a0be08467517bdf30b3ebb11331547560a0748f083f4", "prompt_hash": "ff9045dca4d7404bf32838349109d20b16baf2ca9e626a68f766fa54e3265523", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1145, "doc": {"id": "4015ab002ff8c233d1c7ef26f5156b88", "question": "Bart entered his horse into the contest.  Where did he do this?", "question_concept": "horse", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["circus", "in kentucky", "western movie", "central park", "state fair"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Bart entered his horse into the contest.  Where did he do this?\nA. circus\nB. in kentucky\nC. western movie\nD. central park\nE. state fair\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Bart entered his horse into the contest.  Where did he do this?\nA. circus\nB. in kentucky\nC. western movie\nD. central park\nE. state fair\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Bart entered his horse into the contest.  Where did he do this?\nA. circus\nB. in kentucky\nC. western movie\nD. central park\nE. state fair\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Bart entered his horse into the contest.  Where did he do this?\nA. circus\nB. in kentucky\nC. western movie\nD. central park\nE. state fair\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Bart entered his horse into the contest.  Where did he do this?\nA. circus\nB. in kentucky\nC. western movie\nD. central park\nE. state fair\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.169322490692139", "False"]], [["-3.1693224906921387", "False"]], [["-8.919322967529297", "False"]], [["-8.419322967529297", "False"]], [["-1.6693226099014282", "False"]]], "filtered_resps": [["-5.169322490692139", "False"], ["-3.1693224906921387", "False"], ["-8.919322967529297", "False"], ["-8.419322967529297", "False"], ["-1.6693226099014282", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f905686b9278ebe630fed6cffc5892cd7df8d85fa42e5088ccac1d01a1bebe8f", "prompt_hash": "8e37fec5407fc56e0e2e3ca2ce51b6d6b5d1df93b4dd31abf443cbff82c4af2e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1146, "doc": {"id": "0197ade3bb26d163ab2e284c960c626f", "question": "From where does a snowflake form?", "question_concept": "snowflake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cloud", "snow storm", "billow", "air", "snowstorm"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: From where does a snowflake form?\nA. cloud\nB. snow storm\nC. billow\nD. air\nE. snowstorm\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: From where does a snowflake form?\nA. cloud\nB. snow storm\nC. billow\nD. air\nE. snowstorm\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: From where does a snowflake form?\nA. cloud\nB. snow storm\nC. billow\nD. air\nE. snowstorm\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: From where does a snowflake form?\nA. cloud\nB. snow storm\nC. billow\nD. air\nE. snowstorm\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: From where does a snowflake form?\nA. cloud\nB. snow storm\nC. billow\nD. air\nE. snowstorm\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9097509384155273", "True"]], [["-7.159750938415527", "False"]], [["-7.409750938415527", "False"]], [["-7.409750938415527", "False"]], [["-10.409750938415527", "False"]]], "filtered_resps": [["-0.9097509384155273", "True"], ["-7.159750938415527", "False"], ["-7.409750938415527", "False"], ["-7.409750938415527", "False"], ["-10.409750938415527", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "18a2e200399a3c06955dbe28b5e4ede04f9f475715df71bcc86f47a1b59f285d", "prompt_hash": "bb566613bbe2ffd9117c71d5523f17a96ae342653201da21630a4c27a58d92ce", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1147, "doc": {"id": "a90f9197a13c64089c9ba95bcba275ad", "question": "All the power tools like the drill used for fixing cars made for a very loud workplace where?", "question_concept": "drill", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["basement", "work shop", "tool shed", "repair shop", "store room"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: All the power tools like the drill used for fixing cars made for a very loud workplace where?\nA. basement\nB. work shop\nC. tool shed\nD. repair shop\nE. store room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: All the power tools like the drill used for fixing cars made for a very loud workplace where?\nA. basement\nB. work shop\nC. tool shed\nD. repair shop\nE. store room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: All the power tools like the drill used for fixing cars made for a very loud workplace where?\nA. basement\nB. work shop\nC. tool shed\nD. repair shop\nE. store room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: All the power tools like the drill used for fixing cars made for a very loud workplace where?\nA. basement\nB. work shop\nC. tool shed\nD. repair shop\nE. store room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: All the power tools like the drill used for fixing cars made for a very loud workplace where?\nA. basement\nB. work shop\nC. tool shed\nD. repair shop\nE. store room\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9209909439086914", "False"]], [["-2.9209909439086914", "False"]], [["-7.170990943908691", "False"]], [["-1.670991063117981", "True"]], [["-9.920990943908691", "False"]]], "filtered_resps": [["-3.9209909439086914", "False"], ["-2.9209909439086914", "False"], ["-7.170990943908691", "False"], ["-1.670991063117981", "True"], ["-9.920990943908691", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "711574807a4973ee6dfb8bc49102cdd571ec290ecbea4d3654176621e54d62e3", "prompt_hash": "992ca85ab805e01907b22fba3215fd354dabf380164224ae7edc5ab882e603f0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1148, "doc": {"id": "684204df916cc58d47293960f9c6ed9f", "question": "Applying for a job can make someone feel what sort of emotion, even if they get it?", "question_concept": "applying for job", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["working hard", "frustration", "rejection", "defeat", "stress"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Applying for a job can make someone feel what sort of emotion, even if they get it?\nA. working hard\nB. frustration\nC. rejection\nD. defeat\nE. stress\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Applying for a job can make someone feel what sort of emotion, even if they get it?\nA. working hard\nB. frustration\nC. rejection\nD. defeat\nE. stress\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Applying for a job can make someone feel what sort of emotion, even if they get it?\nA. working hard\nB. frustration\nC. rejection\nD. defeat\nE. stress\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Applying for a job can make someone feel what sort of emotion, even if they get it?\nA. working hard\nB. frustration\nC. rejection\nD. defeat\nE. stress\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Applying for a job can make someone feel what sort of emotion, even if they get it?\nA. working hard\nB. frustration\nC. rejection\nD. defeat\nE. stress\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5826947689056396", "False"]], [["-2.3326947689056396", "False"]], [["-3.8326947689056396", "False"]], [["-5.332695007324219", "False"]], [["-1.8326947689056396", "True"]]], "filtered_resps": [["-3.5826947689056396", "False"], ["-2.3326947689056396", "False"], ["-3.8326947689056396", "False"], ["-5.332695007324219", "False"], ["-1.8326947689056396", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4fe2c790bfd0599ba1fd9982de6f5d576fcebd06feaa4e91874c41c312603310", "prompt_hash": "e041eb9fce0eec07aebc25ae2961fc19bc40a36ae5fce1b7f6794cf15fdd6a01", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1149, "doc": {"id": "a2aa95861ef74bf1ecfc55db505e3982", "question": "A farmer sees a weasel in the woods, where is the farmer?", "question_concept": "weasel", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["chicken coop", "beach", "fairytale", "great outdoors", "corn fields"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: A farmer sees a weasel in the woods, where is the farmer?\nA. chicken coop\nB. beach\nC. fairytale\nD. great outdoors\nE. corn fields\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A farmer sees a weasel in the woods, where is the farmer?\nA. chicken coop\nB. beach\nC. fairytale\nD. great outdoors\nE. corn fields\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A farmer sees a weasel in the woods, where is the farmer?\nA. chicken coop\nB. beach\nC. fairytale\nD. great outdoors\nE. corn fields\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A farmer sees a weasel in the woods, where is the farmer?\nA. chicken coop\nB. beach\nC. fairytale\nD. great outdoors\nE. corn fields\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A farmer sees a weasel in the woods, where is the farmer?\nA. chicken coop\nB. beach\nC. fairytale\nD. great outdoors\nE. corn fields\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5942111015319824", "False"]], [["-6.094211101531982", "False"]], [["-5.594211101531982", "False"]], [["-1.0942109823226929", "True"]], [["-3.3442111015319824", "False"]]], "filtered_resps": [["-2.5942111015319824", "False"], ["-6.094211101531982", "False"], ["-5.594211101531982", "False"], ["-1.0942109823226929", "True"], ["-3.3442111015319824", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ac0a22d7ab16dced96b7595f79ec19718d52cb640de7ce0f89de7752a39c0ada", "prompt_hash": "eb27d54e3fc92f3f28207641a656f1607cf9b8e1b9cd8474741a3b4f8af93d82", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1150, "doc": {"id": "8555dd9667d010018961a2f7d1c22704", "question": "He picked up the perfect pebble, he planned to skip it across the entire small what?", "question_concept": "pebble", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["manual", "lake", "aquarium", "pond", "playground"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: He picked up the perfect pebble, he planned to skip it across the entire small what?\nA. manual\nB. lake\nC. aquarium\nD. pond\nE. playground\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He picked up the perfect pebble, he planned to skip it across the entire small what?\nA. manual\nB. lake\nC. aquarium\nD. pond\nE. playground\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He picked up the perfect pebble, he planned to skip it across the entire small what?\nA. manual\nB. lake\nC. aquarium\nD. pond\nE. playground\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He picked up the perfect pebble, he planned to skip it across the entire small what?\nA. manual\nB. lake\nC. aquarium\nD. pond\nE. playground\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He picked up the perfect pebble, he planned to skip it across the entire small what?\nA. manual\nB. lake\nC. aquarium\nD. pond\nE. playground\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.242893218994141", "False"]], [["-2.9928932189941406", "False"]], [["-9.24289321899414", "False"]], [["-1.492893099784851", "False"]], [["-10.74289321899414", "False"]]], "filtered_resps": [["-4.242893218994141", "False"], ["-2.9928932189941406", "False"], ["-9.24289321899414", "False"], ["-1.492893099784851", "False"], ["-10.74289321899414", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "21af292e90999c6f275c3a62beac27494643ed4b12dcc9aa494b7b4b52df1d4f", "prompt_hash": "d3a90558ff2d6954a5913d665f7b4b9aa1c1c47ca724bdc5fe8e4b6a3102ec16", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1151, "doc": {"id": "84a761f516efce04ab27d7ca8dd25255", "question": "Traveling from new place to new place is likely to be what?", "question_concept": "traveling", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["going somewhere", "exhilarating", "diarrhea", "relocation", "exhausting"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Traveling from new place to new place is likely to be what?\nA. going somewhere\nB. exhilarating\nC. diarrhea\nD. relocation\nE. exhausting\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Traveling from new place to new place is likely to be what?\nA. going somewhere\nB. exhilarating\nC. diarrhea\nD. relocation\nE. exhausting\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Traveling from new place to new place is likely to be what?\nA. going somewhere\nB. exhilarating\nC. diarrhea\nD. relocation\nE. exhausting\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Traveling from new place to new place is likely to be what?\nA. going somewhere\nB. exhilarating\nC. diarrhea\nD. relocation\nE. exhausting\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Traveling from new place to new place is likely to be what?\nA. going somewhere\nB. exhilarating\nC. diarrhea\nD. relocation\nE. exhausting\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.193761348724365", "False"]], [["-3.4437613487243652", "False"]], [["-6.443761348724365", "False"]], [["-5.693761348724365", "False"]], [["-1.9437613487243652", "False"]]], "filtered_resps": [["-5.193761348724365", "False"], ["-3.4437613487243652", "False"], ["-6.443761348724365", "False"], ["-5.693761348724365", "False"], ["-1.9437613487243652", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "91777c29c105a9e6b6af86039aa85bb3fd242a1fb577c6072420918155c82d9f", "prompt_hash": "72111ea2f9e1ee8413fc7c4453369f82911029f323a2915f7d56352f94dfe477", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 1152, "doc": {"id": "45a6becd307342669d9d17474e50b97a", "question": "Turkey only has a small northern part of their country located in part of the what?", "question_concept": "turkey", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["middle east", "oven", "balkan peninsula", "provide meat", "asia minor"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Turkey only has a small northern part of their country located in part of the what?\nA. middle east\nB. oven\nC. balkan peninsula\nD. provide meat\nE. asia minor\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Turkey only has a small northern part of their country located in part of the what?\nA. middle east\nB. oven\nC. balkan peninsula\nD. provide meat\nE. asia minor\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Turkey only has a small northern part of their country located in part of the what?\nA. middle east\nB. oven\nC. balkan peninsula\nD. provide meat\nE. asia minor\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Turkey only has a small northern part of their country located in part of the what?\nA. middle east\nB. oven\nC. balkan peninsula\nD. provide meat\nE. asia minor\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Turkey only has a small northern part of their country located in part of the what?\nA. middle east\nB. oven\nC. balkan peninsula\nD. provide meat\nE. asia minor\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.730207920074463", "False"]], [["-6.230207920074463", "False"]], [["-4.480207920074463", "False"]], [["-7.480207920074463", "False"]], [["-1.730207920074463", "False"]]], "filtered_resps": [["-4.730207920074463", "False"], ["-6.230207920074463", "False"], ["-4.480207920074463", "False"], ["-7.480207920074463", "False"], ["-1.730207920074463", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "cb19bbd68315c39d89d9a0d78a1872fb9658191d85a003e0c2b87ac112226a33", "prompt_hash": "b5af9470547297cbd87bcac303eea9632d522fb6ab63f7b117baca1e43ca2ef8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1153, "doc": {"id": "c509c499bace6de324b39c0d4d0c30fa", "question": "Where might someone store a reusable shopping bag?", "question_concept": "shopping bag", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["supermarket", "home", "mart", "obesity", "closet"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where might someone store a reusable shopping bag?\nA. supermarket\nB. home\nC. mart\nD. obesity\nE. closet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where might someone store a reusable shopping bag?\nA. supermarket\nB. home\nC. mart\nD. obesity\nE. closet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where might someone store a reusable shopping bag?\nA. supermarket\nB. home\nC. mart\nD. obesity\nE. closet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where might someone store a reusable shopping bag?\nA. supermarket\nB. home\nC. mart\nD. obesity\nE. closet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where might someone store a reusable shopping bag?\nA. supermarket\nB. home\nC. mart\nD. obesity\nE. closet\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.6441216468811035", "False"]], [["-1.3941216468811035", "True"]], [["-6.6441216468811035", "False"]], [["-8.894121170043945", "False"]], [["-1.8941216468811035", "False"]]], "filtered_resps": [["-2.6441216468811035", "False"], ["-1.3941216468811035", "True"], ["-6.6441216468811035", "False"], ["-8.894121170043945", "False"], ["-1.8941216468811035", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c11f0955df936b790926f3b38d1ee654cb953f307772266d9a86e5de74d282c3", "prompt_hash": "08ae6bdcc4770040f471666dce9c8b75573afbb857cdd477eb9c84ccc43cf139", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1154, "doc": {"id": "77ddc9134bb27f9962aa2ed5ec5a5ef9", "question": "How could you have fun by yourself with no one around you?", "question_concept": "fun", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fairgrounds", "watching television", "tired", "enjoyable", "friend's house"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: How could you have fun by yourself with no one around you?\nA. fairgrounds\nB. watching television\nC. tired\nD. enjoyable\nE. friend's house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How could you have fun by yourself with no one around you?\nA. fairgrounds\nB. watching television\nC. tired\nD. enjoyable\nE. friend's house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How could you have fun by yourself with no one around you?\nA. fairgrounds\nB. watching television\nC. tired\nD. enjoyable\nE. friend's house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How could you have fun by yourself with no one around you?\nA. fairgrounds\nB. watching television\nC. tired\nD. enjoyable\nE. friend's house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How could you have fun by yourself with no one around you?\nA. fairgrounds\nB. watching television\nC. tired\nD. enjoyable\nE. friend's house\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.0901577472686768", "False"]], [["-0.8401577472686768", "True"]], [["-4.340157508850098", "False"]], [["-3.3401577472686768", "False"]], [["-5.590157508850098", "False"]]], "filtered_resps": [["-2.0901577472686768", "False"], ["-0.8401577472686768", "True"], ["-4.340157508850098", "False"], ["-3.3401577472686768", "False"], ["-5.590157508850098", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d4e7e7fecd3a02fd3a592a8c3ffc9d38e0aa7fe14774231f6d382dd0d33e44fe", "prompt_hash": "947d39a7af06e42f274273e1cefea864874d18513a7a8328c77e435d03ec3426", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1155, "doc": {"id": "715583129369c0c5c9f499c93a1c095e", "question": "The potato might be the official vegetable of what?", "question_concept": "potato", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["vegans", "kitchen cupboard", "restaurants", "chicken", "maryland"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The potato might be the official vegetable of what?\nA. vegans\nB. kitchen cupboard\nC. restaurants\nD. chicken\nE. maryland\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The potato might be the official vegetable of what?\nA. vegans\nB. kitchen cupboard\nC. restaurants\nD. chicken\nE. maryland\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The potato might be the official vegetable of what?\nA. vegans\nB. kitchen cupboard\nC. restaurants\nD. chicken\nE. maryland\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The potato might be the official vegetable of what?\nA. vegans\nB. kitchen cupboard\nC. restaurants\nD. chicken\nE. maryland\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The potato might be the official vegetable of what?\nA. vegans\nB. kitchen cupboard\nC. restaurants\nD. chicken\nE. maryland\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.982550859451294", "False"]], [["-4.982550621032715", "False"]], [["-6.482550621032715", "False"]], [["-8.232550621032715", "False"]], [["-1.232550859451294", "True"]]], "filtered_resps": [["-3.982550859451294", "False"], ["-4.982550621032715", "False"], ["-6.482550621032715", "False"], ["-8.232550621032715", "False"], ["-1.232550859451294", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "2e56614ef65ab9c18cf226113fe2deb628ae4ffa61f06ab5dbe01ab1d49bcbeb", "prompt_hash": "23f201ac0d5a17cd67505781490ef9b58096e72a49c6380fc077190979360ff8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1156, "doc": {"id": "a478e8b7c049781574f7fbb11ba1eec0", "question": "Where is the sky most beautiful?", "question_concept": "sky", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["planetarium", "outdoors", "atmosphere", "night", "photo"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where is the sky most beautiful?\nA. planetarium\nB. outdoors\nC. atmosphere\nD. night\nE. photo\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is the sky most beautiful?\nA. planetarium\nB. outdoors\nC. atmosphere\nD. night\nE. photo\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is the sky most beautiful?\nA. planetarium\nB. outdoors\nC. atmosphere\nD. night\nE. photo\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is the sky most beautiful?\nA. planetarium\nB. outdoors\nC. atmosphere\nD. night\nE. photo\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is the sky most beautiful?\nA. planetarium\nB. outdoors\nC. atmosphere\nD. night\nE. photo\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.3167405128479", "False"]], [["-0.8167405724525452", "True"]], [["-6.3167405128479", "False"]], [["-6.5667405128479", "False"]], [["-8.316740989685059", "False"]]], "filtered_resps": [["-5.3167405128479", "False"], ["-0.8167405724525452", "True"], ["-6.3167405128479", "False"], ["-6.5667405128479", "False"], ["-8.316740989685059", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c34041a41257c10695553c64ba2736ca638dd0da5ffe697643f65e5dce548c3e", "prompt_hash": "bd2be1aab2b2fbb54caeb6344b35eb480358feca4d2101de4d792f0fd5ad92fa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1157, "doc": {"id": "f427f9de6bf580314531baf86de8acbc", "question": "What type of fruit is easily broken in to sections?", "question_concept": "section", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["slide", "citrus", "band", "orchestra", "coconut"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What type of fruit is easily broken in to sections?\nA. slide\nB. citrus\nC. band\nD. orchestra\nE. coconut\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What type of fruit is easily broken in to sections?\nA. slide\nB. citrus\nC. band\nD. orchestra\nE. coconut\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What type of fruit is easily broken in to sections?\nA. slide\nB. citrus\nC. band\nD. orchestra\nE. coconut\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What type of fruit is easily broken in to sections?\nA. slide\nB. citrus\nC. band\nD. orchestra\nE. coconut\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What type of fruit is easily broken in to sections?\nA. slide\nB. citrus\nC. band\nD. orchestra\nE. coconut\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.932190895080566", "False"]], [["-1.1821908950805664", "True"]], [["-4.682190895080566", "False"]], [["-8.182190895080566", "False"]], [["-6.432190895080566", "False"]]], "filtered_resps": [["-4.932190895080566", "False"], ["-1.1821908950805664", "True"], ["-4.682190895080566", "False"], ["-8.182190895080566", "False"], ["-6.432190895080566", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3d3fff7624a398d7e07cc5dd793ce2f26213e9c4b7139480e7e8b205352f8e8e", "prompt_hash": "6bbc12860989e5802a02ae184da2a43f37fb91aa2ab7a2846ad3aac5ddfb9ae3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1158, "doc": {"id": "0f7425ecbe369bf41a230aab92d84132", "question": "Marathoners feel fatigued after running twenty six miles, but some that have pushed them self too hard might be prone to what?", "question_concept": "running twenty six miles", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["excruciating pain", "passing out", "death", "drunk", "exhaustion"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Marathoners feel fatigued after running twenty six miles, but some that have pushed them self too hard might be prone to what?\nA. excruciating pain\nB. passing out\nC. death\nD. drunk\nE. exhaustion\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Marathoners feel fatigued after running twenty six miles, but some that have pushed them self too hard might be prone to what?\nA. excruciating pain\nB. passing out\nC. death\nD. drunk\nE. exhaustion\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Marathoners feel fatigued after running twenty six miles, but some that have pushed them self too hard might be prone to what?\nA. excruciating pain\nB. passing out\nC. death\nD. drunk\nE. exhaustion\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Marathoners feel fatigued after running twenty six miles, but some that have pushed them self too hard might be prone to what?\nA. excruciating pain\nB. passing out\nC. death\nD. drunk\nE. exhaustion\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Marathoners feel fatigued after running twenty six miles, but some that have pushed them self too hard might be prone to what?\nA. excruciating pain\nB. passing out\nC. death\nD. drunk\nE. exhaustion\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.353518486022949", "False"]], [["-3.103518486022949", "False"]], [["-3.853518486022949", "False"]], [["-6.603518486022949", "False"]], [["-2.353518486022949", "False"]]], "filtered_resps": [["-3.353518486022949", "False"], ["-3.103518486022949", "False"], ["-3.853518486022949", "False"], ["-6.603518486022949", "False"], ["-2.353518486022949", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f7961b7ce6df0a2a38b9abc5d63687fa9c93125ef00d760b1208929232a54a2e", "prompt_hash": "682d3941b9406b852bd45f84a2447c9abd4c5677e3a6dff68b09060606ee8540", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 1159, "doc": {"id": "c872c08a95dd28a16479b76f240a4ad5", "question": "Billy liked driving cars.  He was good at it.  But he was rattled ever since his father experienced what?", "question_concept": "driving car", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["transportation", "pollution", "stress", "death", "go somewhere"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Billy liked driving cars.  He was good at it.  But he was rattled ever since his father experienced what?\nA. transportation\nB. pollution\nC. stress\nD. death\nE. go somewhere\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Billy liked driving cars.  He was good at it.  But he was rattled ever since his father experienced what?\nA. transportation\nB. pollution\nC. stress\nD. death\nE. go somewhere\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Billy liked driving cars.  He was good at it.  But he was rattled ever since his father experienced what?\nA. transportation\nB. pollution\nC. stress\nD. death\nE. go somewhere\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Billy liked driving cars.  He was good at it.  But he was rattled ever since his father experienced what?\nA. transportation\nB. pollution\nC. stress\nD. death\nE. go somewhere\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Billy liked driving cars.  He was good at it.  But he was rattled ever since his father experienced what?\nA. transportation\nB. pollution\nC. stress\nD. death\nE. go somewhere\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.325806617736816", "False"]], [["-5.575806617736816", "False"]], [["-4.575806617736816", "False"]], [["-1.325806736946106", "False"]], [["-9.825806617736816", "False"]]], "filtered_resps": [["-6.325806617736816", "False"], ["-5.575806617736816", "False"], ["-4.575806617736816", "False"], ["-1.325806736946106", "False"], ["-9.825806617736816", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f73fe315b3bdff960e5abd588ecf9a2ae09b1e2144e5784345cd04905210b4f0", "prompt_hash": "00d0d49221c6fecd811be34c79b2c9cc670ef77f26a2f098952d17b410442e7d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1160, "doc": {"id": "08d908ed723f813574992195d61386a2", "question": "I am cold, what should I do to stay warm?", "question_concept": "cold", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["stay in bed", "light fire", "freezer", "lay on ice", "spit"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: I am cold, what should I do to stay warm?\nA. stay in bed\nB. light fire\nC. freezer\nD. lay on ice\nE. spit\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: I am cold, what should I do to stay warm?\nA. stay in bed\nB. light fire\nC. freezer\nD. lay on ice\nE. spit\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: I am cold, what should I do to stay warm?\nA. stay in bed\nB. light fire\nC. freezer\nD. lay on ice\nE. spit\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: I am cold, what should I do to stay warm?\nA. stay in bed\nB. light fire\nC. freezer\nD. lay on ice\nE. spit\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: I am cold, what should I do to stay warm?\nA. stay in bed\nB. light fire\nC. freezer\nD. lay on ice\nE. spit\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.003066062927246", "False"]], [["-1.003065824508667", "True"]], [["-7.503066062927246", "False"]], [["-8.253066062927246", "False"]], [["-8.253066062927246", "False"]]], "filtered_resps": [["-5.003066062927246", "False"], ["-1.003065824508667", "True"], ["-7.503066062927246", "False"], ["-8.253066062927246", "False"], ["-8.253066062927246", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7cdcbce557baddfe77047dca99f3a6c0fffce0e7af52319e0e23e740e0084772", "prompt_hash": "de5142d1e34ea48d08aea62609d3d6a0e86ecd0a35d1f7d86a05d74ad00f0f53", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1161, "doc": {"id": "5365fd00ef8cec62ee5685e246a939db", "question": "Copulating with the wrong partner may be ill advised, many diseases can be transferred that can cause different types of what?", "question_concept": "copulating", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["intense pleasure", "ejaculation", "period of rest", "enjoyment", "skin irritation"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Copulating with the wrong partner may be ill advised, many diseases can be transferred that can cause different types of what?\nA. intense pleasure\nB. ejaculation\nC. period of rest\nD. enjoyment\nE. skin irritation\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Copulating with the wrong partner may be ill advised, many diseases can be transferred that can cause different types of what?\nA. intense pleasure\nB. ejaculation\nC. period of rest\nD. enjoyment\nE. skin irritation\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Copulating with the wrong partner may be ill advised, many diseases can be transferred that can cause different types of what?\nA. intense pleasure\nB. ejaculation\nC. period of rest\nD. enjoyment\nE. skin irritation\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Copulating with the wrong partner may be ill advised, many diseases can be transferred that can cause different types of what?\nA. intense pleasure\nB. ejaculation\nC. period of rest\nD. enjoyment\nE. skin irritation\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Copulating with the wrong partner may be ill advised, many diseases can be transferred that can cause different types of what?\nA. intense pleasure\nB. ejaculation\nC. period of rest\nD. enjoyment\nE. skin irritation\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.923461675643921", "False"]], [["-3.423461675643921", "False"]], [["-5.6734619140625", "False"]], [["-4.6734619140625", "False"]], [["-4.4234619140625", "False"]]], "filtered_resps": [["-3.923461675643921", "False"], ["-3.423461675643921", "False"], ["-5.6734619140625", "False"], ["-4.6734619140625", "False"], ["-4.4234619140625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "30f82686b5f0b0e2a54b2a03d97b6f42efc4057f658df13b4ced493be7da25c9", "prompt_hash": "7fc24b7188ff59852cf913b581a68c29033c1ba4da6a10cd3abdacf32c0bcde6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1162, "doc": {"id": "5649bd90dbb57e223fd843b7a4563a0f", "question": "What do audiences clap for?", "question_concept": "audience", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["cinema", "theatre", "movies", "show", "hockey game"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What do audiences clap for?\nA. cinema\nB. theatre\nC. movies\nD. show\nE. hockey game\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do audiences clap for?\nA. cinema\nB. theatre\nC. movies\nD. show\nE. hockey game\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do audiences clap for?\nA. cinema\nB. theatre\nC. movies\nD. show\nE. hockey game\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do audiences clap for?\nA. cinema\nB. theatre\nC. movies\nD. show\nE. hockey game\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do audiences clap for?\nA. cinema\nB. theatre\nC. movies\nD. show\nE. hockey game\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.975464344024658", "False"]], [["-1.7254642248153687", "False"]], [["-5.225464344024658", "False"]], [["-5.725464344024658", "False"]], [["-5.975464344024658", "False"]]], "filtered_resps": [["-2.975464344024658", "False"], ["-1.7254642248153687", "False"], ["-5.225464344024658", "False"], ["-5.725464344024658", "False"], ["-5.975464344024658", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "056a2c4c5413773cf693fdb42ff356f8058126d903062fd2c7ee016a714390c4", "prompt_hash": "95f8172c9ef00d6f9e3a757179b484cade98148e4fa2ed15fd59cd8c8c64a502", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1163, "doc": {"id": "0a2195ae8d4706abc5721578c9991466", "question": "Where would you get a balalaika if you do not have one?", "question_concept": "balalaika", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["orchestra", "music store", "buy music", "make music", "symphony"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you get a balalaika if you do not have one?\nA. orchestra\nB. music store\nC. buy music\nD. make music\nE. symphony\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you get a balalaika if you do not have one?\nA. orchestra\nB. music store\nC. buy music\nD. make music\nE. symphony\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you get a balalaika if you do not have one?\nA. orchestra\nB. music store\nC. buy music\nD. make music\nE. symphony\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you get a balalaika if you do not have one?\nA. orchestra\nB. music store\nC. buy music\nD. make music\nE. symphony\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you get a balalaika if you do not have one?\nA. orchestra\nB. music store\nC. buy music\nD. make music\nE. symphony\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.799713611602783", "False"]], [["-1.2997136116027832", "False"]], [["-6.799713611602783", "False"]], [["-8.799713134765625", "False"]], [["-10.549713134765625", "False"]]], "filtered_resps": [["-2.799713611602783", "False"], ["-1.2997136116027832", "False"], ["-6.799713611602783", "False"], ["-8.799713134765625", "False"], ["-10.549713134765625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "070acdb6231d5c22a4887e8a14906ac9bbb0afc8ac3ef21c70c69487798fe7aa", "prompt_hash": "93cfefd2c41901f98697c5a06a4fffea408cc23c5361557a193e23a325fb863e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1164, "doc": {"id": "5d15989039d46156b417c149728591de", "question": "Hoping for a beautiful day, what did the clouds do that disappointed everyone?", "question_concept": "beautiful", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["homely", "overcast", "hideous", "overrated", "misshapen"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Hoping for a beautiful day, what did the clouds do that disappointed everyone?\nA. homely\nB. overcast\nC. hideous\nD. overrated\nE. misshapen\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Hoping for a beautiful day, what did the clouds do that disappointed everyone?\nA. homely\nB. overcast\nC. hideous\nD. overrated\nE. misshapen\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Hoping for a beautiful day, what did the clouds do that disappointed everyone?\nA. homely\nB. overcast\nC. hideous\nD. overrated\nE. misshapen\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Hoping for a beautiful day, what did the clouds do that disappointed everyone?\nA. homely\nB. overcast\nC. hideous\nD. overrated\nE. misshapen\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Hoping for a beautiful day, what did the clouds do that disappointed everyone?\nA. homely\nB. overcast\nC. hideous\nD. overrated\nE. misshapen\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.568756103515625", "False"]], [["-1.3187562227249146", "False"]], [["-5.318756103515625", "False"]], [["-6.318756103515625", "False"]], [["-7.568756103515625", "False"]]], "filtered_resps": [["-5.568756103515625", "False"], ["-1.3187562227249146", "False"], ["-5.318756103515625", "False"], ["-6.318756103515625", "False"], ["-7.568756103515625", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c33e96409d6f6abdce81686e6436e88b9a32caea5df6a13a5763dcaf7918ba12", "prompt_hash": "0a33e1b79a8f5c43ae92874d257684ed362ebd95a28eae0c99fb9f22b125bbe2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1165, "doc": {"id": "6eb57102b44ab74163d8f9821cbdabd0", "question": "What type of demands to the unions need to be making to go off strike?", "question_concept": "go off strike", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["reasonable", "more money", "not go to work", "return to work", "union"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What type of demands to the unions need to be making to go off strike?\nA. reasonable\nB. more money\nC. not go to work\nD. return to work\nE. union\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What type of demands to the unions need to be making to go off strike?\nA. reasonable\nB. more money\nC. not go to work\nD. return to work\nE. union\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What type of demands to the unions need to be making to go off strike?\nA. reasonable\nB. more money\nC. not go to work\nD. return to work\nE. union\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What type of demands to the unions need to be making to go off strike?\nA. reasonable\nB. more money\nC. not go to work\nD. return to work\nE. union\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What type of demands to the unions need to be making to go off strike?\nA. reasonable\nB. more money\nC. not go to work\nD. return to work\nE. union\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.043557167053223", "False"]], [["-6.043557167053223", "False"]], [["-6.543557167053223", "False"]], [["-1.7935569286346436", "False"]], [["-8.293557167053223", "False"]]], "filtered_resps": [["-4.043557167053223", "False"], ["-6.043557167053223", "False"], ["-6.543557167053223", "False"], ["-1.7935569286346436", "False"], ["-8.293557167053223", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "1c72f6390c031f2ccd8ae53950829c3173badb381ee14bdcc43a8f2fd54a5ed1", "prompt_hash": "59a2292cbf9899977e54d03ea39a09b73bdde73000496e79e014f9040a43d613", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 1166, "doc": {"id": "63861ac5e633db9090704ae315ef6f93", "question": "The landscaper was carefully arching stones together, he was creating an elaborate what over the creek?", "question_concept": "stones", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["park", "made from rocks", "balloon", "field", "bridge"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The landscaper was carefully arching stones together, he was creating an elaborate what over the creek?\nA. park\nB. made from rocks\nC. balloon\nD. field\nE. bridge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The landscaper was carefully arching stones together, he was creating an elaborate what over the creek?\nA. park\nB. made from rocks\nC. balloon\nD. field\nE. bridge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The landscaper was carefully arching stones together, he was creating an elaborate what over the creek?\nA. park\nB. made from rocks\nC. balloon\nD. field\nE. bridge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The landscaper was carefully arching stones together, he was creating an elaborate what over the creek?\nA. park\nB. made from rocks\nC. balloon\nD. field\nE. bridge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The landscaper was carefully arching stones together, he was creating an elaborate what over the creek?\nA. park\nB. made from rocks\nC. balloon\nD. field\nE. bridge\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.4127864837646484", "False"]], [["-3.9127864837646484", "False"]], [["-7.662786483764648", "False"]], [["-7.912786483764648", "False"]], [["-1.1627863645553589", "True"]]], "filtered_resps": [["-3.4127864837646484", "False"], ["-3.9127864837646484", "False"], ["-7.662786483764648", "False"], ["-7.912786483764648", "False"], ["-1.1627863645553589", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "79758290c180f30d8dd9e420505858126d08a437345e3067a38f1d328f4c081a", "prompt_hash": "a30615bc5c5d6169a00caa4b757b41f61a0c493e14382923b49d01884e568cb1", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1167, "doc": {"id": "8058c566a4f488033d00e6520b17caea", "question": "John was not happy with his marriage. He and his wife drifted apart.     All and all, recent turns could be described as what?", "question_concept": "happy", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["inappropriate", "sadness", "unsatisfied", "unfortunate", "disenchanted"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: John was not happy with his marriage. He and his wife drifted apart.     All and all, recent turns could be described as what?\nA. inappropriate\nB. sadness\nC. unsatisfied\nD. unfortunate\nE. disenchanted\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John was not happy with his marriage. He and his wife drifted apart.     All and all, recent turns could be described as what?\nA. inappropriate\nB. sadness\nC. unsatisfied\nD. unfortunate\nE. disenchanted\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John was not happy with his marriage. He and his wife drifted apart.     All and all, recent turns could be described as what?\nA. inappropriate\nB. sadness\nC. unsatisfied\nD. unfortunate\nE. disenchanted\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John was not happy with his marriage. He and his wife drifted apart.     All and all, recent turns could be described as what?\nA. inappropriate\nB. sadness\nC. unsatisfied\nD. unfortunate\nE. disenchanted\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John was not happy with his marriage. He and his wife drifted apart.     All and all, recent turns could be described as what?\nA. inappropriate\nB. sadness\nC. unsatisfied\nD. unfortunate\nE. disenchanted\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.031747341156006", "False"]], [["-5.531747341156006", "False"]], [["-4.281747341156006", "False"]], [["-2.531747341156006", "False"]], [["-2.281747341156006", "False"]]], "filtered_resps": [["-6.031747341156006", "False"], ["-5.531747341156006", "False"], ["-4.281747341156006", "False"], ["-2.531747341156006", "False"], ["-2.281747341156006", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "368cd3166f7c514d991ab9dfbdee941669f229f595469149ffa81244344a477d", "prompt_hash": "7ec20188d0ddcbc95c19b7f1a9b9bc6e71535ebc412578469cc658d843ff820c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1168, "doc": {"id": "57b83653d82b27d32bc39228130f3516", "question": "The poor girls needed a light to see, what was the relationship between that light and finishing her homework?", "question_concept": "light", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["darkness", "cumbersome", "obesity", "forceful", "crucial"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The poor girls needed a light to see, what was the relationship between that light and finishing her homework?\nA. darkness\nB. cumbersome\nC. obesity\nD. forceful\nE. crucial\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The poor girls needed a light to see, what was the relationship between that light and finishing her homework?\nA. darkness\nB. cumbersome\nC. obesity\nD. forceful\nE. crucial\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The poor girls needed a light to see, what was the relationship between that light and finishing her homework?\nA. darkness\nB. cumbersome\nC. obesity\nD. forceful\nE. crucial\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The poor girls needed a light to see, what was the relationship between that light and finishing her homework?\nA. darkness\nB. cumbersome\nC. obesity\nD. forceful\nE. crucial\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The poor girls needed a light to see, what was the relationship between that light and finishing her homework?\nA. darkness\nB. cumbersome\nC. obesity\nD. forceful\nE. crucial\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.0074479579925537", "False"]], [["-7.007448196411133", "False"]], [["-8.757448196411133", "False"]], [["-9.007448196411133", "False"]], [["-0.7574480175971985", "True"]]], "filtered_resps": [["-3.0074479579925537", "False"], ["-7.007448196411133", "False"], ["-8.757448196411133", "False"], ["-9.007448196411133", "False"], ["-0.7574480175971985", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fa539243c0d05fd8e13ba4bb3e71a4c196a926839a6e74c0be023c47866c9a84", "prompt_hash": "c4c6a6c7a666a59c84a711d4de6de25511908a6445c86bfba9593f4caa669bf9", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1169, "doc": {"id": "410f907f817dd7aa8e73291a918d3d86", "question": "Where would you find a ticket booth and see a concert?", "question_concept": "ticket booth", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["clerk", "indoors", "movie theater", "venue", "auditorium"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find a ticket booth and see a concert?\nA. clerk\nB. indoors\nC. movie theater\nD. venue\nE. auditorium\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find a ticket booth and see a concert?\nA. clerk\nB. indoors\nC. movie theater\nD. venue\nE. auditorium\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find a ticket booth and see a concert?\nA. clerk\nB. indoors\nC. movie theater\nD. venue\nE. auditorium\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find a ticket booth and see a concert?\nA. clerk\nB. indoors\nC. movie theater\nD. venue\nE. auditorium\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find a ticket booth and see a concert?\nA. clerk\nB. indoors\nC. movie theater\nD. venue\nE. auditorium\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.539024829864502", "False"]], [["-5.039024829864502", "False"]], [["-6.789024829864502", "False"]], [["-1.289024829864502", "False"]], [["-6.789024829864502", "False"]]], "filtered_resps": [["-2.539024829864502", "False"], ["-5.039024829864502", "False"], ["-6.789024829864502", "False"], ["-1.289024829864502", "False"], ["-6.789024829864502", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3249796d76d95f8a895336e8ac8c47f04e34627f6743b2994bacc051769b7179", "prompt_hash": "2ff4b62904c3d6f74234f60b1eff3c8f05a6aa0e4523ab431fb6d68b71e5f408", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1170, "doc": {"id": "506c2dbfe7b00a82bfdf0507a8de88fb", "question": "Who is not famous for a superhighway with no speed limit?", "question_concept": "superhighway", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["europe", "germany", "industrialized country", "city", "america"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Who is not famous for a superhighway with no speed limit?\nA. europe\nB. germany\nC. industrialized country\nD. city\nE. america\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Who is not famous for a superhighway with no speed limit?\nA. europe\nB. germany\nC. industrialized country\nD. city\nE. america\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Who is not famous for a superhighway with no speed limit?\nA. europe\nB. germany\nC. industrialized country\nD. city\nE. america\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Who is not famous for a superhighway with no speed limit?\nA. europe\nB. germany\nC. industrialized country\nD. city\nE. america\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Who is not famous for a superhighway with no speed limit?\nA. europe\nB. germany\nC. industrialized country\nD. city\nE. america\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.943493127822876", "False"]], [["-5.443492889404297", "False"]], [["-4.693492889404297", "False"]], [["-3.193493127822876", "False"]], [["-7.193492889404297", "False"]]], "filtered_resps": [["-2.943493127822876", "False"], ["-5.443492889404297", "False"], ["-4.693492889404297", "False"], ["-3.193493127822876", "False"], ["-7.193492889404297", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "8f861429164eca4492d9b6934e8886817e6a7795be343be8312245558a2889dc", "prompt_hash": "8f4c4237e9c9406ff6c6725256705f2a09ca4e15d159ee703600c7fea276a886", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1171, "doc": {"id": "42520bf3f93f8de23670044e019001a3", "question": "The low trickle of water revealed a stone, where was the stone found?", "question_concept": "stone", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ocean", "gallbladder", "driveway", "river bed", "creek bed"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: The low trickle of water revealed a stone, where was the stone found?\nA. ocean\nB. gallbladder\nC. driveway\nD. river bed\nE. creek bed\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The low trickle of water revealed a stone, where was the stone found?\nA. ocean\nB. gallbladder\nC. driveway\nD. river bed\nE. creek bed\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The low trickle of water revealed a stone, where was the stone found?\nA. ocean\nB. gallbladder\nC. driveway\nD. river bed\nE. creek bed\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The low trickle of water revealed a stone, where was the stone found?\nA. ocean\nB. gallbladder\nC. driveway\nD. river bed\nE. creek bed\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The low trickle of water revealed a stone, where was the stone found?\nA. ocean\nB. gallbladder\nC. driveway\nD. river bed\nE. creek bed\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.217130422592163", "False"]], [["-5.217130661010742", "False"]], [["-2.967130422592163", "False"]], [["-2.967130422592163", "False"]], [["-1.967130422592163", "False"]]], "filtered_resps": [["-3.217130422592163", "False"], ["-5.217130661010742", "False"], ["-2.967130422592163", "False"], ["-2.967130422592163", "False"], ["-1.967130422592163", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "885ea0e514bb838dda9fb99dfa5adc4ac56b5bd0dfdd0af6907d9fe6b7c92fcf", "prompt_hash": "a4b9c8edb09fa48ddda3ee21fda05ba58ac0a848f1b65d607c93755eda5fbe65", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1172, "doc": {"id": "5e260e1d96187716888cbd968010bb65", "question": "Where is the closest place from where you could borrow salt?", "question_concept": "salt", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ocean water", "table", "shaker", "neighbor's house", "lake"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is the closest place from where you could borrow salt?\nA. ocean water\nB. table\nC. shaker\nD. neighbor's house\nE. lake\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is the closest place from where you could borrow salt?\nA. ocean water\nB. table\nC. shaker\nD. neighbor's house\nE. lake\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is the closest place from where you could borrow salt?\nA. ocean water\nB. table\nC. shaker\nD. neighbor's house\nE. lake\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is the closest place from where you could borrow salt?\nA. ocean water\nB. table\nC. shaker\nD. neighbor's house\nE. lake\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is the closest place from where you could borrow salt?\nA. ocean water\nB. table\nC. shaker\nD. neighbor's house\nE. lake\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.228161334991455", "False"]], [["-2.978161334991455", "False"]], [["-1.4781614542007446", "True"]], [["-3.728161334991455", "False"]], [["-7.978161334991455", "False"]]], "filtered_resps": [["-4.228161334991455", "False"], ["-2.978161334991455", "False"], ["-1.4781614542007446", "True"], ["-3.728161334991455", "False"], ["-7.978161334991455", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9afacd53478c066ab4e40942fc7f2e5d06ebc7782004680c17a4b05304461d58", "prompt_hash": "8cee94b714863756ca793a5da33c1b31b856d153d01cc5b29f1b67ebecc5078f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1173, "doc": {"id": "ed50555f8db2b8f66caf9868dcd7e13b", "question": "No matter what date you put on it, we all know the universe to be what?", "question_concept": "universe", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["very old", "infiniverse", "getting younger", "infinite", "real"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: No matter what date you put on it, we all know the universe to be what?\nA. very old\nB. infiniverse\nC. getting younger\nD. infinite\nE. real\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: No matter what date you put on it, we all know the universe to be what?\nA. very old\nB. infiniverse\nC. getting younger\nD. infinite\nE. real\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: No matter what date you put on it, we all know the universe to be what?\nA. very old\nB. infiniverse\nC. getting younger\nD. infinite\nE. real\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: No matter what date you put on it, we all know the universe to be what?\nA. very old\nB. infiniverse\nC. getting younger\nD. infinite\nE. real\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: No matter what date you put on it, we all know the universe to be what?\nA. very old\nB. infiniverse\nC. getting younger\nD. infinite\nE. real\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.98271644115448", "True"]], [["-5.2327165603637695", "False"]], [["-7.2327165603637695", "False"]], [["-6.4827165603637695", "False"]], [["-5.4827165603637695", "False"]]], "filtered_resps": [["-0.98271644115448", "True"], ["-5.2327165603637695", "False"], ["-7.2327165603637695", "False"], ["-6.4827165603637695", "False"], ["-5.4827165603637695", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d97a6aaea834edd5c8000fb674f4badeeeec1bb56e5d8f4648759a686813cc15", "prompt_hash": "7e6fc503e880304585f01116b74326dd44dd9fbf0fa0404e2ab35085680eb7f3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1174, "doc": {"id": "a8c284637dabc87745a7eb05d4f7fcbc", "question": "A meteor travels through galaxies which are a part of what?", "question_concept": "meteor", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["republic of ireland", "sky", "orbit", "universe", "school"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: A meteor travels through galaxies which are a part of what?\nA. republic of ireland\nB. sky\nC. orbit\nD. universe\nE. school\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A meteor travels through galaxies which are a part of what?\nA. republic of ireland\nB. sky\nC. orbit\nD. universe\nE. school\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A meteor travels through galaxies which are a part of what?\nA. republic of ireland\nB. sky\nC. orbit\nD. universe\nE. school\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A meteor travels through galaxies which are a part of what?\nA. republic of ireland\nB. sky\nC. orbit\nD. universe\nE. school\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A meteor travels through galaxies which are a part of what?\nA. republic of ireland\nB. sky\nC. orbit\nD. universe\nE. school\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.946868896484375", "False"]], [["-5.696868896484375", "False"]], [["-6.196868896484375", "False"]], [["-1.696868896484375", "True"]], [["-10.696868896484375", "False"]]], "filtered_resps": [["-1.946868896484375", "False"], ["-5.696868896484375", "False"], ["-6.196868896484375", "False"], ["-1.696868896484375", "True"], ["-10.696868896484375", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6550198073a761567dcaf5132551f5e903f6ecc38a11dc3f316741b4377994a1", "prompt_hash": "3081a02a084b9ef59306f5a0198f210d50fcf4b11651d09fd183c98b49dad767", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1175, "doc": {"id": "5758a0fb686071e95d95b1cfad5299a0", "question": "What is a person considered a bully known for?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ridiculous", "false information", "made fun of", "brain tumor", "bull rider"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is a person considered a bully known for?\nA. ridiculous\nB. false information\nC. made fun of\nD. brain tumor\nE. bull rider\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a person considered a bully known for?\nA. ridiculous\nB. false information\nC. made fun of\nD. brain tumor\nE. bull rider\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a person considered a bully known for?\nA. ridiculous\nB. false information\nC. made fun of\nD. brain tumor\nE. bull rider\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a person considered a bully known for?\nA. ridiculous\nB. false information\nC. made fun of\nD. brain tumor\nE. bull rider\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a person considered a bully known for?\nA. ridiculous\nB. false information\nC. made fun of\nD. brain tumor\nE. bull rider\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1107418537139893", "True"]], [["-3.3607418537139893", "False"]], [["-2.1107418537139893", "False"]], [["-5.86074161529541", "False"]], [["-7.11074161529541", "False"]]], "filtered_resps": [["-1.1107418537139893", "True"], ["-3.3607418537139893", "False"], ["-2.1107418537139893", "False"], ["-5.86074161529541", "False"], ["-7.11074161529541", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "de0946a97e849e96f03d3cac181676669250309474e928c011de716244db8029", "prompt_hash": "9270a66c41126a322bab4f0aebbe22c805dec4da38597ba03b8fc6daff6eb043", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1176, "doc": {"id": "d986f17acb3ed19c77e3ca3f98c026b9", "question": "She had an interest in the man, what did she want to do with him?", "question_concept": "interest", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["see particular program", "see exhibits", "see people play game", "have conversation", "watch film"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: She had an interest in the man, what did she want to do with him?\nA. see particular program\nB. see exhibits\nC. see people play game\nD. have conversation\nE. watch film\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: She had an interest in the man, what did she want to do with him?\nA. see particular program\nB. see exhibits\nC. see people play game\nD. have conversation\nE. watch film\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: She had an interest in the man, what did she want to do with him?\nA. see particular program\nB. see exhibits\nC. see people play game\nD. have conversation\nE. watch film\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: She had an interest in the man, what did she want to do with him?\nA. see particular program\nB. see exhibits\nC. see people play game\nD. have conversation\nE. watch film\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: She had an interest in the man, what did she want to do with him?\nA. see particular program\nB. see exhibits\nC. see people play game\nD. have conversation\nE. watch film\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.2557268142700195", "False"]], [["-6.5057268142700195", "False"]], [["-8.50572681427002", "False"]], [["-1.5057268142700195", "False"]], [["-9.25572681427002", "False"]]], "filtered_resps": [["-5.2557268142700195", "False"], ["-6.5057268142700195", "False"], ["-8.50572681427002", "False"], ["-1.5057268142700195", "False"], ["-9.25572681427002", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f384fff2cd51fc05659d64c1d54b66236200ffb534fa274c534285ea10f7f3f7", "prompt_hash": "3542e9045e471a0a1e658ecd6438ec03b49e97fe7428e1a30310c29ddf922a4f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1177, "doc": {"id": "4a4f6408fae400ce0beb5bea0f9913e9", "question": "Where is a drug kept in a home bathroom?", "question_concept": "drug", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["nursery", "ghetto", "cupboard", "pharmacy", "medicine cabinet"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where is a drug kept in a home bathroom?\nA. nursery\nB. ghetto\nC. cupboard\nD. pharmacy\nE. medicine cabinet\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is a drug kept in a home bathroom?\nA. nursery\nB. ghetto\nC. cupboard\nD. pharmacy\nE. medicine cabinet\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is a drug kept in a home bathroom?\nA. nursery\nB. ghetto\nC. cupboard\nD. pharmacy\nE. medicine cabinet\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is a drug kept in a home bathroom?\nA. nursery\nB. ghetto\nC. cupboard\nD. pharmacy\nE. medicine cabinet\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is a drug kept in a home bathroom?\nA. nursery\nB. ghetto\nC. cupboard\nD. pharmacy\nE. medicine cabinet\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.823269844055176", "False"]], [["-7.573269844055176", "False"]], [["-4.573269844055176", "False"]], [["-6.323269844055176", "False"]], [["-1.3232697248458862", "True"]]], "filtered_resps": [["-2.823269844055176", "False"], ["-7.573269844055176", "False"], ["-4.573269844055176", "False"], ["-6.323269844055176", "False"], ["-1.3232697248458862", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d865ec80b8a04bcd62ddab7f26d623485ff88a4b689409da52d1954bda234087", "prompt_hash": "dce7b9e546f06d49ec84b33ec9fd194f961b7cdecb27eeb8c50029d31e14f314", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1178, "doc": {"id": "8c655f3a55bde41aad880f138d7a445d", "question": "When cooking sheep meat a lot of people might want to be well?", "question_concept": "sheep", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["ram", "lamb", "done", "ram", "wolf"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: When cooking sheep meat a lot of people might want to be well?\nA. ram\nB. lamb\nC. done\nD. ram\nE. wolf\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: When cooking sheep meat a lot of people might want to be well?\nA. ram\nB. lamb\nC. done\nD. ram\nE. wolf\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: When cooking sheep meat a lot of people might want to be well?\nA. ram\nB. lamb\nC. done\nD. ram\nE. wolf\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: When cooking sheep meat a lot of people might want to be well?\nA. ram\nB. lamb\nC. done\nD. ram\nE. wolf\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: When cooking sheep meat a lot of people might want to be well?\nA. ram\nB. lamb\nC. done\nD. ram\nE. wolf\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.432617664337158", "False"]], [["-1.6826175451278687", "False"]], [["-4.682617664337158", "False"]], [["-7.682617664337158", "False"]], [["-9.1826171875", "False"]]], "filtered_resps": [["-5.432617664337158", "False"], ["-1.6826175451278687", "False"], ["-4.682617664337158", "False"], ["-7.682617664337158", "False"], ["-9.1826171875", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "63738526305d9ca877f555630e022463eb0240780e04218e40478bb9ae513785", "prompt_hash": "221ed4724a6d6984c0e8aedce591e51ea3e6dc1435b5849d73bfea1487eb5bd7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1179, "doc": {"id": "56417ee33b44f0d916bedfb6fd99b0ec", "question": "Where would you sit in a chair while working toward an advanced degree?", "question_concept": "chair", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["aeroport", "church", "furniture store", "university", "living room"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you sit in a chair while working toward an advanced degree?\nA. aeroport\nB. church\nC. furniture store\nD. university\nE. living room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you sit in a chair while working toward an advanced degree?\nA. aeroport\nB. church\nC. furniture store\nD. university\nE. living room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you sit in a chair while working toward an advanced degree?\nA. aeroport\nB. church\nC. furniture store\nD. university\nE. living room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you sit in a chair while working toward an advanced degree?\nA. aeroport\nB. church\nC. furniture store\nD. university\nE. living room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you sit in a chair while working toward an advanced degree?\nA. aeroport\nB. church\nC. furniture store\nD. university\nE. living room\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.832465171813965", "False"]], [["-6.332465171813965", "False"]], [["-7.332465171813965", "False"]], [["-0.8324650526046753", "True"]], [["-7.832465171813965", "False"]]], "filtered_resps": [["-3.832465171813965", "False"], ["-6.332465171813965", "False"], ["-7.332465171813965", "False"], ["-0.8324650526046753", "True"], ["-7.832465171813965", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e8f0fa128761072eb02618f11079ca1262e8b51a36168f1966c2ba6517979cea", "prompt_hash": "c1ea0b81694d05974a3ca6b7fc76bc4712b9d32aaeb485d2f75dcec9a156ceca", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1180, "doc": {"id": "43fb083962f825ae651d88648bbd2f74", "question": "Farm land makes use of what?", "question_concept": "farmland", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["countryside", "michigan", "north dakota", "farming areas", "illinois"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Farm land makes use of what?\nA. countryside\nB. michigan\nC. north dakota\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Farm land makes use of what?\nA. countryside\nB. michigan\nC. north dakota\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Farm land makes use of what?\nA. countryside\nB. michigan\nC. north dakota\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Farm land makes use of what?\nA. countryside\nB. michigan\nC. north dakota\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Farm land makes use of what?\nA. countryside\nB. michigan\nC. north dakota\nD. farming areas\nE. illinois\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.1264166831970215", "False"]], [["-5.8764166831970215", "False"]], [["-7.6264166831970215", "False"]], [["-2.1264166831970215", "False"]], [["-9.126416206359863", "False"]]], "filtered_resps": [["-2.1264166831970215", "False"], ["-5.8764166831970215", "False"], ["-7.6264166831970215", "False"], ["-2.1264166831970215", "False"], ["-9.126416206359863", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "babcd24da137c87a92cc100f72f3b9998410a7e6ae3b98b7e2f6108ed962a5b8", "prompt_hash": "cf4259c88e28556d40de2568b088748c1e2462576bc81549eac54513166792d0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1181, "doc": {"id": "aed771629c8dbd0c2587891e98030607", "question": "A good interview after applying for a job may cause you to feel what?", "question_concept": "applying for job", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["offer", "income", "rejection", "hostile", "hope"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: A good interview after applying for a job may cause you to feel what?\nA. offer\nB. income\nC. rejection\nD. hostile\nE. hope\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A good interview after applying for a job may cause you to feel what?\nA. offer\nB. income\nC. rejection\nD. hostile\nE. hope\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A good interview after applying for a job may cause you to feel what?\nA. offer\nB. income\nC. rejection\nD. hostile\nE. hope\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A good interview after applying for a job may cause you to feel what?\nA. offer\nB. income\nC. rejection\nD. hostile\nE. hope\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A good interview after applying for a job may cause you to feel what?\nA. offer\nB. income\nC. rejection\nD. hostile\nE. hope\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.686112642288208", "True"]], [["-4.936112403869629", "False"]], [["-5.686112403869629", "False"]], [["-6.186112403869629", "False"]], [["-2.186112642288208", "False"]]], "filtered_resps": [["-0.686112642288208", "True"], ["-4.936112403869629", "False"], ["-5.686112403869629", "False"], ["-6.186112403869629", "False"], ["-2.186112642288208", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "fe3a3142a2d932e3dacbc3b527aef55ce16427e84f4d045ae071d2f1f0a0c2e7", "prompt_hash": "99bf3a840a2a0695d45bb7132d0e3664c6ac3b520f94714d50a5c6e1dbfa4967", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1182, "doc": {"id": "d0a42c8180b4e080aa071dd70fce7e03", "question": "Computers have allowed everybody to answer questions they have quickly, but still we seem to be getting duller despite access to this what?", "question_concept": "computers", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["economic boom", "advance knowledge", "produce sound", "teach", "follow instructions"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Computers have allowed everybody to answer questions they have quickly, but still we seem to be getting duller despite access to this what?\nA. economic boom\nB. advance knowledge\nC. produce sound\nD. teach\nE. follow instructions\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Computers have allowed everybody to answer questions they have quickly, but still we seem to be getting duller despite access to this what?\nA. economic boom\nB. advance knowledge\nC. produce sound\nD. teach\nE. follow instructions\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Computers have allowed everybody to answer questions they have quickly, but still we seem to be getting duller despite access to this what?\nA. economic boom\nB. advance knowledge\nC. produce sound\nD. teach\nE. follow instructions\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Computers have allowed everybody to answer questions they have quickly, but still we seem to be getting duller despite access to this what?\nA. economic boom\nB. advance knowledge\nC. produce sound\nD. teach\nE. follow instructions\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Computers have allowed everybody to answer questions they have quickly, but still we seem to be getting duller despite access to this what?\nA. economic boom\nB. advance knowledge\nC. produce sound\nD. teach\nE. follow instructions\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.938927173614502", "False"]], [["-1.438927173614502", "True"]], [["-5.188927173614502", "False"]], [["-3.688927173614502", "False"]], [["-5.938927173614502", "False"]]], "filtered_resps": [["-4.938927173614502", "False"], ["-1.438927173614502", "True"], ["-5.188927173614502", "False"], ["-3.688927173614502", "False"], ["-5.938927173614502", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "74fb496c4cda55184fe1569b953695af47b7374fb593e8d1b81d3354431dbab0", "prompt_hash": "087dbfcc54cac502c6e3e36c793a1b46873c934761ca2203cbb6d5aa1ed5d5ac", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1183, "doc": {"id": "533599262a5dae7c7137cfe69e0e24fb", "question": "There was a long cottage somewhere.  People thought it was haunted.  It was overgrown, there was nothing near it.  It's was far into the what?", "question_concept": "cottage", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["mountains", "countryside", "train", "painting", "village"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: There was a long cottage somewhere.  People thought it was haunted.  It was overgrown, there was nothing near it.  It's was far into the what?\nA. mountains\nB. countryside\nC. train\nD. painting\nE. village\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: There was a long cottage somewhere.  People thought it was haunted.  It was overgrown, there was nothing near it.  It's was far into the what?\nA. mountains\nB. countryside\nC. train\nD. painting\nE. village\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: There was a long cottage somewhere.  People thought it was haunted.  It was overgrown, there was nothing near it.  It's was far into the what?\nA. mountains\nB. countryside\nC. train\nD. painting\nE. village\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: There was a long cottage somewhere.  People thought it was haunted.  It was overgrown, there was nothing near it.  It's was far into the what?\nA. mountains\nB. countryside\nC. train\nD. painting\nE. village\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: There was a long cottage somewhere.  People thought it was haunted.  It was overgrown, there was nothing near it.  It's was far into the what?\nA. mountains\nB. countryside\nC. train\nD. painting\nE. village\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.61565637588501", "False"]], [["-1.6156564950942993", "False"]], [["-7.86565637588501", "False"]], [["-8.115656852722168", "False"]], [["-2.3656563758850098", "False"]]], "filtered_resps": [["-4.61565637588501", "False"], ["-1.6156564950942993", "False"], ["-7.86565637588501", "False"], ["-8.115656852722168", "False"], ["-2.3656563758850098", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "155955af5bf05826946f16a3b0b8563e756b4148ece4d8d3274c073879e853a0", "prompt_hash": "fede3b33307a1696fbab8657fec675feac07b90a8a6c40d726d4db0b9593d36a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1184, "doc": {"id": "edd1634d911614590c6b8ca730df95fe", "question": "Where is knight always portrayed as a hero?", "question_concept": "knight", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["middle ages", "chess board", "kids story", "fairy tale", "castle"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Where is knight always portrayed as a hero?\nA. middle ages\nB. chess board\nC. kids story\nD. fairy tale\nE. castle\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where is knight always portrayed as a hero?\nA. middle ages\nB. chess board\nC. kids story\nD. fairy tale\nE. castle\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where is knight always portrayed as a hero?\nA. middle ages\nB. chess board\nC. kids story\nD. fairy tale\nE. castle\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where is knight always portrayed as a hero?\nA. middle ages\nB. chess board\nC. kids story\nD. fairy tale\nE. castle\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where is knight always portrayed as a hero?\nA. middle ages\nB. chess board\nC. kids story\nD. fairy tale\nE. castle\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.880555510520935", "True"]], [["-2.3805556297302246", "False"]], [["-6.130555629730225", "False"]], [["-3.8805556297302246", "False"]], [["-9.380555152893066", "False"]]], "filtered_resps": [["-1.880555510520935", "True"], ["-2.3805556297302246", "False"], ["-6.130555629730225", "False"], ["-3.8805556297302246", "False"], ["-9.380555152893066", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "22a02934bd01a4c55841e7e8510caed0d77d7ee94587618186d9d5f216f52def", "prompt_hash": "19aba0edffbba7cba5b897ced46ec19c946b999b30e20c3cacbdd40e5152ed61", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1185, "doc": {"id": "9a544e9f4847c41a15fdf47ae7b98d8a", "question": "James is carrying a duffel bag with him because he doesn't have a vehicle of his own and needs a bag to carry his things in while he uses what?", "question_concept": "duffel bag", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["library", "transit", "bus station", "army barracks", "locker room"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: James is carrying a duffel bag with him because he doesn't have a vehicle of his own and needs a bag to carry his things in while he uses what?\nA. library\nB. transit\nC. bus station\nD. army barracks\nE. locker room\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James is carrying a duffel bag with him because he doesn't have a vehicle of his own and needs a bag to carry his things in while he uses what?\nA. library\nB. transit\nC. bus station\nD. army barracks\nE. locker room\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James is carrying a duffel bag with him because he doesn't have a vehicle of his own and needs a bag to carry his things in while he uses what?\nA. library\nB. transit\nC. bus station\nD. army barracks\nE. locker room\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James is carrying a duffel bag with him because he doesn't have a vehicle of his own and needs a bag to carry his things in while he uses what?\nA. library\nB. transit\nC. bus station\nD. army barracks\nE. locker room\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James is carrying a duffel bag with him because he doesn't have a vehicle of his own and needs a bag to carry his things in while he uses what?\nA. library\nB. transit\nC. bus station\nD. army barracks\nE. locker room\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.187053680419922", "False"]], [["-1.4370534420013428", "True"]], [["-6.187053680419922", "False"]], [["-5.187053680419922", "False"]], [["-8.937053680419922", "False"]]], "filtered_resps": [["-5.187053680419922", "False"], ["-1.4370534420013428", "True"], ["-6.187053680419922", "False"], ["-5.187053680419922", "False"], ["-8.937053680419922", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9b6b41c6ba8ddda5c08ac8445fc4b3dfef6ccebacdd1e2e83a19fac4d67d25c8", "prompt_hash": "ef5434ce443fc488a3e8b8f81e29822dcc91cc06bc998df4727d319a01b8d020", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1186, "doc": {"id": "26bd85f05d29863ed777a4f1a4b8fa63", "question": "What would you need if you want to smoke?", "question_concept": "smoke", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["you're stupid", "kill yourself", "roll joint", "cigarette", "lighter fluid."]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: What would you need if you want to smoke?\nA. you're stupid\nB. kill yourself\nC. roll joint\nD. cigarette\nE. lighter fluid.\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you need if you want to smoke?\nA. you're stupid\nB. kill yourself\nC. roll joint\nD. cigarette\nE. lighter fluid.\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you need if you want to smoke?\nA. you're stupid\nB. kill yourself\nC. roll joint\nD. cigarette\nE. lighter fluid.\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you need if you want to smoke?\nA. you're stupid\nB. kill yourself\nC. roll joint\nD. cigarette\nE. lighter fluid.\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you need if you want to smoke?\nA. you're stupid\nB. kill yourself\nC. roll joint\nD. cigarette\nE. lighter fluid.\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.545358180999756", "False"]], [["-6.295358180999756", "False"]], [["-1.7953583002090454", "False"]], [["-1.5453583002090454", "True"]], [["-6.295358180999756", "False"]]], "filtered_resps": [["-4.545358180999756", "False"], ["-6.295358180999756", "False"], ["-1.7953583002090454", "False"], ["-1.5453583002090454", "True"], ["-6.295358180999756", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "c4d09c863eab5cf2c94ea007cb379f5d6ea1012c58866800db1e58b6dba2ee75", "prompt_hash": "1c55b1cf3f15f17e386b284ec41a66562aef406f85899c06c30941d5c9baaafd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1187, "doc": {"id": "3884d82524f2337ce53ce64776293cf7", "question": "James decided that competing was the right choice.   Not competing has a defined outcome, but if he competes then what could happen?", "question_concept": "competing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["might win", "perform better", "enemies", "winners and losers", "lose"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: James decided that competing was the right choice.   Not competing has a defined outcome, but if he competes then what could happen?\nA. might win\nB. perform better\nC. enemies\nD. winners and losers\nE. lose\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: James decided that competing was the right choice.   Not competing has a defined outcome, but if he competes then what could happen?\nA. might win\nB. perform better\nC. enemies\nD. winners and losers\nE. lose\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: James decided that competing was the right choice.   Not competing has a defined outcome, but if he competes then what could happen?\nA. might win\nB. perform better\nC. enemies\nD. winners and losers\nE. lose\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: James decided that competing was the right choice.   Not competing has a defined outcome, but if he competes then what could happen?\nA. might win\nB. perform better\nC. enemies\nD. winners and losers\nE. lose\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: James decided that competing was the right choice.   Not competing has a defined outcome, but if he competes then what could happen?\nA. might win\nB. perform better\nC. enemies\nD. winners and losers\nE. lose\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9059934616088867", "False"]], [["-5.155993461608887", "False"]], [["-5.405993461608887", "False"]], [["-1.9059935808181763", "False"]], [["-5.905993461608887", "False"]]], "filtered_resps": [["-3.9059934616088867", "False"], ["-5.155993461608887", "False"], ["-5.405993461608887", "False"], ["-1.9059935808181763", "False"], ["-5.905993461608887", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "32efaa5e6d06fb342e11410a2b116ed22dc00d5532d7f5f7792e6cdd18f4cec2", "prompt_hash": "44cf24c0327dd78f086e7119f738c6fbe788485757fe79c74d100ffbd740dfcb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 1188, "doc": {"id": "acb3147d946db3b06a596d48e0be56cf", "question": "What could you use to get to some airplanes?", "question_concept": "airplanes", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["taxi", "carry people", "car", "stall", "crash"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What could you use to get to some airplanes?\nA. taxi\nB. carry people\nC. car\nD. stall\nE. crash\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What could you use to get to some airplanes?\nA. taxi\nB. carry people\nC. car\nD. stall\nE. crash\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What could you use to get to some airplanes?\nA. taxi\nB. carry people\nC. car\nD. stall\nE. crash\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What could you use to get to some airplanes?\nA. taxi\nB. carry people\nC. car\nD. stall\nE. crash\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What could you use to get to some airplanes?\nA. taxi\nB. carry people\nC. car\nD. stall\nE. crash\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7923007607460022", "True"]], [["-5.792300701141357", "False"]], [["-5.042300701141357", "False"]], [["-5.292300701141357", "False"]], [["-6.792300701141357", "False"]]], "filtered_resps": [["-0.7923007607460022", "True"], ["-5.792300701141357", "False"], ["-5.042300701141357", "False"], ["-5.292300701141357", "False"], ["-6.792300701141357", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9d25a3224f1c18887c339285736a7953fe975bbd34cea76826b6f1419c7554f0", "prompt_hash": "504dada7ba5c3797668198c5ef21c2842f69321a78363dd2fd64631718819115", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1189, "doc": {"id": "52ab95f9216f1994e37cc08f7f258f13", "question": "What happens when driving?", "question_concept": "driving", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["lack of fuel", "paint scratching", "wheels turning", "tire wear", "traffic accident"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What happens when driving?\nA. lack of fuel\nB. paint scratching\nC. wheels turning\nD. tire wear\nE. traffic accident\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What happens when driving?\nA. lack of fuel\nB. paint scratching\nC. wheels turning\nD. tire wear\nE. traffic accident\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What happens when driving?\nA. lack of fuel\nB. paint scratching\nC. wheels turning\nD. tire wear\nE. traffic accident\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What happens when driving?\nA. lack of fuel\nB. paint scratching\nC. wheels turning\nD. tire wear\nE. traffic accident\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What happens when driving?\nA. lack of fuel\nB. paint scratching\nC. wheels turning\nD. tire wear\nE. traffic accident\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.858262538909912", "False"]], [["-5.858262538909912", "False"]], [["-1.3582624197006226", "True"]], [["-3.108262538909912", "False"]], [["-3.108262538909912", "False"]]], "filtered_resps": [["-4.858262538909912", "False"], ["-5.858262538909912", "False"], ["-1.3582624197006226", "True"], ["-3.108262538909912", "False"], ["-3.108262538909912", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "10dde5f158e81292b653924b565b49e88a136de14a264ee6a763e3462eca0a61", "prompt_hash": "5a0a92bef3aeaa9ee20e6a9c62484736fb23ee582f8f7e72ee4cbe4a4ad99325", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1190, "doc": {"id": "f60641f550d5ee44ac1bedcaf6ad6357", "question": "What are our bodies doing after having food?", "question_concept": "having food", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["falling down", "digesting", "gas", "weight gain", "not hungry"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: What are our bodies doing after having food?\nA. falling down\nB. digesting\nC. gas\nD. weight gain\nE. not hungry\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are our bodies doing after having food?\nA. falling down\nB. digesting\nC. gas\nD. weight gain\nE. not hungry\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are our bodies doing after having food?\nA. falling down\nB. digesting\nC. gas\nD. weight gain\nE. not hungry\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are our bodies doing after having food?\nA. falling down\nB. digesting\nC. gas\nD. weight gain\nE. not hungry\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are our bodies doing after having food?\nA. falling down\nB. digesting\nC. gas\nD. weight gain\nE. not hungry\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.936864852905273", "False"]], [["-1.1868650913238525", "True"]], [["-8.186864852905273", "False"]], [["-8.186864852905273", "False"]], [["-8.686864852905273", "False"]]], "filtered_resps": [["-6.936864852905273", "False"], ["-1.1868650913238525", "True"], ["-8.186864852905273", "False"], ["-8.186864852905273", "False"], ["-8.686864852905273", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "f00250cba570773c84cd19fb4ca8a278cc3a0a60ef35bf95a7e6346ad2591590", "prompt_hash": "038c879f49411014f3da068c6264611a09f12993458492b9d2d288524d3b1420", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1191, "doc": {"id": "d9835ede7a0ed79325de13ca95b85b78", "question": "Why would one try to avoid work?", "question_concept": "going to work", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["making money", "leave home", "success", "malaise", "bad mood"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: Why would one try to avoid work?\nA. making money\nB. leave home\nC. success\nD. malaise\nE. bad mood\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would one try to avoid work?\nA. making money\nB. leave home\nC. success\nD. malaise\nE. bad mood\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would one try to avoid work?\nA. making money\nB. leave home\nC. success\nD. malaise\nE. bad mood\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would one try to avoid work?\nA. making money\nB. leave home\nC. success\nD. malaise\nE. bad mood\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would one try to avoid work?\nA. making money\nB. leave home\nC. success\nD. malaise\nE. bad mood\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.9432621002197266", "False"]], [["-6.443262100219727", "False"]], [["-7.693262100219727", "False"]], [["-1.4432622194290161", "False"]], [["-4.443262100219727", "False"]]], "filtered_resps": [["-3.9432621002197266", "False"], ["-6.443262100219727", "False"], ["-7.693262100219727", "False"], ["-1.4432622194290161", "False"], ["-4.443262100219727", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7d619c60c6f3c65911306f64663976234390a97bf5846e5c0c2aecbade14cd34", "prompt_hash": "4b861b84bf72f957b2620636d5061ca4c4169dc5e70076ab6d06fbfec74dd8aa", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1192, "doc": {"id": "2987db72e66f5fa0015ac64f9b3614ec", "question": "What do you do in order to fly in airplane?", "question_concept": "fly in airplane", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["buy tickets", "passenger", "read", "add gas", "run through checklists"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What do you do in order to fly in airplane?\nA. buy tickets\nB. passenger\nC. read\nD. add gas\nE. run through checklists\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What do you do in order to fly in airplane?\nA. buy tickets\nB. passenger\nC. read\nD. add gas\nE. run through checklists\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What do you do in order to fly in airplane?\nA. buy tickets\nB. passenger\nC. read\nD. add gas\nE. run through checklists\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What do you do in order to fly in airplane?\nA. buy tickets\nB. passenger\nC. read\nD. add gas\nE. run through checklists\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What do you do in order to fly in airplane?\nA. buy tickets\nB. passenger\nC. read\nD. add gas\nE. run through checklists\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.24994158744812", "False"]], [["-2.99994158744812", "False"]], [["-7.749941825866699", "False"]], [["-8.2499418258667", "False"]], [["-1.4999415874481201", "True"]]], "filtered_resps": [["-3.24994158744812", "False"], ["-2.99994158744812", "False"], ["-7.749941825866699", "False"], ["-8.2499418258667", "False"], ["-1.4999415874481201", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7785ba1aac8b93d975f5707b467bc7f91856841a6d1383402700407207ccfb30", "prompt_hash": "d0c5470d4f19f18070660d412771c127144a03e5a1d8a437668b3d4d2ba43887", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 0.0}
{"doc_id": 1193, "doc": {"id": "8b548832703a8c68a788e2f9c0e222ae", "question": "What is another name for the color of the fur of a dog with light colored fur?", "question_concept": "small dog", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fair", "basket", "dog hair", "game", "sun"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What is another name for the color of the fur of a dog with light colored fur?\nA. fair\nB. basket\nC. dog hair\nD. game\nE. sun\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is another name for the color of the fur of a dog with light colored fur?\nA. fair\nB. basket\nC. dog hair\nD. game\nE. sun\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is another name for the color of the fur of a dog with light colored fur?\nA. fair\nB. basket\nC. dog hair\nD. game\nE. sun\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is another name for the color of the fur of a dog with light colored fur?\nA. fair\nB. basket\nC. dog hair\nD. game\nE. sun\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is another name for the color of the fur of a dog with light colored fur?\nA. fair\nB. basket\nC. dog hair\nD. game\nE. sun\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.9132081866264343", "True"]], [["-5.6632080078125", "False"]], [["-4.4132080078125", "False"]], [["-5.4132080078125", "False"]], [["-4.9132080078125", "False"]]], "filtered_resps": [["-0.9132081866264343", "True"], ["-5.6632080078125", "False"], ["-4.4132080078125", "False"], ["-5.4132080078125", "False"], ["-4.9132080078125", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0cd633683e561496834f62f3c1b851ec48c0d7ac5438391f313c50bbb2670d58", "prompt_hash": "6a507efe5457a48f2bf455178abc5de0b5600393b5f6b299b341be496481e175", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1194, "doc": {"id": "1ddd239a2a6438a891cb411b82e7f450", "question": "Sally was bored because she didn't like the junk that was on what?", "question_concept": "junk", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["drawer", "garage", "caddy", "bed", "television"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Sally was bored because she didn't like the junk that was on what?\nA. drawer\nB. garage\nC. caddy\nD. bed\nE. television\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Sally was bored because she didn't like the junk that was on what?\nA. drawer\nB. garage\nC. caddy\nD. bed\nE. television\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Sally was bored because she didn't like the junk that was on what?\nA. drawer\nB. garage\nC. caddy\nD. bed\nE. television\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Sally was bored because she didn't like the junk that was on what?\nA. drawer\nB. garage\nC. caddy\nD. bed\nE. television\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Sally was bored because she didn't like the junk that was on what?\nA. drawer\nB. garage\nC. caddy\nD. bed\nE. television\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.759704828262329", "False"]], [["-3.509704828262329", "False"]], [["-5.25970458984375", "False"]], [["-3.759704828262329", "False"]], [["-2.509704828262329", "False"]]], "filtered_resps": [["-1.759704828262329", "False"], ["-3.509704828262329", "False"], ["-5.25970458984375", "False"], ["-3.759704828262329", "False"], ["-2.509704828262329", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "721edae5b2a4d640d1119ab0d592162ba7b0ea9d69c42e96adcceb170a5830fd", "prompt_hash": "3ad39e92d6df05ff5c2564fec5cdd0f00e3705dabd93f0f3f6c1e978d4b522dd", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1195, "doc": {"id": "6544a50bf9563d52dbd2034e81df0bf3", "question": "The lion sensed his competitor was timid, so what attitude did the lion take?", "question_concept": "timid", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["reckless", "bellicose", "defensive", "aggressive", "dauntless"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: The lion sensed his competitor was timid, so what attitude did the lion take?\nA. reckless\nB. bellicose\nC. defensive\nD. aggressive\nE. dauntless\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The lion sensed his competitor was timid, so what attitude did the lion take?\nA. reckless\nB. bellicose\nC. defensive\nD. aggressive\nE. dauntless\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The lion sensed his competitor was timid, so what attitude did the lion take?\nA. reckless\nB. bellicose\nC. defensive\nD. aggressive\nE. dauntless\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The lion sensed his competitor was timid, so what attitude did the lion take?\nA. reckless\nB. bellicose\nC. defensive\nD. aggressive\nE. dauntless\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The lion sensed his competitor was timid, so what attitude did the lion take?\nA. reckless\nB. bellicose\nC. defensive\nD. aggressive\nE. dauntless\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.44830322265625", "False"]], [["-3.94830322265625", "False"]], [["-3.94830322265625", "False"]], [["-2.69830322265625", "False"]], [["-2.4917140007019043", "False"]]], "filtered_resps": [["-3.44830322265625", "False"], ["-3.94830322265625", "False"], ["-3.94830322265625", "False"], ["-2.69830322265625", "False"], ["-2.4917140007019043", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3035e3c90d96d5110e50f0546f4c78ec82bdb56f5af9a4d8eda5ddbb97983436", "prompt_hash": "8ab71d5554dc9ba67fc2ff631021582d863eec654292036bc5d0714abf724599", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 0.0}
{"doc_id": 1196, "doc": {"id": "5ff6ce8ad88459272ffe23d33db4970a", "question": "John felt a snake slither over him as he rested.  He was afraid to raise his covers for fear of startling it.  Where might he be?", "question_concept": "snake", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pet shops", "oklahoma", "basement", "bedroom", "dreams"]}, "answerKey": "D"}, "target": "D", "arguments": {"gen_args_0": {"arg_0": "Question: John felt a snake slither over him as he rested.  He was afraid to raise his covers for fear of startling it.  Where might he be?\nA. pet shops\nB. oklahoma\nC. basement\nD. bedroom\nE. dreams\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: John felt a snake slither over him as he rested.  He was afraid to raise his covers for fear of startling it.  Where might he be?\nA. pet shops\nB. oklahoma\nC. basement\nD. bedroom\nE. dreams\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: John felt a snake slither over him as he rested.  He was afraid to raise his covers for fear of startling it.  Where might he be?\nA. pet shops\nB. oklahoma\nC. basement\nD. bedroom\nE. dreams\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: John felt a snake slither over him as he rested.  He was afraid to raise his covers for fear of startling it.  Where might he be?\nA. pet shops\nB. oklahoma\nC. basement\nD. bedroom\nE. dreams\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: John felt a snake slither over him as he rested.  He was afraid to raise his covers for fear of startling it.  Where might he be?\nA. pet shops\nB. oklahoma\nC. basement\nD. bedroom\nE. dreams\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.51445198059082", "False"]], [["-4.76445198059082", "False"]], [["-3.014451742172241", "False"]], [["-2.264451742172241", "False"]], [["-5.01445198059082", "False"]]], "filtered_resps": [["-4.51445198059082", "False"], ["-4.76445198059082", "False"], ["-3.014451742172241", "False"], ["-2.264451742172241", "False"], ["-5.01445198059082", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "56b8c2a41a2c2ba4bb3e573ae03aaad5e5a08b1ec3f7b2913e039fe0ccb9241d", "prompt_hash": "ee9d0492fb59130057ee4cd0de9bc6f8b7cc01b6fe9fde422d10a12571ff9ce6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "acc": 1.0}
{"doc_id": 1197, "doc": {"id": "2ca05683157a3cd89d82016f13e560ec", "question": "Where can you find a place to eat in an urban area close to local nightlife?", "question_concept": "place to eat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["city", "downtown", "mall", "shopping center", "own house"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you find a place to eat in an urban area close to local nightlife?\nA. city\nB. downtown\nC. mall\nD. shopping center\nE. own house\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you find a place to eat in an urban area close to local nightlife?\nA. city\nB. downtown\nC. mall\nD. shopping center\nE. own house\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you find a place to eat in an urban area close to local nightlife?\nA. city\nB. downtown\nC. mall\nD. shopping center\nE. own house\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you find a place to eat in an urban area close to local nightlife?\nA. city\nB. downtown\nC. mall\nD. shopping center\nE. own house\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you find a place to eat in an urban area close to local nightlife?\nA. city\nB. downtown\nC. mall\nD. shopping center\nE. own house\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.908116579055786", "False"]], [["-0.9081165790557861", "True"]], [["-5.158116340637207", "False"]], [["-4.908116340637207", "False"]], [["-7.908116340637207", "False"]]], "filtered_resps": [["-3.908116579055786", "False"], ["-0.9081165790557861", "True"], ["-5.158116340637207", "False"], ["-4.908116340637207", "False"], ["-7.908116340637207", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4c4465e972a48c2cc4dfba10dabb7f8e79516cd9ecbab08f03f1d32dee543754", "prompt_hash": "6d8ad22ede8d31e4e39cac518d4d14e99b78dde70cce7a90b98824a1944f3ad6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1198, "doc": {"id": "1a8fbab20bbdf0bbf3961894662d5f7c", "question": "You have to a lot of thinking while studying a new subject, but it is how you gain what?", "question_concept": "thinking", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fatigue", "depression", "best way", "weight", "knowledge"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: You have to a lot of thinking while studying a new subject, but it is how you gain what?\nA. fatigue\nB. depression\nC. best way\nD. weight\nE. knowledge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: You have to a lot of thinking while studying a new subject, but it is how you gain what?\nA. fatigue\nB. depression\nC. best way\nD. weight\nE. knowledge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: You have to a lot of thinking while studying a new subject, but it is how you gain what?\nA. fatigue\nB. depression\nC. best way\nD. weight\nE. knowledge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: You have to a lot of thinking while studying a new subject, but it is how you gain what?\nA. fatigue\nB. depression\nC. best way\nD. weight\nE. knowledge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: You have to a lot of thinking while studying a new subject, but it is how you gain what?\nA. fatigue\nB. depression\nC. best way\nD. weight\nE. knowledge\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.36897611618042", "False"]], [["-7.11897611618042", "False"]], [["-5.61897611618042", "False"]], [["-7.36897611618042", "False"]], [["-0.8689762949943542", "True"]]], "filtered_resps": [["-4.36897611618042", "False"], ["-7.11897611618042", "False"], ["-5.61897611618042", "False"], ["-7.36897611618042", "False"], ["-0.8689762949943542", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "ad52cbafcd2fb6008edfcb8306fa0ae594eb61deeb7b8fe927f86cfafa67b5fd", "prompt_hash": "8cd0bb5f4206595eb7c6b88a1518deea4ebe0ae43aff5964ed1286ca80c0ac34", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1199, "doc": {"id": "5b5d2a8b83282f61c68a870116042f64", "question": "How will you communicate if you are far away from who you want to communicate with?", "question_concept": "communicate", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["think", "talk with people", "talk to people", "speak out", "send email"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: How will you communicate if you are far away from who you want to communicate with?\nA. think\nB. talk with people\nC. talk to people\nD. speak out\nE. send email\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: How will you communicate if you are far away from who you want to communicate with?\nA. think\nB. talk with people\nC. talk to people\nD. speak out\nE. send email\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: How will you communicate if you are far away from who you want to communicate with?\nA. think\nB. talk with people\nC. talk to people\nD. speak out\nE. send email\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: How will you communicate if you are far away from who you want to communicate with?\nA. think\nB. talk with people\nC. talk to people\nD. speak out\nE. send email\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: How will you communicate if you are far away from who you want to communicate with?\nA. think\nB. talk with people\nC. talk to people\nD. speak out\nE. send email\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.733309745788574", "False"]], [["-5.733309745788574", "False"]], [["-3.733309745788574", "False"]], [["-6.733309745788574", "False"]], [["-1.9833098649978638", "False"]]], "filtered_resps": [["-5.733309745788574", "False"], ["-5.733309745788574", "False"], ["-3.733309745788574", "False"], ["-6.733309745788574", "False"], ["-1.9833098649978638", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "d5f522d144c95b8df75c7208c67189f748ec41d2465333dee41ffb7bb6849e1a", "prompt_hash": "78edc94da14e0b93d4bc6833943d274f22501048b49850492720bc2de3dc2a1b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1200, "doc": {"id": "cfa081b5ba90dae4d7ddb5b7ad9d369a", "question": "Why would you not trust your friends after chatting with friends?", "question_concept": "chatting with friends", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["fever", "smoke", "laughing", "coughing", "lie"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Why would you not trust your friends after chatting with friends?\nA. fever\nB. smoke\nC. laughing\nD. coughing\nE. lie\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Why would you not trust your friends after chatting with friends?\nA. fever\nB. smoke\nC. laughing\nD. coughing\nE. lie\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Why would you not trust your friends after chatting with friends?\nA. fever\nB. smoke\nC. laughing\nD. coughing\nE. lie\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Why would you not trust your friends after chatting with friends?\nA. fever\nB. smoke\nC. laughing\nD. coughing\nE. lie\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Why would you not trust your friends after chatting with friends?\nA. fever\nB. smoke\nC. laughing\nD. coughing\nE. lie\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.848244667053223", "False"]], [["-4.098244667053223", "False"]], [["-5.348244667053223", "False"]], [["-3.8482444286346436", "False"]], [["-0.8482444882392883", "True"]]], "filtered_resps": [["-4.848244667053223", "False"], ["-4.098244667053223", "False"], ["-5.348244667053223", "False"], ["-3.8482444286346436", "False"], ["-0.8482444882392883", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "37d5394e347d155ade61c69758ad7a881bcfe055472cd2172458af5b103323b0", "prompt_hash": "01adad1acc8b5fea15fa1adcd9431bd1b4dc94a4a3c4fefb6b563b10143cbeb6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1201, "doc": {"id": "009a7aabffe0583fc2df46656b29c326", "question": "He came from old money and had a fortune, but he made new money making shrewd trades where?", "question_concept": "fortune", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["eat cake", "cookie", "stock market", "real estate", "treasure chest"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: He came from old money and had a fortune, but he made new money making shrewd trades where?\nA. eat cake\nB. cookie\nC. stock market\nD. real estate\nE. treasure chest\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: He came from old money and had a fortune, but he made new money making shrewd trades where?\nA. eat cake\nB. cookie\nC. stock market\nD. real estate\nE. treasure chest\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: He came from old money and had a fortune, but he made new money making shrewd trades where?\nA. eat cake\nB. cookie\nC. stock market\nD. real estate\nE. treasure chest\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: He came from old money and had a fortune, but he made new money making shrewd trades where?\nA. eat cake\nB. cookie\nC. stock market\nD. real estate\nE. treasure chest\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: He came from old money and had a fortune, but he made new money making shrewd trades where?\nA. eat cake\nB. cookie\nC. stock market\nD. real estate\nE. treasure chest\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.3973002433776855", "False"]], [["-7.1473002433776855", "False"]], [["-1.397300124168396", "False"]], [["-6.3973002433776855", "False"]], [["-9.397299766540527", "False"]]], "filtered_resps": [["-5.3973002433776855", "False"], ["-7.1473002433776855", "False"], ["-1.397300124168396", "False"], ["-6.3973002433776855", "False"], ["-9.397299766540527", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "afa87118e25bb7e61a2b5d840429bf938b4f7e7bf1e085df38b0bbf0387c4e72", "prompt_hash": "e23f5d798ced1e54f5ac12a92ae3753a6430a6df3d54ed45808865af222981ca", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1202, "doc": {"id": "2521b3fe6bfd6aeb91f9107dc7c4fbee", "question": "Animals make up a large part of the?", "question_concept": "animal", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["carrying cargo", "favorite", "ecosystem", "nature", "ecology"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: Animals make up a large part of the?\nA. carrying cargo\nB. favorite\nC. ecosystem\nD. nature\nE. ecology\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Animals make up a large part of the?\nA. carrying cargo\nB. favorite\nC. ecosystem\nD. nature\nE. ecology\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Animals make up a large part of the?\nA. carrying cargo\nB. favorite\nC. ecosystem\nD. nature\nE. ecology\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Animals make up a large part of the?\nA. carrying cargo\nB. favorite\nC. ecosystem\nD. nature\nE. ecology\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Animals make up a large part of the?\nA. carrying cargo\nB. favorite\nC. ecosystem\nD. nature\nE. ecology\nAnswer:", "arg_1": " E"}}, "resps": [[["-6.120617866516113", "False"]], [["-6.870617866516113", "False"]], [["-1.6206181049346924", "False"]], [["-7.620617866516113", "False"]], [["-3.1206181049346924", "False"]]], "filtered_resps": [["-6.120617866516113", "False"], ["-6.870617866516113", "False"], ["-1.6206181049346924", "False"], ["-7.620617866516113", "False"], ["-3.1206181049346924", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "bc31635cfe53d8e22ad7153e233aed5b64b0991c4d3fd5c4c099583fce5c512e", "prompt_hash": "b883c61804ab55f6c2c9ba8dba6c01515a1237a3f40c5198fae8b5abb9380782", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1203, "doc": {"id": "3fe45ab3bd4a844ea290050fc0ece8c1_1", "question": "At a shop what can you buy to put your spare unused things?", "question_concept": "shop", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["basement", "cardboard box", "ocean floor", "high school", "container"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: At a shop what can you buy to put your spare unused things?\nA. basement\nB. cardboard box\nC. ocean floor\nD. high school\nE. container\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: At a shop what can you buy to put your spare unused things?\nA. basement\nB. cardboard box\nC. ocean floor\nD. high school\nE. container\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: At a shop what can you buy to put your spare unused things?\nA. basement\nB. cardboard box\nC. ocean floor\nD. high school\nE. container\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: At a shop what can you buy to put your spare unused things?\nA. basement\nB. cardboard box\nC. ocean floor\nD. high school\nE. container\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: At a shop what can you buy to put your spare unused things?\nA. basement\nB. cardboard box\nC. ocean floor\nD. high school\nE. container\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.181396245956421", "False"]], [["-2.181396245956421", "False"]], [["-6.931396484375", "False"]], [["-8.931396484375", "False"]], [["-1.431396245956421", "True"]]], "filtered_resps": [["-3.181396245956421", "False"], ["-2.181396245956421", "False"], ["-6.931396484375", "False"], ["-8.931396484375", "False"], ["-1.431396245956421", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "0c80b87dd9a9d8fab7efc967b8434d117eb81727be203680117d93bc3b732ca0", "prompt_hash": "5d9eac47afc2e4b9ba93cb910e667edb9ef46914fb5bbb0e1096a0b633a0f508", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1204, "doc": {"id": "a2e0f6b5651e5271fcff8d6f5c9adfee", "question": "A person with digestion issues eats a meat-filled breakfast, what does he feel?", "question_concept": "eating breakfast", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["heartburn", "overeating", "happiness", "being satisfied", "gain energy"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: A person with digestion issues eats a meat-filled breakfast, what does he feel?\nA. heartburn\nB. overeating\nC. happiness\nD. being satisfied\nE. gain energy\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: A person with digestion issues eats a meat-filled breakfast, what does he feel?\nA. heartburn\nB. overeating\nC. happiness\nD. being satisfied\nE. gain energy\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: A person with digestion issues eats a meat-filled breakfast, what does he feel?\nA. heartburn\nB. overeating\nC. happiness\nD. being satisfied\nE. gain energy\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: A person with digestion issues eats a meat-filled breakfast, what does he feel?\nA. heartburn\nB. overeating\nC. happiness\nD. being satisfied\nE. gain energy\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: A person with digestion issues eats a meat-filled breakfast, what does he feel?\nA. heartburn\nB. overeating\nC. happiness\nD. being satisfied\nE. gain energy\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6590700745582581", "True"]], [["-4.659070014953613", "False"]], [["-4.909070014953613", "False"]], [["-6.409070014953613", "False"]], [["-4.159070014953613", "False"]]], "filtered_resps": [["-0.6590700745582581", "True"], ["-4.659070014953613", "False"], ["-4.909070014953613", "False"], ["-6.409070014953613", "False"], ["-4.159070014953613", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "349643c8475875a6ffc6b95dfdf0af6bc2efd769435a960b28fd164c3a3567da", "prompt_hash": "f71dd1edef7ffe0781806e8aea5c96c7d6de557397f6c3875dbf2e59d6014fc3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1205, "doc": {"id": "d6900a01a9dd6627b4bb22b0f6d191a5", "question": "What is a prisoner sentenced to do?", "question_concept": "prisoner", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["scape jail", "dream of freedom", "become a hairdresser", "attempt to escape", "do time"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What is a prisoner sentenced to do?\nA. scape jail\nB. dream of freedom\nC. become a hairdresser\nD. attempt to escape\nE. do time\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is a prisoner sentenced to do?\nA. scape jail\nB. dream of freedom\nC. become a hairdresser\nD. attempt to escape\nE. do time\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is a prisoner sentenced to do?\nA. scape jail\nB. dream of freedom\nC. become a hairdresser\nD. attempt to escape\nE. do time\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is a prisoner sentenced to do?\nA. scape jail\nB. dream of freedom\nC. become a hairdresser\nD. attempt to escape\nE. do time\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is a prisoner sentenced to do?\nA. scape jail\nB. dream of freedom\nC. become a hairdresser\nD. attempt to escape\nE. do time\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5889015197753906", "False"]], [["-6.088901519775391", "False"]], [["-9.08890151977539", "False"]], [["-7.588901519775391", "False"]], [["-0.8389016389846802", "True"]]], "filtered_resps": [["-2.5889015197753906", "False"], ["-6.088901519775391", "False"], ["-9.08890151977539", "False"], ["-7.588901519775391", "False"], ["-0.8389016389846802", "True"]], "filter": "none", "metrics": ["acc"], "doc_hash": "27943db3c1f705d980469c8332c28e7136d9f9cad3f9025227c085baefb64097", "prompt_hash": "ba71feaa295cfc45591e88320d18b1bf6f90fb802842a6b68b19bfbabf23208f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1206, "doc": {"id": "8f2976690c83be6b8fa3a1196dfd9722", "question": "Jesse  enjoyed remembering the past because he helped him understand it.  And understanding the past helped him with doing what?", "question_concept": "remembering", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["phoning", "nostalgia", "writing down", "active", "being prepared"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Jesse  enjoyed remembering the past because he helped him understand it.  And understanding the past helped him with doing what?\nA. phoning\nB. nostalgia\nC. writing down\nD. active\nE. being prepared\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Jesse  enjoyed remembering the past because he helped him understand it.  And understanding the past helped him with doing what?\nA. phoning\nB. nostalgia\nC. writing down\nD. active\nE. being prepared\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Jesse  enjoyed remembering the past because he helped him understand it.  And understanding the past helped him with doing what?\nA. phoning\nB. nostalgia\nC. writing down\nD. active\nE. being prepared\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Jesse  enjoyed remembering the past because he helped him understand it.  And understanding the past helped him with doing what?\nA. phoning\nB. nostalgia\nC. writing down\nD. active\nE. being prepared\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Jesse  enjoyed remembering the past because he helped him understand it.  And understanding the past helped him with doing what?\nA. phoning\nB. nostalgia\nC. writing down\nD. active\nE. being prepared\nAnswer:", "arg_1": " E"}}, "resps": [[["-4.631130218505859", "False"]], [["-2.1311302185058594", "False"]], [["-5.131130218505859", "False"]], [["-5.631130218505859", "False"]], [["-2.3811302185058594", "False"]]], "filtered_resps": [["-4.631130218505859", "False"], ["-2.1311302185058594", "False"], ["-5.131130218505859", "False"], ["-5.631130218505859", "False"], ["-2.3811302185058594", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "92fe8ab81b7df2bd8dd37e45797b3aa52c38de1131198a09fc8771d90bdd9607", "prompt_hash": "88efe0ca8f6f62c59f1dfe62470b6f4df09d72785ca51211e832f60d734dfb11", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1207, "doc": {"id": "570be8c1edb8c638603dc5c8cae421cc", "question": "David watched some nesting birds using his binoculars while on vacation.  Where might David be?.", "question_concept": "birds", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["sky", "vaccation", "forest", "countryside", "roof"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: David watched some nesting birds using his binoculars while on vacation.  Where might David be?.\nA. sky\nB. vaccation\nC. forest\nD. countryside\nE. roof\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: David watched some nesting birds using his binoculars while on vacation.  Where might David be?.\nA. sky\nB. vaccation\nC. forest\nD. countryside\nE. roof\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: David watched some nesting birds using his binoculars while on vacation.  Where might David be?.\nA. sky\nB. vaccation\nC. forest\nD. countryside\nE. roof\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: David watched some nesting birds using his binoculars while on vacation.  Where might David be?.\nA. sky\nB. vaccation\nC. forest\nD. countryside\nE. roof\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: David watched some nesting birds using his binoculars while on vacation.  Where might David be?.\nA. sky\nB. vaccation\nC. forest\nD. countryside\nE. roof\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.836149215698242", "False"]], [["-2.586149215698242", "False"]], [["-2.586149215698242", "False"]], [["-3.836149215698242", "False"]], [["-7.336149215698242", "False"]]], "filtered_resps": [["-3.836149215698242", "False"], ["-2.586149215698242", "False"], ["-2.586149215698242", "False"], ["-3.836149215698242", "False"], ["-7.336149215698242", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "e77f705d57978c38de5ee9c0f67ce7a2d62d2ca47cf0ebc7e8cecaddfe811751", "prompt_hash": "b952ce5c0c71a06da2dfe8e4351675c2435f9805b55d63d255a20c9f26a01692", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 0.0}
{"doc_id": 1208, "doc": {"id": "08d3175de59a639be02f2ebc032d56bd", "question": "Where would you find many varieties of plants including a rosebush?", "question_concept": "rosebush", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["kew gardens", "garder", "backyard", "shop", "beautiful garden"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Where would you find many varieties of plants including a rosebush?\nA. kew gardens\nB. garder\nC. backyard\nD. shop\nE. beautiful garden\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where would you find many varieties of plants including a rosebush?\nA. kew gardens\nB. garder\nC. backyard\nD. shop\nE. beautiful garden\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where would you find many varieties of plants including a rosebush?\nA. kew gardens\nB. garder\nC. backyard\nD. shop\nE. beautiful garden\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where would you find many varieties of plants including a rosebush?\nA. kew gardens\nB. garder\nC. backyard\nD. shop\nE. beautiful garden\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where would you find many varieties of plants including a rosebush?\nA. kew gardens\nB. garder\nC. backyard\nD. shop\nE. beautiful garden\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.1174426078796387", "True"]], [["-7.367442607879639", "False"]], [["-5.867442607879639", "False"]], [["-9.117443084716797", "False"]], [["-8.117443084716797", "False"]]], "filtered_resps": [["-1.1174426078796387", "True"], ["-7.367442607879639", "False"], ["-5.867442607879639", "False"], ["-9.117443084716797", "False"], ["-8.117443084716797", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "9041e04105c27c9ec1a395054b5232b24dce0bb482b003d8d04325cd5357b5aa", "prompt_hash": "d1377045ec3c37d05e2ddfe82ccff84de862c7cff53607492c2171d745132eaa", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1209, "doc": {"id": "549cf641318edfc0510fa7c7dbb359e1", "question": "If I did not have a rosebush, where would I get one?", "question_concept": "rosebush", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["pot", "museum", "garden center", "formal garden", "backyard"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: If I did not have a rosebush, where would I get one?\nA. pot\nB. museum\nC. garden center\nD. formal garden\nE. backyard\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If I did not have a rosebush, where would I get one?\nA. pot\nB. museum\nC. garden center\nD. formal garden\nE. backyard\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If I did not have a rosebush, where would I get one?\nA. pot\nB. museum\nC. garden center\nD. formal garden\nE. backyard\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If I did not have a rosebush, where would I get one?\nA. pot\nB. museum\nC. garden center\nD. formal garden\nE. backyard\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If I did not have a rosebush, where would I get one?\nA. pot\nB. museum\nC. garden center\nD. formal garden\nE. backyard\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.5518345832824707", "False"]], [["-8.051834106445312", "False"]], [["-1.8018344640731812", "False"]], [["-9.551834106445312", "False"]], [["-9.301834106445312", "False"]]], "filtered_resps": [["-3.5518345832824707", "False"], ["-8.051834106445312", "False"], ["-1.8018344640731812", "False"], ["-9.551834106445312", "False"], ["-9.301834106445312", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "64e994062cca03ba43da4792c9a50e31ee7988e252afd81d7c3ac2e176860383", "prompt_hash": "2a47d17f268a8d10a27cca8335befebbbff27f4656965d2e2d9bdc3c068285be", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1210, "doc": {"id": "dfa23d3422b7294843447b6950d2b476", "question": "What does a person with a what likely do?", "question_concept": "person", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["feel important", "trust himself", "own house", "electrical circuit", "know what time"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What does a person with a what likely do?\nA. feel important\nB. trust himself\nC. own house\nD. electrical circuit\nE. know what time\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What does a person with a what likely do?\nA. feel important\nB. trust himself\nC. own house\nD. electrical circuit\nE. know what time\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What does a person with a what likely do?\nA. feel important\nB. trust himself\nC. own house\nD. electrical circuit\nE. know what time\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What does a person with a what likely do?\nA. feel important\nB. trust himself\nC. own house\nD. electrical circuit\nE. know what time\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What does a person with a what likely do?\nA. feel important\nB. trust himself\nC. own house\nD. electrical circuit\nE. know what time\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.8305953741073608", "False"]], [["-1.5805953741073608", "True"]], [["-4.83059549331665", "False"]], [["-4.33059549331665", "False"]], [["-5.58059549331665", "False"]]], "filtered_resps": [["-1.8305953741073608", "False"], ["-1.5805953741073608", "True"], ["-4.83059549331665", "False"], ["-4.33059549331665", "False"], ["-5.58059549331665", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "92008eaa72c6260a04974614ee5c7a6839fbac6b103bc1f891fe7d54172e2c67", "prompt_hash": "1d6b200fcc49bb81c9afadaed7f951baa4704d4226294e5ae2f72ff35d6522aa", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1211, "doc": {"id": "1fe90a4aee405e1aa2279442d28803ae", "question": "What are cats often known for?", "question_concept": "cat", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["whiskers", "sharp teeth", "purr", "four legs", "sharp claws"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: What are cats often known for?\nA. whiskers\nB. sharp teeth\nC. purr\nD. four legs\nE. sharp claws\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What are cats often known for?\nA. whiskers\nB. sharp teeth\nC. purr\nD. four legs\nE. sharp claws\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What are cats often known for?\nA. whiskers\nB. sharp teeth\nC. purr\nD. four legs\nE. sharp claws\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What are cats often known for?\nA. whiskers\nB. sharp teeth\nC. purr\nD. four legs\nE. sharp claws\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What are cats often known for?\nA. whiskers\nB. sharp teeth\nC. purr\nD. four legs\nE. sharp claws\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.4173529148101807", "False"]], [["-5.16735315322876", "False"]], [["-0.6673529744148254", "True"]], [["-4.41735315322876", "False"]], [["-2.4173529148101807", "False"]]], "filtered_resps": [["-2.4173529148101807", "False"], ["-5.16735315322876", "False"], ["-0.6673529744148254", "True"], ["-4.41735315322876", "False"], ["-2.4173529148101807", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "3aefafe4a569fa84f2764d7338f77552b7b9af0129d9baa78085880179816d47", "prompt_hash": "4da93405f3035931de4e2e9e251a2690d94de5d5d654600559c05ccbb97c1dff", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1212, "doc": {"id": "01794dde3ca2991615f1aa2f63fb22e3", "question": "As he looked out the window, he knew the landing was happening soon, and it made him nervous, but where would he be soon?", "question_concept": "landing", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["apartment building", "disembark", "stairwell", "deplane", "airport"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: As he looked out the window, he knew the landing was happening soon, and it made him nervous, but where would he be soon?\nA. apartment building\nB. disembark\nC. stairwell\nD. deplane\nE. airport\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: As he looked out the window, he knew the landing was happening soon, and it made him nervous, but where would he be soon?\nA. apartment building\nB. disembark\nC. stairwell\nD. deplane\nE. airport\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: As he looked out the window, he knew the landing was happening soon, and it made him nervous, but where would he be soon?\nA. apartment building\nB. disembark\nC. stairwell\nD. deplane\nE. airport\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: As he looked out the window, he knew the landing was happening soon, and it made him nervous, but where would he be soon?\nA. apartment building\nB. disembark\nC. stairwell\nD. deplane\nE. airport\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: As he looked out the window, he knew the landing was happening soon, and it made him nervous, but where would he be soon?\nA. apartment building\nB. disembark\nC. stairwell\nD. deplane\nE. airport\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.579907178878784", "False"]], [["-2.829907178878784", "False"]], [["-5.329907417297363", "False"]], [["-1.0799071788787842", "True"]], [["-3.329907178878784", "False"]]], "filtered_resps": [["-3.579907178878784", "False"], ["-2.829907178878784", "False"], ["-5.329907417297363", "False"], ["-1.0799071788787842", "True"], ["-3.329907178878784", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "73331d5c83857cb5b2fe0da392f37d0a43e4c3a3701cafd80e3965bcbe812bc2", "prompt_hash": "d73274546cfff6dbec0349a738769c1390689a63523e8e04d37e387a38386221", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 0.0}
{"doc_id": 1213, "doc": {"id": "f794e376672c98ac25d8f70506a26e68", "question": "Where can you find a dogs house?", "question_concept": "dogs", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["found outside", "faithful", "frightening", "cold", "four legs"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Where can you find a dogs house?\nA. found outside\nB. faithful\nC. frightening\nD. cold\nE. four legs\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Where can you find a dogs house?\nA. found outside\nB. faithful\nC. frightening\nD. cold\nE. four legs\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Where can you find a dogs house?\nA. found outside\nB. faithful\nC. frightening\nD. cold\nE. four legs\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Where can you find a dogs house?\nA. found outside\nB. faithful\nC. frightening\nD. cold\nE. four legs\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Where can you find a dogs house?\nA. found outside\nB. faithful\nC. frightening\nD. cold\nE. four legs\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.6747151017189026", "True"]], [["-7.424715042114258", "False"]], [["-8.174715042114258", "False"]], [["-7.674715042114258", "False"]], [["-6.924715042114258", "False"]]], "filtered_resps": [["-0.6747151017189026", "True"], ["-7.424715042114258", "False"], ["-8.174715042114258", "False"], ["-7.674715042114258", "False"], ["-6.924715042114258", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "eb71480cbffcc9a06663f41c0b52dfeb93d2c17fe04b730bc6d5f4c8ec178f8d", "prompt_hash": "07168fe5421ff87777e8c07415bf81a765a546e26fa0929d18fb15c9b86d32f5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1214, "doc": {"id": "ace8fa2943ba8414aebdb74b48906fae", "question": "Tweed is a rare fabric in modern clothing, what brand should I look for when buying it?", "question_concept": "tweed", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["scotland", "brown", "fabric store", "clothing stores", "eddie bauer"]}, "answerKey": "E"}, "target": "E", "arguments": {"gen_args_0": {"arg_0": "Question: Tweed is a rare fabric in modern clothing, what brand should I look for when buying it?\nA. scotland\nB. brown\nC. fabric store\nD. clothing stores\nE. eddie bauer\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Tweed is a rare fabric in modern clothing, what brand should I look for when buying it?\nA. scotland\nB. brown\nC. fabric store\nD. clothing stores\nE. eddie bauer\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Tweed is a rare fabric in modern clothing, what brand should I look for when buying it?\nA. scotland\nB. brown\nC. fabric store\nD. clothing stores\nE. eddie bauer\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Tweed is a rare fabric in modern clothing, what brand should I look for when buying it?\nA. scotland\nB. brown\nC. fabric store\nD. clothing stores\nE. eddie bauer\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Tweed is a rare fabric in modern clothing, what brand should I look for when buying it?\nA. scotland\nB. brown\nC. fabric store\nD. clothing stores\nE. eddie bauer\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.8111793994903564", "False"]], [["-5.811179161071777", "False"]], [["-6.561179161071777", "False"]], [["-7.311179161071777", "False"]], [["-3.3111793994903564", "False"]]], "filtered_resps": [["-3.8111793994903564", "False"], ["-5.811179161071777", "False"], ["-6.561179161071777", "False"], ["-7.311179161071777", "False"], ["-3.3111793994903564", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "b38a1c95c5827889a18d24e07f9b9f916f841569bf3ab0906aee0acbd9d7ccb3", "prompt_hash": "81bb781f236af13f8d5211f8e493738113140a1759e609df5ed20c9748968b97", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "acc": 1.0}
{"doc_id": 1215, "doc": {"id": "21ce6f7c5c3d1ad8cf234988c1ad471f", "question": "If you really wanted a grape, where would you go to get it?", "question_concept": "grape", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["winery", "fruit stand", "field", "kitchen", "food"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: If you really wanted a grape, where would you go to get it?\nA. winery\nB. fruit stand\nC. field\nD. kitchen\nE. food\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: If you really wanted a grape, where would you go to get it?\nA. winery\nB. fruit stand\nC. field\nD. kitchen\nE. food\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: If you really wanted a grape, where would you go to get it?\nA. winery\nB. fruit stand\nC. field\nD. kitchen\nE. food\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: If you really wanted a grape, where would you go to get it?\nA. winery\nB. fruit stand\nC. field\nD. kitchen\nE. food\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: If you really wanted a grape, where would you go to get it?\nA. winery\nB. fruit stand\nC. field\nD. kitchen\nE. food\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.7977719306945801", "True"]], [["-1.79777193069458", "False"]], [["-3.79777193069458", "False"]], [["-6.79777193069458", "False"]], [["-6.79777193069458", "False"]]], "filtered_resps": [["-0.7977719306945801", "True"], ["-1.79777193069458", "False"], ["-3.79777193069458", "False"], ["-6.79777193069458", "False"], ["-6.79777193069458", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "27a451b116af0cdd9b8932aff9f6341c748fd4c1566c5d40f973e9d53129ce3d", "prompt_hash": "33e6e1147826b74cfdaebb4e1af51a8445def30c44d017d49267c8422e0e8c69", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 0.0}
{"doc_id": 1216, "doc": {"id": "6c84e79d0595efd99596faa07c4961d0", "question": "What would you do to a rock when climb up a cliff?", "question_concept": "climb", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["grab", "look down", "throw", "falling", "may fall"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: What would you do to a rock when climb up a cliff?\nA. grab\nB. look down\nC. throw\nD. falling\nE. may fall\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What would you do to a rock when climb up a cliff?\nA. grab\nB. look down\nC. throw\nD. falling\nE. may fall\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What would you do to a rock when climb up a cliff?\nA. grab\nB. look down\nC. throw\nD. falling\nE. may fall\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What would you do to a rock when climb up a cliff?\nA. grab\nB. look down\nC. throw\nD. falling\nE. may fall\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What would you do to a rock when climb up a cliff?\nA. grab\nB. look down\nC. throw\nD. falling\nE. may fall\nAnswer:", "arg_1": " E"}}, "resps": [[["-1.016069769859314", "True"]], [["-4.5160698890686035", "False"]], [["-5.2660698890686035", "False"]], [["-8.016069412231445", "False"]], [["-7.0160698890686035", "False"]]], "filtered_resps": [["-1.016069769859314", "True"], ["-4.5160698890686035", "False"], ["-5.2660698890686035", "False"], ["-8.016069412231445", "False"], ["-7.0160698890686035", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "4b231ea05227e93a3583374aaa041dc0073c8e280a218c56e921e79c5c4b16e6", "prompt_hash": "268b2374e6d09e62c67feafe13e6e64c7e5c9db1012b104f3f986256e69fe1a7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1217, "doc": {"id": "88f1fe6cfbcb1a25f25454341c789463", "question": "His compressor needed a new hose, where did he go?", "question_concept": "hose", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["garden shed", "hardware store", "brothel", "garage", "greenhouse"]}, "answerKey": "B"}, "target": "B", "arguments": {"gen_args_0": {"arg_0": "Question: His compressor needed a new hose, where did he go?\nA. garden shed\nB. hardware store\nC. brothel\nD. garage\nE. greenhouse\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: His compressor needed a new hose, where did he go?\nA. garden shed\nB. hardware store\nC. brothel\nD. garage\nE. greenhouse\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: His compressor needed a new hose, where did he go?\nA. garden shed\nB. hardware store\nC. brothel\nD. garage\nE. greenhouse\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: His compressor needed a new hose, where did he go?\nA. garden shed\nB. hardware store\nC. brothel\nD. garage\nE. greenhouse\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: His compressor needed a new hose, where did he go?\nA. garden shed\nB. hardware store\nC. brothel\nD. garage\nE. greenhouse\nAnswer:", "arg_1": " E"}}, "resps": [[["-3.370589256286621", "False"]], [["-1.620589256286621", "False"]], [["-7.870589256286621", "False"]], [["-8.870589256286621", "False"]], [["-11.370589256286621", "False"]]], "filtered_resps": [["-3.370589256286621", "False"], ["-1.620589256286621", "False"], ["-7.870589256286621", "False"], ["-8.870589256286621", "False"], ["-11.370589256286621", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "10c3ef18f21759d2999b2f6c0a9e6adeac3fe8c06e4d3c45ccd21540993180ae", "prompt_hash": "9307a36513242402b89b23beaeced5125cfd7ae8b4848e44e6a88f24630ab871", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "acc": 1.0}
{"doc_id": 1218, "doc": {"id": "5074bcaf0f700c9f3c8c563067af156a", "question": "The man closed his eyes as the music played, what effect did the music have?", "question_concept": "music", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["coma", "enjoyable", "soothing", "universal", "good or bad"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: The man closed his eyes as the music played, what effect did the music have?\nA. coma\nB. enjoyable\nC. soothing\nD. universal\nE. good or bad\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: The man closed his eyes as the music played, what effect did the music have?\nA. coma\nB. enjoyable\nC. soothing\nD. universal\nE. good or bad\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: The man closed his eyes as the music played, what effect did the music have?\nA. coma\nB. enjoyable\nC. soothing\nD. universal\nE. good or bad\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: The man closed his eyes as the music played, what effect did the music have?\nA. coma\nB. enjoyable\nC. soothing\nD. universal\nE. good or bad\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: The man closed his eyes as the music played, what effect did the music have?\nA. coma\nB. enjoyable\nC. soothing\nD. universal\nE. good or bad\nAnswer:", "arg_1": " E"}}, "resps": [[["-5.280882835388184", "False"]], [["-3.7808825969696045", "False"]], [["-1.5308825969696045", "True"]], [["-6.780882835388184", "False"]], [["-6.780882835388184", "False"]]], "filtered_resps": [["-5.280882835388184", "False"], ["-3.7808825969696045", "False"], ["-1.5308825969696045", "True"], ["-6.780882835388184", "False"], ["-6.780882835388184", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "6c866fd122856398c6ee360eb89a7ace161b3fa717d1267bd62e295b5036eb09", "prompt_hash": "e4285b19c4d455cc53857dd0ee9c25dcedd69e7cab3df9ff8016a3df7a9b8870", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
{"doc_id": 1219, "doc": {"id": "6a253e076cd2af00e17d9950d70daf47", "question": "Setting up framing, truss and beam are some of the first steps in what?", "question_concept": "beam", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["new construction", "warehouse", "driving", "ceiling", "bridge"]}, "answerKey": "A"}, "target": "A", "arguments": {"gen_args_0": {"arg_0": "Question: Setting up framing, truss and beam are some of the first steps in what?\nA. new construction\nB. warehouse\nC. driving\nD. ceiling\nE. bridge\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: Setting up framing, truss and beam are some of the first steps in what?\nA. new construction\nB. warehouse\nC. driving\nD. ceiling\nE. bridge\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: Setting up framing, truss and beam are some of the first steps in what?\nA. new construction\nB. warehouse\nC. driving\nD. ceiling\nE. bridge\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: Setting up framing, truss and beam are some of the first steps in what?\nA. new construction\nB. warehouse\nC. driving\nD. ceiling\nE. bridge\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: Setting up framing, truss and beam are some of the first steps in what?\nA. new construction\nB. warehouse\nC. driving\nD. ceiling\nE. bridge\nAnswer:", "arg_1": " E"}}, "resps": [[["-0.8678935170173645", "True"]], [["-7.617893695831299", "False"]], [["-7.867893695831299", "False"]], [["-8.86789321899414", "False"]], [["-4.867893695831299", "False"]]], "filtered_resps": [["-0.8678935170173645", "True"], ["-7.617893695831299", "False"], ["-7.867893695831299", "False"], ["-8.86789321899414", "False"], ["-4.867893695831299", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "a4bbf137a6d0246a0fc600553ff86ddf616eabd662f73d1115968ec853cef2b5", "prompt_hash": "4e488cf6d30de35f617daa840ea3b19cab880e390b56e7ec91276fe17c15125d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "acc": 1.0}
{"doc_id": 1220, "doc": {"id": "5af7c7860e3be61d4cfd814cc109f9d9", "question": "What is another name for a disk for storing information?", "question_concept": "disk", "choices": {"label": ["A", "B", "C", "D", "E"], "text": ["computer store", "computer to store data", "computer hard drive", "cd player", "usb mouse"]}, "answerKey": "C"}, "target": "C", "arguments": {"gen_args_0": {"arg_0": "Question: What is another name for a disk for storing information?\nA. computer store\nB. computer to store data\nC. computer hard drive\nD. cd player\nE. usb mouse\nAnswer:", "arg_1": " A"}, "gen_args_1": {"arg_0": "Question: What is another name for a disk for storing information?\nA. computer store\nB. computer to store data\nC. computer hard drive\nD. cd player\nE. usb mouse\nAnswer:", "arg_1": " B"}, "gen_args_2": {"arg_0": "Question: What is another name for a disk for storing information?\nA. computer store\nB. computer to store data\nC. computer hard drive\nD. cd player\nE. usb mouse\nAnswer:", "arg_1": " C"}, "gen_args_3": {"arg_0": "Question: What is another name for a disk for storing information?\nA. computer store\nB. computer to store data\nC. computer hard drive\nD. cd player\nE. usb mouse\nAnswer:", "arg_1": " D"}, "gen_args_4": {"arg_0": "Question: What is another name for a disk for storing information?\nA. computer store\nB. computer to store data\nC. computer hard drive\nD. cd player\nE. usb mouse\nAnswer:", "arg_1": " E"}}, "resps": [[["-2.5109381675720215", "False"]], [["-7.5109381675720215", "False"]], [["-1.510938286781311", "False"]], [["-7.0109381675720215", "False"]], [["-10.26093864440918", "False"]]], "filtered_resps": [["-2.5109381675720215", "False"], ["-7.5109381675720215", "False"], ["-1.510938286781311", "False"], ["-7.0109381675720215", "False"], ["-10.26093864440918", "False"]], "filter": "none", "metrics": ["acc"], "doc_hash": "7c6f29bd1cf14ee2c4cfd923b01ed7e83f2ce2ed224a8a8b87e5962b4beccdcf", "prompt_hash": "f85b4d13df172890a0f387f102a406056b4b573810c3fd134faf8de46a35e09e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "acc": 1.0}
