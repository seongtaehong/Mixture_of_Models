INFO 05-24 21:25:37 [importing.py:53] Triton module has been replaced with a placeholder.
INFO 05-24 21:25:37 [__init__.py:239] Automatically detected platform cuda.
2025-05-24:21:25:42 INFO     [__main__:440] Selected Tasks: ['mgsm_direct_en']
2025-05-24:21:25:42 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-05-24:21:25:42 INFO     [evaluator:223] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.1-8B-Instruct', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.7}
INFO 05-24 21:25:53 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 05-24 21:25:54 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 05-24 21:25:56 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
2025-05-24 21:25:56,725 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
WARNING 05-24 21:25:56 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f098d1be6f0>
INFO 05-24 21:25:58 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-24 21:25:58 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 05-24 21:25:58 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.
INFO 05-24 21:26:00 [gpu_model_runner.py:1329] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-24 21:26:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.09it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.02it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.54s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.64s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.44s/it]

INFO 05-24 21:26:08 [loader.py:458] Loading weights took 6.17 seconds
INFO 05-24 21:26:09 [gpu_model_runner.py:1347] Model loading took 14.9889 GiB and 8.563550 seconds
INFO 05-24 21:26:17 [backends.py:420] Using cache directory: /mnt/raid6/hst/.cache/vllm/torch_compile_cache/d6a0cf0e50/rank_0_0 for vLLM's torch.compile
INFO 05-24 21:26:17 [backends.py:430] Dynamo bytecode transform time: 8.10 s
INFO 05-24 21:26:46 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 28.868 s
INFO 05-24 21:26:49 [monitor.py:33] torch.compile takes 8.10 s in total
INFO 05-24 21:26:51 [kv_cache_utils.py:634] GPU KV cache size: 134,704 tokens
INFO 05-24 21:26:51 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.03x
INFO 05-24 21:28:13 [gpu_model_runner.py:1686] Graph capturing finished in 80 secs, took 1.59 GiB
INFO 05-24 21:28:13 [core.py:159] init engine (profile, create kv cache, warmup model) took 124.68 seconds
INFO 05-24 21:28:13 [core_client.py:439] Core engine process 0 ready.
2025-05-24:21:28:21 INFO     [evaluator:286] mgsm_direct_en: Using gen_kwargs: {'do_sample': False, 'until': ['Question:', '</s>', '<|im_end|>']}
2025-05-24:21:28:21 INFO     [api.task:434] Building contexts for mgsm_direct_en on rank 0...
  0%|          | 0/250 [00:00<?, ?it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/250 [00:00<00:00, 1114.93it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 226/250 [00:00<00:00, 1126.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 1126.23it/s]
2025-05-24:21:28:21 INFO     [evaluator_utils:206] Task: ConfigurableTask(task_name=mgsm_direct_en,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    
Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?
Answer:
(end of prompt on previous line)
target string or answer choice index (starting on next line):
18
(end of target on previous line)
2025-05-24:21:28:21 INFO     [evaluator_utils:210] Request: Instance(request_type='generate_until', doc={'question': "Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?", 'answer': None, 'answer_number': 18, 'equation_solution': None}, arguments=("Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\nAnswer:", {'do_sample': False, 'until': ['Question:', '</s>', '<|im_end|>']}), idx=0, metadata=('mgsm_direct_en', 0, 1), resps=[], filtered_resps={}, task_name='mgsm_direct_en', doc_id=0, repeats=1)
2025-05-24:21:28:21 INFO     [evaluator:559] Running generate_until requests
Running generate_until requests:   0%|          | 0/250 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:05<23:58,  5.78s/it, est. speed input: 6.58 toks/s, output: 12.29 toks/s][A
Processed prompts:   1%|          | 2/250 [00:07<13:14,  3.21s/it, est. speed input: 10.72 toks/s, output: 23.25 toks/s][A
Processed prompts:   1%|          | 3/250 [00:09<12:11,  2.96s/it, est. speed input: 11.87 toks/s, output: 31.46 toks/s][A
Processed prompts:   2%|â–         | 4/250 [00:10<08:00,  1.95s/it, est. speed input: 17.35 toks/s, output: 44.84 toks/s][A
Processed prompts:   2%|â–         | 5/250 [00:10<05:41,  1.40s/it, est. speed input: 19.78 toks/s, output: 57.85 toks/s][A
Processed prompts:   2%|â–         | 6/250 [00:12<05:37,  1.38s/it, est. speed input: 20.21 toks/s, output: 66.30 toks/s][A
Processed prompts:   3%|â–Ž         | 7/250 [00:14<07:35,  1.88s/it, est. speed input: 23.07 toks/s, output: 68.80 toks/s][A
Processed prompts:   3%|â–Ž         | 8/250 [00:16<07:13,  1.79s/it, est. speed input: 29.05 toks/s, output: 77.58 toks/s][A
Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 109/250 [00:16<00:05, 25.47it/s, est. speed input: 531.72 toks/s, output: 1631.23 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:16<00:00, 15.02it/s, est. speed input: 939.40 toks/s, output: 3799.28 toks/s]
Running generate_until requests:   0%|          | 1/250 [00:16<1:09:12, 16.68s/it]Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:16<00:00, 14.99it/s]
fatal: not a git repository (or any parent up to mount point /mnt)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
2025-05-24:21:28:49 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-05-24:21:28:49 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: mgsm_direct_en
vllm (pretrained=meta-llama/Llama-3.1-8B-Instruct,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto
|    Tasks     |Version|     Filter      |n-shot|  Metric   |   |Value|   |Stderr|
|--------------|------:|-----------------|-----:|-----------|---|----:|---|-----:|
|mgsm_direct_en|      3|flexible-extract |     0|exact_match|â†‘  |0.272|Â±  |0.0282|
|              |       |remove_whitespace|     0|exact_match|â†‘  |0.000|Â±  |0.0000|

