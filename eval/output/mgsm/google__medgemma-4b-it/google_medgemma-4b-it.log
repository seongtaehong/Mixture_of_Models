INFO 05-24 21:31:43 [importing.py:53] Triton module has been replaced with a placeholder.
INFO 05-24 21:31:44 [__init__.py:239] Automatically detected platform cuda.
2025-05-24:21:31:49 INFO     [__main__:440] Selected Tasks: ['mgsm_direct_en']
2025-05-24:21:31:49 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-05-24:21:31:49 INFO     [evaluator:223] Initializing vllm model, with arguments: {'pretrained': 'google/medgemma-4b-it', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.7}
INFO 05-24 21:32:00 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 05-24 21:32:01 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 05-24 21:32:05 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='google/medgemma-4b-it', speculative_config=None, tokenizer='google/medgemma-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=google/medgemma-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
2025-05-24 21:32:08,337 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
WARNING 05-24 21:32:08 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb4fff99310>
INFO 05-24 21:32:11 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-24 21:32:11 [cuda.py:221] Using Flash Attention backend on V1 engine.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
INFO 05-24 21:32:22 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.
INFO 05-24 21:32:24 [gpu_model_runner.py:1329] Starting to load model google/medgemma-4b-it...
INFO 05-24 21:32:26 [config.py:3614] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
INFO 05-24 21:32:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.29s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.14s/it]

INFO 05-24 21:32:39 [loader.py:458] Loading weights took 10.44 seconds
INFO 05-24 21:32:39 [gpu_model_runner.py:1347] Model loading took 8.5832 GiB and 14.380603 seconds
INFO 05-24 21:32:39 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
INFO 05-24 21:32:56 [backends.py:420] Using cache directory: /mnt/raid6/hst/.cache/vllm/torch_compile_cache/af98f58bd3/rank_0_0 for vLLM's torch.compile
INFO 05-24 21:32:56 [backends.py:430] Dynamo bytecode transform time: 11.19 s
INFO 05-24 21:33:33 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 36.236 s
INFO 05-24 21:33:35 [monitor.py:33] torch.compile takes 11.19 s in total
INFO 05-24 21:33:37 [kv_cache_utils.py:634] GPU KV cache size: 153,760 tokens
INFO 05-24 21:33:37 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.17x
INFO 05-24 21:35:07 [gpu_model_runner.py:1686] Graph capturing finished in 90 secs, took 1.98 GiB
INFO 05-24 21:35:07 [core.py:159] init engine (profile, create kv cache, warmup model) took 148.01 seconds
INFO 05-24 21:35:07 [core_client.py:439] Core engine process 0 ready.
2025-05-24:21:35:09 INFO     [models.vllm_causallms:138] Found 'gemma' in model name, a BOS token will be used as Gemma series models underperform without it.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
2025-05-24:21:35:18 INFO     [evaluator:286] mgsm_direct_en: Using gen_kwargs: {'do_sample': False, 'until': ['Question:', '</s>', '<|im_end|>']}
2025-05-24:21:35:18 INFO     [api.task:434] Building contexts for mgsm_direct_en on rank 0...
  0%|          | 0/250 [00:00<?, ?it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/250 [00:00<00:00, 1051.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 212/250 [00:00<00:00, 1055.04it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 1065.90it/s]
2025-05-24:21:35:18 INFO     [evaluator_utils:206] Task: ConfigurableTask(task_name=mgsm_direct_en,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    
Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?
Answer:
(end of prompt on previous line)
target string or answer choice index (starting on next line):
18
(end of target on previous line)
2025-05-24:21:35:18 INFO     [evaluator_utils:210] Request: Instance(request_type='generate_until', doc={'question': "Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?", 'answer': None, 'answer_number': 18, 'equation_solution': None}, arguments=("Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\nAnswer:", {'do_sample': False, 'until': ['Question:', '</s>', '<|im_end|>']}), idx=0, metadata=('mgsm_direct_en', 0, 1), resps=[], filtered_resps={}, task_name='mgsm_direct_en', doc_id=0, repeats=1)
2025-05-24:21:35:18 INFO     [evaluator:559] Running generate_until requests
Running generate_until requests:   0%|          | 0/250 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:12<50:40, 12.21s/it, est. speed input: 11.55 toks/s, output: 20.97 toks/s][A
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 96/250 [00:12<00:13, 11.06it/s, est. speed input: 690.96 toks/s, output: 1995.88 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:12<00:00, 20.28it/s, est. speed input: 1327.47 toks/s, output: 5192.70 toks/s]
Running generate_until requests:   0%|          | 1/250 [00:12<51:18, 12.36s/it]Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:12<00:00, 20.22it/s]
fatal: not a git repository (or any parent up to mount point /mnt)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
2025-05-24:21:35:41 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-05-24:21:35:41 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: mgsm_direct_en
vllm (pretrained=google/medgemma-4b-it,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto
|    Tasks     |Version|     Filter      |n-shot|  Metric   |   |Value|   |Stderr|
|--------------|------:|-----------------|-----:|-----------|---|----:|---|-----:|
|mgsm_direct_en|      3|flexible-extract |     0|exact_match|â†‘  |0.032|Â±  |0.0112|
|              |       |remove_whitespace|     0|exact_match|â†‘  |0.000|Â±  |0.0000|

